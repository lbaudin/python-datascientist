[
  {
    "objectID": "content/annexes/corrections.html",
    "href": "content/annexes/corrections.html",
    "title": "Corrections",
    "section": "",
    "text": "Seules les chapitres dont les corrections ne sont pas apparentes sont list√©s sur cette page."
  },
  {
    "objectID": "content/annexes/corrections.html#partie-1-manipuler-des-donn√©es",
    "href": "content/annexes/corrections.html#partie-1-manipuler-des-donn√©es",
    "title": "Corrections",
    "section": "Partie 1: manipuler des donn√©es",
    "text": "Partie 1: manipuler des donn√©es\n\nRetour sur Numpy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercices Pandas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercices Geopandas:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWebscraping\n\n\n\n\n\n\n\n\n\n\n\n\n\nExpressions r√©guli√®res:\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapitre sur les API:"
  },
  {
    "objectID": "content/annexes/corrections.html#partie-2-visualiser-les-donn√©es",
    "href": "content/annexes/corrections.html#partie-2-visualiser-les-donn√©es",
    "title": "Corrections",
    "section": "Partie 2: visualiser les donn√©es",
    "text": "Partie 2: visualiser les donn√©es"
  },
  {
    "objectID": "content/annexes/corrections.html#partie-3-mod√©liser",
    "href": "content/annexes/corrections.html#partie-3-mod√©liser",
    "title": "Corrections",
    "section": "Partie 3: mod√©liser",
    "text": "Partie 3: mod√©liser"
  },
  {
    "objectID": "content/annexes/corrections.html#partie-4-natural-language-processing-nlp",
    "href": "content/annexes/corrections.html#partie-4-natural-language-processing-nlp",
    "title": "Corrections",
    "section": "Partie 4: Natural Language Processing (NLP)",
    "text": "Partie 4: Natural Language Processing (NLP)"
  },
  {
    "objectID": "content/annexes/corrections.html#partie-5-introduction-aux-outils-et-m√©thodes-√†-l√©tat-de-lart",
    "href": "content/annexes/corrections.html#partie-5-introduction-aux-outils-et-m√©thodes-√†-l√©tat-de-lart",
    "title": "Corrections",
    "section": "Partie 5: Introduction aux outils et m√©thodes √† l‚Äô√©tat de l‚Äôart",
    "text": "Partie 5: Introduction aux outils et m√©thodes √† l‚Äô√©tat de l‚Äôart"
  },
  {
    "objectID": "content/git/exogit.html",
    "href": "content/git/exogit.html",
    "title": "Un cadavre exquis pour d√©couvrir Git",
    "section": "",
    "text": "Les exercices suivants sont inspir√©s d‚Äôun cours de Git que j‚Äôai\nparticip√© √† construire\n√† l‚ÄôInsee et dont les ressources sont disponibles\nici. L‚Äôid√©e\ndu cadavre exquis est inspir√©e de\ncette ressource et de celle-ci.\nCette partie part du principe que les concepts g√©n√©raux de Git sont\nma√Ætris√©s et qu‚Äôun environnement de travail fonctionnel avec Git est\ndisponible. Un exemple de tel environnement est le JupyterLab ou l‚Äôenvironnement VSCode du\nSSPCloud o√π une extension\nGit est pr√©-install√©e :\nOutre le chapitre pr√©c√©dent, il existe de\nnombreuses ressources sur internet sur le sujet,\nnotamment une s√©rie de ressources construites\npour l‚ÄôInsee sur ce site\net des ressources de la documentation collaborative sur R qu‚Äôest utilitR\n(des √©l√©ments sur la configuration\net pratique sur RStudio). Toutes\nles ressources ne sont donc pas du Python car Git est un outil transversal\nqui doit servir quel que soit le langage de pr√©dilection.\nGit fait parti des pratiques collaboratives\ndevenues standards dans le domaine de l‚Äôopen-source\nmais √©galement de plus en plus communes dans les administrations et entreprises\nde la data science.\nCe chapitre propose, pour simplifier l‚Äôapprentissage,\nd‚Äôutiliser l‚Äô extension Git de JupyterLab ou de VSCode.\nUn tutoriel pr√©sentant l‚Äôextension JupyterLab est disponible\nici.\nVSCode propose\nprobablement, √† l‚Äôheure actuelle, l‚Äôensemble le plus complet.\nCertains passages de ce TD n√©cessitent d‚Äôutiliser la ligne de commande.\nIl est tout √† fait possible de r√©aliser ce TD enti√®rement avec celle-ci.\nCependant, pour une personne d√©butante en Git, l‚Äôutilisation d‚Äôune\ninterface graphique peut constituer un √©l√©ment important pour\nla compr√©hension et l‚Äôadoption de Git. Une fois √† l‚Äôaise avec\nGit, on peut tout √† fait se passer des interfaces graphiques\npour les routines quotidiennes et ne les utiliser que\npour certaines op√©rations o√π elles s‚Äôav√®rent fort pratiques\n(notamment la comparaison de deux fichiers avant de devoir fusionner)."
  },
  {
    "objectID": "content/git/exogit.html#rappels-sur-la-notion-de-d√©p√¥t-distant",
    "href": "content/git/exogit.html#rappels-sur-la-notion-de-d√©p√¥t-distant",
    "title": "Un cadavre exquis pour d√©couvrir Git",
    "section": "Rappels sur la notion de d√©p√¥t distant",
    "text": "Rappels sur la notion de d√©p√¥t distant\nComme expliqu√© dans le chapitre pr√©c√©dent,\nil convient de distinguer\nle d√©p√¥t distant (remote) et la copie ou les copies locales (les clones)\nd‚Äôun d√©p√¥t. Le d√©p√¥t distant est g√©n√©ralement stock√© sur une forge\nlogicielle (Github ou Gitlab) et sert √† centraliser la version\ncollective d‚Äôun projet. Les copies locales sont des copies de travail.\nGit est un syst√®me de contr√¥le de version asynchrone, c‚Äôest-√†-dire\nqu‚Äôon n‚Äôinteragit pas en continu avec le d√©p√¥t distant (comme c‚Äôest le\ncas dans le syst√®me SVN) mais qu‚Äôil est possible d‚Äôavoir une version\nlocale qui se diff√©rencie du d√©p√¥t commun et qu‚Äôon rend coh√©rente\nde temps en temps.\nBien qu‚Äôil soit possible d‚Äôavoir une utilisation hors-ligne de Git,\nc‚Äôest-√†-dire un pur contr√¥le de version local sans d√©p√¥t\ndistant, cela est une utilisation\nrare et qui comporte un int√©r√™t limit√©. L‚Äôint√©r√™t de Git est\nd‚Äôoffrir une mani√®re robuste et efficace d‚Äôinteragir avec un\nd√©p√¥t distant facilitant ainsi la collaboration en √©quipe ou en\nsolitaire.\nPour ces exercices, il est propos√©\nd‚Äôutiliser Github, la forge la plus visible.\nL‚Äôavantage de Github par rapport √† son principal concurrent, Gitlab,\nest que le premier est plus visible, car\nmieux index√© par Google et concentre, en partie pour des raisons historiques, plus\nde d√©veloppeurs Python et R (ce qui est important dans des domaines comme\nle code o√π les externalit√©s de r√©seau jouent)."
  },
  {
    "objectID": "content/git/exogit.html#premi√®re-√©tape-cr√©er-un-compte-github",
    "href": "content/git/exogit.html#premi√®re-√©tape-cr√©er-un-compte-github",
    "title": "Un cadavre exquis pour d√©couvrir Git",
    "section": "Premi√®re √©tape: cr√©er un compte Github",
    "text": "Premi√®re √©tape: cr√©er un compte Github\nLes deux premi√®res √©tapes se font sur Github.\n\n\n Exercice 1 : Cr√©er un compte Github\n\nSi vous n‚Äôen avez pas d√©j√† un, cr√©er un compte sur https://github.com\nCr√©er un d√©p√¥t en suivant les consignes ci-dessous.\n\n\nCr√©ez ce d√©p√¥t priv√©, cela permettra\ndans l‚Äôexercice 2 d‚Äôactiver notre jeton. Vous pourrez le rendre public\napr√®s l‚Äôexercice 2, c‚Äôest comme vous le souhaitez.\nCr√©er ce d√©p√¥t avec un README.md en cliquant sur la case Add a README file\nAjouter un .gitignore en s√©lectionnant le mod√®le Python\n\nConnexion sur https://github.com &gt; + (en haut de la page) &gt; New repository &gt; Renseigner le ‚ÄúRepository name‚Äù &gt; Cocher ‚Äúprivate‚Äù &gt; ‚ÄúCreate repository‚Äù"
  },
  {
    "objectID": "content/git/exogit.html#deuxi√®me-√©tape-cr√©er-un-token-jeton-https",
    "href": "content/git/exogit.html#deuxi√®me-√©tape-cr√©er-un-token-jeton-https",
    "title": "Un cadavre exquis pour d√©couvrir Git",
    "section": "Deuxi√®me √©tape: cr√©er un token (jeton) HTTPS",
    "text": "Deuxi√®me √©tape: cr√©er un token (jeton) HTTPS"
  },
  {
    "objectID": "content/git/exogit.html#principe",
    "href": "content/git/exogit.html#principe",
    "title": "Un cadavre exquis pour d√©couvrir Git",
    "section": "Principe",
    "text": "Principe\nGit est un syst√®me d√©centralis√© de contr√¥le de version :\nles codes sont modifi√©s par chaque personne sur son poste de travail,\npuis sont mis en conformit√© avec la version collective disponible\nsur le d√©p√¥t distant au moment o√π le contributeur le d√©cide.\nIl est donc n√©cessaire que la forge connaisse l‚Äôidentit√© de chacun des\ncontributeurs, afin de d√©terminer qui est l‚Äôauteur d‚Äôune modification apport√©e\naux codes stock√©s dans le d√©p√¥t distant.\nPour que Github reconnaisse un utilisateur proposant des modifications,\nil est n√©cessaire de s‚Äôauthentifier (un d√©p√¥t distant, m√™me public, ne peut pas √™tre modifi√© par n‚Äôimporte qui). L‚Äôauthentification consiste ainsi √† fournir un √©l√©ment que seul vous et la forge √™tes cens√©s conna√Ætre : un mot de passe, une cl√© compliqu√©e, un jeton d‚Äôacc√®s‚Ä¶\nPlus pr√©cis√©ment, il existe deux modalit√©s pour faire conna√Ætre son identit√© √† Github :\n\nune authentification HTTPS (d√©crite ici) : l‚Äôauthentification se fait avec un login et un mot de passe ou avec un token (un mot de passe compliqu√© g√©n√©r√© automatiquement par Github et connu exclusivement du d√©tenteur du compte Github) ;\nune authentification SSH : l‚Äôauthentification se fait par une cl√© crypt√©e disponible sur le poste de travail et que GitHub ou GitLab conna√Æt. Une fois configur√©e, cette m√©thode ne n√©cessite plus de faire conna√Ætre son identit√© : l‚Äôempreinte digitale que constitue la cl√© suffit √† reconna√Ætre un utilisateur.\n\nLa documentation collaborative utilitR pr√©sente les raisons pour lesquelles il convient de favoriser\nla m√©thode HTTPS sur la m√©thode SSH.\n\n\n Note\nDepuis Ao√ªt 2021, Github n‚Äôautorise plus l‚Äôauthentification par mot de passe\nlorsqu‚Äôon interagit (pull/push) avec un d√©p√¥t distant\n(raisons ici).\nIl est n√©cessaire d‚Äôutiliser un token (jeton d‚Äôacc√®s) qui pr√©sente l‚Äôavantage\nd‚Äô√™tre r√©voquable (on peut √† tout moment supprimer un jeton si, par exemple,\non suspecte qu‚Äôil a √©t√© diffus√© par erreur) et √† droits limit√©s\n(le jeton permet certaines op√©rations standards mais\nn‚Äôautorise pas certaines op√©rations d√©terminantes comme la suppression\nd‚Äôun d√©p√¥t).\n√Ä partir de mars 2023 et jusqu‚Äô√† la fin de 2023, GitHub commencera progressivement √† exiger que tous les utilisateurs de GitHub activent une ou plusieurs formes d‚Äôauthentification √† deux facteurs (2FA). Pour plus d‚Äôinformations sur le d√©ploiement de l‚Äôinscription 2FA, consultez cet article de blog. Concr√®tement, cela signifie que vous devrez au choix :\n\nRenseigner votre num√©ro de portable pour valider certaines connexions gr√¢ce √† un code que vous recevrez par sms ;\nInstaller une application d‚Äôauthentification (Ex : Microsoft Authenticator) install√©e sur votre t√©l√©phone qui g√©n√®rera un QR code que vous pourrez scanner depuis github, ce qui ne n√©cessite pas que vous ayez √† fournir votre num√©ro de t√©l√©phone\nUtiliser une clef USB de s√©curit√©\n\nPour choisir entre ces diff√©rentes options, vous pouvez vous rendre sur Settings &gt; Password and authentication &gt; Enable two-factor authentication.\n\n\n\n\n Note\nIl est important de ne jamais stocker un token, et encore moins son mot de passe, dans un projet.\nIl est possible de stocker un mot de passe ou token de mani√®re s√©curis√©e et durable\navec le credential helper de Git. Celui-ci est pr√©sent√© par la suite.\nS‚Äôil n‚Äôest pas possible d‚Äôutiliser le credential helper de Git, un mot de passe\nou token peut √™tre stock√© de mani√®re s√©curis√© dans\nun syst√®me de gestion de mot de passe comme Keepass.\nNe jamais stocker un jeton Github, ou pire un mot de passe, dans un fichier\ntexte non crypt√©. Les logiciels de gestion de mot de passe\n(comme Keepass, recommand√© par l‚ÄôAnssi)\nsont simples\nd‚Äôusage et permettent de ne conserver sur l‚Äôordinateur qu‚Äôune version\nhash√©e du mot de passe qui ne peut √™tre d√©crypt√©e qu‚Äôavec un mot de passe\nconnu de vous seuls."
  },
  {
    "objectID": "content/git/exogit.html#cr√©er-un-jeton",
    "href": "content/git/exogit.html#cr√©er-un-jeton",
    "title": "Un cadavre exquis pour d√©couvrir Git",
    "section": "Cr√©er un jeton",
    "text": "Cr√©er un jeton\nLa documentation officielle comporte un certain nombre de captures d‚Äô√©cran expliquant\ncomment proc√©der.\nNous allons utiliser le credential helper associ√© √† Git pour stocker\nce jeton. Ce credential helper permet de conserver de mani√®re p√©renne\nun jeton (on peut aussi faire en sorte que le mot de passe soit automatiquement\nsupprim√© de la m√©moire de l‚Äôordinateur au bout, par\nexemple, d‚Äôune heure).\nL‚Äôinconv√©nient de cette m√©thode est que Git √©crit en clair le jeton dans\nun fichier de configuration. C‚Äôest pour cette raison qu‚Äôon utilise des jetons\npuisque, si ces derniers sont r√©v√©l√©s, on peut toujours les r√©voquer et √©viter\nles probl√®mes (pour ne pas stocker en clair un jeton il faudrait utiliser\nune librairie suppl√©mentaire comme libsecrets qui est au-del√† du programme\nde ce cours).\nSi vous d√©sirez conserver de mani√®re plus durable ou plus s√©curis√©e votre jeton\n(en ne conservant pas le jeton en clair mais de mani√®re hash√©e),\nest d‚Äôutiliser un gestionnaire de mot de passe comme\nKeepass (recommand√© par l‚ÄôAnssi). N√©anmoins,\nil est recommand√© de tout de m√™me fixer une date d‚Äôexp√©ritation\naux jetons pour limiter les risques de s√©curit√© d‚Äôun token qui fuite\nsans s‚Äôen rendre compte.\n\n\n Exercice 2.0 : Cr√©er un service sur le SSPCloud\nEn amont de l‚Äôexercice 2, pour les utilisateurs\ndu SSPCloud,\nil est recommand√© d‚Äôouvrir un service Jupyter\nen suivant les consignes suivantes:\n\nDans la page Mes services, cliquer sur le bouton Nouveau service ;\nChoisir Jupyter-Python ;\nCliquer sur Configuration Jupyter-Python. ‚ö†Ô∏è ne pas lancer le service\ntout de suite !\nFaire d√©filer les onglets pour arriver sur l‚Äôonglet Git ;\nRemplacer la valeur sous Cache par un nombre important,\npar exemple 36000 pour que le jeton que vous utiliserez soit\nvalable 10 heures ;\nLancer le service.\n\n\n\n\n\n Exercice 2 : Cr√©er et stocker un token\n1Ô∏è‚É£ Suivre la\ndocumentation officielle en ne donnant que les droits repo au jeton (ajouter les droits\nworkflow si vous d√©sirez que votre jeton soit utilisable pour des projets\no√π l‚Äôint√©gration continue est n√©cessaire).\nPour r√©sumer les √©tapes devraient √™tre les suivantes :\nSettings &gt; Developers Settings &gt; Personal Access Token &gt; Generate a new token &gt; ‚ÄúMy bash script‚Äù &gt; Expiration ‚Äú30 days‚Äù &gt; cocher juste ‚Äúrepo‚Äù &gt; Generate token &gt; Le copier\n2Ô∏è‚É£ Ouvrir un terminal depuis Jupyter (par exemple File &gt; New &gt; Terminal) ou VSCode (Terminal &gt; New Terminal).\n3Ô∏è‚É£ [Optionnel] Taper dans le terminal la commande\nqui convient selon votre syst√®me d‚Äôexploitation pour activer le\ncredential helper:\n# Sous mac et linux et le datalab\ngit config --global credential.helper store\n\n# Sous windows\ngit config --global credential.helper manager-core\n4Ô∏è‚É£ R√©cup√©rer, sur la page d‚Äôaccueil de votre d√©p√¥t, l‚Äôurl du d√©p√¥t distant.\nIl prend la forme suivante\nhttps://github.com/&lt;username&gt;/&lt;reponame&gt;.git\nVous pouvez utiliser l‚Äôicone √† droite pour copier l‚Äôurl.\n5Ô∏è‚É£ Retournez dans le Terminal. Taper\ngit clone repo_url\no√π repo_url est l‚Äôurl du d√©p√¥t en question (vous pouvez utiliser\nMAJ+Inser pour coller l‚Äôurl pr√©c√©demment copi√©)\nTapez Entr√©e. Dans le cas d‚Äôun r√©pertoire priv√© et sans credential helper, renseignez ensuite votre identifiant, faites Entr√©e, puis votre personal access token, Entr√©e. Si vous n‚Äôavez pas d‚Äôerreur, cela signifie\nque l‚Äôauthentification a bien fonctionn√© et donc que tout va\nbien. Sinon, il vous suffit de r√©√©crire l‚Äôinstruction git clone et de retenter de taper votre personal access token. Normalement, si vous avez cr√©√© un d√©p√¥t vide dans l‚Äôexercice 1,\nvous avez un message de Git:\n\nwarning: You appear to have cloned an empty repository.\n\nCe n‚Äôest pas une erreur mais il est pr√©f√©rable de suivre la\nconsigne de l‚Äôexercice 1 et de cr√©er un projet non vide.\nLe dossier de votre projet a bien\n√©t√© cr√©√©.\nSi vous avez une erreur, suivez la consigne pr√©sent√©e ci-apr√®s\npour r√©initialiser\nvotre credential helper\n6Ô∏è‚É£ Si vous le d√©sirez, vous pouvez changer la visibilit√© de votre d√©p√¥t\nen le rendant public.\n7Ô∏è‚É£ Stocker le token sur le SSP Cloud (ou un gestionnaire de mot de passe) :\n\nMon Compte -&gt; Services externes -&gt; Jeton d‚Äôacc√®s personnel GitHub\n\n\n\n\n\n Note\nSi vous avez fait une faute de frappe dans le mot de passe ou dans le jeton, il est possible de vider la m√©moire\nde la mani√®re suivante, sous Mac ou Linux :\ngit config --global --unset credential.helper\nSous Windows, si vous avez utilis√© l‚Äôoption manager-core √©voqu√©e ci-dessus, vous pouvez utiliser une interface graphique pour effacer le mot de passe ou jeton erron√©. Pour cela, dans le menu d√©marrer, taper Gestionnaire d'identification (ou Credential Manager si Windows ne trouve pas). Dans l‚Äôinterface graphique qui s‚Äôouvre, il est possible de supprimer le mot de passe ou jeton en question. Apr√®s cela, vous devriez √† nouveau avoir l‚Äôopportunit√© de taper un mot de passe ou jeton lors d‚Äôune authentification HTTPS."
  },
  {
    "objectID": "content/git/exogit.html#envoyer-des-modifications-sur-le-d√©p√¥t-distant-push",
    "href": "content/git/exogit.html#envoyer-des-modifications-sur-le-d√©p√¥t-distant-push",
    "title": "Un cadavre exquis pour d√©couvrir Git",
    "section": "Envoyer des modifications sur le d√©p√¥t distant: push",
    "text": "Envoyer des modifications sur le d√©p√¥t distant: push\n\n\n Exercice 6 : Interagir avec Github\nIl convient maintenant d‚Äôenvoyer les fichiers sur le d√©p√¥t distant.\n\n1Ô∏è‚É£\nL‚Äôobjectif est d‚Äôenvoyer vos modifications vers origin.\nOn va passer par la ligne de commande car les boutons push/pull\nde l‚Äôextension Jupyter ne fonctionnent pas de mani√®re syst√©matique.\nTaper\ngit push origin main\nCela signifie: ‚Äúgit envoie (push) mes modifications sur la\nbranche main (la branche sur laquelle on a travaill√©, on reviendra\ndessus) vers mon d√©p√¥t (alias\norigin)‚Äù\nRemarque : Si vous obtenez l‚Äôerreur suivante error: src refspec hello does not match any, c‚Äôest probablement que vous avez indiqu√© le mauvais nom de branche. La confusion se fait souvent entre le nom main ou master (ancienne norme de branche par d√©faut).\nNormalement, si vous avez utilis√© le credential helper, Git ne\nvous demande pas vos identifiants de connexion. Sinon,\nil faut taper\nvotre identifiant Github et votre mot de passe correspond au personal access token nouvellement cr√©√© !\n2Ô∏è‚É£ Retournez voir le d√©p√¥t sur Github, vous devriez maintenant voir le fichier\n.gitignore s‚Äôafficher en page d‚Äôaccueil."
  },
  {
    "objectID": "content/git/exogit.html#la-fonctionnalit√©-pull",
    "href": "content/git/exogit.html#la-fonctionnalit√©-pull",
    "title": "Un cadavre exquis pour d√©couvrir Git",
    "section": "La fonctionnalit√© pull",
    "text": "La fonctionnalit√© pull\nLa deuxi√®me mani√®re d‚Äôinteragir avec le d√©p√¥t est de r√©cup√©rer des\nr√©sultats disponibles en ligne sur sa copie de travail. On appelle\ncela pull.\nPour le moment, vous √™tes tout seul sur le d√©p√¥t. Il n‚Äôy a donc pas de\npartenaire pour modifier un fichier dans le d√©p√¥t distant. On va simuler ce\ncas en utilisant l‚Äôinterface graphique de Github pour modifier\ndes fichiers. On rapatriera les r√©sultats en local dans un deuxi√®me temps.\n\n\n Exercice 7 : Rapatrier des modifs en local\n1Ô∏è‚É£ Se rendre sur votre d√©p√¥t depuis l‚Äôinterface https://github.com\n\nSe placer sur le fichier README.md et cliquer sur le bouton Edit this file, qui prend la forme d‚Äôun ic√¥ne de crayon.\n\n2Ô∏è‚É£ L‚Äôobjectif est de\ndonner au README.md un titre en ajoutant, au d√©but du document, la ligne suivante :\n# Mon oeuvre d'art surr√©aliste \nSautez une ligne et entrez le texte que vous d√©sirez, sans ponctuation. Par exemple,\nle ch√™ne un jour dit au roseau\n3Ô∏è‚É£ Cliquez sur l‚Äôonglet Preview pour voir le texte mis en forme au format Markdown\n4Ô∏è‚É£ R√©diger un titre et un message compl√©mentaire pour faire le commit. Conserver\nl‚Äôoption par d√©faut Commit directly to the main branch\n5Ô∏è‚É£ Editer √† nouveau le README en cliquant sur le crayon juste au dessus\nde l‚Äôaffichage du contenu du README.\nAjouter une deuxi√®me phrase et corrigez la\nponctuation de la premi√®re. Ecrire un message de commit et valider.\nLe Ch√™ne un jour dit au roseau :\nVous avez bien sujet d'accuser la Nature\n6Ô∏è‚É£ Au dessus de l‚Äôaborescence des fichiers, vous devriez voir s‚Äôafficher le\ntitre du dernier commit. Vous pouvez cliquer dessus pour voir la modification\nque vous avez faite.\n7Ô∏è‚É£ Les r√©sultats sont sur le d√©p√¥t distant mais ne sont pas sur votre\ndossier de travail dans Jupyter ou VSCode. Il faut re-synchroniser votre copie locale\navec le d√©p√¥t distant :\n\nSur VSCode, cliquez simplement sur ... &gt; Pull √† c√¥t√© du bouton qui permet\nde visualiser le graphe Git.\nAvec l‚Äôinterface Jupyter, si cela est possible, appuyez tout simplement sur la petite\nfl√®che vers le bas, qui est celle qui a d√©sormais la pastille orange.\nSi cette fl√®che n‚Äôest pas disponible ou si vous travaillez dans un autre\nenvironnement, vous pouvez utiliser la ligne de\ncommande et taper\n\ngit pull origin main\nCela signifie : ‚Äúgit r√©cup√®re (pull) les modifications sur la\nbranche main vers mon d√©p√¥t (alias\norigin)‚Äù\n8Ô∏è‚É£ Regarder √† nouveau l‚Äôhistorique des commits. Cliquez sur le\ndernier commit et affichez les changements sur le fichier. Vous pouvez\nremarquer la finesse du contr√¥le de version : Git d√©tecte au sein de\nla premi√®re ligne de votre texte que vous avez mis des majuscules\nou de la ponctuation.\n\n\nL‚Äôop√©ration pull permet :\n\nA votre syst√®me local de v√©rifier les modifications sur le d√©p√¥t distant\nque vous n‚Äôauriez pas faites (cette op√©ration s‚Äôappelle fetch)\nDe les fusionner s‚Äôil n‚Äôy a pas de conflit de version ou si les conflits de\nversion sont automatiquement fusionnables (deux modifications d‚Äôun fichier mais\nqui ne portent pas sur le m√™me emplacement)."
  },
  {
    "objectID": "content/git/exogit.html#le-workflow-adopt√©",
    "href": "content/git/exogit.html#le-workflow-adopt√©",
    "title": "Un cadavre exquis pour d√©couvrir Git",
    "section": "Le workflow adopt√©",
    "text": "Le workflow adopt√©\nNous allons adopter le mode de travail le plus simple, le Github Flow.\nIl correspond √† cette forme caract√©ristique d‚Äôarbre:\n\nLa branche main constitue le tronc\nLes branches partent de main et divergent\nLorsque les modifications aboutissent, elles sont int√©gr√©es √† main ;\nla branche en question dispara√Æt :\n\n\n\n\n\n\nIl existe des workflows plus complexes, notamment le Git Flow que j‚Äôutilise\npour d√©velopper ce cours. Ce tutoriel, tr√®s bien fait,\nillustre avec un graphique la complexit√© accrue de ce flow :\n\n\n\n\n\nCette fois, une branche interm√©diaire, par exemple une branche development\nint√®gre des modifications √† tester avant de les int√©grer dans la version\nofficielle (main).\n\n\n Hint\nVous pourrez trouvez des dizaines d‚Äôarticles et d‚Äôouvrages sur ce sujet dont chacun pr√©tend avoir trouv√© la meilleure organisation du travail (Git flow, GitHub flow, GitLab flow‚Ä¶). Ne lisez pas trop ces livres et articles sinon vous serez perdus (un peu comme avec les magazines destin√©s aux jeunes parents‚Ä¶).\nLa m√©thode de travail la plus simple est le Github flow qu‚Äôon vous a propos√© d‚Äôadopter. L‚Äôarborescence est reconnaissable : des branches divergent et reviennent syst√©matiquement vers main.\nPour des projets plus complexes dans des √©quipes d√©veloppant des applications, on pourra utiliser d‚Äôautres m√©thodes de travail, notamment le Git flow. Il n‚Äôexiste pas de r√®gles universelles pour d√©terminer la m√©thode de travail ; l‚Äôimportant c‚Äôest, avant tout, de se mettre d‚Äôaccord sur des r√®gles communes de travail avec votre √©quipe."
  },
  {
    "objectID": "content/git/exogit.html#m√©thode-pour-les-merges",
    "href": "content/git/exogit.html#m√©thode-pour-les-merges",
    "title": "Un cadavre exquis pour d√©couvrir Git",
    "section": "M√©thode pour les merges",
    "text": "M√©thode pour les merges\nLes merges vers main doivent imp√©rativement passer par Github (ou Gitlab). Cela permet de garder une trace explicite de ceux-ci (par exemple ici), sans avoir √† chercher dans l‚Äôarborescence, parfois complexe, d‚Äôun projet.\nLa bonne pratique veut qu‚Äôon fasse un squash commit pour √©viter une inflation du nombre de commits dans main: les branches ont vocation √† proposer une multitude de petits commits, les modifications dans main doivent √™tre simples √† tracer d‚Äôo√π le fait de modifier des petits bouts de code.\nComme on l‚Äôa fait dans un exercice pr√©c√©dent, il est tr√®s pratique d‚Äôajouter dans le corps du message close #xx o√π xx est le num√©ro d‚Äôune issue associ√©e √† la pull request. Lorsque la pull request sera fusionn√©e, l‚Äôissue sera automatiquement ferm√©e et un lien sera cr√©√© entre l‚Äôissue et la pull request. Cela vous permettra de comprendre, plusieurs mois ou ann√©es plus tard comment et pourquoi telle ou telle fonctionnalit√© a √©t√© impl√©ment√©e.\nEn revanche, l‚Äôint√©gration des derni√®res modifications de main vers une branche se fait en local. Si votre branche est en conflit, le conflit doit √™tre r√©solu dans la branche et pas dans main.\nmain doit toujours rester propre."
  },
  {
    "objectID": "content/git/exogit.html#mise-en-pratique",
    "href": "content/git/exogit.html#mise-en-pratique",
    "title": "Un cadavre exquis pour d√©couvrir Git",
    "section": "Mise en pratique",
    "text": "Mise en pratique\n\n\n Exercice 9 : Interactions avec le d√©p√¥t distant\nCet exercice se fait par groupe de trois ou quatre. Il y aura deux r√¥les dans ce sc√©nario :\n\nUne personne aura la responsabilit√© d‚Äô√™tre mainteneur\nDeux √† trois personnes seront d√©veloppeurs.\n\n1Ô∏è‚É£ Le mainteneur cr√©e un d√©p√¥t sur Github. Il/Elle donne des droits au(x) d√©veloppeur(s) du projet (Settings &gt; Manage Access &gt; Invite a collaborator).\n2Ô∏è‚É£ Chaque membre du projet, cr√©e une copie locale du projet gr√¢ce √† la commande git clone ou\navec le bouton Clone a repository de JupyterLab.\nPour cela, r√©cup√©rer l‚Äôurl HTTPS du d√©p√¥t en copiant l‚Äôurl du d√©p√¥t que vous pouvez trouver, par exemple, dans la page d‚Äôaccueil du d√©p√¥t, en dessous de Quick setup ‚Äî if you‚Äôve done this kind of thing before\nEn ligne de commande, cela donnera :\ngit clone https://github.com/&lt;username&gt;/&lt;reponame&gt;.git\n3Ô∏è‚É£ Chaque membre du projet cr√©e un fichier avec son nom et son pr√©nom, selon cette structure nom-prenom.md en √©vitant les caract√®res sp√©ciaux. Il √©crit dedans trois phrases de son choix sans ponctuation ni majuscules (pour pouvoir effectuer une correction ult√©rieurement). Enfin, il commit sur le projet.\nPour rappel, en ligne de commande cela donnera les commandes suivantes √† modifier\ngit add nom-prenom.md\ngit commit -m \"C'est l'histoire de XXXXX\"\n4Ô∏è‚É£ Chacun essaie d‚Äôenvoyer (push) ses modifications locales sur le d√©p√¥t:\ngit push origin main\n5Ô∏è‚É£ A ce stade, une seule personne (la plus rapide) devrait ne pas avoir rencontr√© de rejet du push. C‚Äôest normal, avant d‚Äôaccepter une modification Git v√©rifie en premier lieu la coh√©rence de la branche avec le d√©p√¥t distant. Le premier ayant fait un push a modifi√© le d√©p√¥t commun ; les autres doivent int√©grer ces modifications dans leur version locale (pull) avant d‚Äôavoir le droit de proposer un changement.\nPour celui/celle/ceux dont le push a √©t√© refus√©, faire\ngit pull origin main\npour ramener les modifications distantes en local.\n6Ô∏è‚É£ Taper git log et regarder la mani√®re dont a √©t√© int√©gr√© la modification de votre camarade ayant pu faire son push\nVous remarquerez que les commits de vos camarades sont int√©gr√©s tels quels √†\nl‚Äôhistoire du d√©p√¥t.\n7Ô∏è‚É£ Faire √† nouveau\ngit pull origin main\nLe dernier doit refaire, √† nouveau, les √©tapes 5 √† 7 (dans une √©quipe de quatre\nil faudra encore le refaire une fois).\n\n\n\n\n Warning √† nouveau: ne JAMAIS FAIRE git push force\nQuand on fait face √† un rejet du push, on est tent√© de faire passer en force le push malgr√© la mise en garde pr√©c√©dente.\nIl faut imm√©diatement oublier cette solution, elle cr√©e de nombreux probl√®mes et, en fait, ne r√©sout rien. L‚Äôun des risques est de r√©√©crire enti√®rement l‚Äôhistorique rendant les copies locales, et donc les modifications de vos collaborateurs, caduques. Cela vous vaudra, √† raison, des remontrances de vos partenaires qui perdent le b√©n√©fice de leur historique Git qui, s‚Äôils ont des versions sans push depuis longtemps peuvent avoir diverger fortement du d√©p√¥t ma√Ætre.\n\n\n\n\n Exercice 10 : G√©rer les conflits quand on travaille sur le m√™me fichier\nDans la continuit√© de l‚Äôexercice pr√©c√©dent, chaque personne va travailler sur les fichiers des autres membres de l‚Äô√©quipe.\n1Ô∏è‚É£ Les deux ou trois d√©veloppeurs ajoutent la ponctuation et les majuscules du fichier du premier d√©veloppeur.\n2Ô∏è‚É£ Ils sautent une ligne et ajoutent une phrase (pas tous la m√™me).\n3Ô∏è‚É£ Valider les r√©sultats (git add . et commit) et faire un push\n4Ô∏è‚É£ La personne la plus rapide n‚Äôa, normalement, rencontr√© aucune difficult√© (elle peut s‚Äôarr√™ter temporairement pour regarder ce qui va se passer chez les voisins). Les autres voient leur push refus√© et doivent faire un pull.\nüí• Il y a conflit, ce qui doit √™tre signal√© par un message du type :\nAuto-merging XXXXXX\nCONFLICT (content): Merge conflict in XXXXXX.md\nAutomatic merge failed; fix conflicts and then commit the result.\n5Ô∏è‚É£ Etudier le r√©sultat de git status\n6Ô∏è‚É£ Si vous ouvrez les fichiers incrimin√©s, vous devriez voir des balises du type\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nthis is some content to mess with\ncontent to append\n=======\ntotally different content to merge later\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; new_branch_to_merge_later\n7Ô∏è‚É£ Corriger √† la main les fichiers en choisissant, pour chaque ligne, la version qui vous convient et en retirant les balises. Valider en faisant:\ngit add . && git commit -m \"R√©solution du conflit par XXXX\"\nRemplacer XXXX par votre nom. La balise && permet d‚Äôencha√Æner, en une seule ligne de code, les deux commandes.\n8Ô∏è‚É£ Faire un push. Pour la derni√®re personne, refaire les op√©rations 4 √† 8\n\n\nGit permet donc de travailler, en m√™me temps, sur le m√™me fichier et de limiter le nombre de gestes manuels n√©cessaires pour faire la fusion. Lorsqu‚Äôon travaille sur des bouts diff√©rents du m√™me fichier, on n‚Äôa m√™me pas besoin de faire de modification manuelle, la fusion peut √™tre automatique.\nGit est un outil tr√®s puissant. Mais, il ne remplace pas une bonne organisation du travail. Vous l‚Äôavez vu, ce mode de travail uniquement sur main peut √™tre p√©nible. Les branches prennent tout leur sens dans ce cas.\n\n\n Exercice 11 : Gestion des branches\n1Ô∏è‚É£ Le mainteneur va contribuer directement dans main et ne cr√©e pas de branche. Chaque d√©veloppeur cr√©e une branche, en local nomm√©e contrib-XXXXX o√π XXXXX est le pr√©nom:\ngit checkout -b contrib-XXXXX\n2Ô∏è‚É£ Chaque membre du groupe cr√©e un fichier README.md o√π il √©crit une phrase sujet-verbe-compl√©ment. Le mainteneur est le seul √† ajouter un titre dans le README (qu‚Äôil commit dans main).\n3Ô∏è‚É£ Chacun push le produit de son subconscient sur le d√©p√¥t.\n4Ô∏è‚É£ Les d√©veloppeurs ouvrent, chacun, une pull request sur Github de leur branche vers main. Ils lui donnent un titre explicite.\n5Ô∏è‚É£ Dans la discussion de chaque pull request, le mainteneur demande au d√©veloppeur d‚Äôint√©grer le titre qu‚Äôil a √©crit.\n6Ô∏è‚É£ Chaque d√©veloppeur, en local, int√®gre cette modification en faisant\n# Pour √™tre s√ªr d'√™tre sur sa propre branche\ngit checkout branche-XXXX\ngit merge main\nR√©gler le conflit et valider (add et commit). Pousser le r√©sultat. Le mainteneur choisit une des pull request et la valide avec l‚Äôoption squash commits. V√©rifier sur la page d‚Äôaccueil le r√©sultat.\n7Ô∏è‚É£ L‚Äôauteur (si 2 d√©veloppeurs) ou les deux auteurs (si 3 d√©veloppeurs) de la pull request non valid√©e doivent √† nouveau r√©p√©ter l‚Äôop√©ration 6.\n8Ô∏è‚É£ Une fois le conflit de version r√©gl√© et pouss√©, le mainteneur valide la pull request selon la m√™me proc√©dure que pr√©c√©demment.\n9Ô∏è‚É£ V√©rifier l‚Äôarborescence du d√©p√¥t dans Insights &gt; Network. Votre arbre doit avoir une forme caract√©ristique de ce qu‚Äôon appelle le Github flow:"
  },
  {
    "objectID": "content/git/exogit.html#footnotes",
    "href": "content/git/exogit.html#footnotes",
    "title": "Un cadavre exquis pour d√©couvrir Git",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCe cas de figure arrive lorsqu‚Äôon contribue √† des projets\nsur lesquels on n‚Äôa pas de droit d‚Äô√©criture. Il est alors\nn√©cessaire d‚Äôeffectuer un fork, une copie de ce d√©p√¥t sur laquelle\non dispose de droits.\nDans ce cas de figure, on rencontre g√©n√©ralement un nouvel alias √† c√¥t√© d‚Äôorigin.\nnomm√© upstream (cf.\nle tutoriel Github pour mettre √† jour un fork\net qui pointe vers le d√©p√¥t source √† l‚Äôorigine du fork.\nLa cr√©ation du bouton Fetch upstream par Github facilite grandement\nla mise en coh√©rence d‚Äôupstream et origin et constitue la m√©thode\nrecommand√©e.‚Ü©Ô∏é\nLa commande checkout est un couteau-suisse de la gestion de branche en Git. Elle permet en effet de basculer d‚Äôune branche √† l‚Äôautre, mais aussi d‚Äôen cr√©er, etc.‚Ü©Ô∏é"
  },
  {
    "objectID": "content/git/index.html",
    "href": "content/git/index.html",
    "title": "Git: un outil n√©cessaire pour les data scientists",
    "section": "",
    "text": "Cette partie du site pr√©sente un √©l√©ment qui n‚Äôest pas propre √†\nPython mais qui est n√©anmoins indispensable : la pratique de Git.\nUne grande partie du contenu de la partie provient\nd‚Äôun cours d√©di√© fait avec Romain Avouac.\nLe chapitre de pr√©sentation de Git propose\nune introduction visant √† pr√©senter l‚Äôint√©r√™t d‚Äôutiliser\ncet outil. Une mise en pratique est propos√©e\navec un cadavre exquis."
  },
  {
    "objectID": "content/git/index.html#utilisation-de-git-avec-python",
    "href": "content/git/index.html#utilisation-de-git-avec-python",
    "title": "Git: un outil n√©cessaire pour les data scientists",
    "section": "Utilisation de Git avec Python",
    "text": "Utilisation de Git avec Python\nGit est √† la fois un outil et un langage. Il\nest donc n√©cessaire d‚Äôinstaller, dans un premier\ntemps Git Bash, puis de connecter\nson outil pr√©f√©r√© pour faire du Python (qu‚Äôil\ns‚Äôagisse de Jupyter, VSCode ou PyCharm).\nCette partie est structur√©e en 2 temps :\n\nPr√©sentation de Git\nExercice pour d√©couvrir Git"
  },
  {
    "objectID": "content/modern-ds/elastic_approfondissement.html",
    "href": "content/modern-ds/elastic_approfondissement.html",
    "title": "Approfondissement ElasticSearch pour des recherches de proximit√© g√©ographique",
    "section": "",
    "text": "Pour essayer les exemples pr√©sents dans ce tutoriel :\nCe chapitre est issu du travail produit\ndans le cadre d‚Äôun hackathon de l‚ÄôInsee avec\nRapha√´le Adjerad\net pr√©sente quelques √©l√©ments qui peuvent √™tre utiles\npour l‚Äôenrichissement de donn√©es d‚Äôentreprises\n√† partir d‚Äôun r√©pertoire officiel.\n:warning: Il n√©cessite une version particuli√®re du package elasticsearch pour tenir compte de l‚Äôh√©ritage de la version 7 du moteur Elastic. Pour cela, faire"
  },
  {
    "objectID": "content/modern-ds/elastic_approfondissement.html#objectif",
    "href": "content/modern-ds/elastic_approfondissement.html#objectif",
    "title": "Approfondissement ElasticSearch pour des recherches de proximit√© g√©ographique",
    "section": "Objectif",
    "text": "Objectif\nCe chapitre vise √† approfondir les √©l√©ments pr√©sent√©s sur Elastic pr√©c√©demment. L‚Äôid√©e\nest de se placer dans un contexte op√©rationnel o√π on re√ßoit des informations\nsur des entreprises telles que l‚Äôadresse et la localisation et qu‚Äôon\nd√©sire associer √† des donn√©es administratives consid√©r√©es plus fliables."
  },
  {
    "objectID": "content/modern-ds/elastic_approfondissement.html#r√©plication-de-ce-chapitre",
    "href": "content/modern-ds/elastic_approfondissement.html#r√©plication-de-ce-chapitre",
    "title": "Approfondissement ElasticSearch pour des recherches de proximit√© g√©ographique",
    "section": "R√©plication de ce chapitre",
    "text": "R√©plication de ce chapitre\nComme le pr√©c√©dent, ce chapitre est plus exigeant en termes d‚Äôinfrastructures que les pr√©c√©dents.\nIl n√©cessite un serveur Elastic. Les utilisateurs du\nSSP Cloud pourront r√©pliquer les exemples de ce cours\ncar cette technologie est disponible (que ce soit pour indexer une base ou\npour requ√™ter une base existante).\nLa premi√®re partie de ce tutoriel, qui consiste √† cr√©er une base Sirene g√©olocalis√©e\n√† partir des donn√©es open-data ne n√©cessite pas d‚Äôarchitecture particuli√®re et\npeut ainsi √™tre ex√©cut√©e en utilisant les packages suivants :\n\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "content/modern-ds/elastic_approfondissement.html#sources",
    "href": "content/modern-ds/elastic_approfondissement.html#sources",
    "title": "Approfondissement ElasticSearch pour des recherches de proximit√© g√©ographique",
    "section": "Sources",
    "text": "Sources\nCe chapitre va utiliser plusieurs sources de diffusion de\nl‚ÄôInsee:\n\nLe stock des √©tablissements pr√©sents dans les donn√©es de diffusion Sirene ;\nLes donn√©es Sirene g√©olocalis√©es\n\nLes donn√©es √† siretiser sont celles du registre Fran√ßais des √©missions polluantes\n√©tabli par le Minist√®re de la Transition Energ√©tique. Le jeu de donn√©es\nest disponible sur data.gouv"
  },
  {
    "objectID": "content/modern-ds/elastic_approfondissement.html#importer-la-base-d√©j√†-cr√©√©e",
    "href": "content/modern-ds/elastic_approfondissement.html#importer-la-base-d√©j√†-cr√©√©e",
    "title": "Approfondissement ElasticSearch pour des recherches de proximit√© g√©ographique",
    "section": "Importer la base d√©j√† cr√©√©e",
    "text": "Importer la base d√©j√† cr√©√©e\nLes donn√©es √† utiliser pour constuire une base Sirene g√©olocalis√©e\nsont trop volumineuses pour les serveurs mis √† disposition\ngratuitement par Github pour la compilation de ce site web.\nNous proposons ainsi une version d√©j√† construite, stock√©e\ndans l‚Äôespace de mise √† disposition du SSP Cloud. Ce fichier est\nau format parquet et est ouvert √†\ntous, m√™me pour les personnes ne disposant pas d‚Äôun compte.\nLe code ayant construit cette base est pr√©sent√© ci-dessous.\nPour importer cette base, on utilise les fonctionalit√©s\nde pyarrow qui permettent d‚Äôimporter un fichier sur\nun syst√®me de stockage cloud comme s‚Äôil √©tait\npr√©sent sur le disque :\n\nfrom pyarrow import fs\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\nbucket = \"lgaliana\"\npath = \"diffusion/sirene_geolocalized.parquet\"\n\ns3 = fs.S3FileSystem(endpoint_override=\"http://\"+\"minio.lab.sspcloud.fr\")\n\ndf_geolocalized = pq.ParquetDataset(f'{bucket}/{path}', filesystem=s3).read_pandas().to_pandas()\ndf_geolocalized.head(3)"
  },
  {
    "objectID": "content/modern-ds/elastic_approfondissement.html#reproduire-la-construction-de-la-base",
    "href": "content/modern-ds/elastic_approfondissement.html#reproduire-la-construction-de-la-base",
    "title": "Approfondissement ElasticSearch pour des recherches de proximit√© g√©ographique",
    "section": "Reproduire la construction de la base",
    "text": "Reproduire la construction de la base\nLa premi√®re base d‚Äôentr√©e √† utiliser est disponible sur\ndata.gouv\n\nimport requests\nimport zipfile\n\nurl_download = \"https://www.data.gouv.fr/fr/datasets/r/0651fb76-bcf3-4f6a-a38d-bc04fa708576\"\nreq = requests.get(url_download)\n\nwith open(\"sirene.zip\",'wb') as f:\n  f.write(req.content)\n\nwith zipfile.ZipFile(\"sirene.zip\", 'r') as zip_ref:\n  zip_ref.extractall(\"sirene\")\n\nOn va importer seulement les colonnes utiles et simplifier la structure\npour √™tre en mesure de ne garder que les informations qui nous\nint√©ressent (nom de l‚Äôentreprise, adresse, commune, code postal‚Ä¶)\n\nimport pandas as pd\nimport numpy as np\n\nlist_cols = [\n  'siren', 'siret',\n  'activitePrincipaleRegistreMetiersEtablissement',\n  'complementAdresseEtablissement',\n  'numeroVoieEtablissement',\n  'typeVoieEtablissement',\n  'libelleVoieEtablissement',\n  'codePostalEtablissement',\n  'libelleCommuneEtablissement',\n  'codeCommuneEtablissement',\n  'etatAdministratifEtablissement',\n  'denominationUsuelleEtablissement',\n  'activitePrincipaleEtablissement'\n]\n\ndf = pd.read_csv(\n  \"sirene/StockEtablissement_utf8.csv\",\n  usecols = list_cols)\n\ndf['numero'] = df['numeroVoieEtablissement']\\\n  .replace('-', np.NaN).str.split().str[0]\\\n  .str.extract('(\\d+)', expand=False)\\\n  .fillna(\"0\").astype(int)\n\ndf['numero'] = df['numero'].astype(str).replace(\"0\",\"\")\n\ndf['adresse'] = df['numero'] + \" \" + \\\n  df['typeVoieEtablissement'] + \" \" + \\\n  df['libelleVoieEtablissement']\n\ndf['adresse'] = df['adresse'].replace(np.nan, \"\")\n\ndf = df.loc[df['etatAdministratifEtablissement'] == \"A\"]\n\ndf.rename(\n  {\"denominationUsuelleEtablissement\": \"denom\",\n  \"libelleCommuneEtablissement\": \"commune\",\n  \"codeCommuneEtablissement\" : \"code_commune\",\n  \"codePostalEtablissement\": \"code_postal\"},\n  axis = \"columns\", inplace = True)\n\ndf['ape'] = df['activitePrincipaleEtablissement'].str.replace(\"\\.\", \"\", regex = True)\ndf['denom'] = df[\"denom\"].replace(np.nan, \"\")\n\ndf_siret = df.loc[:, ['siren', 'siret','adresse', 'ape', 'denom', 'commune', 'code_commune','code_postal']]\ndf_siret['code_postal'] = df_siret['code_postal'].replace(np.nan, \"0\").astype(int).astype(str).replace(\"0\",\"\")\n\nOn importe ensuite les donn√©es g√©olocalis√©es\n\nimport zipfile\nimport shutil\nimport os\n\n#os.remove(\"sirene.zip\")\n#shutil.rmtree('sirene/')\n\nurl_geoloc = \"https://files.data.gouv.fr/insee-sirene-geo/GeolocalisationEtablissement_Sirene_pour_etudes_statistiques_utf8.zip\"\nr = requests.get(url_geoloc)  \n\nwith open('geoloc.zip', 'wb') as f:\n    f.write(r.content)\n\nwith zipfile.ZipFile(\"geoloc.zip\", 'r') as zip_ref:\n  zip_ref.extractall(\"geoloc\")\n\ndf_geoloc = pd.read_csv(\n  \"geoloc/GeolocalisationEtablissement_Sirene_pour_etudes_statistiques_utf8.csv\",\n  usecols = [\"siret\", \"epsg\", \"x_longitude\", \"y_latitude\"] , sep = \";\")\n\nIl ne reste plus qu‚Äô√† associer les deux jeux de donn√©es\n\ndf_geolocalized = df_siret.merge(df_geoloc, on = \"siret\") \ndf_geolocalized['code_commune'] = df_geolocalized['code_commune'].astype(str) \n\nSi vous avez acc√®s √† un espace de stockage cloud de type\nS3, il est possible d‚Äôutiliser pyarrow pour enregister\ncette base. Afin de l‚Äôenregistrer dans un espace de stockage\npublic, nous allons l‚Äôenregistrer dans un dossier diffusion1\n\nfrom pyarrow import fs\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\nbucket = \"lgaliana\"\npath = \"diffusion/sirene_geolocalized.parquet\"\n\ns3 = fs.S3FileSystem(endpoint_override=\"http://\"+\"minio.lab.sspcloud.fr\")\n\ntable = pa.Table.from_pandas(df_geolocalized)"
  },
  {
    "objectID": "content/modern-ds/elastic_approfondissement.html#d√©finition-du-mapping",
    "href": "content/modern-ds/elastic_approfondissement.html#d√©finition-du-mapping",
    "title": "Approfondissement ElasticSearch pour des recherches de proximit√© g√©ographique",
    "section": "D√©finition du mapping",
    "text": "D√©finition du mapping\nOn va proc√©der par √©tape en essayant d‚Äôutiliser la structure la plus simple\npossible.\n:one: On s‚Äôoccupe d‚Äôabord de d√©finir le mapping\npour les variables textuelles.\n\nstring_var = [\"adresse\", \"denom\", \"ape\", \"commune\"]\nmap_string = {\"type\": \"text\", \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}}}\nmapping_string = {l: map_string for l in string_var}\n\n:two: Les variables cat√©gorielles sont utilis√©es\npar le biais du type keyword:\n\n# keywords\nkeyword_var = [\"siren\",\"siret\",\"code_commune\",\"code_postal\"]\nmap_keywords = {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}\nmapping_keywords = {l: map_keywords for l in keyword_var}\n\n:three: La nouveaut√© par rapport √† la partie\npr√©c√©dente est l‚Äôutilisation de la\ndimension g√©ographique. Elastic propose\nle type geo_point pour cela.\n\n# geoloc\nmapping_geoloc = {\n  \"location\": {\n    \"type\": \"geo_point\"\n    }\n}    \n\nOn collecte tout cela ensemble dans un\ndictionnaire:\n\n# mapping\nmapping_elastic = {\"mappings\":\n  {\"properties\":\n    {**mapping_string, **mapping_geoloc, **mapping_keywords}\n  }\n}\n\nIl est tout √† fait possible de d√©finir un mapping\nplus raffin√©. Ici, on va privil√©gier\nl‚Äôutilisation d‚Äôun mapping simple pour\nillustrer la recherche par distance\ng√©ographique en priorit√©."
  },
  {
    "objectID": "content/modern-ds/elastic_approfondissement.html#cr√©ation-de-lindex",
    "href": "content/modern-ds/elastic_approfondissement.html#cr√©ation-de-lindex",
    "title": "Approfondissement ElasticSearch pour des recherches de proximit√© g√©ographique",
    "section": "Cr√©ation de l‚Äôindex",
    "text": "Cr√©ation de l‚Äôindex\nPour cr√©er le nouvel index, on s‚Äôassure d‚Äôabord de ne pas\nd√©j√† l‚Äôavoir cr√©√© et on passe le mapping d√©fini\npr√©c√©demment.\n\nif es.indices.exists('sirene'):\n    es.indices.delete('sirene')\n\nes.indices.create(index = \"sirene\", body = mapping_elastic)"
  },
  {
    "objectID": "content/modern-ds/elastic_approfondissement.html#indexation-de-la-base-g√©olocalis√©e",
    "href": "content/modern-ds/elastic_approfondissement.html#indexation-de-la-base-g√©olocalis√©e",
    "title": "Approfondissement ElasticSearch pour des recherches de proximit√© g√©ographique",
    "section": "Indexation de la base g√©olocalis√©e",
    "text": "Indexation de la base g√©olocalis√©e\nPour le moment, l‚Äôindex est vide. Il convient de\nle peupler.\nIl est n√©anmoins n√©cessaire de cr√©er le champ location\nau format attendu par elastic: lat, lon √† partir\nde nos colonnes.\n\ndf_geolocalized['location'] = df_geolocalized['y_latitude'].astype(str) + \", \" + df_geolocalized['x_longitude'].astype(str)\n\nLa fonction suivante permet de structurer chaque\nligne du DataFrame telle qu‚ÄôElastic l‚Äôattend:\n\ndef gen_dict_from_pandas(index_name, df):\n    '''\n    Lit un dataframe pandas Open Food Facts, renvoi un it√©rable = dictionnaire des donn√©es √† indexer, sous l'index fourni\n    '''\n    for i, row in df.iterrows():\n        header= {\"_op_type\": \"index\",\"_index\": index_name,\"_id\": i}\n        yield {**header,**row}\n\nEnfin, on peut industrialiser l‚Äôindexation\nde notre DataFrame en faisant tourner de\nmani√®re successive cette fonction:\n\nfrom elasticsearch.helpers import bulk, parallel_bulk\nfrom collections import deque\ndeque(parallel_bulk(client=es, actions=gen_dict_from_pandas(\"sirene\", df_geolocalized), chunk_size = 1000, thread_count = 4))\n\n\nes.count(index = 'sirene')\n\nObjectApiResponse({'count': 13059694, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}})"
  },
  {
    "objectID": "content/modern-ds/elastic_approfondissement.html#premier-exemple-de-requ√™te-g√©ographique",
    "href": "content/modern-ds/elastic_approfondissement.html#premier-exemple-de-requ√™te-g√©ographique",
    "title": "Approfondissement ElasticSearch pour des recherches de proximit√© g√©ographique",
    "section": "Premier exemple de requ√™te g√©ographique",
    "text": "Premier exemple de requ√™te g√©ographique\n\nex1 = es.search(index = 'sirene', body = '''{\n  \"query\": {\n    \"bool\": {\n      \"must\":\n      { \"match\": { \"denom\":   \"institut national de la statistique\"}}\n      }\n  }\n}\n''')['hits']['hits']\n\necho_insee = pd.json_normalize(ex1)\necho_insee\n\nOn remarque d√©j√† que les intitul√©s ne sont\npas bons. Quand est-il de leurs localisations ?\n\nplot_folium_sirene(\n  echo_insee, yvar = \"_source.y_latitude\",\n  xvar = \"_source.x_longitude\")\n\nCe premier essai nous sugg√®re qu‚Äôil est\nn√©cessaire d‚Äôam√©liorer notre recherche.\nPlusieurs voies sont possibles:\n\nAm√©liorer le preprocessing de nos champs\ntextuels en excluant, par exemple, les\nstopwords ;\nEffectuer une restriction g√©ographique\npour mieux cibler l‚Äôensemble de recherche\nTrouver une variable cat√©gorielle jouant\nle r√¥le de variable de blocage2 pour\nmieux cibler les paires pertinentes\n\nConcernant la restriction\ng√©ographique, Elastic fournit une approche\ntr√®s efficace de ciblage g√©ographique.\nEn connaissant une position approximative\nde l‚Äôentreprise √† rechercher,\nil est ainsi possible de\nrechercher dans un rayon\nd‚Äôune taille plus ou moins grande.\nEn supposant qu‚Äôon connaisse pr√©cis√©ment\nla localisation de l‚ÄôInsee, on peut\nchercher dans un rayon relativement\nrestreint. Si notre position √©tait plus\napproximative, on pourrait rechercher\ndans un rayon plus large.\n\nex2 = es.search(index = 'sirene', body = '''{\n  \"query\": {\n    \"bool\": {\n      \"must\":\n      { \"match\": { \"denom\":   \"institut national de la statistique\"}}\n      ,\n      \"filter\":\n        {\"geo_distance\": {\n          \"distance\": \"1km\",\n          \"location\": {\n            \"lat\": \"48.8168\",\n            \"lon\": \"2.3099\"\n          }\n        }\n      }\n    }\n  }\n}\n''')['hits']['hits']\n\necho_insee = pd.json_normalize(ex2)\necho_insee\n\n\n\n Hint\nConna√Ætre la localisation pr√©cise d‚Äôune\nentreprise\nn√©cessite d√©j√† une bonne remont√©e\nd‚Äôinformation sur celle-ci.\nIl est plus plausible de supposer\nqu‚Äôon dispose, dans une phase amont\nde la chaine de production,\nde l‚Äôadresse de celle-ci.\nDans ce cas, il est utile\nd‚Äôutiliser un service de g√©ocodage,\ncomme l‚ÄôAPI Adresse\nd√©velopp√©e par Etalab.\n\n\nLes r√©sultats sont par construction mieux\ncibl√©s. N√©anmoins ils sont toujours d√©cevants\npuisqu‚Äôon ne parvient pas √† identifier l‚ÄôInsee\ndans les dix meilleurs √©chos.\n\nspecificsearch = es.search(index = 'sirus_2020', body = \n'''{\n  \"query\": {\n    \"bool\": {\n      \"should\":\n          { \"match\": { \"rs_denom\":   \"CPCU - CENTRALE DE BERCY\"}},\n      \"filter\": [\n          {\"geo_distance\": {\n                  \"distance\": \"0.5km\",\n                  \"location\": {\n                        \"lat\": \"48.84329\", \n                        \"lon\": \"2.37396\"\n                              }\n                            }\n            }, \n            { \"prefix\":  { \"apet\": \"3530\" }}\n                ]\n            }\n          }\n}'''\n)"
  },
  {
    "objectID": "content/modern-ds/elastic_approfondissement.html#footnotes",
    "href": "content/modern-ds/elastic_approfondissement.html#footnotes",
    "title": "Approfondissement ElasticSearch pour des recherches de proximit√© g√©ographique",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nConcernant la premi√®re piste, il aurait\nfallu mieux d√©finir notre mapping pour\nautoriser des analyzers. A d√©faut,\nnous pourrons\nutiliser nltk ou spacy pour transformer\nles champs textuels avant d‚Äôenvoyer la requ√™te.\nCette solution pr√©sente l‚Äôinconv√©nient\nde ne pas formatter de la m√™me mani√®re l‚Äôensemble\nindex√© mais pourrait malgr√© tout am√©liorer la pertinence\ndes recherches.‚Ü©Ô∏é\nConcernant la premi√®re piste, il aurait\nfallu mieux d√©finir notre mapping pour\nautoriser des analyzers. A d√©faut,\nnous pourrons\nutiliser nltk ou spacy pour transformer\nles champs textuels avant d‚Äôenvoyer la requ√™te.\nCette solution pr√©sente l‚Äôinconv√©nient\nde ne pas formatter de la m√™me mani√®re l‚Äôensemble\nindex√© mais pourrait malgr√© tout am√©liorer la pertinence\ndes recherches.‚Ü©Ô∏é"
  },
  {
    "objectID": "content/modern-ds/dallE.html",
    "href": "content/modern-ds/dallE.html",
    "title": "G√©n√©ration d‚Äôimages avec Python, DALL-E et StableDiffusion",
    "section": "",
    "text": "Pour tester les exemples pr√©sent√©s dans ce chapitre:\nNote\nL‚Äôutilisation de ce tutoriel est assez exigeante en termes d‚Äôinfrastructure\ncar il est n√©cessaire de disposer de GPU.\nLes GPU sont des ressources rares et assez ch√®res, elle ne sont donc pas mises √† disposition de fa√ßon\naussi ais√©es que les CPU dans les cloud providers. Il s‚Äôagit de plus\nde ressources plus polluantes que les CPU.\nDes GPU sont disponibles sur Google Colab, la proc√©dure pour les activer\nest expliqu√©e ci-dessous. Des GPU sont √©galement disponibles sur le SSPCloud\nmais sont √† utiliser avec parcimonie. Elles ne sont pas mises √† disposition\npar d√©faut car il s‚Äôagit d‚Äôune ressource rare. Ce chapitre, lanc√© depuis le\nbouton en d√©but de chapitre, active cette option pour permettre la r√©plication\ndes exemples.\nHint\nPar d√©faut, Colab n‚Äôutilise pas de GPU mais de la CPU. Il est donc n√©cessaire\nd‚Äô√©diter les param√®tres d‚Äôex√©cution du Notebook\n\nDans le menu Ex√©cution, cliquer sur Modifier le type d'ex√©cution ;\nS√©lectionner GPU sous Acc√©l√©rateur mat√©riel."
  },
  {
    "objectID": "content/modern-ds/dallE.html#installation-de-pytorch",
    "href": "content/modern-ds/dallE.html#installation-de-pytorch",
    "title": "G√©n√©ration d‚Äôimages avec Python, DALL-E et StableDiffusion",
    "section": "Installation de PyTorch",
    "text": "Installation de PyTorch\nPour installer PyTorch, la librairie de Deep Learning\nd√©velopp√©e par Meta, il suffit de suivre les recommandations\nsur le site web officiel.\nDans un Notebook, cela prendra la forme suivante :\n\n!conda install mamba\n!mamba install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch\n\n\n\n Note\nJe propose ici d‚Äôutiliser mamba pour acc√©l√©rer l‚Äôinstallation.\nDes √©l√©ments sur mamba sont disponibles dans l‚Äôintroduction de ce cours."
  },
  {
    "objectID": "content/modern-ds/dallE.html#acc√®s-√†-huggingface",
    "href": "content/modern-ds/dallE.html#acc√®s-√†-huggingface",
    "title": "G√©n√©ration d‚Äôimages avec Python, DALL-E et StableDiffusion",
    "section": "Acc√®s √† HuggingFace",
    "text": "Acc√®s √† HuggingFace\nLa question - non n√©gligeable - de l‚Äôacc√®s √†\nde la GPU mise √† part,\nla r√©utilisation des mod√®les de Stable Diffusion est\ntr√®s facile car la documentation mise √† disposition sur\nHuggingFace est tr√®s bien faite.\nLa premi√®re √©tape est de se cr√©er un compte sur HuggingFace\net se cr√©er un token3. Ce token sera donn√© √† l‚ÄôAPI\nde HuggingFace pour s‚Äôauthentifier.\nL‚ÄôAPI d‚ÄôHuggingFace n√©cessite l‚Äôinstallation du\npackage diffusers.\nDans un Notebook, le code suivant permet d‚Äôinstaller la librairie\nrequise:\n\n!pip install --upgrade diffusers transformers scipy accelerate\n\n\n\n Note\nOn va supposer que le token est stock√© dans une variable\nd‚Äôenvironnement HF_PAT. Cela √©vite d‚Äô√©crire le token\ndans un Notebook qu‚Äôon va\npotentiellement partager, alors que le token\nest un √©l√©ment √† garder secret. Pour l‚Äôimporter\ndans la session Python:\nSi vous n‚Äôavez pas la possibilit√© de rentrer le token dans les variables\nd‚Äôenvironnement, cr√©ez une cellule qui cr√©e la variable\nHF_TOKEN et supprimez l√† de suite pour ne pas l‚Äôoublier avant\nde partager votre token.\n\n\n\nimport os\nHF_TOKEN = os.getenv('HF_PAT')"
  },
  {
    "objectID": "content/modern-ds/dallE.html#footnotes",
    "href": "content/modern-ds/dallE.html#footnotes",
    "title": "G√©n√©ration d‚Äôimages avec Python, DALL-E et StableDiffusion",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIl est notamment possible de r√©utiliser l‚Äôimage g√©n√©r√©e √† des fins commerciales. En revanche, il est interdit de chercher √† nuire √† une personne. Pour cette raison, il est fr√©quent que les visages de personnes c√©l√®bres soient flout√©s pour √©viter la cr√©ation de contenu nuisant √† leur r√©putation.‚Ü©Ô∏é\nComme les autres plateformes du monde de la data science,\nHuggingFace a adopt√© l‚Äôutilisation standardis√©e des\njetons (token) comme m√©thode d‚Äôauthentification. Le jeton est\ncomme un mot de passe sauf qu‚Äôil n‚Äôest pas invent√© par l‚Äôutilisateur\n(ce qui permet qu‚Äôil ne soit pas partag√© avec d‚Äôautres sites web potentiellement\nmoins s√©curis√©s), est r√©vocable (date d‚Äôexpiration ou choix de l‚Äôutilisateur)\net dispose de droits moins importants qu‚Äôun\nmot de passe qui vous permet, potentiellement,\nde changer tous les param√®tres de votre compte. Je recommande vivement l‚Äôutilisation\nd‚Äôun gestionnaire de mot de passe pour\nstocker vos token (si vous utilisez Git, Docker, etc.\nvous en avez potentiellement beaucoup) plut√¥t que\nstocker ces jetons dans des fichiers non s√©curis√©s.‚Ü©Ô∏é\nComme les autres plateformes du monde de la data science,\nHuggingFace a adopt√© l‚Äôutilisation standardis√©e des\njetons (token) comme m√©thode d‚Äôauthentification. Le jeton est\ncomme un mot de passe sauf qu‚Äôil n‚Äôest pas invent√© par l‚Äôutilisateur\n(ce qui permet qu‚Äôil ne soit pas partag√© avec d‚Äôautres sites web potentiellement\nmoins s√©curis√©s), est r√©vocable (date d‚Äôexpiration ou choix de l‚Äôutilisateur)\net dispose de droits moins importants qu‚Äôun\nmot de passe qui vous permet, potentiellement,\nde changer tous les param√®tres de votre compte. Je recommande vivement l‚Äôutilisation\nd‚Äôun gestionnaire de mot de passe pour\nstocker vos token (si vous utilisez Git, Docker, etc.\nvous en avez potentiellement beaucoup) plut√¥t que\nstocker ces jetons dans des fichiers non s√©curis√©s.‚Ü©Ô∏é"
  },
  {
    "objectID": "content/modern-ds/index.html",
    "href": "content/modern-ds/index.html",
    "title": "Partie 5: Introduction aux outils et m√©thodes √† l‚Äô√©tat de l‚Äôart",
    "section": "",
    "text": "Les parties pr√©c√©dentes √©taient tr√®s tourn√©es sur l‚Äôacquisition\nde comp√©tences minimales dans chaque domaine de l‚Äôanalyse de donn√©es.\nCette partie propose des √©l√©ments plus avanc√©s mais plus repr√©sentatifs\ndu travail quotidien du data scientist. Cette partie\npr√©sente la mani√®re dont Python peut √™tre utilis√© dans une architecture\nmoderne de type cloud. Elle illustre la mani√®re dont Python peut\nservir de couteau-suisse faisant l‚Äôinterface entre diff√©rents\nlangages plus efficaces ou plusieurs types de donn√©es.\nCette partie est en cours de construction et pr√©sentera les\n√©l√©ments suivants :"
  },
  {
    "objectID": "content/modern-ds/index.html#contenu-de-la-partie",
    "href": "content/modern-ds/index.html#contenu-de-la-partie",
    "title": "Partie 5: Introduction aux outils et m√©thodes √† l‚Äô√©tat de l‚Äôart",
    "section": "Contenu de la partie",
    "text": "Contenu de la partie"
  },
  {
    "objectID": "content/NLP/04_word2vec.html",
    "href": "content/NLP/04_word2vec.html",
    "title": "M√©thodes de vectorisation : comptages et word embeddings",
    "section": "",
    "text": "Cette page approfondit certains aspects pr√©sent√©s dans la\npartie introductive. Apr√®s avoir travaill√© sur le\nComte de Monte Cristo, on va continuer notre exploration de la litt√©rature\navec cette fois des auteurs anglophones:\nLes donn√©es sont disponibles ici : spooky.csv et peuvent √™tre requ√©t√©es via l‚Äôurl\nhttps://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv.\nLe but va √™tre dans un premier temps de regarder dans le d√©tail les termes les plus fr√©quents utilis√©s par les auteurs, de les repr√©senter graphiquement puis on va ensuite essayer de pr√©dire quel texte correspond √† quel auteur √† partir de diff√©rents mod√®les de vectorisation, notamment les word embeddings.\nCe notebook est librement inspir√© de :\nWarning\nComme dans la partie pr√©c√©dente, il faut t√©l√©charger quelques √©l√©ments\npour que nos librairies de NLP puissent fonctionner correctement.\nEn premier lieu, il convient d‚Äôinstaller les librairies ad√©quates\n(spacy, gensim et sentence_transformers):\n!pip install spacy gensim sentence_transformers\nEnsuite, comme nous allons utiliser √©galement spacy, il convient de t√©l√©charger\nle corpus Anglais. Pour cela, on peut se r√©f√©rer √†\nla documentation de spacy,\nextr√™mement bien faite.\n\nId√©alement, il faut installer le module via la ligne de commande. Dans\nune cellule de notebook Jupyter, faire :\n\n!python -m spacy download en_core_web_sm\n\nSans acc√®s √† la ligne de commande (depuis une instance Docker par exemple),\nfaire :\n\nimport spacy\nspacy.cli.download(\"en_core_web_sm\")\n\nSinon, il est √©galement possible d‚Äôinstaller le module en faisant pointer\npip install vers le fichier ad√©quat sur\nGithub. Pour cela, taper\n\npip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gensim\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n\nfrom gensim.models.word2vec import Word2Vec\nimport gensim.downloader\nfrom sentence_transformers import SentenceTransformer"
  },
  {
    "objectID": "content/NLP/04_word2vec.html#nettoyage-des-donn√©es",
    "href": "content/NLP/04_word2vec.html#nettoyage-des-donn√©es",
    "title": "M√©thodes de vectorisation : comptages et word embeddings",
    "section": "Nettoyage des donn√©es",
    "text": "Nettoyage des donn√©es\nNous allons ainsi √† nouveau utiliser le jeu de donn√©es spooky:\n\ndata_url = 'https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv'\nspooky_df = pd.read_csv(data_url)\n\nLe jeu de donn√©es met ainsi en regard un auteur avec une phrase qu‚Äôil a √©crite:\n\nspooky_df.head()\n\n\nPreprocessing\nEn NLP, la premi√®re √©tape est souvent celle du preprocessing, qui inclut notamment les √©tapes de tokenization et de nettoyage du texte. Comme celles-ci ont √©t√© vues en d√©tail dans le pr√©c√©dent chapitre, on se contentera ici d‚Äôun preprocessing minimaliste : suppression de la ponctuation et des stop words (pour la visualisation et les m√©thodes de vectorisation bas√©es sur des comptages).\nJusqu‚Äô√† pr√©sent, nous avons utilis√© principalement nltk pour le\npreprocessing de donn√©es textuelles. Cette fois, nous proposons\nd‚Äôutiliser la librairie spaCy qui permet de mieux automatiser sous forme de\npipelines de preprocessing.\nPour initialiser le processus de nettoyage,\non va utiliser le corpus en_core_web_sm (voir plus\nhaut pour l‚Äôinstallation de ce corpus):\n\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n\nOn va utiliser un pipe spacy qui permet d‚Äôautomatiser, et de parall√©liser,\nun certain nombre d‚Äôop√©rations. Les pipes sont l‚Äô√©quivalent, en NLP, de\nnos pipelines scikit ou des pipes pandas. Il s‚Äôagit donc d‚Äôun outil\ntr√®s appropri√© pour industrialiser un certain nombre d‚Äôop√©rations de\npreprocessing :\n\ndef clean_docs(texts, remove_stopwords=False, n_process = 4):\n    \n    docs = nlp.pipe(texts, \n                    n_process=n_process,\n                    disable=['parser', 'ner',\n                             'lemmatizer', 'textcat'])\n    stopwords = nlp.Defaults.stop_words\n\n    docs_cleaned = []\n    for doc in docs:\n        tokens = [tok.text.lower().strip() for tok in doc if not tok.is_punct]\n        if remove_stopwords:\n            tokens = [tok for tok in tokens if tok not in stopwords]\n        doc_clean = ' '.join(tokens)\n        docs_cleaned.append(doc_clean)\n        \n    return docs_cleaned\n\nOn applique la fonction clean_docs √† notre colonne pandas.\nLes pandas.Series √©tant it√©rables, elles se comportent comme des listes et\nfonctionnent ainsi tr√®s bien avec notre pipe spacy\n\nspooky_df['text_clean'] = clean_docs(spooky_df['text'])\n\n\nspooky_df.head()\n\n\n\nEncodage de la variable √† pr√©dire\nOn r√©alise un simple encodage de la variable √† pr√©dire :\nil y a trois cat√©gories (auteurs), repr√©sent√©es par des entiers 0, 1 et 2.\nPour cela, on utilise le LabelEncoder de scikit d√©j√† pr√©sent√©\ndans la partie mod√©lisation. On va utiliser la m√©thode\nfit_transform qui permet, en un tour de main, d‚Äôappliquer √† la fois\nl‚Äôentra√Ænement (fit), √† savoir la cr√©ation d‚Äôune correspondance entre valeurs\nnum√©riques et labels, et l‚Äôappliquer (transform) √† la m√™me colonne.\n\nle = LabelEncoder()\nspooky_df['author_encoded'] = le.fit_transform(spooky_df['author'])\n\nOn peut v√©rifier les classes de notre LabelEncoder :\n\nle.classes_\n\n\n\nConstruction des bases d‚Äôentra√Ænement et de test\nOn met de c√¥t√© un √©chantillon de test (20 %) avant toute analyse (m√™me descriptive).\nCela permettra d‚Äô√©valuer nos diff√©rents mod√®les toute √† la fin de mani√®re tr√®s rigoureuse,\npuisque ces donn√©es n‚Äôauront jamais utilis√©es pendant l‚Äôentra√Ænement.\nNotre √©chantillon initial n‚Äôest pas √©quilibr√© (balanced) : on retrouve plus d‚Äôoeuvres de\ncertains auteurs que d‚Äôautres. Afin d‚Äôobtenir un mod√®le qui soit √©valu√© au mieux, nous allons donc stratifier notre √©chantillon de mani√®re √† obtenir une r√©partition similaire d‚Äôauteurs dans nos\nensembles d‚Äôentra√Ænement et de test.\n\nX_train, X_test, y_train, y_test = train_test_split(spooky_df['text_clean'].values, \n                                                    spooky_df['author_encoded'].values, \n                                                    test_size=0.2, \n                                                    random_state=33,\n                                                    stratify = spooky_df['author_encoded'].values)\n\nPar exemple, les textes d‚ÄôEAP repr√©sentent 40 % des √©chantillons d‚Äôentra√Ænement et de test :\n\nprint(100*y_train.tolist().count(0)/(len(y_train)))\nprint(100*y_test.tolist().count(0)/(len(y_test)))\n\nAper√ßu du premier √©l√©ment de X_train :\n\nX_train[0]\n\nOn peut aussi v√©rifier qu‚Äôon est capable de retrouver\nla correspondance entre nos auteurs initiaux avec\nla m√©thode inverse_transform\n\nprint(y_train[0], le.inverse_transform([y_train[0]])[0])"
  },
  {
    "objectID": "content/NLP/04_word2vec.html#statistiques-exploratoires",
    "href": "content/NLP/04_word2vec.html#statistiques-exploratoires",
    "title": "M√©thodes de vectorisation : comptages et word embeddings",
    "section": "Statistiques exploratoires",
    "text": "Statistiques exploratoires\n\nR√©partition des labels\nRefaisons un graphique que nous avons d√©j√† produit pr√©c√©demment pour voir\nla r√©partition de notre corpus entre auteurs :\n\nfig = pd.Series(le.inverse_transform(y_train)).value_counts().plot(kind='bar')\nfig\n\nOn observe une petite asym√©trie : les passages des livres d‚ÄôEdgar Allen Poe sont plus nombreux que ceux des autres auteurs dans notre corpus d‚Äôentra√Ænement, ce qui peut √™tre probl√©matique dans le cadre d‚Äôune t√¢che de classification.\nL‚Äô√©cart n‚Äôest pas dramatique, mais on essaiera d‚Äôen tenir compte dans l‚Äôanalyse en choisissant une m√©trique d‚Äô√©valuation pertinente.\n\n\nMots les plus fr√©quemment utilis√©s par chaque auteur\nOn va supprimer les stopwords pour r√©duire le bruit dans notre jeu\nde donn√©es.\n\n# Suppression des stop words\nX_train_no_sw = clean_docs(X_train, remove_stopwords=True)\nX_train_no_sw = np.array(X_train_no_sw)\n\nPour visualiser rapidement nos corpus, on peut utiliser la technique des\nnuages de mots d√©j√† vue √† plusieurs reprises.\nVous pouvez essayer de faire vous-m√™me les nuages ci-dessous\nou cliquer sur la ligne ci-dessous pour afficher le code ayant\ng√©n√©r√© les figures :\n\nCliquer pour afficher le code üëá\n\ndef plot_top_words(initials, ax, n_words=20):\n    # Calcul des mots les plus fr√©quemment utilis√©s par l'auteur\n    texts = X_train_no_sw[le.inverse_transform(y_train) == initials]\n    all_tokens = ' '.join(texts).split()\n    counts = Counter(all_tokens)\n    top_words = [word[0] for word in counts.most_common(n_words)]\n    top_words_counts = [word[1] for word in counts.most_common(n_words)]\n    \n    # Repr√©sentation sous forme de barplot\n    ax = sns.barplot(ax = ax, x=top_words, y=top_words_counts)\n    ax.set_title(f'Most Common Words used by {initials_to_author[initials]}')\n\n\ninitials_to_author = {\n    'EAP': 'Edgar Allen Poe',\n    'HPL': 'H.P. Lovecraft',\n    'MWS': 'Mary Wollstonecraft Shelley'\n}\n\nfig, axs = plt.subplots(3, 1, figsize = (12,12))\n\nplot_top_words('EAP', ax = axs[0])\nplot_top_words('HPL', ax = axs[1])\nplot_top_words('MWS', ax = axs[2])\n\n\n\nBeaucoup de mots se retrouvent tr√®s utilis√©s par les trois auteurs.\nIl y a cependant des diff√©rences notables : le mot ‚Äúlife‚Äù\nest le plus employ√© par MWS, alors qu‚Äôil n‚Äôappara√Æt pas dans les deux autres tops.\nDe m√™me, le mot ‚Äúold‚Äù est le plus utilis√© par HPL\nl√† o√π les deux autres ne l‚Äôutilisent pas de mani√®re surrepr√©sent√©e.\nIl semble donc qu‚Äôil y ait des particularit√©s propres √† chacun des auteurs\nen termes de vocabulaire,\nce qui laisse penser qu‚Äôil est envisageable de pr√©dire les auteurs √† partir\nde leurs textes dans une certaine mesure."
  },
  {
    "objectID": "content/NLP/04_word2vec.html#pr√©diction-sur-le-set-dentra√Ænement",
    "href": "content/NLP/04_word2vec.html#pr√©diction-sur-le-set-dentra√Ænement",
    "title": "M√©thodes de vectorisation : comptages et word embeddings",
    "section": "Pr√©diction sur le set d‚Äôentra√Ænement",
    "text": "Pr√©diction sur le set d‚Äôentra√Ænement\nNous allons √† pr√©sent v√©rifier cette conjecture en comparant\nplusieurs mod√®les de vectorisation,\ni.e. de transformation du texte en objets num√©riques\npour que l‚Äôinformation contenue soit exploitable dans un mod√®le de classification.\n\nD√©marche\nComme nous nous int√©ressons plus √† l‚Äôeffet de la vectorisation qu‚Äô√† la t√¢che de classification en elle-m√™me,\nnous allons utiliser un algorithme de classification simple (un SVM lin√©aire), avec des param√®tres non fine-tun√©s (c‚Äôest-√†-dire des param√®tres pas n√©cessairement choisis pour √™tre les meilleurs de tous).\n\nclf = LinearSVC(max_iter=10000, C=0.1)\n\nCe mod√®le est connu pour √™tre tr√®s performant sur les t√¢ches de classification de texte, et nous fournira donc un bon mod√®le de r√©f√©rence (baseline). Cela nous permettra √©galement de comparer de mani√®re objective l‚Äôimpact des m√©thodes de vectorisation sur la performance finale.\n\n\n\n\nPour les deux premi√®res m√©thodes de vectorisation\n(bas√©es sur des fr√©quences et fr√©quences relatives des mots),\non va simplement normaliser les donn√©es d‚Äôentr√©e, ce qui va permettre au SVM de converger plus rapidement, ces mod√®les √©tant sensibles aux diff√©rences d‚Äô√©chelle dans les donn√©es.\nOn va √©galement fine-tuner via grid-search\ncertains hyperparam√®tres li√©s √† ces m√©thodes de vectorisation :\n\non teste diff√©rents ranges de n-grams (unigrammes et unigrammes + bigrammes)\non teste avec et sans stop-words\n\nAfin d‚Äô√©viter le surapprentissage,\non va √©valuer les diff√©rents mod√®les via validation crois√©e, calcul√©e sur 4 blocs.\nOn r√©cup√®re √† la fin le meilleur mod√®le selon une m√©trique sp√©cifi√©e.\nOn choisit le score F1,\nmoyenne harmonique de la pr√©cision et du rappel,\nqui donne un poids √©quilibr√© aux deux m√©triques, tout en p√©nalisant fortement le cas o√π l‚Äôune des deux est faible.\nPr√©cis√©ment, on retient le score F1 *micro-averaged* :\nles contributions des diff√©rentes classes √† pr√©dire sont agr√©g√©es,\npuis on calcule le score F1 sur ces donn√©es agr√©g√©es.\nL‚Äôavantage de ce choix est qu‚Äôil permet de tenir compte des diff√©rences\nde fr√©quences des diff√©rentes classes.\n\n\nPipeline de pr√©diction\nOn va utiliser un pipeline scikit ce qui va nous permettre d‚Äôavoir\nun code tr√®s concis pour effectuer cet ensemble de t√¢ches coh√©rentes.\nDe plus, cela va nous assurer de g√©rer de mani√®re coh√©rentes nos diff√©rentes\ntransformations (cf.¬†partie sur les pipelines)\nPour se faciliter la vie, on d√©finit une fonction fit_vectorizers qui\nint√®gre dans un pipeline g√©n√©rique une m√©thode d‚Äôestimation scikit\net fait de la validation crois√©e en cherchant le meilleur mod√®le\n(en excluant/incluant les stopwords et avec unigrammes/bigrammes)\n\ndef fit_vectorizers(vectorizer):\n    pipeline = Pipeline(\n    [\n        (\"vect\", vectorizer()),\n        (\"scaling\", StandardScaler(with_mean=False)),\n        (\"clf\", clf),\n    ]\n    )\n\n    parameters = {\n        \"vect__ngram_range\": ((1, 1), (1, 2)),  # unigrams or bigrams\n        \"vect__stop_words\": (\"english\", None)\n    }\n\n    grid_search = GridSearchCV(pipeline, parameters, scoring='f1_micro',\n                               cv=4, n_jobs=4, verbose=1)\n    grid_search.fit(X_train, y_train)\n\n    best_parameters = grid_search.best_estimator_.get_params()\n    for param_name in sorted(parameters.keys()):\n        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n\n    print(f\"CV scores {grid_search.cv_results_['mean_test_score']}\")\n    print(f\"Mean F1 {np.mean(grid_search.cv_results_['mean_test_score'])}\")\n    \n    return grid_search"
  },
  {
    "objectID": "content/NLP/04_word2vec.html#approche-bag-of-words",
    "href": "content/NLP/04_word2vec.html#approche-bag-of-words",
    "title": "M√©thodes de vectorisation : comptages et word embeddings",
    "section": "Approche bag-of-words",
    "text": "Approche bag-of-words\nOn commence par une approche ‚Äúbag-of-words‚Äù,\ni.e.¬†qui revient simplement √† repr√©senter chaque document par un vecteur\nqui compte le nombre d‚Äôapparitions de chaque mot du vocabulaire dans le document.\n\ncv_bow = fit_vectorizers(CountVectorizer)"
  },
  {
    "objectID": "content/NLP/04_word2vec.html#tf-idf",
    "href": "content/NLP/04_word2vec.html#tf-idf",
    "title": "M√©thodes de vectorisation : comptages et word embeddings",
    "section": "TF-IDF",
    "text": "TF-IDF\nOn s‚Äôint√©resse ensuite √† l‚Äôapproche TF-IDF,\nqui permet de tenir compte des fr√©quences relatives des mots.\nAinsi, pour un mot donn√©, on va multiplier la fr√©quence d‚Äôapparition du mot dans le document (calcul√© comme dans la m√©thode pr√©c√©dente) par un terme qui p√©nalise une fr√©quence √©lev√©e du mot dans le corpus. L‚Äôimage ci-dessous, emprunt√©e √† Chris Albon, illustre cette mesure:\n\nSource: Chris Albon\nLa vectorisation TF-IDF permet donc de limiter l‚Äôinfluence des stop-words\net donc de donner plus de poids aux mots les plus salients d‚Äôun document.\nOn observe clairement que la performance de classification est bien sup√©rieure,\nce qui montre la pertinence de cette technique.\n\ncv_tfidf = fit_vectorizers(TfidfVectorizer)"
  },
  {
    "objectID": "content/NLP/04_word2vec.html#word2vec-avec-averaging",
    "href": "content/NLP/04_word2vec.html#word2vec-avec-averaging",
    "title": "M√©thodes de vectorisation : comptages et word embeddings",
    "section": "Word2vec avec averaging",
    "text": "Word2vec avec averaging\nOn va maintenant explorer les techniques de vectorisation bas√©es sur les\nembeddings de mots, et notamment la plus populaire : Word2Vec.\nL‚Äôid√©e derri√®re est simple, mais a r√©volutionn√© le NLP :\nau lieu de repr√©senter les documents par des\nvecteurs sparse de tr√®s grande dimension (la taille du vocabulaire)\ncomme on l‚Äôa fait jusqu‚Äô√† pr√©sent,\non va les repr√©senter par des vecteurs dense (continus)\nde dimension r√©duite (en g√©n√©ral, autour de 100-300).\nChacune de ces dimensions va repr√©senter un facteur latent,\nc‚Äôest √† dire une variable inobserv√©e,\nde la m√™me mani√®re que les composantes principales produites par une ACP.\n\n\n\n\n\nSource: https://medium.com/@zafaralibagh6/simple-tutorial-on-word-embedding-and-word2vec-43d477624b6d\nPourquoi est-ce int√©ressant ?\nPour de nombreuses raisons, mais pour r√©sumer :\ncela permet de beaucoup mieux capturer la similarit√© s√©mantique entre les documents.\nPar exemple, un humain sait qu‚Äôun document contenant le mot ‚ÄúRoi‚Äù\net un autre document contenant le mot ‚ÄúReine‚Äù ont beaucoup de chance\nd‚Äôaborder des sujets semblables.\nPourtant, une vectorisation de type comptage ou TF-IDF\nne permet pas de saisir cette similarit√© :\nle calcul d‚Äôune mesure de similarit√© (norme euclidienne ou similarit√© cosinus)\nentre les deux vecteurs ne prendra en compte la similarit√© des deux concepts, puisque les mots utilis√©s sont diff√©rents.\nA l‚Äôinverse, un mod√®le word2vec bien entra√Æn√© va capter\nqu‚Äôil existe un facteur latent de type ‚Äúroyaut√©‚Äù,\net la similarit√© entre les vecteurs associ√©s aux deux mots sera forte.\nLa magie va m√™me plus loin : le mod√®le captera aussi qu‚Äôil existe un\nfacteur latent de type ‚Äúgenre‚Äù,\net va permettre de construire un espace s√©mantique dans lequel les\nrelations arithm√©tiques entre vecteurs ont du sens ;\npar exemple :\n\\[\\text{king} - \\text{man} + \\text{woman} ‚âà \\text{queen}\\]\nComment ces mod√®les sont-ils entra√Æn√©s ?\nVia une t√¢che de pr√©diction r√©solue par un r√©seau de neurones simple.\nL‚Äôid√©e fondamentale est que la signification d‚Äôun mot se comprend\nen regardant les mots qui apparaissent fr√©quemment dans son voisinage.\nPour un mot donn√©, on va donc essayer de pr√©dire les mots\nqui apparaissent dans une fen√™tre autour du mot cible.\nEn r√©p√©tant cette t√¢che de nombreuses fois et sur un corpus suffisamment vari√©,\non obtient finalement des embeddings pour chaque mot du vocabulaire,\nqui pr√©sentent les propri√©t√©s discut√©es pr√©c√©demment.\n\nX_train_tokens = [text.split() for text in X_train]\nw2v_model = Word2Vec(X_train_tokens, vector_size=200, window=5, \n                     min_count=1, workers=4)\n\n\nw2v_model.wv.most_similar(\"mother\")\n\nOn voit que les mots les plus similaires √† ‚Äúmother‚Äù\nsont souvent des mots li√©s √† la famille, mais pas toujours.\nC‚Äôest li√© √† la taille tr√®s restreinte du corpus sur lequel on entra√Æne le mod√®le,\nqui ne permet pas de r√©aliser des associations toujours pertinentes.\nL‚Äôembedding (la repr√©sentation vectorielle) de chaque document correspond √† la moyenne des word-embeddings des mots qui le composent :\n\ndef get_mean_vector(w2v_vectors, words):\n    words = [word for word in words if word in w2v_vectors]\n    if words:\n        avg_vector = np.mean(w2v_vectors[words], axis=0)\n    else:\n        avg_vector = np.zeros_like(w2v_vectors['hi'])\n    return avg_vector\n\ndef fit_w2v_avg(w2v_vectors):\n    X_train_vectors = np.array([get_mean_vector(w2v_vectors, words)\n                                for words in X_train_tokens])\n    \n    scores = cross_val_score(clf, X_train_vectors, y_train, \n                         cv=4, scoring='f1_micro', n_jobs=4)\n\n    print(f\"CV scores {scores}\")\n    print(f\"Mean F1 {np.mean(scores)}\")\n    return scores\n\n\ncv_w2vec = fit_w2v_avg(w2v_model.wv)\n\nLa performance chute fortement ;\nla faute √† la taille tr√®s restreinte du corpus, comme annonc√© pr√©c√©demment."
  },
  {
    "objectID": "content/NLP/04_word2vec.html#word2vec-pr√©-entra√Æn√©-averaging",
    "href": "content/NLP/04_word2vec.html#word2vec-pr√©-entra√Æn√©-averaging",
    "title": "M√©thodes de vectorisation : comptages et word embeddings",
    "section": "Word2vec pr√©-entra√Æn√© + averaging",
    "text": "Word2vec pr√©-entra√Æn√© + averaging\nQuand on travaille avec des corpus de taille restreinte,\nc‚Äôest g√©n√©ralement une mauvaise id√©e d‚Äôentra√Æner son propre mod√®le word2vec.\nHeureusement, des mod√®les pr√©-entra√Æn√©s sur de tr√®s gros corpus sont disponibles.\nIls permettent de r√©aliser du transfer learning,\nc‚Äôest-√†-dire de b√©n√©ficier de la performance d‚Äôun mod√®le qui a √©t√© entra√Æn√© sur une autre t√¢che ou bien sur un autre corpus.\nL‚Äôun des mod√®les les plus connus pour d√©marrer est le glove_model de\nGensim (Glove pour Global Vectors for Word Representation)1:\n\nGloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\nSource : https://nlp.stanford.edu/projects/glove/\n\nOn peut le charger directement gr√¢ce √† l‚Äôinstruction suivante :\n\nglove_model = gensim.downloader.load('glove-wiki-gigaword-200')\n\nPar exemple, la repr√©sentation vectorielle de roi est l‚Äôobjet\nmultidimensionnel suivant :\n\nglove_model['king']\n\nComme elle est peu intelligible, on va plut√¥t rechercher les termes les\nplus similaires. Par exemple,\n\nglove_model.most_similar('mother')\n\nOn peut retrouver notre formule pr√©c√©dente\n\\[\\text{king} - \\text{man} + \\text{woman} ‚âà \\text{queen}\\]\ndans ce plongement de mots:\n\nglove_model.most_similar(positive = ['king', 'woman'], negative = ['man'])\n\nVous pouvez vous r√©f√©rer √† ce tutoriel\npour en d√©couvrir plus sur Word2Vec.\nFaisons notre apprentissage par transfert :\n\ncv_w2vec_transfert = fit_w2v_avg(glove_model)\n\nLa performance remonte substantiellement.\nCela √©tant, on ne parvient pas √† faire mieux que les approches basiques,\non arrive √† peine aux performances de la vectorisation par comptage.\nEn effet, pour rappel, les performances sont les suivantes :\n\nperfs = pd.DataFrame(\n    [np.mean(cv_bow.cv_results_['mean_test_score']),\n     np.mean(cv_tfidf.cv_results_['mean_test_score']),\n    np.mean(cv_w2vec),\n    np.mean(cv_w2vec_transfert)],\n    index = ['Bag-of-Words','TF-IDF', 'Word2Vec non pr√©-entra√Æn√©', 'Word2Vec pr√©-entra√Æn√©'],\n    columns = [\"Mean F1 score\"]\n).sort_values(\"Mean F1 score\",ascending = False)\nperfs\n\nLes performences limit√©es du mod√®le Word2Vec sont cette fois certainement dues √† la mani√®re dont\nles word-embeddings sont exploit√©s : ils sont moyenn√©s pour d√©crire chaque document.\nCela a plusieurs limites :\n\non ne tient pas compte de l‚Äôordre et donc du contexte des mots\nlorsque les documents sont longs, la moyennisation peut cr√©er\ndes repr√©sentation bruit√©es."
  },
  {
    "objectID": "content/NLP/04_word2vec.html#contextual-embeddings",
    "href": "content/NLP/04_word2vec.html#contextual-embeddings",
    "title": "M√©thodes de vectorisation : comptages et word embeddings",
    "section": "Contextual embeddings",
    "text": "Contextual embeddings\nLes embeddings contextuels visent √† pallier les limites des embeddings\ntraditionnels √©voqu√©es pr√©c√©demment.\nCette fois, les mots n‚Äôont plus de repr√©sentation vectorielle fixe,\ncelle-ci est calcul√©e dynamiquement en fonction des mots du voisinage, et ainsi de suite.\nCela permet de tenir compte de la structure des phrases\net de tenir compte du fait que le sens d‚Äôun mot est fortement d√©pendant des mots\nqui l‚Äôentourent.\nPar exemple, dans les expressions ‚Äúle pr√©sident Macron‚Äù et ‚Äúle camembert Pr√©sident‚Äù le mot pr√©sident n‚Äôa pas du tout le m√™me r√¥le.\nCes embeddings sont produits par des architectures tr√®s complexes,\nde type Transformer (BERT, etc.).\n\nmodel = SentenceTransformer('all-mpnet-base-v2')\n\n\nX_train_vectors = model.encode(X_train)\n\n\nscores = cross_val_score(clf, X_train_vectors, y_train, \n                         cv=4, scoring='f1_micro', n_jobs=4)\n\nprint(f\"CV scores {scores}\")\nprint(f\"Mean F1 {np.mean(scores)}\")\n\n\nperfs = pd.concat(\n  [perfs,\n  pd.DataFrame(\n    [np.mean(scores)],\n    index = ['Contextual Embedding'],\n    columns = [\"Mean F1 score\"])]\n).sort_values(\"Mean F1 score\",ascending = False)\nperfs\n\nVerdict : on fait tr√®s l√©g√®rement mieux que la vectorisation TF-IDF.\nOn voit donc l‚Äôimportance de tenir compte du contexte.\nMais pourquoi, avec une m√©thode tr√®s compliqu√©e, ne parvenons-nous pas √† battre une m√©thode toute simple ?\nOn peut avancer plusieurs raisons :\n\nle TF-IDF est un mod√®le simple, mais toujours tr√®s performant\n(on parle de ‚Äútough-to-beat baseline‚Äù).\nla classification d‚Äôauteurs est une t√¢che tr√®s particuli√®re et tr√®s ardue,\nqui ne fait pas justice aux embeddings. Comme on l‚Äôa dit pr√©c√©demment, ces derniers se r√©v√®lent particuli√®rement pertinents lorsqu‚Äôil est question de similarit√© s√©mantique entre des textes (clustering, etc.).\n\nDans le cas de notre t√¢che de classification, il est probable que\ncertains mots (noms de personnage, noms de lieux) soient suffisants pour classifier de mani√®re pertinente,\nce que ne permettent pas de capter les embeddings qui accordent √† tous les mots la m√™me importance."
  },
  {
    "objectID": "content/NLP/04_word2vec.html#aller-plus-loin",
    "href": "content/NLP/04_word2vec.html#aller-plus-loin",
    "title": "M√©thodes de vectorisation : comptages et word embeddings",
    "section": "Aller plus loin",
    "text": "Aller plus loin\n\nNous avons entra√Æn√© diff√©rents mod√®les sur l‚Äô√©chantillon d‚Äôentra√Ænement par validation crois√©e, mais nous n‚Äôavons toujours pas utilis√© l‚Äô√©chantillon test que nous avons mis de c√¥t√© au d√©but. R√©aliser la pr√©diction sur les donn√©es de test, et v√©rifier si l‚Äôon obtient le m√™me classement des m√©thodes de vectorisation.\nFaire un vrai split train/test : faire l‚Äôentra√Ænement avec des textes de certains auteurs, et faire la pr√©diction avec des textes d‚Äôauteurs diff√©rents. Cela permettrait de neutraliser la pr√©sence de noms de lieux, de personnages, etc.\nComparer avec d‚Äôautres algorithmes de classification qu‚Äôun SVM\n(Avanc√©) : fine-tuner le mod√®le d‚Äôembeddings contextuels sur la t√¢che de classification"
  },
  {
    "objectID": "content/NLP/04_word2vec.html#footnotes",
    "href": "content/NLP/04_word2vec.html#footnotes",
    "title": "M√©thodes de vectorisation : comptages et word embeddings",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nJeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation.‚Ü©Ô∏é"
  },
  {
    "objectID": "content/NLP/02_exoclean.html",
    "href": "content/NLP/02_exoclean.html",
    "title": "Nettoyer un texte: des exercices pour d√©couvrir l‚Äôapproche bag-of-words",
    "section": "",
    "text": "Cette page approfondit certains aspects pr√©sent√©s dans la\npartie introductive. Apr√®s avoir travaill√© sur le\nComte de Monte Cristo, on va continuer notre exploration de la litt√©rature\navec cette fois des auteurs anglophones :\nLes donn√©es sont disponibles ici : spooky.csv et peuvent √™tre requ√©t√©es via l‚Äôurl\nhttps://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv.\nLe but va √™tre dans un premier temps de regarder dans le d√©tail les termes les plus fr√©quemment utilis√©s par les auteurs, de les repr√©senter graphiquement.\nOn prendra appui sur l‚Äôapproche bag of words pr√©sent√©e dans le chapitre pr√©c√©dent1.\nCe notebook est librement inspir√© de :\nLes chapitres suivants permettront d‚Äôintroduire aux enjeux de mod√©lisation\nde corpus textuels. Dans un premier temps, le mod√®le LDA permettra d‚Äôexplorer\nle principe des mod√®les bay√©siens √† couche cach√©es pour mod√©liser les sujets (topics)\npr√©sents dans un corpus et segmenter ces topics selon les mots qui les composent.\nLe dernier chapitre de la partie visera √†\npr√©dire quel texte correspond √† quel auteur √† partir d‚Äôun mod√®le Word2Vec.\nCela sera un pas suppl√©mentaire dans la formalisation puisqu‚Äôil s‚Äôagira de\nrepr√©senter chaque mot d‚Äôun texte sous forme d‚Äôun vecteur de grande dimension, ce\nqui nous permettra de rapprocher les mots entre eux dans un espace complexe.\nCette technique, dite des plongements de mots (Word Embedding),\npermet ainsi de transformer une information complexe difficilement quantifiable\ncomme un mot\nen un objet num√©rique qui peut ainsi √™tre rapproch√© d‚Äôautres par des m√©thodes\nalg√©briques. Pour d√©couvrir ce concept, ce post de blog\nest particuli√®rement utile. En pratique, la technique des\nplongements de mots permet d‚Äôobtenir des tableaux comme celui-ci:"
  },
  {
    "objectID": "content/NLP/02_exoclean.html#librairies-n√©cessaires",
    "href": "content/NLP/02_exoclean.html#librairies-n√©cessaires",
    "title": "Nettoyer un texte: des exercices pour d√©couvrir l‚Äôapproche bag-of-words",
    "section": "Librairies n√©cessaires",
    "text": "Librairies n√©cessaires\nCette page √©voquera les principales librairies pour faire du NLP, notamment :\n\nWordCloud\nnltk\nSpaCy\nKeras\nTensorFlow\n\nIl faudra √©galement installer les librairies gensim et pywaffle\n\n\n Hint\nComme dans la partie pr√©c√©dente, il faut t√©l√©charger quelques √©l√©ments pour que NTLK puisse fonctionner correctement. Pour cela, faire :\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('genesis')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\n\nLa liste des modules √† importer est assez longue, la voici :\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport base64\nimport string\nimport re\nimport nltk\n\nfrom collections import Counter\nfrom time import time\n# from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords\nfrom sklearn.metrics import log_loss\nimport matplotlib.pyplot as plt\n#!pip install pywaffle\nfrom pywaffle import Waffle\n\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\n\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('genesis')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\n[nltk_data] Downloading package stopwords to /github/home/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /github/home/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package genesis to /github/home/nltk_data...\n[nltk_data]   Unzipping corpora/genesis.zip.\n[nltk_data] Downloading package wordnet to /github/home/nltk_data...\n[nltk_data] Downloading package omw-1.4 to /github/home/nltk_data...\n\n\nTrue"
  },
  {
    "objectID": "content/NLP/02_exoclean.html#donn√©es-utilis√©es",
    "href": "content/NLP/02_exoclean.html#donn√©es-utilis√©es",
    "title": "Nettoyer un texte: des exercices pour d√©couvrir l‚Äôapproche bag-of-words",
    "section": "Donn√©es utilis√©es",
    "text": "Donn√©es utilis√©es\n\n\n Exercice 1 : Importer les donn√©es spooky\nPour ceux qui ont envie de tester leurs connaissances en pandas\n\nImporter le jeu de donn√©es spooky √† partir de l‚ÄôURL https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv sous le nom train. L‚Äôencoding est latin-1\nMettre des majuscules au nom des colonnes.\nRetirer le prefix id de la colonne Id et appeler la nouvelle colonne ID.\nMettre l‚Äôancienne colonne Id en index.\n\n\n\nSi vous ne faites pas l‚Äôexercice 1, pensez √† charger les donn√©es en executant la fonction get_data.py :\n\nimport requests\n\nurl = 'https://raw.githubusercontent.com/linogaliana/python-datascientist/master/content/NLP/get_data.py'\nr = requests.get(url, allow_redirects=True)\nopen('getdata.py', 'wb').write(r.content)\n\nimport getdata\ntrain = getdata.create_train_dataframes()\n\nCe code introduit une base nomm√©e train dans l‚Äôenvironnement.\nLe jeu de donn√©es met ainsi en regard un auteur avec une phrase qu‚Äôil a √©crite :\n\ntrain.head()\n\n\n\n\n\n\n\n\nText\nAuthor\nID\n\n\nId\n\n\n\n\n\n\n\nid26305\nThis process, however, afforded me no means of...\nEAP\n26305\n\n\nid17569\nIt never once occurred to me that the fumbling...\nHPL\n17569\n\n\nid11008\nIn his left hand was a gold snuff box, from wh...\nEAP\n11008\n\n\nid27763\nHow lovely is spring As we looked from Windsor...\nMWS\n27763\n\n\nid12958\nFinding nothing else, not even gold, the Super...\nHPL\n12958\n\n\n\n\n\n\n\nOn peut se rendre compte que les extraits des 3 auteurs ne sont\npas forc√©ment √©quilibr√©s dans le jeu de donn√©es.\nIl faudra en tenir compte dans la pr√©diction.\n\nfig = plt.figure()\ng = sns.barplot(x=['Edgar Allen Poe', 'Mary W. Shelley', 'H.P. Lovecraft'], y=train['Author'].value_counts())\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1765: FutureWarning:\n\nunique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n\n\n\n\n\n\n\n\n\n Note\nL‚Äôapproche bag of words est pr√©sent√©e de\nmani√®re plus extensive dans le chapitre pr√©c√©dent.\nL‚Äôid√©e est d‚Äô√©tudier la fr√©quence des mots d‚Äôun document et la\nsurrepr√©sentation des mots par rapport √† un document de\nr√©f√©rence (appel√© corpus).\nCette approche un peu simpliste mais tr√®s\nefficace : on peut calculer des scores permettant par exemple de faire\nde classification automatique de document par th√®me, de comparer la\nsimilarit√© de deux documents. Elle est souvent utilis√©e en premi√®re analyse,\net elle reste la r√©f√©rence pour l‚Äôanalyse de textes mal\nstructur√©s (tweets, dialogue tchat, etc.).\nLes analyses tf-idf (term frequency-inverse document frequency) ou les\nconstructions d‚Äôindices de similarit√© cosinus reposent sur ce type d‚Äôapproche.\n\n\n\nFr√©quence d‚Äôun mot\nAvant de s‚Äôadonner √† une analyse syst√©matique du champ lexical de chaque\nauteur, on se focaliser dans un premier temps sur un unique mot, le mot fear.\n\n\n Note\nL‚Äôexercice ci-dessous pr√©sente une repr√©sentation graphique nomm√©e\nwaffle chart. Il s‚Äôagit d‚Äôune approche pr√©f√©rable aux\ncamemberts qui sont des graphiques manipulables car l‚Äôoeil humain se laisse\nfacilement berner par cette repr√©sentation graphique qui ne respecte pas\nles proportions.\n\n\n\n\n Exercice 2 : Fr√©quence d'un mot\n\nCompter le nombre de phrases, pour chaque auteur, o√π appara√Æt le mot fear.\nUtiliser pywaffle pour obtenir les graphiques ci-dessous qui r√©sument\nde mani√®re synth√©tique le nombre d‚Äôoccurrences du mot ‚Äúfear‚Äù par auteur.\nRefaire l‚Äôanalyse avec le mot ‚Äúhorror‚Äù.\n\n\n\nA l‚Äôissue de la question 1, vous devriez obtenir le tableau\nde fr√©quence suivant :\n\n\n\n\n\n\n\n\n\nText\nID\nwordtoplot\n\n\nAuthor\n\n\n\n\n\n\n\nEAP\nThis process, however, afforded me no means of...\n2630511008096741351519322166071718908441148621...\n70\n\n\nHPL\nIt never once occurred to me that the fumbling...\n1756912958197641888620836080752790708121117330...\n160\n\n\nMWS\nHow lovely is spring As we looked from Windsor...\n2776322965009121673712799131170076400683052582...\n211\n\n\n\n\n\n\n\nCeci permet d‚Äôobtenir le waffle chart suivant :\n\n\n\n\n\nFigure¬†2: R√©partition du terme fear dans le corpus de nos trois auteurs\n\n\n\n\nOn remarque ainsi de mani√®re tr√®s intuitive\nle d√©s√©quilibre de notre jeu de donn√©es\nlorsqu‚Äôon se focalise sur le terme ‚Äúpeur‚Äù\no√π Mary Shelley repr√©sente pr√®s de 50%\ndes observations.\nSi on reproduit cette analyse avec le terme ‚Äúhorror‚Äù, on peut\nen conclure que la peur est plus √©voqu√©e par Mary Shelley\n(sentiment assez naturel face √† la cr√©ature du docteur Frankenstein) alors\nque Lovecraft n‚Äôa pas vol√© sa r√©putation d‚Äô√©crivain de l‚Äôhorreur !\n\n\n\n\n\n\n\n\n\n\n\nPremier wordcloud\nPour aller plus loin dans l‚Äôanalyse du champ lexical de chaque auteur,\non peut repr√©senter un wordcloud qui permet d‚Äôafficher chaque mot avec une\ntaille proportionnelle au nombre d‚Äôoccurrence de celui-ci.\n\n\n Exercice 3 : Wordcloud\n\nEn utilisant la fonction wordCloud, faire trois nuages de mot pour repr√©senter les mots les plus utilis√©s par chaque auteur.\nCalculer les 25 mots plus communs pour chaque auteur et repr√©senter les trois histogrammes des d√©comptes.\n\n\n\nLe wordcloud pour nos diff√©rents auteurs est le suivant :\n\n\n\n\n\n\n\n\n\nEnfin, si on fait un histogramme des fr√©quences,\ncela donnera :\n\n\n\n\n\n\n\n\n\nOn voit ici que ce sont des mots communs, comme ‚Äúthe‚Äù, ‚Äúof‚Äù, etc. sont tr√®s\npr√©sents. Mais ils sont peu porteurs d‚Äôinformation, on peut donc les √©liminer\navant de faire une analyse syntaxique pouss√©e.\nCeci est une d√©monstration par l‚Äôexemple qu‚Äôil vaut mieux nettoyer le texte avant de\nl‚Äôanalyser (sauf si on est int√©ress√©\npar la loi de Zipf, cf.¬†exercice suivant).\n\n\nApart√©: la loi de Zipf\n\n\n La loi de Zipf\nDans son sens strict, la loi de Zipf pr√©voit que\ndans un texte donn√©, la fr√©quence d‚Äôoccurrence \\(f(n_i)\\) d‚Äôun mot est\nli√©e √† son rang \\(n_i\\) dans l‚Äôordre des fr√©quences par une loi de la forme\n\\(f(n_i) = c/n_i\\) o√π \\(c\\) est une constante. Zipf, dans les ann√©es 1930, se basait sur l‚Äôoeuvre\nde Joyce, Ulysse pour cette affirmation.\nPlus g√©n√©ralement, on peut d√©river la loi de Zipf d‚Äôune distribution exponentielle des fr√©quences : \\(f(n_i) = cn_{i}^{-k}\\). Cela permet d‚Äôutiliser la famille des mod√®les lin√©aires g√©n√©ralis√©s, notamment les r√©gressions poissonniennes, pour mesurer les param√®tres de la loi. Les mod√®les lin√©aire traditionnels en log souffrent en effet, dans ce contexte, de biais (la loi de Zipf est un cas particulier d‚Äôun mod√®le gravitaire, o√π appliquer des OLS est une mauvaise id√©e, cf.¬†Galiana et al. (2020) pour les limites).\n\n\nUn mod√®le exponentiel peut se repr√©senter par un mod√®le de Poisson ou, si\nles donn√©es sont tr√®s dispers√©es, par un mod√®le binomial n√©gatif. Pour\nplus d‚Äôinformations, consulter l‚Äôannexe de Galiana et al. (2020).\nLa technique √©conom√©trique associ√©e pour l‚Äôestimation est\nles mod√®les lin√©aires g√©n√©ralis√©s (GLM) qu‚Äôon peut\nutiliser en Python via le\npackage statsmodels2:\n\\[\n\\mathbb{E}\\bigg( f(n_i)|n_i \\bigg) = \\exp(\\beta_0 + \\beta_1 \\log(n_i))\n\\]\nPrenons les r√©sultats de l‚Äôexercice pr√©c√©dent et enrichissons les du rang et de la fr√©quence d‚Äôoccurrence d‚Äôun mot :\n\ncount_words = pd.DataFrame({'counter' : train\n    .groupby('Author')\n    .apply(lambda s: ' '.join(s['Text']).split())\n    .apply(lambda s: Counter(s))\n    .apply(lambda s: s.most_common())\n    .explode()}\n)\ncount_words[['word','count']] = pd.DataFrame(count_words['counter'].tolist(), index=count_words.index)\ncount_words = count_words.reset_index()\n\ncount_words = count_words.assign(\n    tot_mots_auteur = lambda x: (x.groupby(\"Author\")['count'].transform('sum')),\n    freq = lambda x: x['count'] /  x['tot_mots_auteur'],\n    rank = lambda x: x.groupby(\"Author\")['count'].transform('rank', ascending = False)\n)\n\nCommen√ßons par repr√©senter la relation entre la fr√©quence et le rang:\nNous avons bien, graphiquement, une relation log-lin√©aire entre les deux:\n\ng.figure.get_figure()\n\n\n\n\n\n\n\n\nAvec statsmodels, v√©rifions plus formellement cette relation:\n\nimport statsmodels.api as sm\n\nexog = sm.add_constant(np.log(count_words['rank'].astype(float)))\n\nmodel = sm.GLM(count_words['freq'].astype(float), exog, family = sm.families.Poisson()).fit()\n\n# Afficher les r√©sultats du mod√®le\nprint(model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                   freq   No. Observations:                69301\nModel:                            GLM   Df Residuals:                    69299\nModel Family:                 Poisson   Df Model:                            1\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -23.011\nDate:                Tue, 21 Nov 2023   Deviance:                     0.065676\nTime:                        15:27:18   Pearson chi2:                   0.0656\nNo. Iterations:                     5   Pseudo R-squ. (CS):          0.0002431\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.4388      1.089     -2.239      0.025      -4.574      -0.303\nrank          -0.9831      0.189     -5.196      0.000      -1.354      -0.612\n==============================================================================\n\n\nLe coefficient de la r√©gression est presque 1 ce qui sugg√®re bien une relation\nquasiment log-lin√©aire entre le rang et la fr√©quence d‚Äôoccurrence d‚Äôun mot.\nDit autrement, le mot le plus utilis√© l‚Äôest deux fois plus que le deuxi√®me\nmois le plus fr√©quent qui l‚Äôest trois plus que le troisi√®me, etc."
  },
  {
    "objectID": "content/NLP/02_exoclean.html#nettoyage-dun-texte",
    "href": "content/NLP/02_exoclean.html#nettoyage-dun-texte",
    "title": "Nettoyer un texte: des exercices pour d√©couvrir l‚Äôapproche bag-of-words",
    "section": "Nettoyage d‚Äôun texte",
    "text": "Nettoyage d‚Äôun texte\nLes premi√®res √©tapes dans le nettoyage d‚Äôun texte, qu‚Äôon a\nd√©velopp√© au cours du chapitre pr√©c√©dent, sont :\n\nsuppression de la ponctuation\nsuppression des stopwords\n\nCela passe par la tokenisation d‚Äôun texte, c‚Äôest-√†-dire la d√©composition\nde celui-ci en unit√©s lexicales (les tokens).\nCes unit√©s lexicales peuvent √™tre de diff√©rentes natures,\nselon l‚Äôanalyse que l‚Äôon d√©sire mener.\nIci, on va d√©finir les tokens comme √©tant les mots utilis√©s.\nPlut√¥t que de faire soi-m√™me ce travail de nettoyage,\navec des fonctions mal optimis√©es,\non peut utiliser la librairie nltk comme d√©taill√© pr√©c√©demment.\n\n\n Exercice 4 : Nettoyage du texte\nRepartir de train, notre jeu de donn√©es d‚Äôentra√Ænement. Pour rappel, train a la structure suivante :\n\nTokeniser chaque phrase avec nltk.\nRetirer les stopwords avec nltk.\n\n\n\nPour rappel, au d√©but de l‚Äôexercice, le DataFrame pr√©sente l‚Äôaspect suivant :\n\n\n\n\n\n\n\n\n\nText\nAuthor\nID\nwordtoplot\n\n\nId\n\n\n\n\n\n\n\n\nid26305\nThis process, however, afforded me no means of...\nEAP\n26305\n0\n\n\nid17569\nIt never once occurred to me that the fumbling...\nHPL\n17569\n0\n\n\n\n\n\n\n\nApr√®s tokenisation, il devrait avoir cet aspect :\n\n\nID     Author\n00001  MWS       [Idris, was, well, content, with, this, resolv...\n00002  HPL       [I, was, faint, even, fainter, than, the, hate...\ndtype: object\n\n\nApr√®s le retrait des stopwords, cela donnera :\n\n\n Hint\nLa m√©thode apply est tr√®s pratique ici car nous avons une phrase par ligne. Plut√¥t que de faire un DataFrame par auteur, ce qui n‚Äôest pas une approche tr√®s flexible, on peut directement appliquer la tokenisation\nsur notre DataFrame gr√¢ce √† apply, sans le diviser.\n\n\nCe petit nettoyage permet d‚Äôarriver √† un texte plus int√©ressant en termes d‚Äôanalyse lexicale. Par exemple, si on reproduit l‚Äôanalyse pr√©c√©dente‚Ä¶ :\n\n\n\n\n\n\n\n\n\nPour aller plus loin dans l‚Äôharmonisation d‚Äôun texte, il est possible de\nmettre en place les classes d‚Äô√©quivalence d√©velopp√©es dans la\npartie pr√©c√©dente afin de remplacer diff√©rentes variations d‚Äôun m√™me\nmot par une forme canonique :\n\nla racinisation (stemming) assez fruste mais rapide, notamment\nen pr√©sence de fautes d‚Äôorthographe. Dans ce cas, chevaux peut devenir chev\nmais √™tre ainsi confondu avec chevet ou cheveux.\nCette m√©thode est g√©n√©ralement plus simple √† mettre en oeuvre, quoique\nplus fruste.\nla lemmatisation qui requiert la connaissance des statuts\ngrammaticaux (exemple : chevaux devient cheval).\nElle est mise en oeuvre, comme toujours avec nltk, √† travers un\nmod√®le. En l‚Äôoccurrence, un WordNetLemmatizer (WordNet est une base\nlexicographique ouverte). Par exemple, les mots ‚Äúwomen‚Äù, ‚Äúdaughters‚Äù\net ‚Äúleaves‚Äù seront ainsi lemmatis√©s de la mani√®re suivante :\n\n\nfrom nltk.stem import WordNetLemmatizer\nlemm = WordNetLemmatizer()\n\nfor word in [\"women\",\"daughters\", \"leaves\"]:\n    print(\"The lemmatized form of %s is: {}\".format(lemm.lemmatize(word)) % word)\n\nThe lemmatized form of women is: woman\nThe lemmatized form of daughters is: daughter\nThe lemmatized form of leaves is: leaf\n\n\n\n\n Note\nPour disposer du corpus n√©cessaire √† la lemmatisation, il faut, la premi√®re fois,\nt√©l√©charger celui-ci gr√¢ce aux commandes suivantes :\nimport nltk\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\n\nOn va se restreindre au corpus d‚ÄôEdgar Allan Poe et repartir de la base de donn√©es\nbrute:\n\neap_clean = train[train[\"Author\"] == \"EAP\"]\neap_clean = ' '.join(eap_clean['Text'])\n#Tokenisation na√Øve sur les espaces entre les mots =&gt; on obtient une liste de mots\n#tokens = eap_clean.split()\nword_list = nltk.word_tokenize(eap_clean)\n\n\n\n Exercice 5 : Lemmatisation avec nltk\nUtiliser un WordNetLemmatizer et observer le r√©sultat.\nOptionnel: Effectuer la m√™me t√¢che avec spaCy\n\n\nLe WordNetLemmatizer donnera le r√©sultat suivant :"
  },
  {
    "objectID": "content/NLP/02_exoclean.html#tf-idf-calcul-de-fr√©quence",
    "href": "content/NLP/02_exoclean.html#tf-idf-calcul-de-fr√©quence",
    "title": "Nettoyer un texte: des exercices pour d√©couvrir l‚Äôapproche bag-of-words",
    "section": "TF-IDF: calcul de fr√©quence",
    "text": "TF-IDF: calcul de fr√©quence\nLe calcul tf-idf (term frequency‚Äìinverse document frequency)\npermet de calculer un score de proximit√© entre un terme de recherche et un\ndocument (c‚Äôest ce que font les moteurs de recherche).\n\nLa partie tf calcule une fonction croissante de la fr√©quence du terme de recherche dans le document √† l‚Äô√©tude ;\nLa partie idf calcule une fonction inversement proportionnelle √† la fr√©quence du terme dans l‚Äôensemble des documents (ou corpus).\n\nLe score total, obtenu en multipliant les deux composantes,\npermet ainsi de donner un score d‚Äôautant plus √©lev√© que le terme est surr√©pr√©sent√© dans un document\n(par rapport √† l‚Äôensemble des documents).\nIl existe plusieurs fonctions, qui p√©nalisent plus ou moins les documents longs,\nou qui sont plus ou moins smooth.\n\n\n Exercice 6 : TF-IDF: calcul de fr√©quence\n\nUtiliser le vectoriseur TF-IdF de scikit-learn pour transformer notre corpus en une matrice document x terms. Au passage, utiliser l‚Äôoption stop_words pour ne pas provoquer une inflation de la taille de la matrice. Nommer le mod√®le tfidf et le jeu entra√Æn√© tfs.\nApr√®s avoir construit la matrice de documents x terms avec le code suivant, rechercher les lignes o√π les termes ayant la structure abandon sont non-nuls.\nTrouver les 50 extraits o√π le score TF-IDF est le plus √©lev√© et l‚Äôauteur associ√©. Vous devriez obtenir le classement suivant :\n\n\n\n\nfeature_names = tfidf.get_feature_names_out()\ncorpus_index = [n for n in list(tfidf.vocabulary_.keys())]\nimport pandas as pd\ndf = pd.DataFrame(tfs.todense(), columns=feature_names)\n\ndf.head()\n\n\n\n\n\n\n\n\naaem\nab\naback\nabaft\nabandon\nabandoned\nabandoning\nabandonment\nabaout\nabased\n...\nzodiacal\nzoilus\nzokkar\nzone\nzones\nzopyrus\nzorry\nzubmizzion\nzuro\n√°¬º\n\n\n\n\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n0.0\n0.0\n0.0\n0.0\n0.0\n0.253506\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows √ó 24937 columns\n\n\n\nLes lignes o√π les termes de abandon sont non nuls\nsont les suivantes :\n\n\nIndex([    4,   116,   215,   571,   839,  1042,  1052,  1069,  2247,  2317,\n        2505,  3023,  3058,  3245,  3380,  3764,  3886,  4425,  5289,  5576,\n        5694,  6812,  7500,  9013,  9021,  9077,  9560, 11229, 11395, 11451,\n       11588, 11827, 11989, 11998, 12122, 12158, 12189, 13666, 15259, 16516,\n       16524, 16759, 17547, 18019, 18072, 18126, 18204, 18251],\n      dtype='int64')\n\n\n\n\n\n\n\n\n\naaem\nab\naback\nabaft\nabandon\nabandoned\nabandoning\nabandonment\nabaout\nabased\n...\nzodiacal\nzoilus\nzokkar\nzone\nzones\nzopyrus\nzorry\nzubmizzion\nzuro\n√°¬º\n\n\n\n\n4\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.253506\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n116\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.339101\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n215\n0.0\n0.0\n0.0\n0.0\n0.235817\n0.000000\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n571\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.143788\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n839\n0.0\n0.0\n0.0\n0.0\n0.285886\n0.000000\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows √ó 24937 columns\n\n\n\n\n\nAuthor\nMWS    22\nHPL    15\nEAP    13\nName: Text, dtype: int64\n\n\nLes 10 scores les plus √©lev√©s sont les suivants :\n\nprint(train.iloc[list_fear[:9]]['Text'].values)\n\n['We could not fear we did not.' '\"And now I do not fear death.'\n 'Be of heart and fear nothing.' 'I smiled, for what had I to fear?'\n 'Indeed I had no fear on her account.'\n 'I have not the slightest fear for the result.'\n 'At length, in an abrupt manner she asked, \"Where is he?\" \"O, fear not,\" she continued, \"fear not that I should entertain hope Yet tell me, have you found him?'\n '\"I fear you are right there,\" said the Prefect.'\n 'I went down to open it with a light heart, for what had I now to fear?']\n\n\nOn remarque que les scores les plus √©l√©v√©s sont soient des extraits courts o√π le mot apparait une seule fois, soit des extraits plus longs o√π le mot fear appara√Æt plusieurs fois.\n\n\n Note\nLa matrice document x terms est un exemple typique de matrice sparse puisque, dans des corpus volumineux, une grande diversit√© de vocabulaire peut √™tre trouv√©e."
  },
  {
    "objectID": "content/NLP/02_exoclean.html#approche-contextuelle-les-n-gramms",
    "href": "content/NLP/02_exoclean.html#approche-contextuelle-les-n-gramms",
    "title": "Nettoyer un texte: des exercices pour d√©couvrir l‚Äôapproche bag-of-words",
    "section": "Approche contextuelle: les n-gramms",
    "text": "Approche contextuelle: les n-gramms\nPour √™tre en mesure de mener cette analyse, il est n√©cessaire de t√©l√©charger un corpus suppl√©mentaire :\n\nimport nltk\nnltk.download('genesis')\nnltk.corpus.genesis.words('english-web.txt')\n\n[nltk_data] Downloading package genesis to /github/home/nltk_data...\n[nltk_data]   Package genesis is already up-to-date!\n\n\n['In', 'the', 'beginning', 'God', 'created', 'the', ...]\n\n\nIl s‚Äôagit maintenant de raffiner l‚Äôanalyse.\nOn s‚Äôint√©resse non seulement aux mots et √† leur fr√©quence, mais aussi aux mots qui suivent. Cette approche est essentielle pour d√©sambiguiser les homonymes. Elle permet aussi d‚Äôaffiner les mod√®les ‚Äúbag-of-words‚Äù. Le calcul de n-grams (bigrams pour les co-occurences de mots deux-√†-deux, tri-grams pour les co-occurences trois-√†-trois, etc.) constitue la m√©thode la plus simple pour tenir compte du contexte.\nnltk offre des methodes pour tenir compte du contexte : pour ce faire, nous calculons les n-grams, c‚Äôest-√†-dire l‚Äôensemble des co-occurrences successives de mots n-√†-n.¬†En g√©n√©ral, on se contente de bi-grams, au mieux de tri-grams :\n\nles mod√®les de classification, analyse du sentiment, comparaison de documents, etc. qui comparent des n-grams avec n trop grands sont rapidement confront√©s au probl√®me de donn√©es sparse, cela r√©duit la capacit√© pr√©dictive des mod√®les ;\nles performances d√©croissent tr√®s rapidement en fonction de n, et les co√ªts de stockage des donn√©es augmentent rapidement (environ n fois plus √©lev√© que la base de donn√©es initiale).\n\nOn va, rapidement, regarder dans quel contexte appara√Æt le mot fear dans\nl‚Äôoeuvre d‚ÄôEdgar Allan Poe (EAP). Pour cela, on transforme d‚Äôabord\nle corpus EAP en tokens `nltk :\n\neap_clean = train[train[\"Author\"] == \"EAP\"]\neap_clean = ' '.join(eap_clean['Text'])\ntokens = eap_clean.split()\nprint(tokens[:10])\ntext = nltk.Text(tokens)\nprint(text)\n\n['This', 'process,', 'however,', 'afforded', 'me', 'no', 'means', 'of', 'ascertaining', 'the']\n&lt;Text: This process, however, afforded me no means of...&gt;\n\n\nVous aurez besoin des fonctions BigramCollocationFinder.from_words et BigramAssocMeasures.likelihood_ratio :\n\nfrom nltk.collocations import BigramCollocationFinder\nfrom nltk.metrics import BigramAssocMeasures\n\n\n\n Exercice 7  : n-grams et contexte du mot fear\n\nUtiliser la m√©thode concordance pour afficher le contexte dans lequel appara√Æt le terme fear.\nS√©lectionner et afficher les meilleures collocation, par exemple selon le crit√®re du ratio de vraisemblance.\n\nLorsque deux mots sont fortement associ√©s, cela est parfois d√ª au fait qu‚Äôils apparaissent rarement. Il est donc parfois n√©cessaire d‚Äôappliquer des filtres, par exemple ignorer les bigrammes qui apparaissent moins de 5 fois dans le corpus.\n\nRefaire la question pr√©c√©dente en utilisant toujours un mod√®le BigramCollocationFinder suivi de la m√©thode apply_freq_filter pour ne conserver que les bigrammes pr√©sents au moins 5 fois. Puis, au lieu d‚Äôutiliser la m√©thode de maximum de vraisemblance, testez la m√©thode nltk.collocations.BigramAssocMeasures().jaccard.\nNe s‚Äôint√©resser qu‚Äôaux collocations qui concernent le mot fear\n\n\n\nAvec la m√©thode concordance (question 1),\nla liste devrait ressembler √† celle-ci:\n\n\nExemples d'occurences du terme 'fear' :\nDisplaying 13 of 13 matches:\nd quick unequal spoken apparently in fear as well as in anger. What he said wa\nhutters were close fastened, through fear of robbers, and so I knew that he co\nto details. I even went so far as to fear that, as I occasioned much trouble, \nyears of age, was heard to express a fear \"that she should never see Marie aga\nich must be entirely remodelled, for fear of serious accident I mean the steel\n my arm, and I attended her home. 'I fear that I shall never see Marie again.'\nclusion here is absurd. \"I very much fear it is so,\" replied Monsieur Maillard\nbt of ultimately seeing the Pole. \"I fear you are right there,\" said the Prefe\ner occurred before.' Indeed I had no fear on her account. For a moment there w\nerhaps so,\" said I; \"but, Legrand, I fear you are no artist. It is my firm int\n raps with a hammer. Be of heart and fear nothing. My daughter, Mademoiselle M\ne splendor. I have not the slightest fear for the result. The face was so far \narriers of iron that hemmed me in. I fear you have mesmerized\" adding immediat\n\n\n\n\nM√™me si on peut facilement voir le mot avant et apr√®s, cette liste est assez difficile √† interpr√©ter car elle recoupe beaucoup d‚Äôinformations.\nLa collocation consiste √† trouver les bi-grammes qui\napparaissent le plus fr√©quemment ensemble. Parmi toutes les paires de deux mots observ√©es,\nil s‚Äôagit de s√©lectionner, √† partir d‚Äôun mod√®le statistique, les ‚Äúmeilleures‚Äù.\nOn obtient donc avec cette m√©thode (question 2):\nSi on mod√©lise les meilleures collocations:\nCette liste a un peu plus de sens,\non a des noms de personnages, de lieux mais aussi des termes fr√©quemment employ√©s ensemble\n(Chess Player par exemple).\nEn ce qui concerne les collocations du mot fear:\nSi on m√®ne la m√™me analyse pour le terme love, on remarque que de mani√®re logique, on retrouve bien des sujets g√©n√©ralement accol√©s au verbe :\n\ncollocations_word(\"love\")\n\n[('love', 'me'), ('love', 'he'), ('will', 'love'), ('I', 'love'), ('love', ','), ('you', 'love'), ('the', 'love')]"
  },
  {
    "objectID": "content/NLP/02_exoclean.html#footnotes",
    "href": "content/NLP/02_exoclean.html#footnotes",
    "title": "Nettoyer un texte: des exercices pour d√©couvrir l‚Äôapproche bag-of-words",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nL‚Äôapproche bag of words est d√©j√†, si on la pousse √† ses limites, tr√®s int√©ressante. Elle peut notamment\nfaciliter la mise en coh√©rence de diff√©rents corpus\npar la m√©thode des appariements flous\n(cf.¬†Galiana and Castillo (2022).\nLe chapitre sur ElasticSearch pr√©sent dans cette partie du cours pr√©sente quelques\n√©l√©ments de ce travail sur les donn√©es de l‚ÄôOpenFoodFacts‚Ü©Ô∏é\nLa litt√©rature sur les mod√®les gravitaires, pr√©sent√©e dans Galiana et al. (2020),\ndonne quelques arguments pour privil√©gier les mod√®les GLM √† des mod√®les log-lin√©aires\nestim√©s par moindres carr√©s ordinaires.‚Ü©Ô∏é"
  },
  {
    "objectID": "content/NLP/index.html",
    "href": "content/NLP/index.html",
    "title": "Partie 4 : Natural Language Processing (NLP)",
    "section": "",
    "text": "Les parties pr√©c√©dentes √©taient consacr√©es √† l‚Äôacquisition de comp√©tences\ntransversales pour la valorisation des donn√©es. De mani√®re naturelle,\nnous nous sommes jusqu‚Äô√† pr√©sent plut√¥t consacr√©s\n√† la valorisation de donn√©es structur√©es, d‚Äôune\ndimension certes modeste mais qui ouvraient d√©j√† √©norm√©ment de\nprobl√©matiques √† creuser. Cette partie propose maintenant de se\nconsacrer √† un sujet dont il n‚Äôest pas √©vident a priori que\nles ordinateurs s‚Äôemparent: le langage humain et sa richesse.\nEn effet, si la linguistique propose certes de repr√©senter\nde mani√®re conceptuelle le langage sous une forme de donn√©es, comment\ndes ordinateurs, qui au fond ne connaissent que le 0 et le 1, peuvent-ils\ns‚Äôapproprier cet objet √©minemment complexe qu‚Äôest le langage et qu‚Äôun\nhumain met lui-m√™me des ann√©es √† comprendre et s‚Äôapproprier ?1\nLe traitement du langage naturel - traduction fran√ßaise du concept de\nnatural language processing (NLP) - est l‚Äôensemble des techniques\npermettant aux ordinateurs de comprendre, analyser synth√©tiser et\ng√©n√©rer le langage humain2.\nIl s‚Äôagit d‚Äôun champ disciplinaire √† l‚Äôintersection de la statistique\net de la linguistique qui connait depuis quelques ann√©es un engouement\nimportant que ce soit d‚Äôun point de vue acad√©mique ou op√©rationnel.\nCertaines des applications de ces techniques sont devenues incontournables\ndans nos t√¢ches quotidiennes, notamment les moteurs de recherche, la traduction\nautomatique et plus r√©cemment les chatbots.\nCette partie du cours est consacr√©e √† l‚Äôanalyse des donn√©es textuelles avec\ndes exemples de üìñ pour s‚Äôamuser. Elle est une introduction progressive\n√† ce sujet en se concentrant sur des concepts de base, n√©cessaires √†\nla compr√©hension ult√©rieure de principes plus avanc√©s et de techniques\nsophistiqu√©es3."
  },
  {
    "objectID": "content/NLP/index.html#r√©sum√©-de-la-partie",
    "href": "content/NLP/index.html#r√©sum√©-de-la-partie",
    "title": "Partie 4 : Natural Language Processing (NLP)",
    "section": "R√©sum√© de la partie",
    "text": "R√©sum√© de la partie\nPython est un excellent outil pour l‚Äôanalyse de donn√©es textuelles.\nLes m√©thodes de base ou les librairies sp√©cialis√©es\ncomme NLTK et SpaCy permettent d‚Äôeffectuer ces t√¢ches de mani√®re\ntr√®s efficace. Les ressources en ligne sur le sujet sont tr√®s\nnombreuses. Python est bien mieux outill√© que R pour l‚Äôanalyse de\ndonn√©es textuelles.\nDans un premier temps, cette partie propose\nde revenir sur la mani√®re de structurer et nettoyer un corpus\ntextuel au travers de l‚Äôapproche bag of words (sac de mots).\nElle vise √† montrer comment transformer un corpus en outil propre √† une\nanalyse statistique :\n\nElle propose d‚Äôabord une introduction aux enjeux du nettoyage des donn√©es\ntextuelles √† travers l‚Äôanalyse du Comte de Monte Cristo d‚ÄôAlexandre Dumas\nici qui permet de synth√©tiser rapidement l‚Äôinformation disponible\ndans un large volume de donn√©es (√† l‚Äôimage de la ?@fig-wordcloud-dumas)\nElle propose ensuite une s√©rie d‚Äôexercices sur le nettoyage de textes √† partir des\noeuvres d‚ÄôEdgar Allan Poe, Mary Shelley et H.P. Lovecraft visant √† distinguer la\nsp√©cificit√© du vocabulaire employ√© par chaque auteurs (par exemple ?@fig-waffle-fear). Ces exercices sont\ndisponibles dans le deuxi√®me chapitre de la partie.\n\nEnsuite, nous proposerons d‚Äôexplorer une approche alternative, prenant en compte\nle contexte d‚Äôapparition d‚Äôun mot. L‚Äôintroduction √† la\nLatent Dirichlet Allocation (LDA) sera l‚Äôoccasion de pr√©senter la mod√©lisation\nde documents sous la forme de topics.\nEnfin, nous introduirons aux enjeux de la transformation de champs textuels\nsous forme de vecteurs num√©riques. Pour cela, nous pr√©senterons le principe\nde Word2Vec qui permet ainsi, par exemple,\nmalgr√© une distance syntaxique importante,\nde dire que s√©mantiquement Homme et Femme sont proches.\nCe chapitre est une passerelle vers le concept d‚Äôembedding, v√©ritable\nr√©volution r√©cente du NLP, et qui permet de rapprocher des corpus\nnon seulement sur leur proximit√© syntaxique (partagent-ils par exemple des mots\ncommuns ?) mais aussi sur leur proximit√© s√©mantique (partagent-ils un th√®me ou un sens commun ?).4\nComme l‚Äôillustre la figure suivante, emprunt√©e √† Sebastian Raschka, les concepts que nous allons aborder\ndans cette partie sont indispensables pour √™tre en mesure\nd‚Äôentra√Æner ou r√©utiliser ult√©rieurement un mod√®le\nsophistiqu√© d‚Äôanalyse du langage:\n\n\n\n\n\n(a) Illustration transformer architecture\n\n\nFigure¬†1: Illustration of the original transformer architecture proposed in Attention Is All You Need, 2017\n(source: Sebastien Raschka)"
  },
  {
    "objectID": "content/NLP/index.html#pour-aller-plus-loin",
    "href": "content/NLP/index.html#pour-aller-plus-loin",
    "title": "Partie 4 : Natural Language Processing (NLP)",
    "section": "Pour aller plus loin",
    "text": "Pour aller plus loin\nLa recherche dans le domaine du NLP est tr√®s active. Il est donc recommand√©\nde faire preuve de curiosit√© pour en apprendre plus car une ressource\nunique ne compilera pas l‚Äôensemble des connaissances, a fortiori dans\nun champ de recherche aussi dynamique que le NLP.\nPour approfondir les comp√©tences √©voqu√©es dans ce cours, je recommande vivement\nce cours d‚ÄôHuggingFace.\nPour comprendre l‚Äôarchitecture interne d‚Äôun LLM,\nce post de Sebastian Raschka\nest tr√®s utile."
  },
  {
    "objectID": "content/NLP/index.html#footnotes",
    "href": "content/NLP/index.html#footnotes",
    "title": "Partie 4 : Natural Language Processing (NLP)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEn √©crivant ces lignes j‚Äôai demand√© √† ChatGPT de me faire une blague, la communication entre l‚Äôhumain et la machine est perfectible :\n\nPourquoi les programmeurs d√©testent-ils la plage ?\nParce que le sable change tout le temps de paradigme !\n\nNe l‚Äôayant pas comprise du fait de mon esprit obtus, je lui\nai demand√© de me l‚Äôexpliquer. Voici sa r√©ponse\n\nBien s√ªr, je serais ravi de l‚Äôexpliquer ! Cette blague joue sur le double sens du mot ‚Äúparadigme‚Äù et fait un jeu de mots avec les notions li√©es √† la programmation et √† la plage.\nDans le domaine de la programmation, un ‚Äúparadigme‚Äù d√©signe un mod√®le ou une m√©thode de programmation, comme la programmation orient√©e objet, la programmation fonctionnelle, etc. Les programmeurs sont souvent familiers avec diff√©rents paradigmes de programmation.\nD‚Äôautre part, en contexte de plage, ‚Äúparadigme‚Äù est utilis√© pour signifier ‚Äúmar√©e qui monte et descend‚Äù, se r√©f√©rant √† l‚Äôaction naturelle du sable et de l‚Äôeau sur la plage.\nLa blague joue sur la frustration humoristique que peuvent ressentir les programmeurs lorsqu‚Äôils doivent s‚Äôadapter √† de nouveaux paradigmes de programmation, tout comme le sable de la plage qui change constamment avec les mar√©es. En associant les deux sens du mot ‚Äúparadigme‚Äù, la blague cr√©e une situation absurde et amusante, faisant sourire ceux qui comprennent les r√©f√©rences √† la programmation et √† la plage.\n\nFinalement, j‚Äôai trouv√© son explication plus dr√¥le que sa blague.‚Ü©Ô∏é\nLe concept de natural language processing tend √† remplacer celui\nde text mining dont le p√©rim√®tre √©tait plus sp√©cifique. Le text mining\nrenvoie √† la recherche d‚Äôinformation - la fouille - dans un ensemble de\ntextes. C‚Äôest l‚Äôun des champs d‚Äôapplications\ndu natural language processing mais ce n‚Äôest pas l‚Äôunique.‚Ü©Ô∏é\nPar exemple, le concept d‚Äôembedding - transformation d‚Äôun champ\ntextuel en un vecteur num√©rique multidimensionnel - aujourd‚Äôhui central\ndans le NLP n‚Äôest √©voqu√© qu‚Äô√† quelques reprises. Avant d‚Äôen arriver\nau sujet des embeddings, il est pr√©cieux de comprendre les apports et les\nlimites de concepts comme\nle sac de mot (bag of words) ou la distance\nTF-IDF (term frequency - inverse document frequency). Dans une\noptique introductive, ce cours se focalise donc sur ces derniers pour faciliter\nl‚Äôouverture ult√©rieure de la boite de Pandore que sont les embeddings.‚Ü©Ô∏é\nUn exemple d‚Äôint√©r√™t de ce type d‚Äôapproche est la ?@fig-relevanc-table-embedding.‚Ü©Ô∏é"
  },
  {
    "objectID": "content/modelisation/5_clustering.html",
    "href": "content/modelisation/5_clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Nous allons continuer avec le m√™me jeu de donn√©es que pr√©c√©demment,\nc‚Äôest-√†-dire les r√©sultats des √©lections US 2020 pr√©sent√©s dans l‚Äôintroduction\nde cette partie: les donn√©es de vote aux √©lections pr√©sidentielles am√©ricaines\ncrois√©es √† des variables sociod√©mographiques.\nLe code\nest disponible sur Github.\n#!pip install --upgrade xlrd #colab bug verson xlrd\n#!pip install geopandas\n\nimport requests\n\nurl = 'https://raw.githubusercontent.com/linogaliana/python-datascientist/master/content/modelisation/get_data.py'\nr = requests.get(url, allow_redirects=True)\nopen('getdata.py', 'wb').write(r.content)\n\nimport getdata\nvotes = getdata.create_votes_dataframes()"
  },
  {
    "objectID": "content/modelisation/5_clustering.html#introduction-sur-le-clustering",
    "href": "content/modelisation/5_clustering.html#introduction-sur-le-clustering",
    "title": "Clustering",
    "section": "Introduction sur le clustering",
    "text": "Introduction sur le clustering\nJusqu‚Äô√† pr√©sent, nous avons fait de l‚Äôapprentissage supervis√© puisque nous\nconnaissions la vraie valeur de la variable √† expliquer/pr√©dire (y). Ce n‚Äôest plus le cas avec\nl‚Äôapprentissage non supervis√©.\nLe clustering est un champ d‚Äôapplication de l‚Äôapprentissage non-supervis√©.\nIl s‚Äôagit d‚Äôexploiter l‚Äôinformation disponible pour regrouper des observations\nqui se ressemblent.\nL‚Äôobjectif est de cr√©er des clusters d‚Äôobservations pour lesquels :\n\nau sein de chaque cluster, les observations sont homog√®nes (variance intra-cluster minimale)\nles clusters ont des profils h√©t√©rog√®nes, c‚Äôest-√†-dire qu‚Äôils se distinguent les uns des autres (variance inter-cluster maximale)\n\nEn Machine Learning, les m√©thodes de clustering sont tr√®s utilis√©es pour\nfaire de la recommandation. En faisant, par exemple, des classes homog√®nes de\nconsommateurs, il est plus facile d‚Äôidentifier et cibler des comportements\npropres √† chaque classe de consommateurs.\nCes m√©thodes ont √©galement un int√©r√™t en √©conomie et sciences sociales parce qu‚Äôelles permettent\nde regrouper des observations sans a priori et ainsi interpr√©ter une variable\nd‚Äôint√©r√™t √† l‚Äôaune de ces r√©sultats. Cette publication sur la s√©gr√©gation spatiale utilisant des donn√©es de t√©l√©phonie mobile\nutilise par exemple cette approche.\nLes m√©thodes de clustering peuvent aussi intervenir en amont d‚Äôun probl√®me de classification (dans des\nprobl√®mes d‚Äôapprentissage semi-supervis√©).\nLe manuel Hands-on machine learning with scikit-learn, Keras et TensorFlow pr√©sente dans le\nchapitre d√©di√© √† l‚Äôapprentissage non supervis√© quelques exemples.\nDans certaines bases de donn√©es, on peut se retrouver avec quelques exemples labellis√©s mais la plupart sont\nnon labellis√©s. Les labels ont par exemple √©t√© faits manuellement par des experts.\nPar exemple, supposons que dans la base MNIST des chiffres manuscrits, les chiffres ne soient pas labellis√©s\net que l‚Äôon se demande quelle est la meilleure strat√©gie pour labelliser cette base.\nOn pourrait regarder des images de chiffres manuscrits au hasard de la base et les labelliser.\nLes auteurs du livre montrent qu‚Äôil existe toutefois une meilleure strat√©gie.\nIl vaut mieux appliquer un algorithme de clustering en amont pour regrouper les images ensemble et avoir une\nimage repr√©sentative par groupe, et labelliser ces images repr√©sentatives au lieu de labelliser au hasard.\nLes m√©thodes de clustering sont nombreuses.\nNous allons nous pencher sur la plus intuitive : les k-means."
  },
  {
    "objectID": "content/modelisation/5_clustering.html#les-k-means",
    "href": "content/modelisation/5_clustering.html#les-k-means",
    "title": "Clustering",
    "section": "Les k-means",
    "text": "Les k-means\n\nPrincipe\nL‚Äôobjectif des k-means est de partitionner l‚Äôespace des observations en trouvant des points (centroids) jouant le r√¥le de centres de gravit√© pour lesquels les observations proches peuvent √™tre regroup√©es dans une classe homog√®ne.\nL‚Äôalgorithme k-means fonctionne par it√©ration, en initialisant les centro√Ødes puis en les mettant √† jour √† chaque\nit√©ration, jusqu‚Äô√† ce que les centro√Ødes se stabilisent. Quelques exemples de clusters issus de la m√©thode k-means :\n\n\n\n\n\n\n\n Hint\nL‚Äôobjectif des k-means est de trouver une partition des donn√©es \\(S=\\{S_1,...,S_K\\}\\) telle que\n\\[\n\\arg\\min_{S} \\sum_{i=1}^K \\sum_{x \\in S_i} ||x - \\mu_i||^2\n\\]\navec \\(\\mu_i\\) la moyenne des \\(x_i\\) dans l‚Äôensemble de points \\(S_i\\)\n\n\n\n# packages utiles\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nfrom sklearn.cluster import KMeans #pour kmeans\nimport seaborn as sns #pour scatterplots\n\n\n\n Exercice 1 : Principe des k-means\n\nImporter les donn√©es et se restreindre aux variables 'Unemployment_rate_2019', 'Median_Household_Income_2019', 'Percent of adults with less than a high school diploma, 2015-19', \"Percent of adults with a bachelor's degree or higher, 2015-19\" et bien s√ªr 'per_gop'. Appelez cette base restreinte df2 et enlevez les valeurs manquantes.\nFaire un k-means avec \\(k=4\\).\nCr√©er une variable label dans votes stockant le r√©sultat de la typologie\nAfficher cette typologie sur une carte.\nChoisir les variables Median_Household_Incomme_2019 et Unemployment_rate_2019 et repr√©senter le nuage de points en colorant diff√©remment\nen fonction du label obtenu.\nRepr√©senter la distribution du vote pour chaque cluster\n\n\n\nLa carte obtenue √† la question 4, qui permet de\nrepr√©senter spatialement nos groupes, est\nla suivante :\n\n\n\n\n\n\n\n\n\nLe nuage de point de la question 5, permettant de repr√©senter\nla relation entre Median_Household_Incomme_2019\net Unemployment_rate_2019, aura l‚Äôaspect suivant :\n\n\n\n\n\n\n\n\n\nEnfin, l‚Äôhistogramme des votes pour chaque cluster est :\n\n\n\n\n\n\n\n\n\n\n\n Hint\nIl faut noter plusieurs points sur l‚Äôalgorithme impl√©ment√© par d√©faut par scikit-learn, que l‚Äôon peut lire dans\nla documentation :\n- l‚Äôalgorithme impl√©ment√© par d√©faut est kmeans ++ (cf.¬†param√®tre init). Cela signifie que\nl‚Äôinitialisation des centro√Ødes est faite de mani√®re intelligente pour que les centro√Ødes initiaux soient choisis\nafin de ne pas √™tre trop proches.\n- l‚Äôalgorithme va √™tre d√©marr√© avec n_init centro√Ødes diff√©rents et le mod√®le va choisir la meilleure initialisation\nen fonction de l‚Äôinertia du mod√®le, par d√©faut √©gale √† 10.\nLe mod√®le renvoie les cluster_centers_, les labels labels_, l‚Äôinertia inertia_ et le nombre d‚Äôit√©rations\nn_iter_.\n\n\n\n\nChoisir le nombre de clusters\nLe nombre de clusters est fix√© par le mod√©lisateur.\nIl existe plusieurs fa√ßons de fixer ce nombre :\n\nconnaissance a priori du probl√®me ;\nanalyse d‚Äôune m√©trique sp√©cifique pour d√©finir le nombre de clusters √† choisir ;\netc.\n\nIl y a un arbitrage √† faire\nentre biais et variance :\nun trop grand nombre de clusters implique une variance\nintra-cluster tr√®s faible (sur-apprentissage, m√™me s‚Äôil n‚Äôest jamais possible de d√©terminer\nle vrai type d‚Äôune observation puisqu‚Äôon est en apprentissage non supervis√©).\nSans connaissance a priori du nombre de clusters, on peut recourir √† deux familles de m√©thodes :\n\nLa m√©thode du coude (elbow method): On prend le point d‚Äôinflexion de la courbe\nde performance du mod√®le. Cela repr√©sente le moment o√π ajouter un cluster\n(complexit√© croissante du mod√®le) n‚Äôapporte que des gains mod√©r√©s dans la\nmod√©lisation des donn√©es.\nLe score de silhouette : On mesure la similarit√© entre un point et les autres points\ndu cluster par rapport aux autres clusters. Plus sp√©cifiquement :\n\n\nSilhouette value is a measure of how similar an object is to its own cluster\n(cohesion) compared to other clusters (separation). The silhouette ranges\nfrom ‚àí1 to +1, where a high value indicates that the object is\nwell matched to its own cluster and poorly matched to neighboring\nclusters. If most objects have a high value, then the clustering\nconfiguration is appropriate. If many points have a low or negative\nvalue, then the clustering configuration may have too many or too few clusters\nSource: Wikipedia\n\nLe score de silhouette d‚Äôune observation est donc √©gal √†\n(m_nearest_cluster - m_intra_cluster)/max( m_nearest_cluster,m_intra_cluster)\no√π m_intra_cluster est la moyenne des distances de l‚Äôobservation aux observations du m√™me cluster\net m_nearest_cluster est la moyenne des distances de l‚Äôobservation aux observations du cluster le plus proche.\nLe package yellowbrick fournit une extension utile √† scikit pour repr√©senter\nfacilement la performance en clustering.\n\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\n\nvisualizer = KElbowVisualizer(model, k=(2,12))\nvisualizer.fit(df2[xvars])        # Fit the data to the visualizer\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\nKElbowVisualizer(ax=&lt;Axes: &gt;, estimator=KMeans(n_clusters=11), k=(2, 12))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KElbowVisualizerKElbowVisualizer(ax=&lt;Axes: &gt;, estimator=KMeans(n_clusters=11), k=(2, 12))estimator: KMeansKMeans(n_clusters=11)KMeansKMeans(n_clusters=11)\n\n\n\n\n\n\n\n\n\n\n\n&lt;Axes: title={'center': 'Distortion Score Elbow for KMeans Clustering'}, xlabel='k', ylabel='distortion score'&gt;\n\n\n&lt;Figure size 768x528 with 0 Axes&gt;\n\n\n\n\n\n\n\nPour la m√©thode du coude, la courbe\nde performance du mod√®le marque un coude l√©ger √† \\(k=4\\). Le mod√®le initial\nsemblait donc appropri√©.\nyellowbrick permet √©galement de repr√©senter des silhouettes mais\nl‚Äôinterpr√©tation est moins ais√©e et le co√ªt computationnel plus √©lev√© :\n\nfrom yellowbrick.cluster import SilhouetteVisualizer\n\nfig, ax = plt.subplots(2, 2, figsize=(15,8))\nj=0\nfor i in [3, 4, 6, 10]:\n    j += 1\n    '''\n    Create KMeans instance for different number of clusters\n    '''\n    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)\n    q, mod = divmod(j, 2)\n    '''\n    Create SilhouetteVisualizer instance with KMeans instance\n    Fit the visualizer\n    '''\n    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])\n    ax[q-1][mod].set_title(\"k = \" + str(i))\n    visualizer.fit(df2[xvars])\n\n\n\n\n\n\nLe score de silhouette offre une repr√©sentation plus riche que la courbe coud√©e.\nSur ce graphique, les barres verticales en rouge et en pointill√© repr√©sentent le score de silhouette\nglobal pour chaque k choisi. On voit par exemple que pour tous les k repr√©sent√©s ici, le\nscore de silhouette se situe entre 0.5 et 0.6 et varie peu.\nEnsuite, pour un k donn√©, on va avoir la repr√©sentation des scores de silhouette de chaque\nobservation, regroup√©es par cluster.\nPar exemple, pour k = 4, ici, on voit bien quatre couleurs diff√©rentes qui sont les 4 clusters mod√©lis√©s.\nLes ordonn√©es sont toutes les observations clusteris√©es et en abscisses on a le score de silhouette de\nchaque observation. Si au sein d‚Äôun cluster, les observations ont un score de silhouette plus faible que le\nscore de silhouette global (ligne verticale en rouge), cela signifie que les observations du clusters sont\ntrop proches des autres clusters.\nGr√¢ce √† cette repr√©sentation, on peut aussi se rendre compte de la taille relative des clusters. Par exemple,\navec k = 3, on voit qu‚Äôon a deux clusters cons√©quents et un plus ‚Äúpetit‚Äù cluster relativement aux deux autres.\nCela peut nous permettre de choisir des clusters de tailles homog√®nes ou non.\nEnfin, quand le score de silhouette est n√©gatif, cela signifie que la moyenne des distances de l‚Äôobservation\naux observations du cluster le plus proche est inf√©rieure √† la moyenne des distances de l‚Äôobservation aux\nobservations de son cluster. Cela signifie que l‚Äôobservation est mal class√©e.\n\n\nAutres m√©thodes de clustering\nIl existe de nombreuses autres m√©thodes de clustering. Parmi les plus connues, on peut citer deux exemples en particulier :\n\nDBSCAN\nles m√©langes de Gaussiennes\n\n\nDBSCAN\nL‚Äôalgorithme DBSCAN est impl√©ment√© dans sklearn.cluster.\nIl peut √™tre utilis√© pour faire de la d√©tection d‚Äôanomalies\nnotamment.\nEn effet, cette m√©thode repose sur le clustering en r√©gions o√π la densit√©\ndes observations est continue, gr√¢ce √† la notion de voisinage selon une certaine distance epsilon.\nPour chaque observation, on va regarder si dans son voisinage selon une distance epsilon, il y a des voisins. S‚Äôil y a au\nmoins min_samples voisins, alors l‚Äôobservation sera une core instance.\nLes observations qui ne sont pas des core instances et qui n‚Äôen ont pas dans leur voisinage selon une distance espilon\nvont √™tre d√©tect√©es comme des anomalies.\n\n\nLes m√©langes de gaussiennes\nEn ce qui concerne la th√©orie, voir le cours Probabilit√©s num√©riques et statistiques computationnelles, M1 Jussieu, V.Lemaire et T.Rebafka\nSe r√©f√©rer notamment aux notebooks pour l‚Äôalgorithme EM pour m√©lange gaussien.\nDans sklearn, les m√©langes gaussiens sont impl√©ment√©s dans sklearn.mixture comme GaussianMixture.\nLes param√®tres importants sont alors le nombre de gaussiennes n_components et le nombre d‚Äôinitiatisations n_init.\nIl est possible de faire de la d√©tection d‚Äôanomalie savec les m√©langes de gaussiennes.\n\n\n Pour aller plus loin\nIl existe de nombreuses autres m√©thodes de clustering :\n\nLocal outlier factor ;\nbayesian gaussian mixture models ;\ndiff√©rentes m√©thodes de clustering hi√©rarchique ;\netc."
  },
  {
    "objectID": "content/modelisation/3_regression.html",
    "href": "content/modelisation/3_regression.html",
    "title": "R√©gression : une introduction",
    "section": "",
    "text": "Nous allons partir du m√™me jeu de donn√©es que pr√©c√©demment,\nc‚Äôest-√†-dire les r√©sultats des √©lections US 2020 pr√©sent√©s dans l‚Äôintroduction\nde cette partie: les donn√©es de vote aux √©lections pr√©sidentielles am√©ricaines\ncrois√©es √† des variables sociod√©mographiques.\nLe code\nest disponible sur Github.\n#!pip install --upgrade xlrd #colab bug verson xlrd\n#!pip install geopandas\n\nimport requests\n\nurl = 'https://raw.githubusercontent.com/linogaliana/python-datascientist/master/content/modelisation/get_data.py'\nr = requests.get(url, allow_redirects=True)\nopen('getdata.py', 'wb').write(r.content)\n\nimport getdata\nvotes = getdata.create_votes_dataframes()\nLe pr√©c√©dent chapitre visait √† proposer un premier mod√®le pour comprendre\nles comt√©s o√π le parti R√©publicain l‚Äôemporte. La variable d‚Äôint√©r√™t √©tant\nbimodale (victoire ou d√©faite), on √©tait dans le cadre d‚Äôun mod√®le de\nclassification.\nMaintenant, sur les m√™mes donn√©es, on va proposer un mod√®le de r√©gression\npour expliquer le score du parti R√©publicain. La variable est donc continue.\nNous ignorerons le fait que ses bornes se trouvent entre 0 et 100 et donc\nqu‚Äôil faudrait, pour √™tre rigoureux, transformer l‚Äô√©chelle afin d‚Äôavoir\ndes donn√©es dans cet intervalle.\nCe chapitre utilise toujours le m√™me jeu de donn√©es, pr√©sent√© dans l‚Äôintroduction\nde cette partie: les donn√©es de vote aux √©lections pr√©sidentielles am√©ricaines\ncrois√©es √† des variables sociod√©mographiques.\nLe code\nest disponible sur Github."
  },
  {
    "objectID": "content/modelisation/3_regression.html#principe-g√©n√©ral",
    "href": "content/modelisation/3_regression.html#principe-g√©n√©ral",
    "title": "R√©gression : une introduction",
    "section": "Principe g√©n√©ral",
    "text": "Principe g√©n√©ral\nLe principe g√©n√©ral de la r√©gression consiste √† trouver une loi \\(h_\\theta(X)\\)\ntelle que\n\\[\nh_\\theta(X) = \\mathbb{E}_\\theta(Y|X)\n\\]\nCette formalisation est extr√™mement g√©n√©raliste et ne se restreint d‚Äôailleurs\npar √† la r√©gression lin√©aire.\nEn √©conom√©trie, la r√©gression offre une alternative aux m√©thodes de maximum\nde vraisemblance et aux m√©thodes des moments. La r√©gression est un ensemble\ntr√®s vaste de m√©thodes, selon la famille de mod√®les\n(param√©triques, non param√©triques, etc.) et la structure de mod√®les.\n\nLa r√©gression lin√©aire\nC‚Äôest la mani√®re la plus simple de repr√©senter la loi \\(h_\\theta(X)\\) comme\ncombinaison lin√©aire de variables \\(X\\) et de param√®tres \\(\\theta\\). Dans ce\ncas,\n\\[\n\\mathbb{E}_\\theta(Y|X) = X\\beta\n\\]\nCette relation est encore, sous cette formulation, th√©orique. Il convient\nde l‚Äôestimer √† partir des donn√©es observ√©es \\(y\\). La m√©thode des moindres\ncarr√©s consiste √† minimiser l‚Äôerreur quadratique entre la pr√©diction et\nles donn√©es observ√©es (ce qui explique qu‚Äôon puisse voir la r√©gression comme\nun probl√®me de Machine Learning). En toute g√©n√©ralit√©, la m√©thode des\nmoindres carr√©s consiste √† trouver l‚Äôensemble de param√®tres \\(\\theta\\)\ntel que\n\\[\n\\theta = \\arg \\min_{\\theta \\in \\Theta} \\mathbb{E}\\bigg[ \\left( y - h_\\theta(X) \\right)^2 \\bigg]\n\\]\nCe qui, dans le cadre de la r√©gression lin√©aire, s‚Äôexprime de la mani√®re suivante :\n\\[\n\\beta = \\arg\\min \\mathbb{E}\\bigg[ \\left( y - X\\beta \\right)^2 \\bigg]\n\\]\nLorsqu‚Äôon am√®ne le mod√®le th√©orique (\\(\\mathbb{E}_\\theta(Y|X) = X\\beta\\)) aux donn√©es,\non formalise le mod√®le de la mani√®re suivante :\n\\[\nY = X\\beta + \\epsilon\n\\]\nAvec une certaine distribution du bruit \\(\\epsilon\\) qui d√©pend\ndes hypoth√®ses faites. Par exemple, avec des\n\\(\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)\\) i.i.d., l‚Äôestimateur \\(\\beta\\) obtenu\nest √©quivalent √† celui du Maximum de Vraisemblance dont la th√©orie asymptotique\nnous assure l‚Äôabsence de biais, la variance minimale (borne de Cramer-Rao).\n\n# packages utiles\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n\n\n Exercice 1a : R√©gression lin√©aire avec scikit\nCet exercice vise √† illustrer la mani√®re d‚Äôeffectuer une r√©gression lin√©aire avec scikit.\nDans ce domaine,\nstatsmodels est nettement plus complet, ce que montrera l‚Äôexercice suivant.\nL‚Äôint√©r√™t principal de faire\ndes r√©gressions avec scikit est de pouvoir comparer les r√©sultats d‚Äôune r√©gression lin√©aire\navec d‚Äôautres mod√®les de r√©gression. Cependant, le chapitre sur les\npipelines montrera qu‚Äôon peut tr√®s bien ins√©rer, avec quelques efforts\nde programmation orient√©e objet, une r√©gression statsmodels dans\nun pipeline scikit.\nL‚Äôobjectif est d‚Äôexpliquer le score des R√©publicains en fonction de quelques\nvariables. Contrairement au chapitre pr√©c√©dent, o√π on se focalisait sur\nun r√©sultat binaire (victoire/d√©faite des R√©publicains), cette\nfois on va chercher √† mod√©liser directement le score des R√©publicains.\n\nA partir de quelques variables, par exemple, ‚ÄòUnemployment_rate_2019‚Äô, ‚ÄòMedian_Household_Income_2019‚Äô, ‚ÄòPercent of adults with less than a high school diploma, 2015-19‚Äô, ‚ÄúPercent of adults with a bachelor‚Äôs degree or higher, 2015-19‚Äù, expliquer la variable per_gop √† l‚Äôaide d‚Äôun √©chantillon d‚Äôentra√Ænement X_train constitu√© au pr√©alable.\n\n:warning: utiliser la variable Median_Household_Income_2019\nen log sinon son √©chelle risque d‚Äô√©craser tout effet.\n\nAfficher les valeurs des coefficients, constante comprise\nEvaluer la pertinence du mod√®le avec le \\(R^2\\) et la qualit√© du fit avec le MSE.\nRepr√©senter un nuage de points des valeurs observ√©es\net des erreurs de pr√©diction. Observez-vous\nun probl√®me de sp√©cification ?\n\n\n\n\n# packages utiles\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n\n\n Exercice 1b : R√©gression lin√©aire avec statsmodels\nCet exercice vise √† illustrer la mani√®re d‚Äôeffectuer une r√©gression lin√©aire avec statsmodels qui offre des fonctionnalit√©s plus proches de celles de R, et moins orient√©es Machine Learning.\nL‚Äôobjectif est toujours d‚Äôexpliquer le score des R√©publicains en fonction de quelques\nvariables.\n\nA partir de quelques variables, par exemple, ‚ÄòUnemployment_rate_2019‚Äô, ‚ÄòMedian_Household_Income_2019‚Äô, ‚ÄòPercent of adults with less than a high school diploma, 2015-19‚Äô, ‚ÄúPercent of adults with a bachelor‚Äôs degree or higher, 2015-19‚Äù, expliquer la variable per_gop. :warning: utiliser la variable Median_Household_Income_2019\nen log sinon son √©chelle risque d‚Äô√©craser tout effet.\nAfficher un tableau de r√©gression.\nEvaluer la pertinence du mod√®le avec le R^2.\nUtiliser l‚ÄôAPI formula pour r√©gresser le score des r√©publicains en fonction de la variable Unemployment_rate_2019, de Unemployment_rate_2019 au carr√© et du log de\nMedian_Household_Income_2019.\n\n\n\n\n\n Hint\nPour sortir une belle table pour un rapport sous \\(\\LaTeX\\), il est possible d‚Äôutiliser\nla m√©thode Summary.as_latex. Pour un rapport HTML, on utilisera Summary.as_html\n\n\n\n\n Note\nLes utilisateurs de R retrouveront des √©l√©ments tr√®s familiers avec statsmodels,\nnotamment la possibilit√© d‚Äôutiliser une formule pour d√©finir une r√©gression.\nLa philosophie de statsmodels est similaire √† celle qui a influ√© sur la construction\ndes packages stats et MASS de R: offrir une librairie g√©n√©raliste, proposant\nune large gamme de mod√®les. N√©anmoins, statsmodels b√©n√©ficie de sa jeunesse\npar rapport aux packages R. Depuis les ann√©es 1990, les packages R visant\n√† proposer des fonctionalit√©s manquantes dans stats et MASS se sont\nmultipli√©s alors que statsmodels, enfant des ann√©es 2010, n‚Äôa eu qu‚Äô√†\nproposer un cadre g√©n√©ral (les generalized estimating equations) pour\nenglober ces mod√®les.\n\n\n\n\nLa r√©gression logistique\nCe mod√®le s‚Äôapplique √† une distribution binaire.\nDans ce cas, \\(\\mathbb{E}\\_{\\theta}(Y|X) = \\mathbb{P}\\_{\\theta}(Y = 1|X)\\).\nLa r√©gression logistique peut √™tre vue comme un mod√®le lin√©aire en probabilit√©:\n\\[\n\\text{logit}\\bigg(\\mathbb{E}\\_{\\theta}(Y|X)\\bigg) = \\text{logit}\\bigg(\\mathbb{P}\\_{\\theta}(Y = 1|X)\\bigg) = X\\beta\n\\]\nLa fonction \\(\\text{logit}\\) est \\(]0,1[ \\to \\mathbb{R}: p \\mapsto \\log(\\frac{p}{1-p})\\).\nElle permet ainsi de transformer une probabilit√© dans \\(\\mathbb{R}\\).\nSa fonction r√©ciproque est la sigmo√Øde (\\(\\frac{1}{1 + e^{-x}}\\)),\nobjet central du Deep Learning.\nIl convient de noter que les probabilit√©s ne sont pas observ√©es, c‚Äôest l‚Äôoutcome\nbinaire (0/1) qui l‚Äôest. Cela am√®ne √† voir la r√©gression logistique de deux\nmani√®res diff√©rentes :\n\nEn √©conom√©trie, on s‚Äôint√©resse au mod√®le latent qui d√©termine le choix de\nl‚Äôoutcome. Par exemple, si on observe les choix de participer ou non au march√©\ndu travail, on va mod√©liser les facteurs d√©terminant ce choix ;\nEn Machine Learning, le mod√®le latent n‚Äôest n√©cessaire que pour classifier\ndans la bonne cat√©gorie les observations\n\nL‚Äôestimation des param√®tres \\(\\beta\\) peut se faire par maximum de vraisemblance\nou par r√©gression, les deux solutions sont √©quivalentes sous certaines\nhypoth√®ses.\n\n\n Note\nPar d√©faut, scikit applique une r√©gularisation pour p√©naliser les mod√®les\npeu parcimonieux (comportement diff√©rent\nde celui de statsmodels). Ce comportement par d√©faut est √† garder √† l‚Äôesprit\nsi l‚Äôobjectif n‚Äôest pas de faire de la pr√©diction.\n\n\n\n# packages utiles\nfrom sklearn.linear_model import LogisticRegression\nimport sklearn.metrics\n\n\n\n Exercice 2a : R√©gression logistique avec scikit\nAvec scikit, en utilisant √©chantillons d‚Äôapprentissage et d‚Äôestimation :\n\nEvaluer l‚Äôeffet des variables d√©j√† utilis√©es sur la probabilit√© des R√©publicains\nde gagner. Affichez la valeur des coefficients.\nD√©duire une matrice de confusion et\nune mesure de qualit√© du mod√®le.\nSupprimer la r√©gularisation gr√¢ce au param√®tre penalty. Quel effet sur les param√®tres estim√©s ?\n\n\n\n\n# packages utiles\nfrom scipy import stats\n\n\n\n Exercice 2b : R√©gression logistique avec statmodels\nEn utilisant √©chantillons d‚Äôapprentissage et d‚Äôestimation :\n\nEvaluer l‚Äôeffet des variables d√©j√† utilis√©es sur la probabilit√© des R√©publicains\nde gagner.\nFaire un test de ratio de vraisemblance concernant l‚Äôinclusion de la variable de (log)-revenu.\n\n\n\n\n\n Hint\nLa statistique du test est :\n\\[\nLR = -2\\log\\bigg(\\frac{\\mathcal{L}_{\\theta}}{\\mathcal{L}_{\\theta_0}}\\bigg) = -2(\\mathcal{l}_{\\theta} - \\mathcal{l}_{\\theta_0})\n\\]"
  },
  {
    "objectID": "content/modelisation/1_modelevaluation.html",
    "href": "content/modelisation/1_modelevaluation.html",
    "title": "Evaluer la qualit√© d‚Äôun mod√®le",
    "section": "",
    "text": "Nous allons ici voir des m√©thodes g√©n√©rales permettant de s‚Äôassurer que le mod√®le\nde Machine Learning mobilis√© est de qualit√©. Ce chapitre ne pr√©sente pas\nd‚Äôexercice ou de code, il est l√† pour pr√©senter certains concepts\nque nous appliquerons dans les prochains chapitres."
  },
  {
    "objectID": "content/modelisation/1_modelevaluation.html#d√©couper-l√©chantillon",
    "href": "content/modelisation/1_modelevaluation.html#d√©couper-l√©chantillon",
    "title": "Evaluer la qualit√© d‚Äôun mod√®le",
    "section": "D√©couper l‚Äô√©chantillon",
    "text": "D√©couper l‚Äô√©chantillon\nLe chapitre pr√©c√©dent pr√©sentait le pipeline simple ci-dessous\npour introduire √† la notion d‚Äôentra√Ænement d‚Äôun mod√®le:\n\n\n\n\n\nCe pipeline fait abstraction d‚Äôhypoth√®ses exog√®nes √† l‚Äôestimation\nmais qui sont √† faire sur des param√®tres\ncar elles affectent la performance de la pr√©diction.\nPar exemple, de nombreux mod√®les proposent une p√©nalisation des mod√®les\nnon parcimonieux pour √©viter le sur-apprentissage. Le choix de la p√©nalisation\nid√©ale d√©pend de la structure des donn√©es et n‚Äôest jamais connue, ex-ante\npar le mod√©lisateur. Faut-il p√©naliser fortement ou non le mod√®le ? En l‚Äôabsence\nd‚Äôargument th√©orique, on aura tendance √† tester plusieurs param√®tres de\np√©nalisation et choisir celui qui permet la meilleure pr√©diction.\nLa notion de validation crois√©e permettra de g√©n√©raliser cette approche. Ces param√®tres\nqui affectent la pr√©diction seront pas la suite appel√©s des\nhyperparam√®tres. Comme nous allons le voir, nous allons aboutir √† un\nraffinement de l‚Äôapproche pour obtenir un pipeline ayant plut√¥t cet aspect:"
  },
  {
    "objectID": "content/modelisation/1_modelevaluation.html#le-probl√®me-du-sur-apprentissage",
    "href": "content/modelisation/1_modelevaluation.html#le-probl√®me-du-sur-apprentissage",
    "title": "Evaluer la qualit√© d‚Äôun mod√®le",
    "section": "Le probl√®me du sur-apprentissage",
    "text": "Le probl√®me du sur-apprentissage\nLe but du Machine Learning est de calibrer l‚Äôalgorithme sur des exemples\nconnus (donn√©es labellis√©es) afin de g√©n√©raliser √† des\nexemples nouveaux (√©ventuellement non labellis√©s).\nOn vise donc de bonnes qualit√©s\npr√©dictives et non un ajustement parfait\naux donn√©es historiques.\nIl existe un arbitrage biais-variance dans la qualit√© d‚Äôestimation[^1]. Soit \\(h(X,\\theta)\\) un mod√®le statistique. On\npeut d√©composer l‚Äôerreur d‚Äôestimation en deux parties :\n\\[\n\\mathbb{E}\\bigg[(y - h(\\theta,X))^2 \\bigg] = \\underbrace{ \\bigg( y - \\mathbb{E}(h_\\theta(X)) \\bigg)^2}_{\\text{biais}^2} + \\underbrace{\\mathbb{V}\\big(h(\\theta,X)\\big)}_{\\text{variance}}\n\\]\nIl y a ainsi un compromis √† faire entre biais et variance. Un mod√®le peu parcimonieux, c‚Äôest-√†-dire proposant un grand nombre de param√®tres, va, en g√©n√©ral, avoir un faible biais mais une grande variance. En effet, le mod√®le va tendre √† se souvenir d‚Äôune combinaison de param√®tres √† partir d‚Äôun grand nombre d‚Äôexemples sans √™tre capable d‚Äôapprendre la r√®gle qui permette de structurer les donn√©es.\n[^1]! Cette formule permet de bien comprendre la th√©orie statistique asymptotique, notamment le th√©or√®me de Cramer-Rao. Dans la classe des estimateurs sans biais, c‚Äôest-√†-dire dont le premier terme est nul, trouver l‚Äôestimateur √† variance minimale revient √† trouver l‚Äôestimateur qui minimise \\(\\mathbb{E}\\bigg[(y - h_\\theta(X))^2 \\bigg]\\). C‚Äôest la d√©finition m√™me de la r√©gression, ce qui, quand on fait des hypoth√®ses suppl√©mentaires sur le mod√®le statistique, explique le th√©or√®me de Cramer-Rao.\nPar exemple, la ligne verte ci-dessous est trop d√©pendante des donn√©es et risque de produire une erreur plus importante que la ligne noire (qui moyennise plus) sur de nouvelles donn√©es.\n\n\n\n\n\nPour renforcer la validit√© externe d‚Äôun mod√®le, il est ainsi commun, en Machine Learning:\n\nd‚Äôestimer un mod√®le sur un jeu de donn√©es (jeu d‚Äôapprentissage ou training set) mais d‚Äô√©valuer la performance, et donc la pertinence du mod√®le, sur d‚Äôautres donn√©es, qui n‚Äôont pas √©t√© mobilis√©es lors de la phase d‚Äôestimation (jeu de validation, de test ou testing set) ;\nd‚Äôavoir des mesures de performances qui p√©nalisent fortement les mod√®les peu parcimonieux (BIC) ou conduire une premi√®re phase de s√©lection de variable (par des m√©thodes de LASSO‚Ä¶)\n\nPour d√©composer un mod√®le en jeu d‚Äôestimation et de test,\nla meilleure m√©thode est d‚Äôutiliser les fonctionnalit√©s de scikit de la mani√®re suivante :\n\nfrom sklearn.model_selection import train_test_split\nxTrain, xTest, yTrain, yTest = train_test_split(\n  x, y,\n  test_size = 0.2,\n  random_state = 0\n  )\n\nLa proportion d‚Äôobservations dans le jeu de test est contr√¥l√©e par l‚Äôargument test_size.\nLa proportion optimale n‚Äôexiste pas.\nLa r√®gle du pouce habituelle est d‚Äôassigner al√©atoirement 20 % des observations\ndans l‚Äô√©chantillon de test pour garder suffisamment d‚Äôobservations\ndans l‚Äô√©chantillon d‚Äôestimation.\n\n\n Hint \nLorsqu‚Äôon travaille avec des s√©ries temporelles, l‚Äô√©chantillonnage al√©atoire des observations n‚Äôa pas vraiment de sens. Il vaut mieux tester la qualit√© de l‚Äôobservation sur des p√©riodes distingu√©es.\n\n\n\n\n Note\nAvec des donn√©es multi-niveaux,\ncomme c‚Äôest le cas de donn√©es g√©ographiques ou de donn√©es individuelles avec des variables de classe,\nil peut √™tre int√©ressant d‚Äôutiliser un √©chantillonnage stratifi√©.\nCela permet de garder une proportion √©quivalente de chaque groupe dans les deux jeux de donn√©es de test ou d‚Äôapprentissage.\nCe type d‚Äô√©chantillonnage stratifi√© est √©galement possible avec scikit.\n\n\nL‚Äôexercice sur les SVM illustre cette construction et la mani√®re\ndont elle facilite l‚Äô√©valuation de la qualit√© d‚Äôun mod√®le."
  },
  {
    "objectID": "content/modelisation/1_modelevaluation.html#validation-crois√©e",
    "href": "content/modelisation/1_modelevaluation.html#validation-crois√©e",
    "title": "Evaluer la qualit√© d‚Äôun mod√®le",
    "section": "Validation crois√©e",
    "text": "Validation crois√©e\nCertains algorithmes font intervenir des hyperparam√®tres,\nc‚Äôest-√†-dire des param√®tres exog√®nes qui d√©terminent la pr√©diction mais ne sont pas estim√©s.\nLa validation crois√©e est une m√©thode permettant de choisir la valeur du param√®tre\nqui optimise la qualit√© de la pr√©diction en agr√©geant\ndes scores de performance sur des d√©coupages diff√©rents de l‚Äô√©chantillon d‚Äôapprentissage.\nLa validation crois√©e permet d‚Äô√©valuer les performances de mod√®les diff√©rents (SVM, random forest, etc.) ou, coupl√© √† une strat√©gie de grid search de trouver les valeurs des hyperparam√®tres qui aboutissent √† la meilleure pr√©diction.\n\n\n Note\nL‚Äô√©tape de d√©coupage de l‚Äô√©chantillon de validation crois√©e est √† distinguer de l‚Äô√©tape split_sample_test. A ce stade, on a d√©j√† partitionn√© les donn√©es en √©chantillon d‚Äôapprentissage et test. C‚Äôest l‚Äô√©chantillon d‚Äôapprentissage qu‚Äôon d√©coupe en sous-morceaux.\n\n\nLa m√©thode la plus commune est la validation crois√©e k-fold.\nOn partitionne les donn√©es en \\(K\\) morceaux et on consid√®re chaque pli, tour √† tour, comme un √©chantillon\nde test en apprenant sur les \\(K-1\\) √©chantillons restants. Les \\(K\\) indicateurs ainsi calcul√©s sur les \\(K\\) √©chantillons de test peuvent √™tre moyenn√©s et\ncompar√©s pour plusieurs valeurs des hyperparam√®tres.\n\n\n\n\n\nIl existe d‚Äôautres types de validation crois√©e, notamment la leave one out qui consiste √† consid√©rer une fois\nexactement chaque observation comme l‚Äô√©chantillon de test (une n-fold cross validation)."
  },
  {
    "objectID": "content/modelisation/1_modelevaluation.html#mesurer-la-performance",
    "href": "content/modelisation/1_modelevaluation.html#mesurer-la-performance",
    "title": "Evaluer la qualit√© d‚Äôun mod√®le",
    "section": "Mesurer la performance",
    "text": "Mesurer la performance\nJusqu‚Äô√† pr√©sent, nous avons pass√© sous silence la question du support de \\(y\\), c‚Äôest-√†-dire\nde l‚Äô√©tendue des valeurs de notre variable d‚Äôint√©r√™t.\nEn pratique, la distribution des \\(y\\)\nva n√©anmoins d√©terminer deux questions cruciales : la m√©thode et l‚Äôindicateur de performance.\nEn apprentissage supervis√©, on distingue en g√©n√©ral les probl√®mes de:\n\nClassification : la variable \\(y\\) est discr√®te\nR√©gression : la variable \\(y\\) est continue\n\nLes deux approches ne sont pas sans lien. On peut par exemple voir le mod√®le √©conom√©trique de choix d‚Äôoffre de travail comme un probl√®me de classification (participation ou non au march√© du travail) ou de r√©gression (r√©gression sur un mod√®le √† variable latente)\n\nClassification\nLa plupart des crit√®res de performance sont construits √† partir de la matrice de confusion:\n\n\n\nImage emprunt√©e √† https://www.lebigdata.fr/confusion-matrix-definition\n\n\nA partir des 4 coins de cette matrice, il existe plusieurs mesure de performance\n\n\n\n\n\n\n\n\nCrit√®re\nMesure\nCalcul\n\n\n\n\nAccuracy\nTaux de classification correcte\nDiagonale du tableau: \\(\\frac{TP+TN}{TP+FP+FN+FP}\\)\n\n\nPrecision\nTaux de vrais positifs\nLigne des pr√©dictions positives : \\(\\frac{TP}{TP+FP}\\)\n\n\nRecall (rappel)\nCapacit√© √† identifier les labels positifs\nColonne des pr√©dictions positives : \\(\\frac{TP}{TP+FN}\\)\n\n\nF1 Score\nMesure synth√©tique (moyenne harmonique) de la pr√©cision et du rappel\n\\(2 \\frac{precision \\times recall}{precision + recall}\\)\n\n\n\nEn pr√©sence de classes d√©sequilibr√©es, la\nF-mesure est plus pertinente pour √©valuer les\nperformances mais l‚Äôapprentissage restera\nmauvais si l‚Äôalgorithme est sensible √† ce\nprobl√®me. Notamment, si on d√©sire avoir une performance √©quivalente sur les classes minoritaires, il faut g√©n√©ralement les sur-pond√©rer (ou faire un √©chantillonnage stratifi√©) lors de la constitution de l‚Äô√©chantillon d‚Äôobservation.\nIl est possible de construire des mod√®les √† partir des probabilit√©s pr√©dites d‚Äôappartenir √† la classe d‚Äôint√©r√™t. Pour cela, on fixe un seuil \\(c\\) tel que\n\\[\n\\mathbb{P}(y_i=1|X_i) &gt; c \\Rightarrow \\widehat{y}_i = 1\n\\]\nPlus on augmente \\(c\\), plus on est s√©lectif sur le crit√®re d‚Äôappartenance √† la classe.\nLe rappel, i.e.¬†le taux de faux n√©gatifs, diminue. Mais on augmente le nombre de positifs manqu√©s. Pour chaque valeur de \\(c\\) correspond une matrice de confusion et donc des mesures de performances.\nLa courbe ROC est un outil classique pour repr√©senter en un graphique l‚Äôensemble de ces\ninformations en faisant varier \\(c\\) de 0 √† 1:\n\n\n\n\n\nL‚Äôaire sous la courbe (AUC) permet d‚Äô√©valuer quantitativement le meilleur mod√®le au\nsens de ce crit√®re. L‚ÄôAUC repr√©sente la probabilit√© que le mod√®le soit capable de distinguer entre la classe positive et n√©gative.\n\n\nR√©gression\nEn Machine Learning, les indicateurs de performance en r√©gression sont les suivants :\n\n\n\n\n\n\n\nNom\nFormule\n\n\n\n\nMean squared error\n\\(MSE = \\mathbb{E}\\left[(y - h_\\theta(X))^2\\right]\\)\n\n\nRoot Mean squared error\n\\(RMSE = \\sqrt{\\mathbb{E}\\left[(y - h_\\theta(X))^2\\right]}\\)\n\n\nMean Absolute Error\n\\(MAE = \\mathbb{E} \\bigg[ \\lvert y - h_\\theta(X) \\rvert \\bigg]\\)\n\n\nMean Absolute Percentage Error\n\\(MAE = \\mathbb{E}\\left[ \\left\\lvert \\frac{y - h_\\theta(X)}{y} \\right\\rvert \\right]\\)\n\n\n\nL‚Äô√©conom√®tre se focalise moins sur la qualit√© de la pr√©diction et utilisera\nd‚Äôautres crit√®res pour √©valuer la qualit√© d‚Äôun mod√®le (certains, comme le BIC, sont\n√† regarder aussi dans une optique Machine Learning): \\(R^2\\), \\(BIC\\),\n\\(AIC\\), log-likelihood, etc."
  },
  {
    "objectID": "content/modelisation/index.html",
    "href": "content/modelisation/index.html",
    "title": "Partie 3: mod√©liser",
    "section": "",
    "text": "Les data scientists sont souvent associ√©s √† la mise en oeuvre\nde mod√®les complexes d‚Äôintelligence artificielle.\nLe succ√®s m√©diatique de ce type d‚Äôoutils, notamment ChatGPT,\nn‚Äôy est pas pour rien. Cependant, la mod√©lisation n‚Äôest souvent\nqu‚Äôune\nphase du travail du data scientist, un peu comme la visualisation.\nD‚Äôailleurs, dans certaines organisations o√π la division des t√¢ches\nest plus pouss√©e, les data engineers sont au moins aussi\nimpliqu√©s dans la phase de mod√©lisation que les data scientists.\nC‚Äôest souvent un p√©ch√© de jeunesse de penser qu‚Äôon peut r√©sumer\nle travail du data scientist exclusivement √† la phase de mod√©lisation.\nCette derni√®re d√©pend tr√®s fortement de la qualit√© du travail de\nnettoyage et structuration des donn√©es mis en oeuvre en amont. La\nmise en oeuvre de mod√®les complexes, qui s‚Äôaccomodent de donn√©es\npeu structur√©es, est gourmande en ressources et co√ªteuse. Ce ne sont\ndonc qu‚Äôun nombre limit√© d‚Äôacteurs qui peuvent entra√Æner, ex nihilo,\ndes grands mod√®les de langage1, capables de d√©penser au moins 300 000 dollars\ndans l‚Äôentra√Ænement d‚Äôun mod√®le, avant m√™me toute phase d‚Äôinf√©rence (Izsak, Berchansky, and Levy 2021).\nCes besoins computationnels pour entra√Æner de grands mod√®les de langage sont\nd‚Äôailleurs assez gourmands en √©nergie, ce qui peut amener √†\ndes empreintes carbones non n√©gligeables (Strubell, Ganesh, and McCallum 2019; Arcep 2019).\nHeureusement, il est possible de mettre en oeuvre des mod√©lisations plus\nl√©g√®res (celles que nous pr√©senterons dans les prochains chapitres)\nou de r√©utiliser des mod√®les pr√©-entra√Æn√©s pour les sp√©cialiser\nsur un nouveau jeu de donn√©es (principe du fine tuning2).\nEn fait, pour √™tre plus pertinent que des approches plus parcimonieuses,\nles techniques de deep learning, notamment\nles r√©seaux de neurones, n√©cessitent soit des volumes de donn√©es tr√®s\nimportants (des millions voire dizaine de millions d‚Äôobservations) soit\ndes donn√©es √† la structure complexe comme le langage naturel ou les images.\nDans de nombreux cas, des mod√®les plus simples comme les techniques d‚Äôapprentissage\nautomatique (machine learning) suffisent largement."
  },
  {
    "objectID": "content/modelisation/index.html#la-mod√©lisation-une-approche-au-coeur-de-la-statistique",
    "href": "content/modelisation/index.html#la-mod√©lisation-une-approche-au-coeur-de-la-statistique",
    "title": "Partie 3: mod√©liser",
    "section": "La mod√©lisation, une approche au coeur de la statistique",
    "text": "La mod√©lisation, une approche au coeur de la statistique\nUn mod√®le statistique\nest une repr√©sentation simplifi√©e et structur√©e d‚Äôun ph√©nom√®ne r√©el,\nconstruite √† partir d‚Äôobservations regroup√©es dans un ensemble partiel de donn√©es.\nUn mod√®le vise √† capturer les relations et les sch√©mas sous-jacents au sein de ces donn√©es, permettant ainsi de formuler des hypoth√®ses, d‚Äôeffectuer des pr√©dictions et d‚Äôextrapoler des conclusions au-del√†\nde l‚Äôensemble de donn√©es mesur√©es.\nLes mod√®les statistiques fournissent ainsi un cadre analytique pour explorer, comprendre et interpr√©ter les informations contenues dans les donn√©es.\nDans le domaine de la recherche √©conomique, ils peuvent servir √†\nassocier certains param√®tres structurants des mod√®les de comportement\n√©conomique √† des valeurs quantitatives.\nLes mod√®les statistiques, comme les mod√®les √©conomiques\npr√©sentent n√©anmoins toujours une part d‚Äôirr√©alisme (Friedman 1953; Salmon 2010)\net accepter de mani√®re trop litt√©rale les implications d‚Äôun mod√®le, m√™me s‚Äôil\na de bonnes performances pr√©dictives, peut √™tre dangereux et relever d‚Äôun biais\nscientiste. On s√©lectionne plut√¥t le moins mauvais mod√®le\nque le vrai processus g√©n√©rateur des donn√©es.\nRepr√©senter la r√©alit√© sous la forme d‚Äôun mod√®le est un principe √† la\nbase de la statistique comme discipline scientifique et ayant des\napplications dans de nombreux champs disciplinaires : √©conomie,\nsociologie, g√©ographique, biologie, physique, etc.\nSelon les disciplines, le nom donn√© peut varier mais on retrouve\nr√©guli√®rement la m√™me approche scientifique : le mod√©lisateur\nconstruit des relations entre plusieurs variables th√©oriques\nayant des contreparties empiriques afin d‚Äôexpliquer tel ou tel\nprocessus.\nDans l‚Äôenseignement de l‚ÄôENSAE ce type d‚Äôapproche empirique se retrouve\nprincipalement dans deux types d‚Äôapproches : le machine learning et\nl‚Äô√©conom√©trie. La diff√©rence est certes\ns√©mantique - la r√©gression lin√©aire peut √™tre consid√©r√©e comme une\ntechnique de machine learning ou d‚Äô√©conom√©trie - mais elle est\n√©galement conceptuelle :\n\nDans le domaine du machine learning,\nla structure impos√©e par le mod√©lisateur est minimale et ce sont plut√¥t\nles algorithmes qui, sur des crit√®res de performance statistique, vont\namener √† choisir une loi math√©matique qui correspond au mieux aux donn√©es ;\nEn √©conom√©trie,\nles hypoth√®ses de structure des lois sont plus fortes (m√™me dans un cadre semi ou non-param√©trique) et sont plus souvent impos√©es\npar le mod√©lisateur.\n\nDans cette partie du cours, nous allons principalement\nparler de machine learning car il s‚Äôagit d‚Äôune perspective\nplus op√©rationnelle que l‚Äô√©conom√©trie qui est plus directement associ√©e\n√† des concepts statistiques complexes comme la th√©orie asymptotique.\nL‚Äôadoption du machine learning dans la litt√©rature √©conomique a √©t√© longue\ncar la structuration des donn√©es est souvent le\npendant empirique d‚Äôhypoth√®ses th√©oriques sur le comportement des acteurs ou des march√©s (Athey and Imbens 2019; Charpentier, Flachaire, and Ly 2018).\nPour caricaturer, l‚Äô√©conom√©trie s‚Äôattacherait √† comprendre la causalit√© de certaines variables sur une autre.\nCela implique que ce qui int√©resse l‚Äô√©conom√®tre\nest principalement de l‚Äôestimation des param√®tres (et l‚Äôincertitude\nsur l‚Äôestimation de ceux-ci) qui permettent de quantifier l‚Äôeffet d‚Äôune\nvariation d‚Äôune variable sur une autre.\nToujours pour caricaturer,\nle machine learning se focaliserait\nsur un simple objectif pr√©dictif en exploitant les relations de corr√©lations entre les variables.\nDans cette perspective, l‚Äôimportant n‚Äôest pas la causalit√© mais le fait qu‚Äôune variation\nde \\(x\\)% d‚Äôune variable permette d‚Äôanticiper un changement de \\(\\beta x\\) de la variable\nd‚Äôint√©r√™t ; peu importe la raison.\nMullainathan and Spiess (2017) ont ainsi, pour simplifier, propos√© la diff√©rence fondamentale qui\nsuit : l‚Äô√©conom√©trie se pr√©occupe de \\(\\widehat{\\beta}\\) l√† o√π le machine learning\nse focalise sur \\(\\widehat{y}\\). Les deux sont bien s√ªr reli√©s dans un cadre\nlin√©aire mais cette diff√©rence d‚Äôapproche a des implications importantes\nsur la structure des mod√®les √©tudi√©s, notamment leur parcimonie3."
  },
  {
    "objectID": "content/modelisation/index.html#quelques-d√©finitions",
    "href": "content/modelisation/index.html#quelques-d√©finitions",
    "title": "Partie 3: mod√©liser",
    "section": "Quelques d√©finitions",
    "text": "Quelques d√©finitions\nDans cette partie du cours nous allons employer un certain nombre\nde termes devenus familiers aux praticiens du machine learning\nmais qui m√©ritent d‚Äô√™tre explicit√©s.\n\nMachine learning et deep learning\nJusqu‚Äô√† pr√©sent nous avons beaucoup utilis√©, sans le d√©finir, le\nconcept de machine learning, dont la traduction fran√ßaise est\napprentissage automatique mais le terme anglo-saxon est suffisamment\nutilis√© pour √™tre consid√©r√© comme standard.\nLe machine learning est un ensemble de techniques algorithmiques\nqui permettent aux ordinateurs d‚Äôapprendre, √† partir d‚Äôexemples, √† ajuster un mod√®le\nsans avoir explicitement d√©fini celui-ci. A partir d‚Äôalgorithmes it√©ratifs et d‚Äôune\nm√©trique de performance, des r√®gles de classification ou de pr√©diction vont permettre\nde mettre en relation des caract√©ristiques (features) avec une variable d‚Äôint√©r√™t (label)4.\nDe nombreux algorithmes existent et se distinguent sur la mani√®re d‚Äôintroduire une structure plus ou\nmoins formelle dans la relation entre les variables observ√©es. Nous n‚Äôallons voir que quelques-uns\nde ces algorithmes : support vector machine (SVM), r√©gression logistique, arbres de d√©cision, for√™ts\nal√©atoires, etc. Simples √† mettre en oeuvre gr√¢ce √† la librairie Scikit-Learn, ils permettront\nd√©j√† de comprendre la d√©marche originale du machine learning que vous pourrez approfondir\nult√©rieurement.\nAu sein de la grande famille des algorithmes de machine learning, tendent de plus √† plus √† devenir\nautonomes les techniques de r√©seaux de neurone. Les techniques qui s‚Äôappuient sur les r√©seaux de neurones sont regroup√©s\ndans une famille qu‚Äôon\nappelle deep learning (apprentissage profond en Fran√ßais).\nCes r√©seaux sont inspir√©s du fonctionnement du cerveau humain et sont compos√©s de nombreuses couches de neurones interconnect√©s.\nLa structure canonique bien connue est illustr√©e dans la Figure¬†1.\nLe deep learning est int√©ressant pour cr√©er des mod√®les capables d‚Äôapprendre de repr√©sentations\nde donn√©es complexes et abstraites √† partir de donn√©es brutes,\nce qui √©vite parfois la complexe t√¢che de d√©finir manuellement des caract√©ristiques sp√©cifiques √† cibler.\nLes champs de l‚Äôanalyse d‚Äôimage (computer vision) ou du traitement du langage naturel sont les principaux\ncas d‚Äôapplication de ces m√©thodes.\n\n\n\n\n\n\n\n\n\nFigure¬†1: Exemple de structure d‚Äôun r√©seau de neurones (source: lebigdata.fr)\n\n\nNous n‚Äôallons pas vraiment parler dans ce cours de deep learning car ces mod√®les, pour √™tre pertinents, n√©cessitent\nsoit des donn√©es structur√©es d‚Äôun volume important (ce qui est rarement disponible\nen open data) soit des cas d‚Äôusage sp√©cifiques, plus avanc√©s que ne le permet\nun cours d‚Äôintroduction. L‚Äôorganisation HuggingFace, cr√©atrice de la\nplateforme du m√™me nom facilitant la r√©utilisation de mod√®les de deep learning\npropose d‚Äôexcellents cours sur le sujet, notamment sur\nle traitement du langage naturel (NLP).\nNous ferons du traitement du langage naturel dans la prochaine partie de ce cours mais\nde mani√®re plus modeste en revenant sur les concepts n√©cessaires avant de mettre en oeuvre\nune mod√©lisation sophistiqu√©e du langage.\n\n\nApprentissage supervis√© ou non supervis√©\nUne ligne de clivage importante entre les m√©thodes √† mettre en oeuvre est le fait d‚Äôobserver ou non\nle label (la variable \\(y\\)) qu‚Äôon d√©sire mod√©liser.\nPrenons par exemple un site de commerce qui dispose\nd‚Äôinformations sur ses clients comme l‚Äô√¢ge, le sexe, le lieu de r√©sidence.\nCe site peut d√©sirer\nexploiter cette information de diff√©rentes mani√®res pour mod√©liser le comportement d‚Äôachat.\nEn premier lieu, ce site peut d√©sirer\nanticiper le volume d‚Äôachat d‚Äôun nouveau client ayant certaines caract√©ristiques.\nDans ce cas, il est possible d‚Äôutiliser les montants d√©pens√©s par d‚Äôautres clients en fonction de leurs\ncaract√©ristiques. L‚Äôinformation pour notre nouveau client n‚Äôest pas mesur√©e mais elle peut s‚Äôappuyer\nsur un ensemble d‚Äôobservations de la m√™me variable.\nMais il est tout √† fait possible d‚Äôentra√Æner un mod√®le sur un label qu‚Äôon ne mesure pas, en supposant\nqu‚Äôil fasse sens. Par exemple notre site de commerce peut d√©sirer d√©terminer, en fonction des\ncaract√©ristiques de notre nouveau client et de sa client√®le existante, s‚Äôil appartient √† tel ou\ntel groupe de consommateurs : les d√©pensiers, les √©conomes‚Ä¶ Bien s√ªr on ne sait jamais a priori\n√† quel groupe appartient un consommateur mais le rapprochement entre consommateurs ayant un comportement\nsimilaire permettra de donner du sens √† cette cat√©gorie. Dans ce cas, l‚Äôalgorithme apprendra √† reconna√Ætre\nquelles caract√©ristiques sont structurantes dans la constitution de groupes au comportement similaire et\npermettra d‚Äôassocier tout nouveau consommateur √† un groupe.\nCes deux exemples illustrent l‚Äôapproche diff√©rente selon qu‚Äôon essaie de construire des mod√®les\nsur un label observ√© ou non. Cela constitue m√™me l‚Äôune des dualit√©s fondamentale dans les\ntechniques de machine learning :\n\nApprentissage supervis√© : la valeur cible est connue et peut √™tre utilis√©e pour √©valuer la qualit√© d‚Äôun mod√®le ;\nApprentissage non supervis√© : la valeur cible est inconnue et ce sont des crit√®res statistiques qui vont amener\n√† s√©lectionner la structure de donn√©es la plus plausible.\n\nCette partie du cours illustrera ces deux approches de mani√®re diff√©rente √† partir du m√™me\njeu de donn√©es, les r√©sultats des √©lections am√©ricaines.\nDans le cas de l‚Äôapprentissage supervis√©, nous chercherons √† mod√©liser directement\nle r√©sultat des candidats aux √©lections (soit le score soit le gagnant). Dans\nle cas de l‚Äôapprentissage non supervis√©, nous essaierons de regrouper les\nterritoires au comportement de vote similaire en fonction de facteurs\nsocio-d√©mographiques.\n\n\nClassification et r√©gression\nUne deuxi√®me dualit√© fondamentale qui est d√©terminante dans le choix de la m√©thode de machine learning\n√† mettre en oeuvre est la nature du label. S‚Äôagit-il d‚Äôune variable continue ou d‚Äôune variable\ndiscr√®te, c‚Äôest-√†-dire prenant un nombre limit√© de modalit√©s ?\nCette diff√©rence de nature entre les donn√©es am√®ne √† distinguer deux types d‚Äôapproche :\n\nDans les probl√©matiques de classification, o√π notre label \\(y\\) a un nombre fini de valeurs5,\non cherche √† pr√©dire dans quelle classe ou √† quel groupe il est possible de rattacher nos donn√©es.\nPar exemple, si vous prenez du caf√© le matin, faites-vous partie du groupe des personnes ronchons au lever ?\nLes m√©triques de performance utilisent g√©n√©ralement la proportion de bonnes ou mauvaises classifications\npour estimer la qualit√© d‚Äôun mod√®le.\nDans les probl√©matiques de r√©gression, o√π notre label est une grandeur num√©rique, on\ncherche √† pr√©dire directement la valeur de notre variable dans le mod√®le. Par exemple, si vous\navez tel ou tel √¢ge, quel est votre d√©pense quotidienne en fast food. Les m√©triques\nde performance sont g√©n√©ralement des moyennes plus ou moins sophistiqu√©es d‚Äô√©carts entre\nla pr√©diction et la valeur observ√©e.\n\nEn r√©sum√©, l‚Äôaide-m√©moire suivante, issue de l‚Äôaide de Scikit-Learn, peut d√©j√† donner de premiers enseignements sur les diff√©rentes familles de mod√®les:\n\n\n\n\n\n\n\n\n\nFigure¬†2: Une cheatsheet des algorithmes disponibles dans Scikit-Learn"
  },
  {
    "objectID": "content/modelisation/index.html#donn√©es",
    "href": "content/modelisation/index.html#donn√©es",
    "title": "Partie 3: mod√©liser",
    "section": "Donn√©es",
    "text": "Donn√©es\nLa plupart des exemples de cette partie s‚Äôappuient sur les r√©sultats des\n√©lections US 2020 au niveau comt√©s. Plusieurs bases sont utilis√©es pour\ncela :\n\nLes donn√©es √©lectorales sont une reconstruction √† partir des donn√©es du MIT election lab\npropos√©es sur Github par tonmcg\nou directement disponibles sur le site du MIT Election Lab\nLes donn√©es socio√©conomiques (population, donn√©es de revenu et de pauvret√©,\ntaux de ch√¥mage, variables d‚Äô√©ducation) proviennent de l‚ÄôUSDA (source)\nLe shapefile vient des donn√©es du Census Bureau. Le fichier peut\n√™tre t√©l√©charg√© directement depuis cet url:\nhttps://www2.census.gov/geo/tiger/GENZ2019/shp/cb_2019_us_county_20m.zip\n\nLe code pour construire une base unique √† partir de ces sources diverses\nest disponible ci-dessous :\n\n\nimport urllib\nimport urllib.request\nimport os\nimport zipfile\nfrom urllib.request import Request, urlopen\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\n\ndef download_url(url, save_path):\n    with urllib.request.urlopen(url) as dl_file:\n        with open(save_path, 'wb') as out_file:\n            out_file.write(dl_file.read())\n\n\ndef create_votes_dataframes():\n    \n  Path(\"data\").mkdir(parents=True, exist_ok=True)\n  \n  \n  download_url(\"https://www2.census.gov/geo/tiger/GENZ2019/shp/cb_2019_us_county_20m.zip\", \"data/shapefile\")\n  with zipfile.ZipFile(\"data/shapefile\", 'r') as zip_ref:\n      zip_ref.extractall(\"data/counties\")\n  \n  shp = gpd.read_file(\"data/counties/cb_2019_us_county_20m.shp\")\n  shp = shp[~shp[\"STATEFP\"].isin([\"02\", \"69\", \"66\", \"78\", \"60\", \"72\", \"15\"])]\n  shp\n  \n  df_election = pd.read_csv(\"https://raw.githubusercontent.com/tonmcg/US_County_Level_Election_Results_08-20/master/2020_US_County_Level_Presidential_Results.csv\")\n  df_election.head(2)\n  population = pd.read_excel(\"https://www.ers.usda.gov/webdocs/DataFiles/48747/PopulationEstimates.xls?v=290.4\", header = 2).rename(columns = {\"FIPStxt\": \"FIPS\"})\n  education = pd.read_excel(\"https://www.ers.usda.gov/webdocs/DataFiles/48747/Education.xls?v=290.4\", header = 4).rename(columns = {\"FIPS Code\": \"FIPS\", \"Area name\": \"Area_Name\"})\n  unemployment = pd.read_excel(\"https://www.ers.usda.gov/webdocs/DataFiles/48747/Unemployment.xls?v=290.4\", header = 4).rename(columns = {\"fips_txt\": \"FIPS\", \"area_name\": \"Area_Name\", \"Stabr\": \"State\"})\n  income = pd.read_excel(\"https://www.ers.usda.gov/webdocs/DataFiles/48747/PovertyEstimates.xls?v=290.4\", header = 4).rename(columns = {\"FIPStxt\": \"FIPS\", \"Stabr\": \"State\", \"Area_name\": \"Area_Name\"})\n  \n  \n  dfs = [df.set_index(['FIPS', 'State']) for df in [population, education, unemployment, income]]\n  data_county = pd.concat(dfs, axis=1)\n  df_election = df_election.merge(data_county.reset_index(), left_on = \"county_fips\", right_on = \"FIPS\")\n  df_election['county_fips'] = df_election['county_fips'].astype(str).str.lstrip('0')\n  shp['FIPS'] = shp['GEOID'].astype(str).str.lstrip('0')\n  votes = shp.merge(df_election, left_on = \"FIPS\", right_on = \"county_fips\")\n  \n  req = Request('https://dataverse.harvard.edu/api/access/datafile/3641280?gbrecs=false')\n  req.add_header('User-Agent', 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:77.0) Gecko/20100101 Firefox/77.0')\n  content = urlopen(req)\n  df_historical = pd.read_csv(content, sep = \"\\t\")\n  #df_historical = pd.read_csv('https://dataverse.harvard.edu/api/access/datafile/3641280?gbrecs=false', sep = \"\\t\")\n  \n  df_historical = df_historical.dropna(subset = [\"FIPS\"])\n  df_historical[\"FIPS\"] = df_historical[\"FIPS\"].astype(int)\n  df_historical['share'] = df_historical['candidatevotes']/df_historical['totalvotes']\n  df_historical = df_historical[[\"year\", \"FIPS\", \"party\", \"candidatevotes\", \"share\"]]\n  df_historical['party'] = df_historical['party'].fillna(\"other\")\n  \n  df_historical_wide = df_historical.pivot_table(index = \"FIPS\", values=['candidatevotes',\"share\"], columns = [\"year\",\"party\"])\n  df_historical_wide.columns = [\"_\".join(map(str, s)) for s in df_historical_wide.columns.values]\n  df_historical_wide = df_historical_wide.reset_index()\n  df_historical_wide['FIPS'] = df_historical_wide['FIPS'].astype(str).str.lstrip('0')\n  votes['FIPS'] = votes['GEOID'].astype(str).str.lstrip('0')\n  votes = votes.merge(df_historical_wide, on = \"FIPS\")\n  votes[\"winner\"] =  np.where(votes['votes_gop'] &gt; votes['votes_dem'], 'republican', 'democrats') \n\n  return votes\n\n\nCette partie n‚Äôest absolument pas exhaustive. Elle constitue un point\nd‚Äôentr√©e dans le sujet √† partir d‚Äôune s√©rie d‚Äôexemples sur un fil rouge.\nDe nombreux mod√®les plus appronfondis, que ce soit en √©conom√©trie ou en machine learning\nm√©riteraient d‚Äô√™tre √©voqu√©s. Pour les personnes d√©sirant en savoir plus sur les\nmod√®les √©conom√©triques, qui seront moins √©voqu√©s que ceux de machine learning,\nje recommande la lecture de Turrell and contributors (2021)."
  },
  {
    "objectID": "content/modelisation/index.html#r√©f√©rences",
    "href": "content/modelisation/index.html#r√©f√©rences",
    "title": "Partie 3: mod√©liser",
    "section": "R√©f√©rences",
    "text": "R√©f√©rences\n\n\nArcep. 2019. ‚ÄúL‚Äôempreinte Carbone Du Num√©rique.‚Äù Rapport de l‚ÄôArcep.\n\n\nAthey, Susan, and Guido W Imbens. 2019. ‚ÄúMachine Learning Methods That Economists Should Know About.‚Äù Annual Review of Economics 11: 685‚Äì725.\n\n\nCharpentier, Arthur, Emmanuel Flachaire, and Antoine Ly. 2018. ‚ÄúEconometrics and Machine Learning.‚Äù Economie Et Statistique 505 (1): 147‚Äì69.\n\n\nFriedman, Milton. 1953. ‚ÄúThe Methodology of Positive Economics.‚Äù In Essays in Positive Economics. Chicago: The University of Chicago Press.\n\n\nIzsak, Peter, Moshe Berchansky, and Omer Levy. 2021. ‚ÄúHow to Train BERT with an Academic Budget.‚Äù https://arxiv.org/abs/2104.07705.\n\n\nMullainathan, Sendhil, and Jann Spiess. 2017. ‚ÄúMachine Learning: An Applied Econometric Approach.‚Äù Journal of Economic Perspectives 31 (2): 87‚Äì106. https://doi.org/10.1257/jep.31.2.87.\n\n\nSalmon, Pierre. 2010. ‚ÄúLe Probl√®me Du r√©alisme Des Hypoth√®ses En √©conomie Politique.‚Äù\n\n\nStrubell, Emma, Ananya Ganesh, and Andrew McCallum. 2019. ‚ÄúEnergy and Policy Considerations for Deep Learning in NLP.‚Äù https://arxiv.org/abs/1906.02243.\n\n\nTurrell, Arthur, and contributors. 2021. Coding for Economists. Online. https://aeturrell.github.io/coding-for-economists."
  },
  {
    "objectID": "content/modelisation/index.html#footnotes",
    "href": "content/modelisation/index.html#footnotes",
    "title": "Partie 3: mod√©liser",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNous reviendrons de mani√®re √©pisodique\nsur ce principe des grands mod√®les de langage\nqui sont devenus, en quelques ann√©es,\ncentraux dans l‚Äô√©cosyst√®me de la data science mais sont √©galement\namen√©s √† devenir des outils grands publics, √† la mani√®re de ChatGPT.‚Ü©Ô∏é\nHistoriquement, cette approche n√©cessitait de disposer de donn√©es labellis√©es donc d‚Äô√™tre\ndans un cadre d‚Äôapprentissage supervis√©.\nCependant, avec l‚Äôutilisation de plus en plus\nfr√©quente de donn√©es non structur√©es, sans labels, a √©merg√© une approche int√©ressante\nqui ne n√©cessite plus forc√©ment de labelliser des volumes importants de donn√©es en amont :\nle reinforcement learning with human feedback.\nCet article d‚ÄôAndrew Ng revient sur la mani√®re dont cette approche\nchange la donne dans l‚Äôentra√Ænement ou le r√©-entra√Ænement de mod√®les.‚Ü©Ô∏é\nComme nous l‚Äôavons dit, cette diff√©renciation est un peu\ncaricaturale, notamment maintenant que les √©conomistes sont\nplus familiaris√©s aux concepts d‚Äô√©valuation de performance\npr√©dictive sur des sous-ensembles d‚Äôapprentissage et de test (mais\nl‚Äô√©volution est lente).\nLa recherche en machine learning est quant √† elle tr√®s dynamique\nsur la question de l‚Äôexplicabilit√© et de l‚Äôinterpr√©tabilit√©\ndes mod√®les de machine learning, notamment autour du concept\nde valeurs de Shapley.‚Ü©Ô∏é\nPour faire l‚Äôanalogie avec le cadre √©conom√©trique, les features sont les variables explicatives\nou covariates (la matrice \\(X\\)) et le label est la variable expliqu√©e (\\(y\\)).‚Ü©Ô∏é\nNous allons nous focaliser sur le cas binaire, le plus simple. Dans ce type de probl√®mes,\nla variable \\(y\\) a deux modalit√©s: gagnant-perdant, 0-1, oui-non‚Ä¶ N√©anmoins il existe de\nnombreux cas d‚Äôusage o√π la variable dispose de plus de modalit√©s, par exemples des\nscores de satisfaction entre 0 et 5 ou A et D. La mise en oeuvre de mod√®les est plus\ncomplexe mais l‚Äôid√©e g√©n√©rale est souvent de se ramener √† un ensemble de mod√®les dichotomiques\npour pouvoir appliquer des m√©triques simples et stables.‚Ü©Ô∏é"
  },
  {
    "objectID": "content/visualisation/matplotlib.html",
    "href": "content/visualisation/matplotlib.html",
    "title": "De beaux graphiques avec python : mise en pratique",
    "section": "",
    "text": "La pratique de la visualisation se fera, dans ce cours, en r√©pliquant des graphiques qu‚Äôon peut trouver sur\nla page de l‚Äôopen data de la ville de Paris\nici.\nCe TP vise √† initier :\nNous verrons par la suite la mani√®re de construire des cartes facilement avec\ndes formats √©quivalents.\nSi vous √™tes int√©ress√©s par R ,\nune version tr√®s proche de ce TP est\ndisponible dans ce cours d‚Äôintroduction √† R pour l‚ÄôENS.\nNote\n√ätre capable de construire des visualisations de donn√©es\nint√©ressantes est une comp√©tence n√©cessaire √† tout\ndata scientist ou chercheur. Pour am√©liorer\nla qualit√© de ces visualisations, il est recommand√©\nde suivre certains conseils donn√©s par des sp√©cialistes\nde la dataviz sur la s√©miologie graphique.\nLes bonnes visualisations de donn√©es, comme celles du New York Times,\nreposent certes sur des outils adapt√©s (des librairies JavaScript)\nmais aussi sur certaines r√®gles de repr√©sentation qui permettent\nde comprendre en quelques secondes le message d‚Äôune visualisation.\nCe post de blog\nest une ressource qu‚Äôil est utile de consulter r√©guli√®rement.\nCe post de blog d‚ÄôAlbert Rapp montre bien comment construire graduellement une bonne visualisation\nde donn√©es."
  },
  {
    "objectID": "content/visualisation/matplotlib.html#donn√©es",
    "href": "content/visualisation/matplotlib.html#donn√©es",
    "title": "De beaux graphiques avec python : mise en pratique",
    "section": "Donn√©es",
    "text": "Donn√©es\nUn sous-ensemble des donn√©es de Paris Open Data a √©t√© mis √† disposition\npour faciliter l‚Äôimport.\nIl s‚Äôagit d‚Äôune extraction, qui commence √† dater, des donn√©es disponibles\nsur le site o√π seules les colonnes\nqui servent √† cet exercice ont √©t√© conserv√©es."
  },
  {
    "objectID": "content/visualisation/matplotlib.html#premi√®res-productions-graphiques-avec-lapi-matplotlib-de-pandas",
    "href": "content/visualisation/matplotlib.html#premi√®res-productions-graphiques-avec-lapi-matplotlib-de-pandas",
    "title": "De beaux graphiques avec python : mise en pratique",
    "section": "Premi√®res productions graphiques avec l‚ÄôAPI Matplotlib de Pandas",
    "text": "Premi√®res productions graphiques avec l‚ÄôAPI Matplotlib de Pandas\nChercher √† produire une visualisation parfaite du premier coup est\nillusoire. Il est beaucoup plus r√©aliste d‚Äôam√©liorer graduellement\nune repr√©sentation graphique afin, petit √† petit, de mettre en\navant les effets de structure dans un jeu de donn√©es.\nNous allons donc commencer par nous repr√©senter la distribution\ndes passages aux principales stations de mesure.\nPour cela nous allons produire\nrapidement un barplot puis l‚Äôam√©liorer graduellement.\nDans cette partie, nous allons ainsi\nreproduire les deux premiers graphiques de la\npage d‚Äôanalyse des donn√©es :\nLes 10 compteurs avec la moyenne horaire la plus √©lev√©e et Les 10 compteurs ayant comptabilis√© le plus de v√©los. Les valeurs chiffr√©es des graphiques seront diff√©rentes de celles de la page en ligne, c‚Äôest normal, nous travaillons sur des donn√©es plus anciennes.\n\n\n Exercice 1 : Importer les donn√©es et produire un premier graphique\nLes donn√©es comportent plusieurs dimensions pouvant faire l‚Äôobjet d‚Äôune\nanalyse statistique. Il est donc n√©cessaire dans un premier temps\nde synth√©tiser celles-ci par des agr√©gations afin d‚Äôavoir un\ngraphique lisible.\n\nImporter les donn√©es de compteurs de v√©los. Vous pouvez utiliser l‚Äôurl https://minio.lab.sspcloud.fr/projet-formation/diffusion/python-datascientist/bike.csv.\n\n\n\n‚ö†Ô∏è Warning sur le format de donn√©es\n\nIl s‚Äôagit de donn√©es\ncompress√©es au format gzip, il faut donc utiliser l‚Äôoption compression = 'gzip'\n\n\nGarder les dix bornes √† la moyenne la plus √©lev√©e.\nComme pour obtenir un graphique ordonn√© du plus grand au plus petit,\nil faut avoir les donn√©es ordonn√©es du plus petit au\nplus grand (oui c‚Äôest bizarre mais c‚Äôest comme √ßa‚Ä¶), r√©ordonner\nles donn√©es ;\nEn premier lieu, sans se pr√©occuper des √©l√©ments de style ni de la beaut√©\ndu graphique, cr√©er la structure du barplot (diagramme en batons) de la\npage d‚Äôanalyse des donn√©es.\nPour pr√©parer le travail sur la deuxi√®me figure, ne conserver\nque les 10 compteurs ayant comptabilis√©s le plus de v√©los\nComme pour la question 3, cr√©er un barplot\npour reproduire la figure 2 de l‚Äôopen data parisien\n\n\n\n\n\n\nPremi√®res lignes n√©cessaires pour cet exercice :\n\n\n\n\n\n\n\n\n\n\nIdentifiant du compteur\nNom du compteur\nIdentifiant du site de comptage\nNom du site de comptage\nComptage horaire\nDate et heure de comptage\nDate d'installation du site de comptage\n\n\n\n\n0\n100003096-SC\n97 avenue Denfert Rochereau SO-NE\n100003096\n97 avenue Denfert Rochereau SO-NE\n1\n2019-08-01T02:00:00Z\n2012-02-22\n\n\n1\n100003096-SC\n97 avenue Denfert Rochereau SO-NE\n100003096\n97 avenue Denfert Rochereau SO-NE\n0\n2019-08-01T01:00:00Z\n2012-02-22\n\n\n2\n100003096-SC\n97 avenue Denfert Rochereau SO-NE\n100003096\n97 avenue Denfert Rochereau SO-NE\n0\n2019-08-01T04:00:00Z\n2012-02-22\n\n\n\n\n\n\n\n\n\n\n\n\nLes 10 principales stations √† l'issue de la question 2\n\n\n\n\n\n\n\n\n\n\nComptage horaire\n\n\nNom du compteur\n\n\n\n\n\n26 boulevard de M√©nilmontant SE-NO\n109.462847\n\n\n35 boulevard de Menilmontant NO-SE\n117.180643\n\n\n21 boulevard Saint Michel S-N\n117.730884\n\n\n67 boulevard Voltaire SE-NO\n119.208018\n\n\n72 boulevard Voltaire NO-SE\n124.391365\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1 sans travail sur le style:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2 sans travail sur le style:\n\n\n\n\n\n\n\n\n\n\n\n\nOn commence √† avoir quelque chose qui commence √† transmettre\nun message synth√©tique sur la nature des donn√©es.\nOn peut n√©anmoins remarquer plusieurs √©l√©ments probl√©matiques\n(par exemple les labels) mais\naussi des √©l√©ments ne correspondant pas (les titres des axes, etc.) ou\nmanquants (le nom du graphique‚Ä¶).\nComme les graphiques produits par Pandas suivent la logique tr√®s flexible\nde matplotlib, il est possible de les customiser. Cependant, c‚Äôest\nsouvent beaucoup de travail et la grammaire matplotlib n‚Äôest\npas aussi normalis√©e que celle de ggplot en R.\nIl peut √™tre pr√©f√©rable de directement\nutiliser seaborn, qui offre quelques arguments pr√™ts √† l‚Äôemploi."
  },
  {
    "objectID": "content/visualisation/matplotlib.html#utiliser-directement-seaborn",
    "href": "content/visualisation/matplotlib.html#utiliser-directement-seaborn",
    "title": "De beaux graphiques avec python : mise en pratique",
    "section": "Utiliser directement seaborn",
    "text": "Utiliser directement seaborn\nVous pouvez repartir des deux dataframes pr√©c√©dents. On va supposer qu‚Äôils se\nnomment df1 et df2.\nLa figure comporte maintenant un message mais il est encore peu\nlisible. Il y a plusieurs mani√®res de faire un barplot en seaborn. Les deux principales\nsont :\n\nsns.catplot ;\nsns.barplot.\n\nOn propose d‚Äôutiliser sns.catplot pour cet exercice.\n\n\n Exercice 2 : Un peu de style !\n\nR√©initialiser l‚Äôindex des dataframes df1 et df2\npour avoir une colonne ‚ÄòNom du compteur‚Äô. R√©ordonner les donn√©es\nde mani√®re d√©croissante pour obtenir un graphique ordonn√© dans\nle bon sens avec seaborn.\nRefaire le graphique pr√©c√©dent avec la fonction catplot de seaborn. Pour\ncontr√¥ler la taille du graphique vous pouvez utiliser les arguments height et\naspect.\nAjouter les titres des axes et le titre du graphique pour le premier graphique\nEssayez de colorer en rouge l‚Äôaxe des x. Vous pouvez pr√©-d√©finir un\nstyle avec sns.set_style(\"ticks\", {\"xtick.color\": \"red\"})\n\n\n\nA l‚Äôissue de la question 2, c‚Äôest-√†-dire en utilisant\nseaborn pour reproduire de mani√®re minimale\nun barplot, on obtient :\n\n\n\n\n\n\n\n\n\nApr√®s quelques r√©glages esth√©tiques, √† l‚Äôissue des questions 3 et 4,\non obtient une figure proche de celle de l‚Äôopen data parisien.\n\n\n\n\n\n\n\n\n\nOn comprend\nainsi que le boulevard de S√©bastopol est le plus emprunt√©,\nce qui ne vous suprendra pas si vous faites du v√©lo √† Paris.\nN√©anmoins, si vous n‚Äô√™tes pas familiers avec la g√©ographie parisienne,\ncela sera peu informatif pour vous, vous allez avoir besoin d‚Äôune\nrepr√©sentation graphique suppl√©mentaire: une carte ! Nous verrons\nceci lors d‚Äôun prochain chapitre.\n\n\n Exercice 3 : reproduire la figure 2\nEn suivant l‚Äôapproche graduelle de l‚Äôexercice 2,\nrefaire le graphique Les 10 compteurs ayant comptabilis√© le plus de v√©los.\n\n\nLes diagrammes en batons (barplot) sont extr√™mement communs mais\nqu‚Äôils transmettent. Sur le plan s√©miologique,\nles lollipop charts sont pr√©f√©rables : ils\ntransmettent la m√™me information mais avec moins de bruit\n(la largeur des barres du barplot noie un peu l‚Äôinformation).\n\n\n Exercice 3 (bis) : reproduire la figure 2 avec un lollipop chart\nEn suivant l‚Äôapproche graduelle de l‚Äôexercice 2,\nrefaire le graphique Les 10 compteurs ayant comptabilis√© le plus de v√©los.\n\n\n\n\nText(0, 0.5, 'La somme des v√©los comptabilis√©s sur la p√©riode s√©lectionn√©e')"
  },
  {
    "objectID": "content/visualisation/matplotlib.html#premi√®res-agr√©gations-temporelles",
    "href": "content/visualisation/matplotlib.html#premi√®res-agr√©gations-temporelles",
    "title": "De beaux graphiques avec python : mise en pratique",
    "section": "Premi√®res agr√©gations temporelles",
    "text": "Premi√®res agr√©gations temporelles\nOn va maintenant se concentrer sur la dimension spatiale de notre\njeu de donn√©es √† travers deux approches :\n\nUn diagramme en barre synth√©tisant l‚Äôinformation de notre jeu de donn√©es\nde mani√®re mensuelle ;\nDes s√©ries instructives sur la dynamique temporelle. Cela sera l‚Äôobjet de la prochaine partie.\n\nPour commencer, reproduisons la troisi√®me figure qui est, encore une fois,\nun barplot. La premi√®re question implique une premi√®re rencontre avec\nune donn√©e temporelle √† travers une op√©ration assez classique en s√©ries\ntemporelles : changer le format d‚Äôune date pour pouvoir faire une agr√©gation\n√† un pas de temps plus large.\n\n\n Exercice 4: barplot des comptages mensuels\n\nUtiliser to_datetime du package Pandas pour transformer la variable Date et heure de comptage\nen horodatage car le type de celle-ci a √©t√© mal interpr√©t√© √† la lecture du fichier.\nLe format √† utiliser est %Y-%m-%dT%H:%M:%SZ. Il peut √™tre n√©cessaire d‚Äôutiliser √©galement l‚Äôoption errors='coerce'\nCr√©er une variable month\ndont le format respecte, par exemple, le sch√©ma 2019-08 gr√¢ce √† la bonne option de la m√©thode dt.to_period\nAppliquer les conseils pr√©c√©dents pour construire et am√©liorer\ngraduellement un graphique afin d‚Äôobtenir une figure similaire\n√† la 3e production sur la page de l‚Äôopen data parisien.\nQuestion optionnelle: repr√©senter la m√™me information sous forme de lollipop\n\n\n\n\n\n\n\n\n\n\n\n\nmonth\nvalue\n\n\n\n\n0\n2019-08\n33.637536\n\n\n1\n2019-09\n55.831038\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\nSi vous pr√©f√©rez repr√©senter cela sous forme de lollipop1:\n\n\n\n\n\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;"
  },
  {
    "objectID": "content/visualisation/matplotlib.html#premi√®re-s√©rie-temporelle",
    "href": "content/visualisation/matplotlib.html#premi√®re-s√©rie-temporelle",
    "title": "De beaux graphiques avec python : mise en pratique",
    "section": "Premi√®re s√©rie temporelle",
    "text": "Premi√®re s√©rie temporelle\nIl est plus commun de repr√©senter sous forme de s√©rie\nles donn√©es ayant une dimension temporelle.\n\n\n Exercice 5: barplot des comptages mensuels\n\nCr√©er une variable day qui transforme l‚Äôhorodatage en format journalier\ndu type 2021-05-01 avec dt.day.\nReproduire la figure de la page d‚Äôopen data\n\n\n\n\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n\n&lt;matplotlib.collections.PolyCollection at 0x7fb390f88220&gt;"
  },
  {
    "objectID": "content/visualisation/matplotlib.html#des-graphiques-dynamiques-avec-plotly",
    "href": "content/visualisation/matplotlib.html#des-graphiques-dynamiques-avec-plotly",
    "title": "De beaux graphiques avec python : mise en pratique",
    "section": "Des graphiques dynamiques avec Plotly",
    "text": "Des graphiques dynamiques avec Plotly\n\nContexte\nL‚Äôinconv√©nient des figures avec ggplot est que celles-ci ne permettent\npas d‚Äôinteraction avec le lecteur. Toute l‚Äôinformation doit donc √™tre\ncontenue dans la figure ce qui peut la rendre difficile √† lire.\nSi la figure est bien faite, avec diff√©rents niveaux d‚Äôinformation, cela\npeut bien fonctionner.\nIl est n√©anmoins plus simple, gr√¢ce aux technologies web, de proposer des\nvisualisations √† plusieurs niveaux. Un premier niveau d‚Äôinformation, celui du\ncoup d‚Äôoeil, peut suffire √† assimiler les principaux messages de la\nvisualisation. Ensuite, un comportement plus volontaire de recherche\nd‚Äôinformation secondaire peut permettre d‚Äôen savoir plus. Les visualisations\nr√©actives, qui sont maintenant la norme dans le monde de la dataviz,\npermettent ce type d‚Äôapproche : le lecteur d‚Äôune visualisation peut passer\nsa souris √† la recherche d‚Äôinformation compl√©mentaire (par exemple les\nvaleurs exactes) ou cliquer pour faire appara√Ætre des informations compl√©mentaires\nsur la visualisation ou autour.\nCes visualisations reposent sur le m√™me triptyque que l‚Äôensemble de l‚Äô√©cosyst√®me\nweb : HTML, CSS et JavaScript. Les utilisateurs de Python\nne vont jamais manipuler directement ces langages, qui demandent une\ncertaine expertise, mais vont utiliser des librairies au niveau de R qui g√©n√®reront automatiquement tout le code HTML, CSS et JavaScript\npermettant de cr√©er la figure.\n\n\nLa librairie Plotly\nLe package Plotly est une surcouche √† la librairie Javascript\nPlotly.js qui permet de cr√©er et manipuler des objets graphiques de mani√®re\ntr√®s flexible afin de produire des objets r√©actifs sans avoir √† recourir\n√† Javascript.\nLe point d‚Äôentr√©e recommand√© est le module plotly.express\n(documentation ici) qui offre une arborescence\nriche mais n√©anmoins intuitive pour construire des graphiques\n(objets plotly.graph_objects.Figure) pouvant √™tre modifi√©s a posteriori\nsi besoin (par exemple pour customiser les axes).\n\n\n Visualiser les figures produites par Plotly\nDans un notebook Jupyter classique, les lignes suivantes de code permettent\nd‚Äôafficher le r√©sultat d‚Äôune commande Plotly sous un bloc de code :\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected = True)\nPour JupyterLab, l‚Äôextension jupyterlab-plotly s‚Äôav√®re n√©cessaire:\n!jupyter labextension install jupyterlab-plotly\n\n\n\n\nR√©plication de l‚Äôexemple pr√©c√©dent avec Plotly\nLes repr√©sentations fig√©es comme celles ci-dessus\nsont approri√©es pour des rapports ou articles.\nN√©anmoins\nLes modules suivants seront n√©cessaires pour construire des graphiques\navec plotly:\n\n\n Exercice 7: un barplot avec Plotly\nL‚Äôobjectif est de reconstuire le premier diagramme en barre rouge avec Plotly.\n\nR√©alisez le graphique en utilisant la fonction ad√©quate avec plotly.express et‚Ä¶\n\nNe pas prendre le\nth√®me par d√©faut mais un √† fond blanc, pour avoir un r√©sultat ressemblant\n√† celui propos√© sur le site de l‚Äôopen-data.\nPour la couleur rouge,\nvous pouvez utiliser l‚Äôargument color_discrete_sequence.\nNe pas oublier de nommer les axes\nPensez √† la couleur du texte de l‚Äôaxe inf√©rieur\n\nTester un autre th√®me, √† fond sombre. Pour les couleurs, faire un\ngroupe stockant les trois plus fortes valeurs puis les autres.\n\n\n\nLa premi√®re question permet de construire le graphique suivant :\n\n\n\n                                                \n\n\nAlors qu‚Äôavec le th√®me sombre (question 2), on obtient :\n\n\n\n                                                \n\n\nCette repr√©sentation montre bien le caract√®re sp√©cial de l‚Äôann√©e 2020. Pour\nrappeller au lecteur distrait la nature particuli√®re de la p√©riode, marqu√©e\npar un premier confinement qu‚Äôon voit bien dans les donn√©es, on pourrait,\navec l‚Äôaide de la documentation,\najouter deux barres verticales pour marquer les dates de d√©but et\nde fin de cette p√©riode."
  },
  {
    "objectID": "content/visualisation/matplotlib.html#bonus",
    "href": "content/visualisation/matplotlib.html#bonus",
    "title": "De beaux graphiques avec python : mise en pratique",
    "section": "Bonus",
    "text": "Bonus\nEn bonus, l‚Äô√©tat d‚Äôesprit des habitu√©s de ggplot2\nquand ils d√©couvrent plotnine:"
  },
  {
    "objectID": "content/visualisation/matplotlib.html#footnotes",
    "href": "content/visualisation/matplotlib.html#footnotes",
    "title": "De beaux graphiques avec python : mise en pratique",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nJ‚Äôai retir√© la couleur sur l‚Äôaxe des ordonn√©es qui, je trouve,\napporte peu √† la figure voire d√©grade la compr√©hension du message.‚Ü©Ô∏é"
  },
  {
    "objectID": "content/manipulation/07_dask.html",
    "href": "content/manipulation/07_dask.html",
    "title": "Introduction √† dask gr√¢ce aux donn√©es DVF",
    "section": "",
    "text": "Pour essayer les exemples pr√©sents dans ce tutoriel :\nLa documentation compl√®te sur Dask se trouve sur https://docs.dask.org/.\nLe projet requiert l‚Äôinstallation de dask. Afin d‚Äôavoir\nla distribution compl√®te on utilise la commande suivante :"
  },
  {
    "objectID": "content/manipulation/07_dask.html#pourquoi-utiliser-dask",
    "href": "content/manipulation/07_dask.html#pourquoi-utiliser-dask",
    "title": "Introduction √† dask gr√¢ce aux donn√©es DVF",
    "section": "Pourquoi utiliser Dask ?",
    "text": "Pourquoi utiliser Dask ?\nOn peut se r√©f√©rer √† la page https://docs.dask.org/en/stable/why.html\nPlusieurs points sont mis en avant dans la documentation officielle et sont r√©sum√©s ci-dessous:\n- Dask ressemble fortement en termes de syntaxe √† pandas et numpy ;\n- Dask peut √™tre utilis√© sur un ordinateur seul ou sur un cloud cluster. Avec Dask, on peut traiter des bases de 100GB sur un ordinateur portable, voire m√™me 1TB sans m√™me avoir besoin d‚Äôun cluster big data ;\n- Dask requiert peu de temps d‚Äôinstallation puisqu‚Äôil peut √™tre install√© avec le gestionnaire de packages conda (il est m√™me livr√© dans la distribution par d√©faut d‚ÄôAnaconda)\n\nComment Dask se compare √† Spark ?\nDans le monde du big-data, un √©cosyst√®me concurrent existe: Spark. Globalement, lorsqu‚Äôon a compris la logique\nde l‚Äôun, il est tr√®s facile de faire la transition vers l‚Äôautre si besoin1.\n\n\nSpark est √©crit en Scala √† l‚Äôorigine. Le package pyspark permet d‚Äô√©crire en Python et s‚Äôassure de la traduction en Python afin d‚Äôinteragir avec les machines virtuelles Java (JVM) n√©cessaires pour la parall√©lisation des op√©rations Spark. Dask est quant √† lui √©crit en Python, ce qui est un √©cosyst√®me plus l√©ger. Pour gagner en performance, il permet d‚Äôinteragir avec du code C/C++ entre autres ;\nL‚Äôinstallation de Spark est plus lourde que celle de Dask\nSpark est un projet Apache en lui-m√™me alors que Dask intervient comme une composante de l‚Äôunivers Python;\nSpark est un peu plus vieux (2010 versus 2014 pour Dask) ;\nSpark permet de tr√®s bien faire des op√©rations classiques SQL et des ETLs, et proposer ses propres librairies de parall√©lisation de mod√®les de machine learning. Pour faire du machine learning avec Spark il faut aller piocher dans Spark MLLib. Dask permet quant √† lui de bien interagir avec scikit-learn et de faire de la mod√©lisation.\n\nGlobalement, il faut retenir que Dask comme Spark ne sont int√©ressants que pour des donn√©es dont le traitement engendre des probl√®mes de RAM. Autrement, il\nvaut mieux se contenter de pandas."
  },
  {
    "objectID": "content/manipulation/07_dask.html#d√©monstration-de-quelques-features-de-dask",
    "href": "content/manipulation/07_dask.html#d√©monstration-de-quelques-features-de-dask",
    "title": "Introduction √† dask gr√¢ce aux donn√©es DVF",
    "section": "D√©monstration de quelques features de Dask",
    "text": "D√©monstration de quelques features de Dask\n\nPr√©sentation du Dask.DataFrame\nNous allons utiliser les donn√©es immobili√®res DVF pour montrer quelques √©l√©ments clefs de Dask.\n\n# Import dvf files \nimport pandas as pd\nimport dask.dataframe as dd\n\nd_urls = {\n    \"2019\" : 'https://www.data.gouv.fr/fr/datasets/r/3004168d-bec4-44d9-a781-ef16f41856a2',\n    \"2020\" : \"https://www.data.gouv.fr/fr/datasets/r/90a98de0-f562-4328-aa16-fe0dd1dca60f\",\n    \"2021\": \"https://www.data.gouv.fr/fr/datasets/r/817204ac-2202-4b4a-98e7-4184d154d98c\"\n}\n\n\ndef import_dvf_one_year(year, dict_url = d_urls):\n    df = pd.read_csv(dict_url[year], sep = \"|\", decimal=\",\")\n    df[\"year\"] = year\n    return df\n\ndef import_dvf_all_years(dict_url = d_urls):\n    dfs = [import_dvf_one_year(y, dict_url) for y in dict_url.keys()]\n    df = pd.concat(dfs).reset_index()\n    df = df.drop([\"level_0\", \"level_1\"], axis=1)\n    return df\n\nDans un premier temps, on va utiliser pandas pour\nimporter une ann√©e de donn√©es (mill√©sime 2019), ces derni√®res tenant en m√©moire\nsur un ordinateur normalement dot√© en RAM2:\n\ndvf = import_dvf_one_year(\"2019\")\ndvf.shape\ndvf.head()\n\n/tmp/ipykernel_3840/455432909.py:13: DtypeWarning:\n\nColumns (18,23,24,26,28,41) have mixed types. Specify dtype option on import or set low_memory=False.\n\n\n\n\n\n\n\n\n\n\nIdentifiant de document\nReference document\n1 Articles CGI\n2 Articles CGI\n3 Articles CGI\n4 Articles CGI\n5 Articles CGI\nNo disposition\nDate mutation\nNature mutation\n...\nNombre de lots\nCode type local\nType local\nIdentifiant local\nSurface reelle bati\nNombre pieces principales\nNature culture\nNature culture speciale\nSurface terrain\nyear\n\n\n\n\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n04/01/2019\nVente\n...\n1\n2.0\nAppartement\nNaN\n20.0\n1.0\nNaN\nNaN\nNaN\n2019\n\n\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n04/01/2019\nVente\n...\n1\n3.0\nD√©pendance\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\n2019\n\n\n2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n04/01/2019\nVente\n...\n2\n2.0\nAppartement\nNaN\n62.0\n3.0\nNaN\nNaN\nNaN\n2019\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n08/01/2019\nVente\n...\n0\n1.0\nMaison\nNaN\n90.0\n4.0\nS\nNaN\n940.0\n2019\n\n\n4\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n07/01/2019\nVente\n...\n0\n1.0\nMaison\nNaN\n101.0\n5.0\nS\nNaN\n490.0\n2019\n\n\n\n\n5 rows √ó 44 columns\n\n\n\nIci on travaille sur un DataFrame d‚Äôenviron 3.5 millions de lignes et 44 variables.\nL‚Äôobjet dvf est un pandas.DataFrame\nqui tient en m√©moire sur le SSP Cloud ou sur les serveurs utilis√©s\npour construire ce site web.\n\n\n Exercice 1\nOn aurait pu lire directement les csv dans un dask.DataFrame avec le read_csv de dask. Comme exercice, vous pouvez essayer de le faire\npour une ann√©e (analogue de la fonction import_dvf_one_year) puis sur toutes les donn√©es (analogue de la fonction import_dvf_all_years).\n\n\nOn peut cr√©er une structure Dask directement √† partir\nd‚Äôun DataFrame pandas avec la m√©thode from_pandas.\n\ndvf_dd = dd.from_pandas(dvf, npartitions=10) \ndvf_dd\n\nDask DataFrame Structure:\n\n\n\n\n\n\n\nIdentifiant de document\nReference document\n1 Articles CGI\n2 Articles CGI\n3 Articles CGI\n4 Articles CGI\n5 Articles CGI\nNo disposition\nDate mutation\nNature mutation\nValeur fonciere\nNo voie\nB/T/Q\nType de voie\nCode voie\nVoie\nCode postal\nCommune\nCode departement\nCode commune\nPrefixe de section\nSection\nNo plan\nNo Volume\n1er lot\nSurface Carrez du 1er lot\n2eme lot\nSurface Carrez du 2eme lot\n3eme lot\nSurface Carrez du 3eme lot\n4eme lot\nSurface Carrez du 4eme lot\n5eme lot\nSurface Carrez du 5eme lot\nNombre de lots\nCode type local\nType local\nIdentifiant local\nSurface reelle bati\nNombre pieces principales\nNature culture\nNature culture speciale\nSurface terrain\nyear\n\n\nnpartitions=10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nint64\nstring\nstring\nfloat64\nfloat64\nstring\nstring\nstring\nstring\nfloat64\nstring\nstring\nint64\nfloat64\nstring\nint64\nstring\nstring\nfloat64\nstring\nfloat64\nstring\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nint64\nfloat64\nstring\nfloat64\nfloat64\nfloat64\nstring\nstring\nfloat64\nstring\n\n\n362713\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3264417\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3627129\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n\n\n\nDask Name: to_pyarrow_string, 2 graph layers\n\n\nPour souligner la diff√©rence avec un pandas.DataFrame,\nl‚Äôaffichage diff√®re. Seule la structure du dask.DataFrame\nest affich√©e et non son contenu car les donn√©es\ndask ne sont pas charg√©es en m√©moire.\n\n\n Warning\nAttention, Dask ne peut cr√©er un Dask.DataFrame √† partir d‚Äôun pandas.DataFrame multi-index√©.\nDans ce cas il a fallu faire un reset_index() pour avoir un unique index.\n\n\nOn a ainsi la structure de notre dask.DataFrame, soit environ 3.5 millions de lignes, avec 44 colonnes en 10 partitions, soit environ 350 000 observations par partition.\nIl faut savoir que Dask produit des Array, Bag et DataFrames, qui fonctionnent comme dans Numpy et Pandas (il est possible de cr√©er d‚Äôautres structures ad hoc, cf plus loin).\nDask, comme Spark et en fait comme la plupart des frameworks permettant de\ntraiter des donn√©es plus volumineuses que la RAM disponible,\nrepose sur le principe du partitionnement et de la parall√©lisation\ndes op√©rations. Les donn√©es ne sont jamais import√©es dans leur\nensemble mais par bloc. Un plan des op√©rations √† effectuer est\nensuite appliquer sur chaque bloc (nous reviendrons\nsur ce principe), ind√©pendamment. La particularit√© de Dask,\npar rapport √† Spark,\nest que chaque bloc est un pandas.DataFrame, ce qui\nrend tr√®s facile l‚Äôapplication de manipulations de donn√©es\ntraditionnelles sur des sources volumineuses:\n\n\n\n\n\nLe site de Dask cite une r√®gle qui est la suivante :\n\n‚ÄúHave 5 to 10 times as much RAM as the size of your dataset‚Äù,\n@mckinney2017apache, 10 things I hate about pandas\n\nSur disque, en sauvegardant en CSV, on\nobtient une base de 1.4GB. Si l‚Äôon suit la r√®gle du pouce donn√©e plus haut, on va avoir besoin d‚Äôune RAM entre 7-14GB pour traiter la donn√©e, en fonction de nos traitements qui seront plus ou moins intensifs. Autrement dit, si on a moins de 8GB de RAM, il devient int√©ressant de faire appel √† dask, sinon il vaut mieux privil√©gier pandas (sauf si on fait des\ntraitements tr√®s intensifs en calculs).\nIl existe un autre objet dask, les Array pour reprendre la logique de numpy. De la m√™me mani√®re qu‚Äôun dask.DataFrame est en quelque sorte un ensemble de pandas.DataFrame, un dask.Array est un ensemble de numpy.Array qui sont plus importants en taille que la RAM. On pourra utiliser les op√©rations courantes numpy avec dask de la m√™me mani√®re que le dask DataFrame r√©plique la logique du pandas DataFrame.\n\n\n Hint\nLe choix du nombre de partition (10) est arbitraire ici. Bien qu‚Äôon puisse\ntrouver des r√®gles du pouce pour fixer un nombre optimal de\npartitions, cela d√©pend de beaucoup de facteurs et, en pratique,\nrien ne remplace l‚Äôessai-erreur. Par exemple, la documentation Dask recommande des blocs d‚Äôenviron\n100MB\nce qui peut convenir pour des ordinateurs √† la RAM limit√©e mais n‚Äôa pas\nforc√©ment de sens pour des machines ayant 16GB de RAM.\nUn nombre important de partition va permettre de faire des op√©rations\nsur des petits blocs de donn√©es, ce qui permettra de gagner en vitesse\nd‚Äôex√©cution. Le prix √† payer est beaucoup d‚Äôinput/output car\nDask va passer du temps √† lire beaucoup de blocs de donn√©es et √©crire\ndes bases interm√©diaires.\n\n\nOn peut acc√©der aux index que couvrent les partitions de la mani√®re suivante :\n\ndvf_dd.divisions\n\n(0,\n 362713,\n 725426,\n 1088139,\n 1450852,\n 1813565,\n 2176278,\n 2538991,\n 2901704,\n 3264417,\n 3627129)\n\n\nAutrement dit, la premi√®re partition couvrira les lignes 0 √† 362713. La deuxi√®me les lignes 362714 √† 725426, etc.\nEt on peut directement acc√©der √† une partition gr√¢ce aux crochets []:\n\ndvf_dd.partitions[0]\n\nDask DataFrame Structure:\n\n\n\n\n\n\n\nIdentifiant de document\nReference document\n1 Articles CGI\n2 Articles CGI\n3 Articles CGI\n4 Articles CGI\n5 Articles CGI\nNo disposition\nDate mutation\nNature mutation\nValeur fonciere\nNo voie\nB/T/Q\nType de voie\nCode voie\nVoie\nCode postal\nCommune\nCode departement\nCode commune\nPrefixe de section\nSection\nNo plan\nNo Volume\n1er lot\nSurface Carrez du 1er lot\n2eme lot\nSurface Carrez du 2eme lot\n3eme lot\nSurface Carrez du 3eme lot\n4eme lot\nSurface Carrez du 4eme lot\n5eme lot\nSurface Carrez du 5eme lot\nNombre de lots\nCode type local\nType local\nIdentifiant local\nSurface reelle bati\nNombre pieces principales\nNature culture\nNature culture speciale\nSurface terrain\nyear\n\n\nnpartitions=1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nint64\nstring\nstring\nfloat64\nfloat64\nstring\nstring\nstring\nstring\nfloat64\nstring\nstring\nint64\nfloat64\nstring\nint64\nstring\nstring\nfloat64\nstring\nfloat64\nstring\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nint64\nfloat64\nstring\nfloat64\nfloat64\nfloat64\nstring\nstring\nfloat64\nstring\n\n\n362713\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n\n\n\nDask Name: blocks, 3 graph layers\n\n\n\n\nLa ‚Äúlazy evaluation‚Äù\nDask fait de la ‚Äúlazy evaluation‚Äù. Cela signifie que le r√©sultat n‚Äôest calcul√© que si on le demande explicitement. Dans le cas, contraire, ce que l‚Äôon appelle un dask task graph est produit (on verra plus bas comment voir ce graph).\nPour demander explicitement le r√©sultat d‚Äôun calcul, il faut utiliser la\nm√©thode compute.\nA noter que certaines m√©thodes vont d√©clencher un compute directement, comme par exemple len ou head.\nPar exemple, pour afficher le contenu des 100 premi√®res lignes :\n\ndvf_dd.loc[0:100,:].compute()\n\n\n\n\n\n\n\n\nIdentifiant de document\nReference document\n1 Articles CGI\n2 Articles CGI\n3 Articles CGI\n4 Articles CGI\n5 Articles CGI\nNo disposition\nDate mutation\nNature mutation\n...\nNombre de lots\nCode type local\nType local\nIdentifiant local\nSurface reelle bati\nNombre pieces principales\nNature culture\nNature culture speciale\nSurface terrain\nyear\n\n\n\n\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n04/01/2019\nVente\n...\n1\n2.0\nAppartement\nNaN\n20.0\n1.0\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n2019\n\n\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n04/01/2019\nVente\n...\n1\n3.0\nD√©pendance\nNaN\n0.0\n0.0\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n2019\n\n\n2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n04/01/2019\nVente\n...\n2\n2.0\nAppartement\nNaN\n62.0\n3.0\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n2019\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n08/01/2019\nVente\n...\n0\n1.0\nMaison\nNaN\n90.0\n4.0\nS\n&lt;NA&gt;\n940.0\n2019\n\n\n4\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n07/01/2019\nVente\n...\n0\n1.0\nMaison\nNaN\n101.0\n5.0\nS\n&lt;NA&gt;\n490.0\n2019\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n96\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n04/01/2019\nVente\n...\n0\n1.0\nMaison\nNaN\n48.0\n2.0\nS\n&lt;NA&gt;\n935.0\n2019\n\n\n97\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n07/01/2019\nVente\n...\n0\nNaN\n&lt;NA&gt;\nNaN\nNaN\nNaN\nP\n&lt;NA&gt;\n3264.0\n2019\n\n\n98\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n18/01/2019\nVente\n...\n0\nNaN\n&lt;NA&gt;\nNaN\nNaN\nNaN\nP\n&lt;NA&gt;\n2870.0\n2019\n\n\n99\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n07/01/2019\nVente\n...\n0\nNaN\n&lt;NA&gt;\nNaN\nNaN\nNaN\nT\n&lt;NA&gt;\n1423.0\n2019\n\n\n100\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n03/01/2019\nVente\n...\n0\nNaN\n&lt;NA&gt;\nNaN\nNaN\nNaN\nP\n&lt;NA&gt;\n93.0\n2019\n\n\n\n\n101 rows √ó 44 columns\n\n\n\nCe qui est pratique avec dask.dataframe c‚Äôest que de nombreuses m√©thodes sont semblables √† celles de pandas. Par exemple, si l‚Äôon souhaite connaitre les types de locaux pr√©sents dans la base en 2019:\n\ndvf_dd.loc[:,\"Type local\"].value_counts().compute()\n\nType local\nMaison                                      712958\nAppartement                                 656819\nD√©pendance                                  496414\nLocal industriel. commercial ou assimil√©    143265\nName: count, dtype: int64[pyarrow]\n\n\nA titre de comparaison, comparons les temps de calculs entre pandas et dask ici:\n\nimport time\nstart_time = time.time()\ndvf_dd.loc[:,\"Type local\"].value_counts().compute()\nprint(f\"{time.time() - start_time} seconds\")\n\n3.3733575344085693 seconds\n\n\n\nstart_time = time.time()\ndvf.loc[:,\"Type local\"].value_counts()\nprint(f\"{time.time() - start_time} seconds\")\n\n0.12499499320983887 seconds\n\n\nOn se rend compte que le pandas.DataFrame a un temps de calcul plus court, mais c‚Äôest parce que dask va nous servir avant tout √† lire des bases dont le traitement exc√®de notre RAM. Donc, cette comparaison n‚Äôexistera tout simplement pas car le pandas.DataFrame n‚Äôaura pas √©t√© charg√© en RAM. On voit dans cet exemple que lorsque le traitement du DataFrame tient en RAM, l‚Äôutilisation de Dask est inutile.\nLes m√©thodes dans Dask peuvent √™tre chain√©es, comme dans pandas, par exemple, on pourra √©crire:\n\nmean_by_year = dvf_dd.loc[~dvf_dd[\"Surface terrain\"].isna(),[\"Surface terrain\", \"year\"]].groupby(\"year\").mean()\n\n\nmean_by_year.compute()\n\n\n\n\n\n\n\n\nSurface terrain\n\n\nyear\n\n\n\n\n\n2019\n3064.685954\n\n\n\n\n\n\n\nLe principe de la lazy evaluation est donc d‚Äôannoncer √† Dask\nqu‚Äôon va effectuer une s√©rie d‚Äôop√©ration qui ne vont se r√©aliser\nque lorsqu‚Äôon fera un appel √† compute. Dask, quant √† lui,\nse chargera d‚Äôoptimiser les traitements.\nComme le plan d‚Äôaction peut devenir difficile √† suivre si on\nd√©sire effectuer beaucoup d‚Äôop√©rations encha√Æn√©es, on peut\nvouloir visualiser le graph de computation de dask.\nAvec celui-ci, on voit toutes les √©tapes que jusqu‚Äôici dask n‚Äôa pas execut√©\net qu‚Äôil va devoir ex√©cuter pour calculer le r√©sultat (compute()).\n\nmean_by_year.dask\n\nEn l‚Äôoccurence on voit l‚Äôencha√Ænement des √©tapes\nfrom_pandas(), getitem, isna, inv et loc-series qui r√©sultent de nos filtres sur le DataFrame. Ensuite,\non voit les √©tapes de groupby et, enfin, pour calculer la moyenne il convient de faire la somme et la division. Toutes ces √©tapes vont √™tre effectu√©es quand on appelle compute() et pas avant (lazy evaluation).\nAfin de voir la structure du dask.DataFrame on peut utiliser la m√©thode visualize()\n\ndvf_dd.visualize() # attention graphviz est requis\n\n\n\n\n\n\n\n\n\n\n Note\ngraphviz est requis pour ce graphique. S‚Äôil n‚Äôest pas install√© dans votre environnement, faire :\n!pip install graphviz\n\n\nPour construire de v√©ritables pipelines de donn√©es,\nles principes du pipe de pandas √©voqu√© dans cette partie du cours et celui des pipelines scikit, √©voqu√© dans un chapitre d√©di√©\nont √©t√© import√©s dans dask.\n\n\nProbl√®mes de lecture dus √† des types probl√©matiques\nLa m√©thode read_csv de dask va inf√©rer les types du DataFrame √† partir d‚Äô√©chantillon, et va les impl√©menter sur tout le DataFrame seulement au moment d‚Äôune √©tape compute.\nIl peut donc y avoir des erreurs de types d√ªs √† un √©chantillon ne prenant pas en compte certains cas particuliers, causant des erreurs dans la lecture du fichier.\nDans ce cas, et comme de mani√®re g√©n√©rale avec pandas, il peut √™tre recommand√© de faire appel au param√®tre dtype de read_csv - qui est un dict - (la doc de dask nous dit aussi que l‚Äôon peut augmenter la taille de l‚Äô√©chantllon sample)."
  },
  {
    "objectID": "content/manipulation/07_dask.html#utiliser-dask-avec-le-format-parquet",
    "href": "content/manipulation/07_dask.html#utiliser-dask-avec-le-format-parquet",
    "title": "Introduction √† dask gr√¢ce aux donn√©es DVF",
    "section": "Utiliser Dask avec le format parquet",
    "text": "Utiliser Dask avec le format parquet\nLe format parquet tend √† devenir le format\nde r√©f√©rence dans le monde de la data science.\nUne pr√©sentation extensive de celui-ci est disponible\ndans le chapitre d√©di√©.\ndask permet de lire le format parquet, et plus pr√©cis√©ment d‚Äôutiliser des fonctionnalit√©s sp√©cifiques √† ce format. La lecture et l‚Äô√©criture en parquet reposent par d√©faut sur pyarrow. On peut aussi utiliser fastparquet et pr√©ciser dans la lecture/√©criture ce que l‚Äôon souhaite des deux.\n\ndvf_net = dvf.loc[:,[ 'Date mutation', 'Nature mutation', 'Valeur fonciere', 'Commune', \n       'Code commune', 'Type local', 'Identifiant local', 'Surface reelle bati',\n       'Nombre pieces principales', 'Nature culture',\n       'Nature culture speciale', 'Surface terrain', 'year']]\n\nOn va utiliser l‚Äôengine par d√©faut pour\nl‚Äô√©criture de parquet qui est pyarrow (faire pip install pyarrow si vous ne l‚Äôavez pas d√©j√† install√©). to_parquet qui est une m√©thode pandas a √©t√© √©galement √©tendue aux objets dask:\n\ndvf_net.to_parquet(\"dvf/\", partition_cols=\"year\")\n\nLorsqu‚Äôil est partitionn√©, le format parquet am√®ne √† une structure\nde fichiers similaire √† celle-ci:\npath\n‚îî‚îÄ‚îÄ to\n    ‚îî‚îÄ‚îÄ table\n        ‚îú‚îÄ‚îÄ gender=male\n        ‚îÇ   ‚îú‚îÄ‚îÄ ...\n        ‚îÇ   ‚îÇ\n        ‚îÇ   ‚îú‚îÄ‚îÄ country=US\n        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data.parquet\n        ‚îÇ   ‚îú‚îÄ‚îÄ country=CN\n        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data.parquet\n        ‚îÇ   ‚îî‚îÄ‚îÄ ...\n        ‚îî‚îÄ‚îÄ gender=female\n            ‚îú‚îÄ‚îÄ ...\n            ‚îÇ\n            ‚îú‚îÄ‚îÄ country=US\n            ‚îÇ   ‚îî‚îÄ‚îÄ data.parquet\n            ‚îú‚îÄ‚îÄ country=CN\n            ‚îÇ   ‚îî‚îÄ‚îÄ data.parquet\n            ‚îî‚îÄ‚îÄ ...\nOn peut alors facilement traiter un sous-√©chantillon des donn√©es,\npar exemple l‚Äôann√©e 2019:\n\ndvf_2019 = dd.read_parquet(\"dvf/year=2019/\", columns=[\"Date mutation\", \"Valeur fonciere\"]) # On peut s√©lectionner directement les deux colonnes\n\nLorsqu‚Äôil faudra passer √† l‚Äô√©chelle, on changera le chemin en \"dvf/\npour utiliser l‚Äôensemble des donn√©es."
  },
  {
    "objectID": "content/manipulation/07_dask.html#a-quoi-sert-persist",
    "href": "content/manipulation/07_dask.html#a-quoi-sert-persist",
    "title": "Introduction √† dask gr√¢ce aux donn√©es DVF",
    "section": "A quoi sert persist ?",
    "text": "A quoi sert persist ?\nPar d√©faut, compute ex√©cute l‚Äôensemble du plan et ne conserve\nen m√©moire que le r√©sultat de celui-ci. Les donn√©es interm√©diaires\nne sont pas conserv√©es. Si on d√©sire r√©utiliser une partie de celui-ci,\npar exemple les premi√®res √©tapes, on devra donc r√©-effectuer\nles calculs.\nIl est possible de garder une partie des donn√©es en m√©moire avec persist(). Les donn√©es sont sauvegard√©es dans des objets appel√©s Futures. Cela peut √™tre int√©ressant si un bloc particulier de donn√©es est utilis√© dans plusieurs compute ou si l‚Äôon a besoin de voir ce qu‚Äôil y a √† l‚Äôint√©rieur souvent.\n\ndvf_dd_mem = dvf_dd.persist()\n\n\nstart_time = time.time()\ndvf_dd_mem.head()\nprint(f\"{time.time() - start_time} seconds\")\n\n\nstart_time = time.time()\ndvf_dd.head()\nprint(f\"{time.time() - start_time} seconds\")\n\n0.33945131301879883 seconds\n\n\nOn a bien un temps plus important avec le dask.DataFrame initial, compar√© avec celui sur lequel on a utilis√© persist. L‚Äôop√©ration qu‚Äôon r√©alise ici √©tant peu complexe, la diff√©rence n‚Äôest pas substantielle. Elle serait beaucoup plus marqu√©e avec un jeu de donn√©es plus volumineux ou des √©tapes intensives en calcul."
  },
  {
    "objectID": "content/manipulation/07_dask.html#aller-plus-loin-utiliser-le-decorator-dask.delayed-pour-parall√©liser-du-code",
    "href": "content/manipulation/07_dask.html#aller-plus-loin-utiliser-le-decorator-dask.delayed-pour-parall√©liser-du-code",
    "title": "Introduction √† dask gr√¢ce aux donn√©es DVF",
    "section": "Aller plus loin: Utiliser le decorator dask.delayed pour parall√©liser du code",
    "text": "Aller plus loin: Utiliser le decorator dask.delayed pour parall√©liser du code\nIl est possible de parall√©liser des fonctions par exemple en utilisant le decorator dask.delayed. Cela permet de rendre les fonctions lazy. Cela signifie que lorsqu‚Äôon appelle la fonction, un delayed object est construit. Pour avoir le r√©sultat, il faut faire un compute. Pour aller plus loin: https://tutorial.dask.org/03_dask.delayed.html.\nPrenons par exemple des fonctions permettant de calculer\ndes aires et des p√©rim√®tres. Comme il s‚Äôagit d‚Äôune op√©ration\ntr√®s peu complexe, on ajoute un d√©lai de calcul avec time.sleep\npour que le timer ne nous sugg√®re pas que l‚Äôop√©ration est\ninstantan√©e.\n\ndef aire_carre(longueur):\n    time.sleep(1)\n    return longueur**2\n\ndef perimetre_carre(longueur):\n    time.sleep(1)\n    return 4*longueur\n\ndef ajout_aire_perim(a, b):\n    return a + b\n\nSans timer, c‚Äôest-√†-dire de mani√®re classique,\non ferait nos appels de fonctions de la\nmani√®re suivante :\n\nstart_time = time.time()\ncar1 = aire_carre(7)\ncar2 = perimetre_carre(9)\ncar3 = ajout_aire_perim(car1, car2)\ncar3\nprint(time.time() - start_time)\n\n2.001528739929199\n\n\nAvec le d√©corateur dask.delayed, on d√©finit\nnos fonctions de la mani√®re suivante :\n\nimport dask\n\n@dask.delayed\ndef aire_carre(longueur):\n    time.sleep(1)\n    return longueur**2\n\n@dask.delayed\ndef perimetre_carre(longueur):\n    time.sleep(1)\n    return 4*longueur\n\n@dask.delayed\ndef ajout_aire_perim(a, b):\n    return a + b\n\nL‚Äôappel de fonctions est identique\n\ncar1 = aire_carre(7)\ncar2 = perimetre_carre(9)\ncar3 = ajout_aire_perim(car1, car2)\n\nCependant, en fait rien n‚Äôa √©t√© calcul√©, si l‚Äôon souhaite le r√©sultat, il faut appeler compute:\n\nstart_time = time.time()\ncar3.compute()\nprint(time.time() - start_time)\n\n1.002249002456665\n\n\nIci l‚Äôint√©r√™t est assez limit√©, mais on voit que l‚Äôon r√©duit quand m√™me de 2 √† 1 seconde le temps de calcul. Mais l‚Äôid√©e derri√®re est que l‚Äôon a transform√© car3 en un objet Delayed. Cela a g√©n√©r√© un task graph permettant de parall√©liser certaines op√©rations.\nIci il est important de noter que les fonctions que l‚Äôon parall√©lise doivent mettre un certain temps, sinon il n‚Äôy aura pas de gain de performance (si on retire le time.sleep il n‚Äôy a pas de gain de performance car le fait de parall√©liser rajoute en fait du temps vu que chaque fonction a un temps de calcul trop faible pour que la parall√©lisation soit int√©ressante).\n\ncar3.visualize() # on peut visualiser le task graph et voir ce qui est fait en parall√®le \n\n\n\n\n\n\n\n\nIl y a des exercices int√©ressants dans la doc de Dask sur les objets Delayed, notamment sur la parall√©lisation de s√©quence de traitement de donn√©es. Ils donnent l‚Äôexemple d‚Äôun ensemble de csv ayant le m√™me format dont on veut r√©sumer un indicateur final. On peut appliquer le decorator √† une fonction permettant de lire le csv, puis utiliser une boucle for pour lire chaque fichier et appliquer les traitements. Ensuite, il faudra appeler compute sur l‚Äôobjet final que l‚Äôon souhaite.\nPour aller plus loin sur l‚Äôutilisation de Dask sur un cluster voir https://tutorial.dask.org/04_distributed.html."
  },
  {
    "objectID": "content/manipulation/07_dask.html#remerciements",
    "href": "content/manipulation/07_dask.html#remerciements",
    "title": "Introduction √† dask gr√¢ce aux donn√©es DVF",
    "section": "Remerciements",
    "text": "Remerciements\nCe chapitre a √©t√© r√©dig√© avec Rapha√´le Adjerad."
  },
  {
    "objectID": "content/manipulation/07_dask.html#footnotes",
    "href": "content/manipulation/07_dask.html#footnotes",
    "title": "Introduction √† dask gr√¢ce aux donn√©es DVF",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCe site web est test√© sur les serveurs d‚Äôint√©gration\ncontinue mis √† disposition gratuitement par Github. Ces\nderniers sont des machines √† la m√©moire limit√©e. Il s‚Äôagit\nd‚Äôun bon exemple d‚Äôint√©r√™t de dask: avec pandas, on ne\npeut tester les exemples sur les trois mill√©simes disponibles\ncar la volum√©trie acc√®de la RAM disponible.‚Ü©Ô∏é\nCe site web est test√© sur les serveurs d‚Äôint√©gration\ncontinue mis √† disposition gratuitement par Github. Ces\nderniers sont des machines √† la m√©moire limit√©e. Il s‚Äôagit\nd‚Äôun bon exemple d‚Äôint√©r√™t de dask: avec pandas, on ne\npeut tester les exemples sur les trois mill√©simes disponibles\ncar la volum√©trie acc√®de la RAM disponible.‚Ü©Ô∏é"
  },
  {
    "objectID": "content/manipulation/04c_API_TP.html",
    "href": "content/manipulation/04c_API_TP.html",
    "title": "R√©cup√©rer des donn√©es avec des API depuis Python",
    "section": "",
    "text": "La partie utilisant l‚ÄôAPI DVF n‚Äôest plus √† jour, elle sera mise √† jour prochainement."
  },
  {
    "objectID": "content/manipulation/04c_API_TP.html#introduction-quest-ce-quune-api",
    "href": "content/manipulation/04c_API_TP.html#introduction-quest-ce-quune-api",
    "title": "R√©cup√©rer des donn√©es avec des API depuis Python",
    "section": "Introduction : Qu‚Äôest-ce qu‚Äôune API ?",
    "text": "Introduction : Qu‚Äôest-ce qu‚Äôune API ?\n\nD√©finition\nPour expliquer le principe d‚Äôune API, je vais reprendre le d√©but de\nla fiche d√©di√©e dans la documentation collaborative\nutilitR que je recommande de lire :\n\nUne Application Programming Interface (ou API) est une interface de programmation qui permet d‚Äôutiliser une application existante pour restituer des donn√©es. Le terme d‚ÄôAPI peut √™tre para√Ætre intimidant, mais il s‚Äôagit simplement d‚Äôune fa√ßon de mettre √† disposition des donn√©es : plut√¥t que de laisser l‚Äôutilisateur consulter directement des bases de donn√©es (souvent volumineuses et complexes), l‚ÄôAPI lui propose de formuler une requ√™te qui est trait√©e par le serveur h√©bergeant la base de donn√©es, puis de recevoir des donn√©es en r√©ponse √† sa requ√™te.\nD‚Äôun point de vue informatique, une API est une porte d‚Äôentr√©e clairement identifi√©e par laquelle un logiciel offre des services √† d‚Äôautres logiciels (ou utilisateurs). L‚Äôobjectif d‚Äôune API est de fournir un point d‚Äôacc√®s √† une fonctionnalit√© qui soit facile √† utiliser et qui masque les d√©tails de la mise en oeuvre. Par exemple, l‚ÄôAPI Sirene permet de r√©cup√©rer la raison sociale d‚Äôune entreprise √† partir de son identifiant Siren en interrogeant le r√©f√©rentiel disponible sur Internet directement depuis un script R, sans avoir √† conna√Ætre tous les d√©tails du r√©pertoire Sirene.\n√Ä l‚ÄôInsee comme ailleurs, la connexion entre les bases de donn√©es pour les nouveaux projets tend √† se r√©aliser par des API. L‚Äôacc√®s √† des donn√©es par des API devient ainsi de plus en plus commun et est amen√© √† devenir une comp√©tence de base de tout utilisateur de donn√©es.\nutilitR\n\n\n\nAvantages des API\nA nouveau, citons la documentation utilitR :\nLes API pr√©sentent de multiples avantages :\n\n\nLes API rendent les programmes plus reproductibles. En effet, gr√¢ce aux API, il est possible de mettre √† jour facilement les donn√©es utilis√©es par un programme si celles-ci √©voluent. Cette flexibilit√© accrue pour l‚Äôutilisateur √©vite au producteur de donn√©es d‚Äôavoir √† r√©aliser de multiples extractions, et r√©duit le probl√®me de la coexistence de versions diff√©rentes des donn√©es.\nGr√¢ce aux API, l‚Äôutilisateur peut extraire facilement une petite partie d‚Äôune base de donn√©es plus cons√©quente.\nLes API permettent de mettre √† disposition des donn√©es tout en limitant le nombre de personnes ayant acc√®s aux bases de donn√©es elles-m√™mes.\nGr√¢ce aux API, il est possible de proposer des services sur mesure pour les utilisateurs (par exemple, un acc√®s sp√©cifique pour les gros utilisateurs).\n\nutilitR\n\nL‚Äôutilisation accrue d‚ÄôAPI dans le cadre de strat√©gies open-data est l‚Äôun\ndes piliers des 15 feuilles de route minist√©rielles\nen mati√®re d‚Äôouverture, de circulation et de valorisation des donn√©es publiques.\n\n\nUtilisation des API\nCitons encore une fois\nla documentation utilitR :\n\nUne API peut souvent √™tre utilis√©e de deux fa√ßons : par une interface Web, et par l‚Äôinterm√©diaire d‚Äôun logiciel (R, Python‚Ä¶). Par ailleurs, les API peuvent √™tre propos√©es avec un niveau de libert√© variable pour l‚Äôutilisateur :\n\nsoit en libre acc√®s (l‚Äôutilisation n‚Äôest pas contr√¥l√©e et l‚Äôutilisateur peut utiliser le service comme bon lui semble)‚ÄØ;\nsoit via la g√©n√©ration d‚Äôun compte et d‚Äôun jeton d‚Äôacc√®s qui permettent de s√©curiser l‚Äôutilisation de l‚ÄôAPI et de limiter le nombre de requ√™tes.\n\nutilitR\n\nDe nombreuses API n√©cessitent une authentification, c‚Äôest-√†-dire un\ncompte utilisateur afin de pouvoir acc√©der aux donn√©es.\nDans un premier temps,\nnous regarderons exclusivement les API ouvertes sans restriction d‚Äôacc√®s.\nCertains exercices et exemples permettront n√©anmoins d‚Äôessayer des API\navec restrictions d‚Äôacc√®s."
  },
  {
    "objectID": "content/manipulation/04c_API_TP.html#requ√™ter-une-api",
    "href": "content/manipulation/04c_API_TP.html#requ√™ter-une-api",
    "title": "R√©cup√©rer des donn√©es avec des API depuis Python",
    "section": "Requ√™ter une API",
    "text": "Requ√™ter une API\n\nPrincipe g√©n√©ral\n\nL‚Äôutilisation de l‚Äôinterface Web est utile dans une d√©marche exploratoire mais trouve rapidement ses limites, notamment lorsqu‚Äôon consulte r√©guli√®rement l‚ÄôAPI. L‚Äôutilisateur va rapidement se rendre compte qu‚Äôil est beaucoup plus commode d‚Äôutiliser une API via un logiciel de traitement pour automatiser la consultation ou pour r√©aliser du t√©l√©chargement de masse. De plus, l‚Äôinterface Web n‚Äôexiste pas syst√©matiquement pour toutes les API.\nLe mode principal de consultation d‚Äôune API consiste √† adresser une requ√™te √† cette API via un logiciel adapt√© (R, Python, Java‚Ä¶). Comme pour l‚Äôutilisation d‚Äôune fonction, l‚Äôappel d‚Äôune API comprend des param√®tres qui sont d√©taill√©es dans la documentation de l‚ÄôAPI.\nutilitR\n\nVoici les √©l√©ments importants √† avoir en t√™te sur les requ√™tes (j‚Äôemprunte encore\n√† utilitR) :\n\nLe point d‚Äôentr√©e d‚Äôun service offert par une API se pr√©sente sous la forme d‚Äôune URL (adresse web).\nChaque service propos√© par une API a sa propre URL. Par exemple, dans le cas de l‚ÄôOpenFood Facts,\nl‚ÄôURL √† utiliser pour obtenir des informations sur un produit particulier (l‚Äôidentifiant 737628064502)\nest https://world.openfoodfacts.org/api/v0/product/737628064502.json\nCette URL doit √™tre compl√©t√©e avec diff√©rents param√®tres qui pr√©cisent la requ√™te (par exemple l‚Äôidentifiant Siren). Ces param√®tres viennent s‚Äôajouter √† l‚ÄôURL, souvent √† la suite de ?. Chaque service propos√© par une API a ses propres param√®tres, d√©taill√©s dans la documentation.\nLorsque l‚Äôutilisateur soumet sa requ√™te, l‚ÄôAPI lui renvoie une r√©ponse structur√©e contenant l‚Äôensemble des informations demand√©es. Le r√©sultat envoy√© par une API est majoritairement aux formats JSON ou XML (deux formats dans lesquels les informations sont hi√©rarchis√©es de mani√®re emboit√©e). Plus rarement, certains services proposent une information sous forme plate (de type csv).\n\nDu fait de la dimension hi√©rarchique des formats JSON ou XML,\nle r√©sultat n‚Äôest pas toujours facile √† r√©cup√©rer mais\nPython propose d‚Äôexcellents outils pour cela (meilleurs que ceux de R).\nCertains packages, notamment json, facilitent l‚Äôextraction de champs d‚Äôune sortie d‚ÄôAPI.\nDans certains cas, des packages sp√©cifiques √† une API ont √©t√© cr√©√©s pour simplifier l‚Äô√©criture d‚Äôune requ√™te ou la r√©cup√©ration du r√©sultat. Par exemple, le package\npynsee\npropose des options qui seront retranscrites automatiquement dans l‚ÄôURL de\nrequ√™te pour faciliter le travail sur les donn√©es Insee.\n\n\nIllustration avec une API de l‚ÄôAdeme pour obtenir des diagnostics energ√©tiques\nLe diagnostic de performance √©nerg√©tique (DPE)\nrenseigne sur la performance √©nerg√©tique d‚Äôun logement ou d‚Äôun b√¢timent,\nen √©valuant sa consommation d‚Äô√©nergie et son impact en terme d‚Äô√©missions de gaz √† effet de serre.\nLes donn√©es des performances √©nerg√©tiques des b√¢timents sont\nmises √† disposition par l‚ÄôAdeme.\nComme ces donn√©es sont relativement\nvolumineuses, une API peut √™tre utile lorsqu‚Äôon ne s‚Äôint√©resse\nqu‚Äô√† un sous-champ des donn√©es.\nUne documentation et un espace de test de l‚ÄôAPI sont disponibles\nsur le site API GOUV1.\nSupposons qu‚Äôon d√©sire r√©cup√©rer une centaine de valeurs pour la commune\nde Villieu-Loyes-Mollon dans l‚ÄôAin (code Insee 01450).\nL‚ÄôAPI comporte plusieurs points d‚Äôentr√©e. Globalement, la racine\ncommune est :\n\nhttps://koumoul.com/data-fair/api/v1/datasets/dpe-france\n\nEnsuite, en fonction de l‚ÄôAPI d√©sir√©e, on va ajouter des √©l√©ments\n√† cette racine. En l‚Äôoccurrence, on va utiliser\nl‚ÄôAPI field qui permet de r√©cup√©rer des lignes en fonction d‚Äôun\nou plusieurs crit√®res (pour nous, la localisation g√©ographique):\nL‚Äôexemple donn√© dans la documentation technique est\n\nGET https://koumoul.com/data-fair/api/v1/datasets/dpe-france/values/{field}\n\nce qui en Python se traduira par l‚Äôutilisation de la m√©thode get du\npackage Request\nsur un url dont la structure est la suivante :\n\nil commencera par https://koumoul.com/data-fair/api/v1/datasets/dpe-france/values/ ;\nil sera ensuite suivi par des param√®tres de recherche. Le champ {field}\ncommence ainsi g√©n√©ralement par un ? qui permet ensuite de sp√©cifier des param√®tres\nsous la forme nom_parameter=value\n\nA la lecture de la documentation, les premiers param√®tres qu‚Äôon d√©sire :\n\nLe nombre de pages, ce qui nous permet d‚Äôobtenir un certain nombre d‚Äô√©chos. On\nva seulement r√©cup√©rer 10 pages ce qui correspond √† une centaine d‚Äô√©chos. On va\nn√©anmoins pr√©ciser qu‚Äôon veut 100 √©chos\nLe format de sortie. On va privil√©gier le JSON qui est un format standard dans le\nmonde des API. Python offre beaucoup de flexibilit√© gr√¢ce √† l‚Äôun de\nses objets de base, √† savoir le dictionnaire (type dict), pour manipuler de tels\nfichiers\nLe code commune des donn√©es qu‚Äôon d√©sire obtenir. Comme on l‚Äôa √©voqu√©,\non va r√©cup√©rer les donn√©es dont le code commune est 01450. D‚Äôapr√®s la doc,\nil convient de passer le code commune sous le format:\ncode_insee_commune_actualise:{code_commune}. Pour √©viter tout risque de\nmauvais formatage, on va utiliser %3A pour signifier :, %2A pour signifier * et\n%22 pour signifier \".\nD‚Äôautres param√®tres annexes, sugg√©r√©s par la documentation\n\nCela nous donne ainsi un URL dont la structure est la suivante :\n\ncode_commune = \"01450\"\nsize = 100\napi_root = \"https://koumoul.com/data-fair/api/v1/datasets/dpe-france/lines\"\nurl_api = f\"{api_root}?page=1&after=10&format=json&q_mode=simple&qs=code_insee_commune_actualise\" + \"%3A%22\" + f\"{code_commune}\" + \"%22\" + f\"&size={size}&select=\" + \"%2A&sampling=neighbors\"\n\nSi vous introduisez cet URL dans votre navigateur, vous devriez aboutir\nsur un JSON non format√©2. En Python,\non peut utiliser requests pour r√©cup√©rer les donn√©es3 :\n\nimport requests\nimport pandas as pd\n\nreq = requests.get(url_api)\nwb = req.json()\n\nPrenons par exemple les 1000 premiers caract√®res du r√©sultat, pour se donner\nune id√©e du r√©sultat et se convaincre que notre filtre au niveau\ncommunal est bien pass√© :\nprint(req.content[:1000])\nb‚Äô{‚Äútotal‚Äù: 121,‚Äúnext‚Äù: ‚Äúhttps://koumoul.com/data-fair/api/v1/datasets/dpe-france/lines?after=102721&format=json&q_mode=simple&qs=code_insee_commune_actualise%3A%2201450%22&size=100&select=*&sampling=neighbors‚Äù,‚Äúresults‚Äù: [\\n {‚Äúclasse_consommation_energie‚Äù: ‚ÄúD‚Äù,‚Äútr001_modele_dpe_type_libelle‚Äù: ‚ÄúVente‚Äù,‚Äúannee_construction‚Äù: 1947,‚Äú_geopoint‚Äù: ‚Äú45.925922,5.229964‚Äù,‚Äúlatitude‚Äù: 45.925922,‚Äúsurface_thermique_lot‚Äù: 117.16,‚Äú_i‚Äù: 487,‚Äútr002_type_batiment_description‚Äù: ‚ÄúMaison Individuelle‚Äù,‚Äúgeo_adresse‚Äù: ‚ÄúRue de la Brugni8re 01800 Villieu-Loyes-Mollon‚Äù,‚Äú_rand‚Äù: 23215,‚Äúcode_insee_commune_actualise‚Äù: ‚Äú01450‚Äù,‚Äúestimation_ges‚Äù: 53,‚Äúgeo_score‚Äù: 0.4,‚Äúclasse_estimation_ges‚Äù: ‚ÄúE‚Äù,‚Äúnom_methode_dpe‚Äù: ‚ÄúM9thode Facture‚Äù,‚Äútv016_departement_code‚Äù: ‚Äú01‚Äù,‚Äúconsommation_energie‚Äù: 178,‚Äúdate_etablissement_dpe‚Äù: ‚Äú2013-06-13‚Äù,‚Äúlongitude‚Äù: 5.229964,‚Äú_score‚Äù: null,‚Äô\nIci, il n‚Äôest m√™me pas n√©cessaire en premi√®re approche\nd‚Äôutiliser le package json, l‚Äôinformation\n√©tant d√©j√† tabul√©e dans l‚Äô√©cho renvoy√© (on a la m√™me information pour tous les pays):\nOn peut donc se contenter de Pandas pour transformer nos donn√©es en\nDataFrame et Geopandas pour convertir en donn√©es\ng√©ographiques :\n\nimport pandas as pandas\nimport geopandas as gpd\n\ndef get_dpe_from_url(url):\n\n    req = requests.get(url)\n    wb = req.json()\n    df = pd.json_normalize(wb[\"results\"])\n\n    dpe = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude), crs = 4326)\n    dpe = dpe.dropna(subset = ['longitude', 'latitude'])\n\n    return dpe\n\ndpe = get_dpe_from_url(url_api)\ndpe.head(2)\n\n/opt/mamba/lib/python3.9/site-packages/geopandas/_compat.py:123: UserWarning:\n\nThe Shapely GEOS version (3.12.0-CAPI-1.18.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n\n/tmp/ipykernel_3709/2008334648.py:2: UserWarning:\n\nShapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n\nimport os\nos.environ['USE_PYGEOS'] = '0'\nimport geopandas\n\nIn a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n\n\n\n\n\n\n\n\n\n\nclasse_consommation_energie\ntr001_modele_dpe_type_libelle\nannee_construction\n_geopoint\nlatitude\nsurface_thermique_lot\n_i\ntr002_type_batiment_description\ngeo_adresse\n_rand\n...\nclasse_estimation_ges\nnom_methode_dpe\ntv016_departement_code\nconsommation_energie\ndate_etablissement_dpe\nlongitude\n_score\n_id\nversion_methode_dpe\ngeometry\n\n\n\n\n0\nD\nVente\n1947\n45.925922,5.229964\n45.925922\n117.16\n487\nMaison Individuelle\nRue de la Brugni√®re 01800 Villieu-Loyes-Mollon\n23215\n...\nE\nM√©thode Facture\n01\n178.00\n2013-06-13\n5.229964\nNone\n04JZNel3WCJYcfsHpCcHv\nNaN\nPOINT (5.22996 45.92592)\n\n\n2\nD\nNeuf\n2006\n45.923421,5.223777\n45.923421\n90.53\n689\nMaison Individuelle\nChemin du Pont-vieux 01800 Villieu-Loyes-Mollon\n401672\n...\nC\nFACTURE - DPE\n01\n227.99\n2013-06-11\n5.223777\nNone\nrkdV2lJn2wxaidVBaHBFY\nV2012\nPOINT (5.22378 45.92342)\n\n\n\n\n2 rows √ó 23 columns\n\n\n\nEssayons de repr√©senter sur une carte ces DPE avec les\nann√©es de construction des logements.\nAvec Folium, on obtient la carte interactive suivante :\n\nimport seaborn as sns\nimport folium\n\npalette = sns.color_palette(\"coolwarm\", 8)\n\ndef interactive_map_dpe(dpe):\n\n    # convert in number\n    dpe['color'] = [ord(dpe.iloc[i]['classe_consommation_energie'].lower()) - 96 for i in range(len(dpe))]\n    dpe = dpe.loc[dpe['color']&lt;=7]\n    dpe['color'] = [palette.as_hex()[x] for x in dpe['color']]\n\n\n    center = dpe[['latitude', 'longitude']].mean().values.tolist()\n    sw = dpe[['latitude', 'longitude']].min().values.tolist()\n    ne = dpe[['latitude', 'longitude']].max().values.tolist()\n\n    m = folium.Map(location = center, tiles='OpenStreetMap')\n\n    # I can add marker one by one on the map\n    for i in range(0,len(dpe)):\n        folium.Marker([dpe.iloc[i]['latitude'], dpe.iloc[i]['longitude']],\n                    popup=f\"Ann√©e de construction: {dpe.iloc[i]['annee_construction']}, &lt;br&gt;DPE: {dpe.iloc[i]['classe_consommation_energie']}\",\n                    icon=folium.Icon(color=\"black\", icon=\"home\", icon_color = dpe.iloc[i]['color'])).add_to(m)\n\n    m.fit_bounds([sw, ne])\n\n    return m\n\nm = interactive_map_dpe(dpe)\n\n/opt/mamba/lib/python3.9/site-packages/geopandas/geodataframe.py:1443: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n# Afficher la carte\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nUn catalogue incomplet d‚ÄôAPI existantes\nDe plus en plus de sites mettent des API √† disposition des d√©veloppeurs et autres curieux.\nPour en citer quelques-unes tr√®s connues :\n\nTwitter  : https://dev.twitter.com/rest/public\nFacebook  : https://developers.facebook.com/\nInstagram  : https://www.instagram.com/developer/\nSpotify  : https://developer.spotify.com/web-api/\n\nCependant, il est int√©ressant de ne pas se restreindre √† celles-ci dont les\ndonn√©es ne sont pas toujours les plus int√©ressantes. Beaucoup\nde producteurs de donn√©es, priv√©s comme publics, mettent √† disposition\nleurs donn√©es sous forme d‚ÄôAPI.\n\nAPI gouv : beaucoup d‚ÄôAPI officielles de l‚ÄôEtat fran√ßais\net acc√®s √† de la documentation\nInsee : https://api.insee.fr/catalogue/ et pynsee\nP√¥le Emploi : https://www.emploi-store-dev.fr/portail-developpeur-cms/home.html\nSNCF : https://data.sncf.com/api\nBanque Mondiale : https://datahelpdesk.worldbank.org/knowledgebase/topics/125589"
  },
  {
    "objectID": "content/manipulation/04c_API_TP.html#lapi-dvf-acc√©der-√†-des-donn√©es-de-transactions-immobili√®res-simplement",
    "href": "content/manipulation/04c_API_TP.html#lapi-dvf-acc√©der-√†-des-donn√©es-de-transactions-immobili√®res-simplement",
    "title": "R√©cup√©rer des donn√©es avec des API depuis Python",
    "section": "L‚ÄôAPI DVF : acc√©der √† des donn√©es de transactions immobili√®res simplement",
    "text": "L‚ÄôAPI DVF : acc√©der √† des donn√©es de transactions immobili√®res simplement\n‚ö†Ô∏è Cette partie n√©cessite une mise √† jour pour privil√©gier l‚ÄôAPI DVF du Cerema.\nLe site DVF (demandes de valeurs fonci√®res) permet de visualiser toutes les donn√©es relatives aux mutations √† titre on√©reux (ventes de maisons, appartements, garages‚Ä¶) r√©alis√©es durant les 5 derni√®res ann√©es.\nUn site de visualisation est disponible sur https://app.dvf.etalab.gouv.fr/.\nCe site est tr√®s complet quand il s‚Äôagit de conna√Ætre le prix moyen au m√®tre\ncarr√© d‚Äôun quartier ou de comparer des r√©gions entre elles.\nL‚ÄôAPI DVF permet d‚Äôaller plus loin afin de r√©cup√©rer les r√©sultats dans\nun logiciel de traitement de donn√©es. Elle a √©t√© r√©alis√©e par\nChristian Quest et le code\nsource est disponible sur Github .\nLes crit√®res de recherche sont les suivants :\n- code_commune = code INSEE de la commune (ex: 94068)\n- section = section cadastrale (ex: 94068000CQ)\n- numero_plan = identifiant de la parcelle, (ex: 94068000CQ0110)\n- lat + lon + dist (optionnel): pour une recherche g√©ographique, dist est par d√©faut un rayon de 500m\n- code_postal\nLes filtres de s√©lection compl√©mentaires :\n- nature_mutation (Vente, etc)\n- type_local (Maison, Appartement, Local, D√©pendance)\nLes requ√™tes sont de la forme : http://api.cquest.org/dvf?code_commune=29168.\n\n\n Exercice 1 : Exploiter l'API DVF\n\nRechercher toutes les transactions existantes dans DVF √† Plogoff (code commune 29168, en Bretagne).\nAfficher les cl√©s du JSON et en d√©duire le nombre de transactions r√©pertori√©es.\nN‚Äôafficher que les transactions portant sur des maisons.\nUtiliser l‚ÄôAPI geo pour\nr√©cup√©rer le d√©coupage communal de la ville de Plogoff.\nRepr√©senter l‚Äôhistogramme des prix de vente.\n\nN‚Äôh√©sitez pas √† aller plus loin en jouant sur des variables de\ngroupes par exemple.\n\n\nLe r√©sultat de la question 2 devrait\nressembler au DataFrame suivant :\nL‚Äôhistogramme des prix de vente (question 4) aura l‚Äôaspect suivant :\nOn va faire une carte des ventes en affichant le prix de l‚Äôachat.\nLa cartographie r√©active sera pr√©sent√©e dans les chapitres\nconsacr√©s √† la visualisation de donn√©es.\nSupposons que le DataFrame des ventes s‚Äôappelle ventes. Il faut d‚Äôabord le\nconvertir\nen objet geopandas.\nAvant de faire une carte, on va convertir\nles limites de la commune de Plogoff en geoJSON pour faciliter\nsa repr√©sentation avec folium\n(voir la doc geopandas √† ce propos):\nPour repr√©senter graphiquement, on peut utiliser le code suivant (essayez de\nle comprendre et pas uniquement de l‚Äôex√©cuter).\n\n# Afficher la carte\nm"
  },
  {
    "objectID": "content/manipulation/04c_API_TP.html#g√©ocoder-des-donn√©es-gr√¢ce-aux-api-officielles",
    "href": "content/manipulation/04c_API_TP.html#g√©ocoder-des-donn√©es-gr√¢ce-aux-api-officielles",
    "title": "R√©cup√©rer des donn√©es avec des API depuis Python",
    "section": "G√©ocoder des donn√©es gr√¢ce aux API officielles",
    "text": "G√©ocoder des donn√©es gr√¢ce aux API officielles\nPour pouvoir faire cet exercice\n\n!pip install xlrd\n\nJusqu‚Äô√† pr√©sent, nous avons travaill√© sur des donn√©es o√π la dimension\ng√©ographique √©tait d√©j√† pr√©sente ou relativement facile √† int√©grer.\nCe cas id√©al ne se rencontre pas n√©cessairement dans la pratique.\nOn dispose parfois de localisations plus ou moins pr√©cises et plus ou\nmoins bien formatt√©es pour d√©terminer la localisation de certains\nlieux.\nDepuis quelques ann√©es, un service officiel de g√©ocodage a √©t√© mis en place.\nCelui-ci est gratuit et permet de mani√®re efficace de coder des adresses\n√† partir d‚Äôune API. Cette API, connue sous le\nnom de la Base d‚ÄôAdresses Nationale (BAN) a b√©n√©fici√© de la mise en commun de donn√©es de plusieurs\nacteurs (collectivit√©s locales, Poste) et de comp√©tences d‚Äôacteurs\ncomme Etalab. La documentation de celle-ci est disponible √† l‚Äôadresse\nhttps://api.gouv.fr/les-api/base-adresse-nationale.\nPour illustrer la mani√®re de g√©ocoder des donn√©es avec Python, nous\nallons partir de la base\ndes r√©sultats des auto-√©coles √† l‚Äôexamen du permis sur l‚Äôann√©e 2018.\nCes donn√©es n√©cessitent un petit peu de travail pour √™tre propres √† une\nanalyse statistique.\nApr√®s avoir renomm√© les colonnes, nous n‚Äôallons conserver que\nles informations relatives au permis B (permis voiture classique) et\nles auto-√©coles ayant pr√©sent√© au moins 20 personnes √† l‚Äôexamen.\n\nimport pandas as pd\nimport xlrd\nimport geopandas as gpd\n\ndf = pd.read_excel(\"https://www.data.gouv.fr/fr/datasets/r/d4b6b072-8a7d-4e04-a029-8cdbdbaf36a5\", header = [0,1])\n\n# Le Excel a des noms de colonne emboit√©es, \n# on nettoie\nindex_0 = [\"\" if df.columns[i][0].startswith(\"Unnamed\") else df.columns[i][0] for i in range(len(df.columns))]\nindex_1 = [df.columns[i][1] for i in range(len(df.columns))]\nkeep_index = [True if el in ('', \"B\") else False for el in index_0] \ncols = [index_0[i] + \" \" + index_1[i].replace(\"+\", \"_\") for i in range(len(df.columns))]\ndf.columns = cols\ndf = df.loc[:, keep_index]\ndf.columns = df.columns.str.replace(\"(^ |¬∞)\", \"\", regex = True).str.replace(\" \", \"_\")\n\n# On garde le sous-√©chantillon d'int√©r√™t\ndf = df.dropna(subset = ['B_NB'])\ndf = df.loc[~df[\"B_NB\"].astype(str).str.contains(\"(\\%|\\.)\"),:]\ndf['B_NB'] = df['B_NB'].astype(int)\ndf['B_TR'] = df['B_TR'].str.replace(\",\", \".\").str.replace(\"%\",\"\").astype(float)\ndf = df.loc[df[\"B_NB\"]&gt;20]\n\n/tmp/ipykernel_3709/3216706257.py:19: UserWarning:\n\nThis pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n\n\n\nSur cet √©chantillon, le taux de r√©ussite moyen √©tait, en 2018, de 58.02%\nNos informations g√©ographiques prennent la forme suivante :\n\ndf.loc[:,['Adresse','CP','Ville']].head(5)\n\n\n\n\n\n\n\n\nAdresse\nCP\nVille\n\n\n\n\n0\n56 RUE CHARLES ROBIN\n01000\nBOURG EN BRESSE\n\n\n2\n7, avenue Revermont\n01250\nCeyzeriat\n\n\n3\n72 PLACE DE LA MAIRIE\n01000\nSAINT-DENIS LES BOURG\n\n\n4\n6 RUE DU LYCEE\n01000\nBOURG EN BRESSE\n\n\n5\n9 place Edgard Quinet\n01000\nBOURG EN BRESSE\n\n\n\n\n\n\n\nAutrement dit, nous disposons d‚Äôune adresse, d‚Äôun code postal et d‚Äôun nom\nde ville. Ces informations peuvent servir √† faire une recherche\nsur la localisation d‚Äôune auto-√©cole puis, √©ventuellement, de se restreindre\n√† un sous-√©chantillon.\n\nUtiliser l‚ÄôAPI BAN\nLa documentation officielle de l‚ÄôAPI\npropose un certain nombre d‚Äôexemples de mani√®re de g√©olocaliser des donn√©es.\nDans notre situation, deux points d‚Äôentr√©e paraissent int√©ressants:\n\nL‚ÄôAPI /search/ qui repr√©sente un point d‚Äôentr√©e avec des URL de la forme\nhttps://api-adresse.data.gouv.fr/search/?q=\\&lt;adresse\\&gt;&postcode=\\&lt;codepostal\\&gt;&limit=1\nL‚ÄôAPI /search/csv qui prend un CSV en entr√©e et retourne ce m√™me CSV avec\nles observations g√©ocod√©es. La requ√™te prend la forme suivante, en apparence\nmoins simple √† mettre en oeuvre :\ncurl -X POST -F data=@search.csv -F columns=adresse -F columns=postcode https://api-adresse.data.gouv.fr/search/csv/\n\nLa tentation serait forte d‚Äôutiliser la premi√®re m√©thode avec une boucle sur les\nlignes de notre DataFrame pour g√©ocoder l‚Äôensemble de notre jeu de donn√©es.\nCela serait n√©anmoins une mauvaise id√©e car les communications entre notre\nsession Python et les serveurs de l‚ÄôAPI seraient beaucoup trop nombreuses\npour offrir des performances satisfaisantes.\nPour vous en convaincre, vous pouvez ex√©cuter le code suivant sur un petit\n√©chantillon de donn√©es (par exemple 100 comme ici) et remarquer que le temps\nd‚Äôex√©cution est assez important\n\nimport time\n\ndfgeoloc = df.loc[:, ['Adresse','CP','Ville']].apply(lambda s: s.str.lower().str.replace(\",\",\" \"))\ndfgeoloc['url'] = (dfgeoloc['Adresse'] + \"+\" + dfgeoloc['Ville'].str.replace(\"-\",'+')).str.replace(\" \",\"+\")\ndfgeoloc['url'] = 'https://api-adresse.data.gouv.fr/search/?q=' + dfgeoloc['url'] + \"&postcode=\" + df['CP'] + \"&limit=1\"\ndfgeoloc = dfgeoloc.dropna()\n\nstart_time = time.time()\n\ndef get_geoloc(i):\n    print(i)\n    return gpd.GeoDataFrame.from_features(requests.get(dfgeoloc['url'].iloc[i]).json()['features'])\n\nlocal = [get_geoloc(i) for i in range(len(dfgeoloc.head(10)))]\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\nComme l‚Äôindique la documentation, si on d√©sire industrialiser notre processus\nde g√©ocodage, on va privil√©gier l‚ÄôAPI CSV.\nPour obtenir une requ√™te CURL coh√©rente avec le format d√©sir√© par l‚ÄôAPI\non va √† nouveau utiliser Requests mais cette fois avec des param√®tres\nsuppl√©mentaires:\n\ndata va nous permettre de passer des param√®tres √† CURL (√©quivalents aux -F\nde la requ√™te CURL) :\n\ncolumns: Les colonnes utilis√©es pour localiser une donn√©e. En l‚Äôoccurrence,\non utilise l‚Äôadresse et la ville (car les codes postaux n‚Äô√©tant pas uniques,\nun m√™me nom de voirie peut se trouver dans plusieurs villes partageant le m√™me\ncode postal) ;\npostcode: Le code postal de la ville. Id√©alement nous aurions utilis√©\nle code Insee mais nous ne l‚Äôavons pas dans nos donn√©es ;\nresult_columns: on restreint les donn√©es √©chang√©es avec l‚ÄôAPI aux\ncolonnes qui nous int√©ressent. Cela permet d‚Äôacc√©l√©rer les processus (on\n√©change moins de donn√©es) et de r√©duire l‚Äôimpact carbone de notre activit√©\n(moins de transferts = moins d‚Äô√©nergie d√©pens√©e). En l‚Äôoccurrence, on ne ressort\nque les donn√©es g√©olocalis√©es et un score de confiance en la g√©olocalisation ;\n\nfiles: permet d‚Äôenvoyer un fichier via CURL.\n\nLes donn√©es sont r√©cup√©r√©es avec request.post. Comme il s‚Äôagit d‚Äôune\ncha√Æne de caract√®re, nous pouvons directement la lire avec Pandas en\nutilisant io.StringIO pour √©viter d‚Äô√©crire des donn√©es interm√©diaires.\nLe nombre d‚Äô√©chos semblant √™tre limit√©, il\nest propos√© de proc√©der par morceaux\n(ici, le jeu de donn√©es est d√©coup√© en 5 morceaux).\n\nimport requests\nimport io   \nimport numpy as np\nimport time\n\nparams = {\n    'columns': ['Adresse', 'Ville'],\n    'postcode': 'CP',\n    'result_columns': ['result_score', 'latitude', 'longitude'],\n}\n\ndf[['Adresse','CP','Ville']] = df.loc[:, ['Adresse','CP','Ville']].apply(lambda s: s.str.lower().str.replace(\",\",\" \"))\n\ndef geoloc_chunk(x):\n    dfgeoloc = x.loc[:, ['Adresse','CP','Ville']]\n    dfgeoloc.to_csv(\"datageocodage.csv\", index=False)\n    response = requests.post('https://api-adresse.data.gouv.fr/search/csv/', data=params, files={'data': ('datageocodage.csv', open('datageocodage.csv', 'rb'))})\n    geoloc = pd.read_csv(io.StringIO(response.text), dtype = {'CP': 'str'})\n    return geoloc\n    \nstart_time = time.time()\ngeodata = [geoloc_chunk(dd) for dd in np.array_split(df, 10)]\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\nCette m√©thode est beaucoup plus rapide et permet ainsi, une fois retourn√© √† nos\ndonn√©es initiales, d‚Äôavoir un jeu de donn√©es g√©olocalis√©.\n\n# Retour aux donn√©es initiales\ngeodata = pd.concat(geodata, ignore_index = True)\ndf_xy = df.merge(geodata, on = ['Adresse','CP','Ville'])\ndf_xy = df_xy.dropna(subset = ['latitude','longitude'])\n\n# Mise en forme pour le tooltip\ndf_xy['text'] = (\n    df_xy['Raison_Sociale'] + '&lt;br&gt;' +\n    df_xy['Adresse'] + '&lt;br&gt;' +\n    df_xy['Ville'] + '&lt;br&gt;Nombre de candidats:' + df_xy['B_NB'].astype(str)\n)\ndf_xy.filter(\n    ['Raison_Sociale','Adresse','CP','Ville','latitude','longitude'],\n    axis = \"columns\"\n).sample(10)\n\n\n\n\n\n\n\n\nRaison_Sociale\nAdresse\nCP\nVille\nlatitude\nlongitude\n\n\n\n\n3907\nBEGON BIS\n176 bis rue de cabochon\n41000\nblois\n47.577073\n1.303151\n\n\n5689\nAUTO-ECOLE DANJOU\n24 rue de landrecies\n59360\nle cateau\n50.104758\n3.545127\n\n\n2314\nECF ROUDAUT\n22 rue de la porte\n29200\nbrest\n48.384001\n-4.499404\n\n\n977\nAE Avenir Conduite\n6 avenue des rosiers\n13109\nsimiane collongue\n43.430206\n5.428678\n\n\n6622\nJOFFRE AE\n132 avenue joffre\n66000\nperpignan\n42.711406\n2.890075\n\n\n11111\nGTA DRIVE AUTO ECOLE\n43 rue jean-jaur√®s\n97200\nfort de france\n14.609408\n-61.070442\n\n\n6295\nALOHA PERMIS\n16 rue d'alsace - r√©sidence sologne\n62700\nbruay-la-buissiere\n50.480136\n2.547684\n\n\n2646\nGRENADE\n13 c all√©es alsace-lorraine\n31330\ngrenade sur garonne\n43.769966\n1.293552\n\n\n1038\nECOLE DE CONDUITE MARYLINE CHERR\n3 rue emile fassin\n13200\narles\n43.674361\n4.629041\n\n\n441\nREFLEX LA TRINITE AUTO ECOLE\n2 boulevard stalingrad\n06340\nla trinite\n43.740554\n7.311126\n\n\n\n\n\n\n\nIl ne reste plus qu‚Äô√† utiliser Geopandas\net nous serons en mesure de faire une carte des localisations des auto-√©coles :\n\n# Transforme en geopandas pour les cartes\nimport geopandas as gpd\ndfgeo = gpd.GeoDataFrame(\n    df_xy,\n    geometry = gpd.points_from_xy(df_xy.longitude, df_xy.latitude)\n)\n\nNous allons repr√©senter les stations dans l‚ÄôEssonne avec un zoom initialement\nsur les villes de Massy et Palaiseau. Le code est le suivant :\n\nimport folium\n\n# Repr√©senter toutes les auto√©coles de l'Essonne\ndf_91 = df_xy.loc[df_xy[\"Dept\"] == \"091\"]\n\n# Centrer la vue initiale sur Massy-Palaiseau\ndf_pal = df_xy.loc[df_xy['Ville'].isin([\"massy\", \"palaiseau\"])]\ncenter = df_pal[['latitude', 'longitude']].mean().values.tolist()\nsw = df_pal[['latitude', 'longitude']].min().values.tolist()\nne = df_pal[['latitude', 'longitude']].max().values.tolist()\n\nm = folium.Map(location = center, tiles='OpenStreetMap')\n\n# I can add marker one by one on the map\nfor i in range(0,len(df_91)):\n    folium.Marker([df_91.iloc[i]['latitude'], df_91.iloc[i]['longitude']],\n                  popup=df_91.iloc[i]['text'],\n                  icon=folium.Icon(icon='car', prefix='fa')).add_to(m)\n\nm.fit_bounds([sw, ne])\n\nCe qui permet d‚Äôobtenir la carte:\n\n# Afficher la carte\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nVous pouvez aller plus loin avec l‚Äôexercice suivant.\n\n\n Exercice 2 : Quelles sont les auto-√©coles les plus proches de chez moi ?\nOn va supposer que vous cherchez, dans un rayon donn√© autour d‚Äôun centre ville,\nles auto-√©coles disponibles.\n\n\nFonction n√©cessaire pour cet exercice\n\nCet exercice n√©cessite une fonction pour cr√©er un cercle\nautour d‚Äôun point\n(source ici).\nLa voici :\nfrom functools import partial\nimport pyproj\nfrom shapely.ops import transform\nfrom shapely.geometry import Point\n\nproj_wgs84 = pyproj.Proj('+proj=longlat +datum=WGS84')\n\n\ndef geodesic_point_buffer(lat, lon, km):\n    # Azimuthal equidistant projection\n    aeqd_proj = '+proj=aeqd +lat_0={lat} +lon_0={lon} +x_0=0 +y_0=0'\n    project = partial(\n        pyproj.transform,\n        pyproj.Proj(aeqd_proj.format(lat=lat, lon=lon)),\n        proj_wgs84)\n    buf = Point(0, 0).buffer(km * 1000)  # distance in metres\n    return transform(project, buf).exterior.coords[:]\n\n\nPour commencer, utiliser l‚ÄôAPI Geo\npour la ville de Palaiseau.\nAppliquer la fonction geodesic_point_buffer au centre ville de Palaiseau\nNe conserver que les auto-√©coles dans ce cercle et les ordonner\n\nSi vous avez la r√©ponse √† la question 3, n‚Äôh√©sitez pas √† la soumettre sur Github afin que je compl√®te la correction üòâ !\n\n\n\n\nERROR 1: PROJ: proj_create_from_database: Open of /opt/mamba/share/proj failed\n\n\n\n\n/opt/mamba/lib/python3.9/site-packages/shapely/ops.py:276: FutureWarning:\n\nThis function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n\n\n\nPour se convaincre, de notre cercle constitu√© lors de\nla question 2, on peut repr√©senter une carte.\nOn a bien un cercle centr√© autour de Palaiseau :\n\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "content/manipulation/04c_API_TP.html#exercices-suppl√©mentaires",
    "href": "content/manipulation/04c_API_TP.html#exercices-suppl√©mentaires",
    "title": "R√©cup√©rer des donn√©es avec des API depuis Python",
    "section": "Exercices suppl√©mentaires",
    "text": "Exercices suppl√©mentaires\n\nD√©couvrir l‚ÄôAPI d‚ÄôOpenFoodFacts\nPour vous aidez, vous pouvez regarder une exemple de structure du JSON ici :\nhttps://world.openfoodfacts.org/api/v0/product/3274080005003.json en particulier la cat√©gorie nutriments.\n\n\n Exercice 3 : Retrouver des produits dans l'openfood facts üçï\nVoici une liste de code-barres:\n3274080005003,  5449000000996, 8002270014901, 3228857000906, 3017620421006, 8712100325953\nUtiliser l‚ÄôAPI d‚Äôopenfoodfacts\n(l‚ÄôAPI, pas depuis le CSV !)\npour retrouver les produits correspondants\net leurs caract√©ristiques nutritionnelles.\nLe panier para√Æt-il √©quilibr√© ? üç´\nR√©cup√©rer l‚ÄôURL d‚Äôune des images et l‚Äôafficher dans votre navigateur.\n\n\nVoici par exemple la photo du produit ayant le code-barre 5449000000996. Vous le reconnaissez ?"
  },
  {
    "objectID": "content/manipulation/04c_API_TP.html#footnotes",
    "href": "content/manipulation/04c_API_TP.html#footnotes",
    "title": "R√©cup√©rer des donn√©es avec des API depuis Python",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLa documentation est √©galement disponible ici‚Ü©Ô∏é\nLe JSON est un format tr√®s appr√©ci√© dans le domaine du big data\ncar il permet d‚Äôempiler des donn√©es\nqui ne sont pas compl√®tes. Il\ns‚Äôagit d‚Äôun des formats privil√©gi√©s du paradigme No-SQL pour lequel\ncet excellent cours propose plus de d√©tails.‚Ü©Ô∏é\nSuivant les API, nous avons soit besoin de rien de plus si nous parvenons directement √† obtenir un json, soit devoir utiliser un parser comme BeautifulSoup dans le cas contraire. Ici, le JSON peut √™tre format√© relativement ais√©ment.‚Ü©Ô∏é"
  },
  {
    "objectID": "content/manipulation/03_geopandas_TP.html",
    "href": "content/manipulation/03_geopandas_TP.html",
    "title": "Pratique de geopandas avec les donn√©es v√©lib",
    "section": "",
    "text": "Dans ce TP,\nnous allons apprendre √† importer et\nmanipuler des donn√©es spatiales avec\nPython.\nCe langage propose\ndes fonctionnalit√©s tr√®s int√©ressantes pour ce type de\ndonn√©es complexes qui le rendent capable de se comporter\ncomme un logiciel de SIG1.\nGr√¢ce √† la librairie Geopandas, une extension\nde Pandas aux donn√©es spatiales, les\ndonn√©es g√©ographiques pourront √™tre manipul√©es\ncomme n‚Äôimporte quel type de donn√©es avec Python.\nLa complexit√© induite par la dimension spatiale ne sera pas ressentie.\nIllustration du principe des donn√©es spatiales (documentation de `sf`, l'√©quivalent de `Geopandas` en `R`)\n\n\n\n![](https://user-images.githubusercontent.com/520851/50280460-e35c1880-044c-11e9-9ed7-cc46754e49db.jpg){width=\"70%\"}\nCe chapitre illustre √† partir d‚Äôexemples pratiques certains principes centraux de l‚Äôanalyse de donn√©es :\nSi vous √™tes int√©ress√©s par R,\nune version tr√®s proche de ce TP est disponible dans ce cours de R.\nNote\nLe package cartiflette est exp√©rimental\net n‚Äôest disponible que sur\nGithub, pas sur PyPi.\nIl est amen√© √† √©voluer rapidement et cette page sera mise √† jour\nquand de nouvelles fonctionalit√©s (notamment l‚Äôutilisation d‚ÄôAPI)\nseront disponibles pour encore simplifier la r√©cup√©ration de\ncontours g√©ographiques.\nPour installer cartiflette, il est n√©cessaire d‚Äôutiliser les commandes suivantes\ndepuis un Jupyter Notebook (si vous utilisez la ligne de commande directement,\nvous pouvez retirer les ! en d√©but de ligne):\nCes commandes permettent de r√©cup√©rer l‚Äôensemble du code\nsource depuis Github"
  },
  {
    "objectID": "content/manipulation/03_geopandas_TP.html#pr√©liminaires",
    "href": "content/manipulation/03_geopandas_TP.html#pr√©liminaires",
    "title": "Pratique de geopandas avec les donn√©es v√©lib",
    "section": "Pr√©liminaires",
    "text": "Pr√©liminaires\nAvant de se lancer dans le TD, il est n√©cessaire d‚Äôinstaller quelques\nlibrairies qui ne sont pas disponibles par d√©faut, dans l‚Äôenvironnement Python\nde base de la data science. Pour installer celles-ci depuis une\ncellule de notebook Jupyter, le code suivant est √† ex√©cuter :\n\n!pip install pandas fiona shapely pyproj rtree # √† faire obligatoirement en premier pour utiliser rtree ou pygeos pour les jointures spatiales\n!pip install contextily\n!pip install geopandas\n!pip install pygeos\n!pip install topojson\n\nApr√®s installations,\nles packages √† importer pour progresser\ndans ce chapitre sont les suivants :\n\nimport geopandas as gpd\nimport contextily as ctx\nimport matplotlib.pyplot as plt\n\nLes instructions d‚Äôinstallation du package cartiflette\nsont quant √† elles d√©taill√©es dans le chapitre\npr√©c√©dent.\n\n!pip install requests py7zr geopandas openpyxl tqdm s3fs PyYAML xlrd\n!pip install git+https://github.com/inseefrlab/cartiflette@80b8a5a28371feb6df31d55bcc2617948a5f9b1a\n\n\nfrom cartiflette.s3 import download_vectorfile_url_all"
  },
  {
    "objectID": "content/manipulation/03_geopandas_TP.html#lire-et-enrichir-des-donn√©es-spatiales",
    "href": "content/manipulation/03_geopandas_TP.html#lire-et-enrichir-des-donn√©es-spatiales",
    "title": "Pratique de geopandas avec les donn√©es v√©lib",
    "section": "Lire et enrichir des donn√©es spatiales",
    "text": "Lire et enrichir des donn√©es spatiales\nDans cette partie,\nnous utiliserons\nles fonds de carte de l‚ÄôIGN dont\nla mise √† disposition est facilit√©e\npar le projet cartiflette2.\n\n\n Exercice 1: d√©couverte des objets g√©ographiques\nEn premier lieu, on r√©cup√®re des donn√©es g√©ographiques gr√¢ce\nau package cartiflette.\n\nUtiliser\nle code ci-dessous pour\nt√©l√©charger les donn√©es communales (produit Admin Express de l‚ÄôIGN)\ndes d√©partements de la petite couronne (75, 92, 93 et 94)\nde mani√®re simplifi√©e gr√¢ce au package\ncartiflette:\n\n\ncommunes_borders = download_vectorfile_url_all(\n    crs = 4326,\n    values = [\"75\", \"92\", \"93\", \"94\"],\n    borders=\"COMMUNE\",\n    vectorfile_format=\"topojson\",\n    filter_by=\"DEPARTEMENT\",\n    source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n    year=2022)\n\n\nRegarder les premi√®res lignes des donn√©es. Identifier la diff√©rence avec\nun dataframe standard.\n\n\n\n\n\n\n\n\n\n\nid\nID\nNOM\nNOM_M\nINSEE_COM\nSTATUT\nPOPULATION\nINSEE_CAN\nINSEE_ARR\nINSEE_DEP\nINSEE_REG\nSIREN_EPCI\nsource\ngeometry\n\n\n\n\n0\nCOMMUNE_0000000009736048\nNaN\nParis\nPARIS\n75056\nCapitale d'√©tat\n2165423\nNR\n1\n75\n11\n200054781\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.36421 48.81640, 2.36333 48.81615, ...\n\n\n0\nCOMMUNE_0000000009736037\nNaN\nLevallois-Perret\nLEVALLOIS-PERRET\n92044\nCommune simple\n66082\n16\n2\n92\n11\n200054781/200057982\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.28739 48.90364, 2.28846 48.90302, ...\n\n\n1\nCOMMUNE_0000000009736055\nNaN\nBois-Colombes\nBOIS-COLOMBES\n92009\nCommune simple\n28841\n11\n2\n92\n11\n200054781/200057990\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.26639 48.90629, 2.26645 48.90615, ...\n\n\n2\nCOMMUNE_0000000009736538\nNaN\nMalakoff\nMALAKOFF\n92046\nCommune simple\n30950\n18\n1\n92\n11\n200054781/200057966\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.27818 48.81425, 2.28066 48.81469, ...\n\n\n3\nCOMMUNE_0000000009736038\nNaN\nClichy\nCLICHY\n92024\nCommune simple\n63089\n09\n2\n92\n11\n200054781/200057990\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.30377 48.89415, 2.30258 48.89487, ...\n\n\n\n\n\n\n\n\nAfficher le crs de communes_borders. Ce dernier contr√¥le la\ntransformation de l‚Äôespace tridimensionnel terrestre en une surface plane.\nUtiliser to_crs pour transformer les donn√©es en Lambert 93, le\nsyst√®me officiel (code EPSG 2154).\nAfficher les communes des Hauts de Seine (d√©partement 92) et utiliser la m√©thode\nplot\nNe conserver que Paris et r√©pr√©senter les fronti√®res sur une carte : quel est le probl√®me pour\nune analyse de Paris intramuros?\n\nOn remarque rapidement le probl√®me.\nOn ne dispose ainsi pas des limites des arrondissements parisiens, ce\nqui appauvrit grandement la carte de Paris.\n\nCette fois, utiliser l‚Äôargument borders=\"COMMUNE_ARRONDISSEMENT\" pour obtenir\nun fonds de carte consolid√© des communes avec les arrondissements dans les grandes villes.\nConvertir en Lambert 93."
  },
  {
    "objectID": "content/manipulation/03_geopandas_TP.html#le-syst√®me-de-projection",
    "href": "content/manipulation/03_geopandas_TP.html#le-syst√®me-de-projection",
    "title": "Pratique de geopandas avec les donn√©es v√©lib",
    "section": "Le syst√®me de projection",
    "text": "Le syst√®me de projection\nUn concept central dans les logiciels de SIG est la notion de\nprojection. L‚Äôexercice pr√©c√©dent imposait parfois certaines projections\nsans expliquer l‚Äôimportance de ces choix. Python, comme\ntout SIG, permet une gestion coh√©rente des projections.\nObservez les variations significatives\nde proportions pour certains pays selon les projections\nchoisies:\n\nhtml`&lt;div&gt;${container_projection}&lt;/div&gt;`\n\n\n\n\n\n\n\ncontainer_projection = html`&lt;div class=\"container\"&gt;\n  &lt;div class=\"row\"&gt;\n    &lt;div class=\"projection\"&gt;\n      &lt;div class=\"projection-label\"&gt;Choisir une projection&lt;/div&gt;\n      ${viewof projection}\n    &lt;/div&gt;\n  &lt;/div&gt;\n  &lt;div class=\"row\"&gt;\n    &lt;div class=\"projectedMap\"&gt;\n      ${projectedMap}\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/div&gt;`\n\n\n\n\n\n\n\nviewof projection = projectionInput({\n  name: \"\",\n  value: \"Mercator\"\n})\n\n\n\n\n\n\n\nimport {projectionInput} from \"@fil/d3-projections\"\nimport {map} from \"@linogaliana/base-map\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprojectedMap = map(projection,\n                   {\n                     //svg: true,\n                     value: projection.options,\n                     width: width_projected_map,\n                     //height: 300,\n                     //rotate: [0, -90],\n                     //inertia: true,\n                     show_equator: true,\n                     background: \"#f1f0eb\"\n                     \n                     //show_structure: true\n                   })\n\n\n\n\n\n\n\nwidth_projected_map = screen.width/2\n\n\n\n\n\n\n\n\n Exercice 2 : Les projections, repr√©sentations et approximations\nVoici un code utilisant encore\ncartiflette\npour r√©cup√©rer les fronti√®res fran√ßaises (d√©coup√©es par r√©gion):\n\nfrance = download_vectorfile_url_all(\n      values = \"metropole\",\n      crs = 4326,\n      borders = \"REGION\",\n      vectorfile_format=\"topojson\",\n      filter_by=\"FRANCE_ENTIERE\",\n      source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n      year=2022)\n\n\nS‚Äôamuser √† repr√©senter les limites de la France avec plusieurs projections:\n\n\nMercator WGS84 (EPSG: 4326)\nProjection healpix (+proj=healpix +lon_0=0 +a=1)\nProjection pr√©vue pour Tahiti (EPSG: 3304)\nProjection Albers pr√©vue pour Etats-Unis (EPSG: 5070)\n\n\nCalculer la superficie en \\(km^2\\)\ndes r√©gions fran√ßaises dans les deux syst√®mes de projection suivants :\nWorld Mercator WGS84 (EPSG: 3395) et Lambert 93 (EPSG: 2154). Calculer la diff√©rence en \\(km^2\\)\npour chaque r√©gion.\n\n\n\nAvec la question 1 illustrant quelques cas pathologiques,\non comprend que les projections ont un effet d√©formant\nqui se voit bien lorsqu‚Äôon les repr√©sente c√¥te √† c√¥te sous\nforme de cartes :\n\n\n\n\n\n\n(a) Mercator WGS84 (EPSG: 4326)\n\n\n\n\n\n(b) Projection healpix (+proj=healpix +lon_0=0 +a=1)\n\n\n\n\n\n\n\n(c) Projection pr√©vue pour Tahiti (EPSG: 3304)\n\n\n\n\n\n(d) Projection Albers pr√©vue pour Etats-Unis (EPSG: 5070)\n\n\n\nFigure¬†1: Comparaison des projections\n\n\nCependant le probl√®me n‚Äôest pas que visuel, il est √©galement\nnum√©rique. Les calculs g√©om√©triques am√®nent √† des diff√©rences\nassez notables selon le syst√®me de r√©f√©rence utilis√©.\nOn peut repr√©senter ces approximations sur une carte3 pour se faire\nune id√©e des r√©gions o√π l‚Äôerreur de mesure est la plus importante.\n\n\n/opt/mamba/lib/python3.9/site-packages/geopandas/plotting.py:730: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n\n\n\n\n\n\n\nCe type d‚Äôerreur de mesure est normal √† l‚Äô√©chelle du territoire fran√ßais.\nLes projections h√©rit√®res du Mercator d√©forment les distances,\nsurtout lorqu‚Äôon se rapproche de l‚Äô√©quateur ou des p√¥les.\n\n\n\n\n\n(a) Exemple de reprojection de pays depuis le site thetruesize.com\n\n\n\n\n\n(b) ‚ÄúDon‚Äôt trust the Mercator projection‚Äù sur Reddit\n\n\nFigure¬†2: La projection Mercator, une vision d√©formante\n\n\nPour aller plus loin, la carte interactive\nsuivante, construite par Nicolas Lambert, issue de\nce notebook Observable, illustre l‚Äôeffet\nd√©formant de la projection Mercator, et de quelques-unes autres,\nsur notre perception de la taille des pays.\n\n\nVoir la carte interactive\n\n\nhtml`&lt;div class=\"grid-container\"&gt;\n  &lt;div class=\"viewof-projection\"&gt;${viewof projectionBertin}&lt;/div&gt;\n  &lt;div class=\"viewof-mycountry\"&gt;${viewof mycountry}&lt;/div&gt;\n  &lt;div class=\"map-bertin\"&gt;${mapBertin}&lt;/div&gt;\n&lt;/div&gt;`\n\n\n\n\n\n\n\n\nimport {map as mapBertin, viewof projection as projectionBertin, viewof mycountry} from \"@neocartocnrs/impact-of-projections-on-areas\"\n\n\n\n\n\n\nIl n‚Äôest donc pas suprenant que nos d√©formations soient exacerb√©es aux\nextr√®mes du territoire m√©tropolitain.\nSi les approximations sont l√©g√®res sur de petits territoires,\nles erreurs peuvent √™tre\nnon n√©gligeables √† l‚Äô√©chelle de la France.\nIl faut donc syst√©matiquement\nrepasser les donn√©es dans le syst√®me de projection Lambert 93 (le\nsyst√®me officiel pour la m√©tropole) avant d‚Äôeffectuer des calculs g√©om√©triques."
  },
  {
    "objectID": "content/manipulation/03_geopandas_TP.html#utiliser-des-donn√©es-g√©ographiques-comme-des-couches-graphiques",
    "href": "content/manipulation/03_geopandas_TP.html#utiliser-des-donn√©es-g√©ographiques-comme-des-couches-graphiques",
    "title": "Pratique de geopandas avec les donn√©es v√©lib",
    "section": "Utiliser des donn√©es g√©ographiques comme des couches graphiques",
    "text": "Utiliser des donn√©es g√©ographiques comme des couches graphiques\nSouvent, le d√©coupage communal ne sert qu‚Äôen fond de cartes, pour donner des\nrep√®res. En compl√©ment de celui-ci, on peut d√©sirer exploiter\nun autre jeu de donn√©es.\nOn va partir des donn√©es de localisation des\nstations velib,\ndisponibles sur le site d‚Äôopen data de la ville de Paris et\nrequ√™tables directement en utilisant un URL\n\nurl = \"https://opendata.paris.fr/explore/dataset/velib-emplacement-des-stations/download/?format=geojson&timezone=Europe/Berlin&lang=fr\"\n\nDans le prochain exercice, nous proposons de cr√©er rapidement une\ncarte comprenant trois couches :\n\nLes localisations de stations sous forme de points ;\nLes bordures des communes et arrondissements pour contextualiser ;\nLes bordures des d√©partements en traits plus larges pour contextualiser √©galement.\n\nNous irons plus loin dans le travail cartographique dans le prochain\nchapitre. Mais √™tre en mesure de positionner rapidement\nses donn√©es sur une carte est\ntoujours utile dans un travail exploratoire.\nEn amont de l‚Äôexercice,\nutiliser la fonction suivante du package cartiflette pour r√©cup√©rer\nle fonds de carte des d√©partements de la petite couronne:\n\nidf = download_vectorfile_url_all(\n      values = \"11\",\n      crs = 4326,\n      borders = \"DEPARTEMENT\",\n      vectorfile_format=\"topojson\",\n      filter_by=\"REGION\",\n      source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n      year=2022)\n\npetite_couronne_departements = idf.loc[idf['INSEE_DEP'].isin([\"75\",\"92\",\"93\",\"94\"])].to_crs(2154)\n\n\n\n Exercice 3: importer et explorer les donn√©es velib\nOn commence par r√©cup√©rer les donn√©es n√©cessaires √† la production\nde cette carte.\n\nEn utilisant l‚ÄôURL pr√©c√©dent, importer les donn√©es velib sous le nom station\nV√©rifier la projection g√©ographique de station (attribut crs). Si celle-ci est diff√©rente des donn√©es communales, reprojeter ces\nderni√®res dans le m√™me syst√®me de projection que les stations de v√©lib\nNe conserver que les 50 principales stations (variable capacity)\n\nOn peut maintenant construire la carte de mani√®re s√©quentielle avec la m√©thode plot en s‚Äôaidant de cette documentation\n\nEn premier lieu, gr√¢ce √† boundary.plot,\nrepr√©senter la couche de base des limites des communes et arrondissements:\n\nUtiliser les options edgecolor = \"black\" et linewidth = 0.5\nNommer cet objet base\n\nAjouter la couche des d√©partements avec les options edgecolor = \"blue\" et linewidth = 0.7\nAjouter les positions des stations\net ajuster la taille en fonction de la variable capacity. L‚Äôesth√©tique des points obtenus peut √™tre contr√¥l√© gr√¢ce aux options color = \"red\" et alpha = 0.4.\nRetirer les axes et ajouter un titre avec les options ci-dessous:\n\nbase.set_axis_off()\nbase.set_title(\"Les 50 principales stations de V√©lib\")\n\n\nLa couche de base obtenue √† l‚Äôissue de la question 4.\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nPuis en y ajoutant les limites d√©partementales (question 5).\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nPuis les stations (question 6).\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nLa carte finale, apr√®s mise en forme:\n\n\n&lt;Axes: title={'center': 'Les 50 principales stations de V√©lib'}&gt;"
  },
  {
    "objectID": "content/manipulation/03_geopandas_TP.html#jointures-spatiales",
    "href": "content/manipulation/03_geopandas_TP.html#jointures-spatiales",
    "title": "Pratique de geopandas avec les donn√©es v√©lib",
    "section": "Jointures spatiales",
    "text": "Jointures spatiales\nLes jointures attributaires fonctionnent comme avec un Pandas classique.\nPour conserver un objet spatial in fine, il faut faire attention √† utiliser en premier (base de gauche) l‚Äôobjet Geopandas.\nEn revanche, l‚Äôun des int√©r√™ts des objets Geopandas est qu‚Äôon peut √©galement faire une jointure sur la dimension spatiale gr√¢ce √† plusieurs fonctions.\nLa documentation √† laquelle se r√©f√©rer est ici.\nUne version p√©dagogique pour R se trouve dans la documentation utilitR.\n\n\n Exercice 4: Associer les stations aux communes et arrondissements auxquels elles appartiennent\nDans cet exercice, on va supposer que :\n\nles localisations des stations velib\nsont stock√©es dans un dataframe nomm√© stations\nles donn√©es administratives\nsont dans un dataframe nomm√© petite_couronne.\n\n\nFaire une jointure spatiale pour enrichir les donn√©es de stations en y ajoutant des informations de petite_couronne. Appeler cet objet stations_info.\nCr√©er les objets stations_19e et arrondissement_19e pour stocker, respectivement,\nles stations appartenant au 19e et les limites de l‚Äôarrondissement.\nRepr√©senter la carte des stations du 19e arrondissement avec le code suivant :\n\nbase = petite_couronne.loc[petite_couronne['INSEE_DEP']==\"75\"].boundary.plot(edgecolor = \"k\", linewidth=0.5)\narrondissement_19e.boundary.plot(ax = base, edgecolor = \"red\", linewidth=0.9)\nstations_19.plot(ax = base, color = \"red\", alpha = 0.4)\nbase.set_axis_off()\nbase.set_title(\"Les stations V√©lib du 19e arrondissement\")\nbase\n\nCompter le nombre de stations velib et le nombre de places velib par arrondissement ou commune. Repr√©senter sur une carte chacune des informations\nRepr√©senter les m√™mes informations mais en densit√© (diviser par la surface de l‚Äôarrondissement ou commune en km2)\n\n\n\n\n\n&lt;Axes: title={'center': 'Les stations V√©lib du 19e arrondissement'}&gt;\n\n\n\n\n\n\n\n\n\nCarte obtenue √† la question 4 :\n\n\n/opt/mamba/lib/python3.9/site-packages/geopandas/plotting.py:730: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nAvec la carte de la question 4, bas√©e sur des aplats de couleurs (choropleth map), le lecteur est victime d‚Äôune illusion classique. Les arrondissements les plus visibles sur la carte sont les plus grands. D‚Äôailleurs c‚Äôest assez logique qu‚Äôils soient √©galement mieux pourvus en velib. M√™me si l‚Äôoffre de velib est probablement plus reli√©e √† la densit√© de population et d‚Äô√©quipements, on peut penser que l‚Äôeffet taille joue et qu‚Äôainsi on est victime d‚Äôune illusion avec la carte pr√©c√©dente.\nSi on repr√©sente plut√¥t la capacit√© sous forme de densit√©, pour tenir compte de la taille diff√©rente des arrondissements, les conclusions sont invers√©es et correspondent mieux aux attentes d‚Äôun mod√®le centre-p√©riph√©rie. Les arrondissements centraux sont mieux pourvus, cela se voit encore mieux avec des ronds proportionnels plut√¥t qu‚Äôune carte chorol√®pthe.\n\n\n/opt/mamba/lib/python3.9/site-packages/geopandas/plotting.py:730: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "content/manipulation/03_geopandas_TP.html#exercice-suppl√©mentaire",
    "href": "content/manipulation/03_geopandas_TP.html#exercice-suppl√©mentaire",
    "title": "Pratique de geopandas avec les donn√©es v√©lib",
    "section": "Exercice suppl√©mentaire",
    "text": "Exercice suppl√©mentaire\nLes exercices pr√©c√©dents ont permis de se familiariser au traitement de donn√©es\nspatiales. N√©anmoins il arrive de devoir jongler plus avec la\ndimension g√©om√©trique par exemple pour changer d‚Äô√©chelle ou introduire\ndes fusions/dissolutions de g√©om√©tries.\nImaginons que chaque utilisateur de velib se d√©place exclusivement\nvers la station la plus proche (√† supposer qu‚Äôil n‚Äôy a jamais p√©nurie\nou surcapacit√©). Quelle est la carte de la couverture des v√©libs ?\nPour r√©pondre √† ce type de question, on utilise fr√©quemment la\nla tesselation de Vorono√Ø,\nune op√©ration classique pour transformer des points en polygones. L‚Äôexercice suivant\npermet de se familiariser avec cette approche4.\nExercice √† venir"
  },
  {
    "objectID": "content/manipulation/03_geopandas_TP.html#footnotes",
    "href": "content/manipulation/03_geopandas_TP.html#footnotes",
    "title": "Pratique de geopandas avec les donn√©es v√©lib",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nD‚Äôailleurs, le logiciel de cartographie sp√©cialis√© QGIS, s‚Äôappuie sur Python\npour les manipulations de donn√©es n√©cessaires avant de r√©aliser une carte.‚Ü©Ô∏é\nLa librairie Python est encore exp√©rimentale mais\nles prochaines semaines devraient permettre de combler ce manque.\nUne documentation interactive illustrant le code n√©cessaire pour reproduire\ntelle ou telle carte est disponible sur linogaliana.github.io/cartiflette-website.‚Ü©Ô∏é\nCette carte n‚Äôest pas trop soign√©e, c‚Äôest normal nous verrons comment\nfaire de belles cartes ult√©rieurement.‚Ü©Ô∏é\nDans ce document de travail sur donn√©es de t√©l√©phonie mobile, on montre n√©anmoins que cette approche n‚Äôest pas sans biais\nsur des ph√©nom√®nes o√π l‚Äôhypoth√®se de proximit√© spatiale est\ntrop simplificatrice.‚Ü©Ô∏é"
  },
  {
    "objectID": "content/manipulation/02b_pandas_TP.html",
    "href": "content/manipulation/02b_pandas_TP.html",
    "title": "Pratique de pandas : un exemple complet",
    "section": "",
    "text": "Les exemples de ce TP sont visualisables sous forme de Jupyter Notebooks:\nDans cette s√©rie d‚Äôexercices Pandas,\nnous allons d√©couvrir comment manipuler plusieurs\njeux de donn√©es avec Python.\nSi vous √™tes int√©ress√©s par R,\nune version tr√®s proche de ce TP est\ndisponible dans ce cours.\nDans ce tutoriel, nous allons utiliser deux sources de donn√©es :\nLa librairie pynsee n‚Äôest pas install√©e par d√©faut avec Python. Avant de pouvoir l‚Äôutiliser,\nil est n√©cessaire de l‚Äôinstaller :\n!pip install xlrd\n!pip install pynsee\nToutes les d√©pendances indispensables √©tant install√©es, il suffit\nmaintenant d‚Äôimporter les librairies qui seront utilis√©es\npendant ces exercices :\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pynsee\nimport pynsee.download"
  },
  {
    "objectID": "content/manipulation/02b_pandas_TP.html#importer-les-donn√©es",
    "href": "content/manipulation/02b_pandas_TP.html#importer-les-donn√©es",
    "title": "Pratique de pandas : un exemple complet",
    "section": "Importer les donn√©es",
    "text": "Importer les donn√©es\n\nImport d‚Äôun csv de l‚ÄôAdeme\nL‚ÄôURL d‚Äôacc√®s aux donn√©es peut √™tre conserv√© dans une variable ad hoc :\n\nurl = \"https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert\"\n\nL‚Äôobjectif du premier exercice est de se familiariser √† l‚Äôimport et l‚Äôaffichage de donn√©es\navec Pandas.\n\n\n Exercice 1: Importer un CSV et explorer la structure de donn√©es\n\nImporter les donn√©es de l‚ÄôAdeme √† l‚Äôaide du package Pandas et de la commande consacr√©e pour l‚Äôimport de csv. Nommer le DataFrame obtenu emissions1.\nUtiliser les m√©thodes ad√©quates afin d‚Äôafficher pour les 10 premi√®res valeurs, les 15 derni√®res et un √©chantillon al√©atoire de 10 valeurs gr√¢ce aux m√©thodes ad√©quates du package Pandas.\nTirer 5 pourcents de l‚Äô√©chantillon sans remise.\nNe conserver que les 10 premi√®res lignes et tirer al√©atoirement dans celles-ci pour obtenir un DataFrame de 100 donn√©es.\nFaire 100 tirages √† partir des 6 premi√®res lignes avec une probabilit√© de 1/2 pour la premi√®re observation et une probabilit√© uniforme pour les autres.\n\n\n\nEn cas de blocage √† la question 1\n\nLire la documentation de read_csv (tr√®s bien faite) ou chercher des exemples\nen ligne pour d√©couvrir cette fonction."
  },
  {
    "objectID": "content/manipulation/02b_pandas_TP.html#premi√®res-manipulations-de-donn√©es",
    "href": "content/manipulation/02b_pandas_TP.html#premi√®res-manipulations-de-donn√©es",
    "title": "Pratique de pandas : un exemple complet",
    "section": "Premi√®res manipulations de donn√©es",
    "text": "Premi√®res manipulations de donn√©es\nLe chapitre pr√©c√©dent √©voquait quelques manipulations traditionnelles\nde donn√©es. Les principales sont rappel√©es ici :\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR√©ordonner le DataFrame\n\n\n\n\nLa cheatsheet suivante est tr√®s pratique puisqu‚Äôelle illustre ces diff√©rentes\nfonctions. Il est recommand√© de r√©guli√®rement\nla consulter :\n\n\n\nCheasheet Pandas\n\n\nL‚Äôobjectif du prochain exercice est de se familiariser aux principales manipulations de donn√©es\nsur un sous-ensemble de la table des √©missions de gaz carbonique.\n\n\n Exercice 2: D√©couverte des verbes de Pandas pour manipuler des donn√©es\nEn premier lieu, on propose de se familiariser avec les op√©rations sur\nles colonnes.\n\nCr√©er un dataframe emissions_copy ne conservant que les colonnes\nINSEE commune, Commune, Autres transports et Autres transports international\n\n\n\nIndice pour cette question\n\n\n\n\n\n\n\n\nComme les noms de variables sont peu pratiques, les renommer de la\nmani√®re suivante :\n\nINSEE commune \\(\\to\\) code_insee\nAutres transports \\(\\to\\) transports\nAutres transports international \\(\\to\\) transports_international\n\n\n\n\nIndice pour cette question\n\n\n\n\n\n\n\n\nOn propose, pour simplifier, de remplacer les valeurs manquantes (NA)\npar la valeur 0. Utiliser la\nm√©thode fillna\npour transformer les valeurs manquantes en 0.\nCr√©er les variables suivantes :\n\ndep: le d√©partement. Celui-ci peut √™tre cr√©√© gr√¢ce aux deux premiers caract√®res de code_insee en appliquant la m√©thode str ;\ntransports_total: les √©missions du secteur transports (somme des deux variables)\n\n\n\n\nIndice pour cette question\n\n\n\n\n\n\n\n\nOrdonner les donn√©es du plus gros pollueur au plus petit\npuis ordonner les donn√©es\ndu plus gros pollueur au plus petit par d√©partement (du 01 au 95).\n\n\n\nIndice pour cette question\n\n\n\n\n\n\n\n\nNe conserver que les communes appartenant aux d√©partements 13 ou 31.\nOrdonner ces communes du plus gros pollueur au plus petit.\n\n\n\nIndice pour cette question\n\n\n\n\n\n\n\n\nCalculer les √©missions totales par d√©partements\n\n\n\nIndice pour cette question\n\n\n‚ÄúGrouper par‚Äù = groupby\n‚Äú√©missions totales‚Äù = agg({***: \"sum\"})\n\n\n\n\nA la question 5, quand on ordonne les communes exclusivement √† partir de la variable\ntransports_total, on obtient ainsi:\n\n\n\n\n\n\n\n\n\ncode_insee\nCommune\ntransports\ntransports_international\ndep\ntransports_total\n\n\n\n\n31108\n77291\nLE MESNIL-AMELOT\n133834.090767\n3.303394e+06\n77\n3.437228e+06\n\n\n31099\n77282\nMAUREGARD\n133699.072712\n3.303394e+06\n77\n3.437093e+06\n\n\n31111\n77294\nMITRY-MORY\n89815.529858\n2.202275e+06\n77\n2.292090e+06\n\n\n\n\n\n\n\nA la question 6, on obtient ce classement :\n\n\n\n\n\n\n\n\n\ncode_insee\nCommune\ntransports\ntransports_international\ndep\ntransports_total\n\n\n\n\n4438\n13096\nSAINTES-MARIES-DE-LA-MER\n271182.758578\n0.000000\n13\n271182.758578\n\n\n4397\n13054\nMARIGNANE\n245375.418650\n527360.799265\n13\n772736.217915\n\n\n11684\n31069\nBLAGNAC\n210157.688544\n403717.366279\n31\n613875.054823\n\n\n\n\n\n\n\n\nImport des donn√©es de l‚ÄôInsee\nEn ce qui concerne nos informations communales, on va utiliser l‚Äôune des\nsources de l‚ÄôInsee les plus utilis√©es : les donn√©es Filosofi.\nAfin de faciliter la r√©cup√©ration de celles-ci, nous allons\nutiliser le package communautaire pynsee :\n\n\n Note\nLe package pynsee comporte deux principaux points d‚Äôentr√©e:\n\nLes API de l‚ÄôInsee, ce qui sera illustr√© dans le chapitre consacr√©.\nQuelques jeux de donn√©es directement issus du site web de\nl‚ÄôInsee (insee.fr)\n\nDans ce chapitre, nous allons exclusivement utiliser cette deuxi√®me\napproche. Cela se fera par le module pynsee.download.\nLa liste des donn√©es disponibles depuis ce package est ici.\nLa fonction download_file attend un identifiant unique\npour savoir quelle base de donn√©es aller chercher et\nrestructurer depuis le\nsite insee.fr.\n\n\nConna√Ætre la liste des bases disponibles\n\nPour conna√Ætre la liste des bases disponibles, vous\npouvez utiliser la fonction meta = pynsee.get_file_list()\napr√®s avoir fait import pynsee.\nCelle-ci renvoie un DataFrame dans lequel on peut\nrechercher, par exemple gr√¢ce √† une recherche\nde mots-clefs :\n\nmeta = pynsee.get_file_list()\nmeta.loc[meta['label'].str.contains(r\"Filosofi.*2016\")]\n\npynsee.download's metadata rely on volunteering contributors and their manual updates\nget_file_list does not provide data from official Insee's metadata API\nConsequently, please report any issue\n\n\n\n\n\n\n\n\n\nid\nname\nlabel\ncollection\nlink\ntype\nzip\nbig_zip\ndata_file\ntab\n...\nlabel_col\ndate_ref\nmeta_file\nseparator\ntype_col\nlong_col\nval_col\nencoding\nlast_row\nmissing_value\n\n\n\n\n79\nFILOSOFI_COM_2016\nFILOSOFI_COM\nDonn√©es Filosofi niveau communal ‚Äì 2016\nFILOSOFI\nhttps://www.insee.fr/fr/statistiques/fichier/4...\nxls\nTrue\nFalse\nbase-cc-filosofi-2016.xls\nCOM\n...\nNaN\n2016-01-01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n80\nFILOSOFI_EPCI_2016\nFILOSOFI_EPCI\nDonn√©es Filosofi niveau EPCI ‚Äì 2016\nFILOSOFI\nhttps://www.insee.fr/fr/statistiques/fichier/4...\nxls\nTrue\nFalse\nbase-cc-filosofi-2016.xls\nEPCI\n...\nNaN\n2016-01-01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n81\nFILOSOFI_ARR_2016\nFILOSOFI_ARR\nDonn√©es Filosofi niveau arondissement ‚Äì 2016\nFILOSOFI\nhttps://www.insee.fr/fr/statistiques/fichier/4...\nxls\nTrue\nFalse\nbase-cc-filosofi-2016.xls\nARR\n...\nNaN\n2016-01-01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n82\nFILOSOFI_DEP_2016\nFILOSOFI_DEP\nDonn√©es Filosofi niveau d√©partemental ‚Äì 2016\nFILOSOFI\nhttps://www.insee.fr/fr/statistiques/fichier/4...\nxls\nTrue\nFalse\nbase-cc-filosofi-2016.xls\nDEP\n...\nNaN\n2016-01-01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n83\nFILOSOFI_REG_2016\nFILOSOFI_REG\nDonn√©es Filosofi niveau r√©gional ‚Äì 2016\nFILOSOFI\nhttps://www.insee.fr/fr/statistiques/fichier/4...\nxls\nTrue\nFalse\nbase-cc-filosofi-2016.xls\nREG\n...\nNaN\n2016-01-01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n84\nFILOSOFI_METRO_2016\nFILOSOFI_METRO\nDonn√©es Filosofi niveau France m√©tropolitaine ...\nFILOSOFI\nhttps://www.insee.fr/fr/statistiques/fichier/4...\nxls\nTrue\nFalse\nbase-cc-filosofi-2016.xls\nMETRO\n...\nNaN\n2016-01-01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n85\nFILOSOFI_AU2010_2016\nFILOSOFI_AU2010\nDonn√©es Filosofi niveau aire urbaine ‚Äì 2016\nFILOSOFI\nhttps://www.insee.fr/fr/statistiques/fichier/4...\nxls\nTrue\nFalse\nbase-cc-filosofi-2016.xls\nAU2010\n...\nNaN\n2016-01-01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n86\nFILOSOFI_UU2010_2016\nFILOSOFI_UU2010\nDonn√©es Filosofi niveau unit√© urbaine ‚Äì 2016\nFILOSOFI\nhttps://www.insee.fr/fr/statistiques/fichier/4...\nxls\nTrue\nFalse\nbase-cc-filosofi-2016.xls\nUU2010\n...\nNaN\n2016-01-01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n87\nFILOSOFI_ZE2010_2016\nFILOSOFI_ZE2010\nDonn√©es Filosofi niveau zone d‚Äôemploi ‚Äì 2016\nFILOSOFI\nhttps://www.insee.fr/fr/statistiques/fichier/4...\nxls\nTrue\nFalse\nbase-cc-filosofi-2016.xls\nZE2010\n...\nNaN\n2016-01-01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n9 rows √ó 24 columns\n\n\n\nIci, meta['label'].str.contains(r\"Filosofi.*2016\") signifie:\n‚Äúpandas trouve moi tous les labels o√π sont contenus les termes Filosofi et 2016.‚Äù\n(.* signifiant ‚Äúpeu m‚Äôimporte le nombre de mots ou caract√®res entre‚Äù)\n\n\n\nOn va utiliser les donn√©es Filosofi (donn√©es de revenus) au niveau communal de 2016.\nCe n‚Äôest pas la m√™me ann√©e que les donn√©es d‚Äô√©mission de CO2, ce n‚Äôest donc pas parfaitement rigoureux,\nmais cela permettra tout de m√™me d‚Äôillustrer\nles principales fonctionnalit√©s de Pandas\nLe point d‚Äôentr√©e principal de la fonction pynsee est la fonction download_file.\nLe code pour t√©l√©charger les donn√©es est le suivant :\n\nfrom pynsee.download import download_file\nfilosofi = download_file(\"FILOSOFI_COM_2016\")\n\nLe DataFrame en question a l‚Äôaspect suivant :\n\n\n\n\n\n\n\n\n\nCODGEO\nLIBGEO\nNBMENFISC16\nNBPERSMENFISC16\nMED16\nPIMP16\nTP6016\nTP60AGE116\nTP60AGE216\nTP60AGE316\n...\nPPEN16\nPPAT16\nPPSOC16\nPPFAM16\nPPMINI16\nPPLOGT16\nPIMPOT16\nD116\nD916\nRD16\n\n\n\n\n16997\n46253\nSaint-Chamarand\n89\n196\n19185.23809523809\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n24793\n63386\nSaint-Pierre-Roche\n204\n457\n20378.571428571428\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n329\n01384\nSaint-Rambert-en-Bugey\n946\n2035\n17868.46153846154\n39\n20\nNaN\nNaN\nNaN\n...\n29.1\n7\n8.2\n3.3\n2.7\n2.2\n-12.6\n9348.09523809524\n28880\n3.0893994192858227\n\n\n23460\n61423\nSaint-Martin-d'√âcublei\n255\n686.5\n21940.666666666668\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n5187\n15257\nVezels-Roussy\n59\n122\n17124.2\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows √ó 29 columns\n\n\n\nPandas a g√©r√© automatiquement les types de variables. Il le fait relativement bien, mais une v√©rification est toujours utile pour les variables qui ont un statut sp√©cifique.\nPour les variables qui ne sont pas en type float alors qu‚Äôelles devraient l‚Äô√™tre, on modifie leur type.\n\nfilosofi.loc[:, filosofi.columns[2:]] = (\n  filosofi.loc[:, filosofi.columns[2:]]\n  .apply(pd.to_numeric, errors='coerce')\n)\n\nUn simple coup d‚Äôoeil sur les donn√©es\ndonne une id√©e assez pr√©cise de la mani√®re dont les donn√©es sont organis√©es.\nOn remarque que certaines variables de filosofi semblent avoir beaucoup de valeurs manquantes (secret statistique)\nalors que d‚Äôautres semblent compl√®tes.\nSi on d√©sire exploiter filosofi, il faut faire attention √† la variable choisie.\nNotre objectif √† terme va √™tre de relier l‚Äôinformation contenue entre ces\ndeux jeux de donn√©es. En effet, sinon, nous risquons d‚Äô√™tre frustr√© : nous allons\nvouloir en savoir plus sur les √©missions de gaz carbonique mais seront tr√®s\nlimit√©s dans les possibilit√©s d‚Äôanalyse sans ajout d‚Äôune information annexe\nissue de filosofi."
  },
  {
    "objectID": "content/manipulation/02b_pandas_TP.html#les-indices",
    "href": "content/manipulation/02b_pandas_TP.html#les-indices",
    "title": "Pratique de pandas : un exemple complet",
    "section": "Les indices",
    "text": "Les indices\nLes indices sont des √©l√©ments sp√©ciaux d‚Äôun DataFrame puisqu‚Äôils permettent d‚Äôidentifier certaines observations.\nIl est tout √† fait possible d‚Äôutiliser plusieurs indices, par exemple si on a des niveaux imbriqu√©s.\nPour le moment, on va prendre comme acquis que les codes communes (dits aussi codes Insee) permettent\nd‚Äôidentifier de mani√®re unique une commune. Un exercice ult√©rieur permettra de s‚Äôen assurer.\nPandas propose un syst√®me d‚Äôindice qui permet d‚Äôordonner les variables mais √©galement de gagner\nen efficacit√© sur certains traitements, comme des recherches d‚Äôobservations. Le prochain\nexercice illustre cette fonctionnalit√©.\n\n\n Exercice 3 : Les indices\nOn suppose ici qu‚Äôon peut se fier aux codes communes. En effet, on a un m√™me ordre de grandeur de communes dans les deux bases.\n\nprint(emissions[['INSEE commune', 'Commune']].nunique())\nprint(filosofi[['CODGEO', 'LIBGEO']].nunique())\n\nINSEE commune    35798\nCommune          33338\ndtype: int64\nCODGEO    34932\nLIBGEO    32676\ndtype: int64\n\n\n\nFixer comme indice la variable de code commune dans les deux bases.\nRegarder le changement que cela induit sur le display du DataFrame\nLes deux premiers chiffres des codes communes sont le num√©ro de d√©partement.\nCr√©er une variable de d√©partement dep dans emissions et filosofi\nCalculer les √©missions totales par secteur pour chaque d√©partement.\nMettre en log ces r√©sultats dans un objet emissions_log.\nGarder 5 d√©partements et produire un barplot gr√¢ce √† la m√©thode plot (la\nfigure n‚Äôa pas besoin d‚Äô√™tre vraiment propre, c‚Äôest seulement pour illustrer\ncette m√©thode)\nRepartir de emissions.\nCalculer les √©missions totales par d√©partement et sortir la liste\ndes 10 principaux √©metteurs de CO2 et des 5 d√©partements les moins √©metteurs.\nEssayer de comprendre pourquoi ce sont ces d√©partements qui apparaissent en t√™te\ndu classement. Pour cela, il peut √™tre utile de regarder les caract√©ristiques de ces\nd√©partements dans filosofi\n\n\n\nEn pratique, l‚Äôutilisation des indices en Pandas peut √™tre pi√©geuse, notamment lorsqu‚Äôon\nassocie des sources de donn√©es.\nIl est plut√¥t recommand√© de ne pas les utiliser ou de les utiliser avec parcimonie,\ncela pourra √©viter de mauvaises surprises."
  },
  {
    "objectID": "content/manipulation/02b_pandas_TP.html#restructurer-les-donn√©es",
    "href": "content/manipulation/02b_pandas_TP.html#restructurer-les-donn√©es",
    "title": "Pratique de pandas : un exemple complet",
    "section": "Restructurer les donn√©es",
    "text": "Restructurer les donn√©es\nQuand on a plusieurs informations pour un m√™me individu ou groupe, on\nretrouve g√©n√©ralement deux types de structure de donn√©es :\n\nformat wide : les donn√©es comportent des observations r√©p√©t√©es, pour un m√™me individu (ou groupe), dans des colonnes diff√©rentes\nformat long : les donn√©es comportent des observations r√©p√©t√©es, pour un m√™me individu, dans des lignes diff√©rentes avec une colonne permettant de distinguer les niveaux d‚Äôobservations\n\nUn exemple de la distinction entre les deux peut √™tre pris √† l‚Äôouvrage de r√©f√©rence d‚ÄôHadley Wickham, R for Data Science:\n\n\n\nDonn√©es long et wide\n\n\nL‚Äôaide m√©moire suivante aidera √† se rappeler les fonctions √† appliquer si besoin :\n\n\n\n\n\nLe fait de passer d‚Äôun format wide au format long (ou vice-versa)\npeut √™tre extr√™mement pratique car certaines fonctions sont plus ad√©quates sur une forme de donn√©es ou sur l‚Äôautre.\nEn r√®gle g√©n√©rale, avec Python comme avec R, les formats long sont souvent pr√©f√©rables.\nLes formats wide sont plut√¥t pens√©s pour des tableurs comme Excel ou on dispose d‚Äôun nombre r√©duit\nde lignes √† partir duquel faire des tableaux crois√©s dynamiques.\nLe prochain exercice propose donc une telle restructuration de donn√©es.\nLes donn√©es de l‚ÄôADEME, et celles de l‚ÄôInsee √©galement, sont au format\nwide.\nLe prochain exercice illustre l‚Äôint√©r√™t de faire la conversion long \\(\\to\\) wide\navant de faire un graphique.\n\n\n Exercice 5: Restructurer les donn√©es : wide to long\n\nCr√©er une copie des donn√©es de l‚ÄôADEME en faisant df_wide = emissions_wide.copy()\nRestructurer les donn√©es au format long pour avoir des donn√©es d‚Äô√©missions par secteur en gardant comme niveau d‚Äôanalyse la commune (attention aux autres variables identifiantes).\nFaire la somme par secteur et repr√©senter graphiquement\nGarder, pour chaque d√©partement, le secteur le plus polluant"
  },
  {
    "objectID": "content/manipulation/02b_pandas_TP.html#combiner-les-donn√©es",
    "href": "content/manipulation/02b_pandas_TP.html#combiner-les-donn√©es",
    "title": "Pratique de pandas : un exemple complet",
    "section": "Combiner les donn√©es",
    "text": "Combiner les donn√©es\n\nTravail pr√©liminaire\nJusqu‚Äô√† pr√©sent lorsque nous avons produit des statistiques descriptives,\ncelles-ci √©taient univari√©es, c‚Äôest-√†-dire que nous produisions de l‚Äôinformation\nsur une variable mais nous ne la mettions pas en lien avec une autre. Pourtant,\non est rapidement amen√© √† d√©sirer expliquer certaines statistiques agr√©g√©es √† partir\nde caract√©ristiques issues d‚Äôune autre source de donn√©es. Cela implique\ndonc d‚Äôassocier des jeux de donn√©es,\nautrement dit de mettre en lien deux jeux de donn√©es\npr√©sentant le m√™me niveau d‚Äôinformation.\nOn appelle ceci faire un merge ou un join. De mani√®re illustr√©e,\nceci revient √† effectuer ce type d‚Äôop√©ration :\n\n\n\n\n\nAvant de faire ceci, il est n√©anmoins n√©cessaire de s‚Äôassurer que les variables\ncommunes entre les bases de donn√©es pr√©sentent le bon niveau d‚Äôinformation.\n\n\n Exercice 6: v√©rification des cl√©s de jointure\nOn commence par v√©rifier les dimensions des DataFrames et la structure de certaines variables cl√©s.\nEn l‚Äôoccurrence, les variables fondamentales pour lier nos donn√©es sont les variables communales.\nIci, on a deux variables g√©ographiques: un code commune et un nom de commune.\n\nV√©rifier les dimensions des DataFrames.\nIdentifier dans filosofi les noms de communes qui correspondent √† plusieurs codes communes et s√©lectionner leurs codes. En d‚Äôautres termes, identifier les LIBGEO tels qu‚Äôil existe des doublons de CODGEO et les stocker dans un vecteur x (conseil: faire attention √† l‚Äôindex de x).\n\nOn se focalise temporairement sur les observations o√π le libell√© comporte plus de deux codes communes diff√©rents\n\nQuestion 3. Regarder dans filosofi ces observations.\nQuestion 4. Pour mieux y voir, r√©ordonner la base obtenue par order alphab√©tique.\nQuestion 5. D√©terminer la taille moyenne (variable nombre de personnes: NBPERSMENFISC16) et quelques statistiques descriptives de ces donn√©es.\nComparer aux m√™mes statistiques sur les donn√©es o√π libell√©s et codes communes co√Øncident.\nQuestion 6. V√©rifier les grandes villes (plus de 100 000 personnes),\nla proportion de villes pour lesquelles un m√™me nom est associ√© √† diff√©rents codes commune.\nQuestion 7. V√©rifier dans filosofi les villes dont le libell√© est √©gal √† Montreuil.\nV√©rifier √©galement celles qui contiennent le terme ‚ÄòSaint-Denis‚Äô.\n\n\n\nCe petit exercice permet de se rassurer car les libell√©s dupliqu√©s\nsont en fait des noms de commune identiques mais qui ne sont pas dans le m√™me d√©partement.\nIl ne s‚Äôagit donc pas d‚Äôobservations dupliqu√©es.\nOn peut donc se fier aux codes communes, qui eux sont uniques.\n\n\nAssocier des donn√©es\nUne information que l‚Äôon cherche √† obtenir s‚Äôobtient de moins en moins √† partir d‚Äôune unique base de donn√©es. Il devient commun de devoir combiner des donn√©es issues de sources diff√©rentes.\nNous allons ici nous focaliser sur le cas le plus favorable qui est la situation o√π une information permet d‚Äôapparier de mani√®re exacte deux bases de donn√©es (autrement nous serions dans une situation, beaucoup plus complexe, d‚Äôappariement flou). La situation typique est l‚Äôappariement entre deux sources de donn√©es selon un identifiant individuel ou un identifiant de code commune, ce qui est notre cas.\nIl est recommand√© de lire ce guide assez complet sur la question des jointures avec R qui donne des recommandations √©galement utiles en Python.\nDans le langage courant du statisticien,\non utilise de mani√®re indiff√©rente les termes merge ou join. Le deuxi√®me terme provient de la syntaxe SQL.\nQuand on fait du Pandas, on utilise plut√¥t la commande merge.\n\n\n\n\n\n\n\n Exercice 7: Calculer l'empreinte carbone par habitant\nEn premier lieu, on va calculer l‚Äôempreinte carbone de chaque commune.\n\nCr√©er une variable emissions qui correspond aux √©missions totales d‚Äôune commune\nFaire une jointure √† gauche entre les donn√©es d‚Äô√©missions et les donn√©es de cadrage2.\nCalculer l‚Äôempreinte carbone (√©missions totales / population).\n\nA ce stade nous pourrions avoir envie d‚Äôaller vers la mod√©lisation pour essayer d‚Äôexpliquer\nles d√©terminants de l‚Äôempreinte carbone √† partir de variables communales.\nUne approche inf√©rentielle n√©cessite n√©anmoins pour √™tre pertinente de\nv√©rifier en amont des statistiques descriptives.\n\nSortir un histogramme en niveau puis en log de l‚Äôempreinte carbone communale.\n\nAvec une meilleure compr√©hension de nos donn√©es, nous nous rapprochons\nde la statistique inf√©rentielle. N√©anmoins, nous avons jusqu‚Äô√† pr√©sent\nconstruit des statistiques univari√©es mais n‚Äôavons pas cherch√© √† comprendre\nles r√©sultats en regardant le lien avec d‚Äôautres variables.\nCela nous am√®ne vers la statistique bivari√©e, notamment l‚Äôanalyse des corr√©lations.\nCe travail est important puisque toute mod√©lisation ult√©rieure consistera √†\nraffiner l‚Äôanalyse des corr√©lations pour tenir compte des corr√©lations crois√©es\nentre multiples facteurs. On propose ici de faire cette analyse\nde mani√®re minimale.\n\nRegarder la corr√©lation entre les variables de cadrage et l‚Äôempreinte carbone. Certaines variables semblent-elles pouvoir potentiellement influer sur l‚Äôempreinte carbone ?\n\n\n\nA l‚Äôissue de la question 5, le graphique des corr√©lations est le suivant :\n\n\n&lt;Axes: ylabel='index'&gt;"
  },
  {
    "objectID": "content/manipulation/02b_pandas_TP.html#exercices-bonus",
    "href": "content/manipulation/02b_pandas_TP.html#exercices-bonus",
    "title": "Pratique de pandas : un exemple complet",
    "section": "Exercices bonus",
    "text": "Exercices bonus\nLes plus rapides d‚Äôentre vous sont invit√©s √† aller un peu plus loin en s‚Äôentra√Ænant avec des exercices bonus qui proviennent du site de Xavier Dupr√©. 3 notebooks en lien avec numpy et pandas vous y sont propos√©s :\n\nCalcul Matriciel, Optimisation : √©nonc√© / corrig√©\nDataFrame et Graphes : √©nonc√© / corrig√©\nPandas et it√©rateurs : √©nonc√© / corrig√©"
  },
  {
    "objectID": "content/manipulation/02b_pandas_TP.html#footnotes",
    "href": "content/manipulation/02b_pandas_TP.html#footnotes",
    "title": "Pratique de pandas : un exemple complet",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPar manque d‚Äôimagination, on est souvent tent√© d‚Äôappeler notre\ndataframe principal df ou data. C‚Äôest souvent une mauvaise id√©e puisque\nce nom n‚Äôest pas tr√®s informatif quand on relit le code quelques semaines\nplus tard. L‚Äôautodocumentation, approche qui consiste √† avoir un code\nqui se comprend de lui-m√™me, est une bonne pratique et il est donc recommand√©\nde donner un nom simple mais efficace pour conna√Ætre la nature du dataset en question.‚Ü©Ô∏é\nId√©alement, il serait n√©cessaire de s‚Äôassurer que cette jointure n‚Äôintroduit\npas de biais. En effet, comme nos ann√©es de r√©f√©rence ne sont pas forc√©ment identiques,\nil peut y avoir un mismatch entre nos deux sources. Le TP √©tant d√©j√† long, nous n‚Äôallons pas dans cette voie.\nLes lecteurs int√©ress√©s pourront effectuer une telle analyse en exercice suppl√©mentaire.‚Ü©Ô∏é"
  },
  {
    "objectID": "content/manipulation/01_numpy.html",
    "href": "content/manipulation/01_numpy.html",
    "title": "Numpy, la brique de base de la data science",
    "section": "",
    "text": "Pour essayer les exemples pr√©sents dans ce tutoriel :\nIl est recommand√© de r√©guli√®rement se r√©f√©rer √†\nla cheatsheet numpy et √† la\ndoc officielle en cas de doute\nsur une fonction.\nDans ce chapitre, on ne d√©rogera pas √† la convention qui s‚Äôest impos√©e\nd‚Äôimporter Numpy de la\nmani√®re suivante :\nimport numpy as np\nNous allons √©galement fixer la racine du g√©n√©rateur al√©atoire de nombres\nafin d‚Äôavoir des r√©sultats reproductibles :\nnp.random.seed(12345)\nNote\nLes auteurs\nde numpy pr√©conisent d√©sormais\nde privil√©gier l‚Äôutilisation de\ng√©n√©rateurs via la fonction default_rng() plut√¥t que la simple utilisation de numpy.random.\nAfin d‚Äô√™tre en phase avec les codes qu‚Äôon peut trouver partout sur internet, nous\nconservons encore np.random.seed mais cela peut √™tre amen√© √† √©voluer.\nSi les scripts suivants sont ex√©cut√©s dans un Notebook Jupyter,\nil est recommand√© d‚Äôutiliser les param√®tres suivants\npour contr√¥ler le rendu:"
  },
  {
    "objectID": "content/manipulation/01_numpy.html#le-concept-darray",
    "href": "content/manipulation/01_numpy.html#le-concept-darray",
    "title": "Numpy, la brique de base de la data science",
    "section": "Le concept d‚Äôarray",
    "text": "Le concept d‚Äôarray\nLe concept central de NumPy (Numerical Python) est\nl‚Äôarray qui est un tableau de donn√©es multidimensionnel.\nL‚Äôarray numpy peut √™tre unidimensionnel et s‚Äôapparenter √† un\nvecteur (1d-array),\nbidimensionnel et ainsi s‚Äôapparenter √† une matrice (2d-array) ou,\nde mani√®re plus g√©n√©rale,\nprendre la forme d‚Äôun objet\nmultidimensionnel (Nd-array).\nLes tableaux simples (uni ou bi-dimensionnels) sont faciles √† se repr√©senter et seront particuli√®rement\nutilis√©s dans le paradigme des DataFrames mais\nla possibilit√© d‚Äôavoir des objets multidimensionnels permettra d‚Äôexploiter des\nstructures tr√®s complexes.\nUn DataFrame sera construit √† partir d‚Äôune collection\nd‚Äôarray uni-dimensionnels (les variables de la table), ce qui permettra d‚Äôeffectuer des op√©rations coh√©rentes\n(et optimis√©es) avec le type de la variable.\nPar rapport √† une liste,\n\nun array ne peut contenir qu‚Äôun type de donn√©es (integer, string, etc.),\ncontrairement √† une liste.\nles op√©rations impl√©ment√©es par numpy seront plus efficaces et demanderont moins\nde m√©moire\n\nLes donn√©es g√©ographiques constitueront une construction un peu plus complexe qu‚Äôun DataFrame traditionnel.\nLa dimension g√©ographique prend la forme d‚Äôun tableau plus profond, au moins bidimensionnel\n(coordonn√©es d‚Äôun point)."
  },
  {
    "objectID": "content/manipulation/01_numpy.html#cr√©er-un-array",
    "href": "content/manipulation/01_numpy.html#cr√©er-un-array",
    "title": "Numpy, la brique de base de la data science",
    "section": "Cr√©er un array",
    "text": "Cr√©er un array\nOn peut cr√©er un array de plusieurs mani√®res. Pour cr√©er un array √† partir d‚Äôune liste,\nil suffit d‚Äôutiliser la m√©thode array:\n\nnp.array([1,2,5])\n\narray([1, 2, 5])\n\n\nIl est possible d‚Äôajouter un argument dtype pour contraindre le type du array:\n\nnp.array([[\"a\",\"z\",\"e\"],[\"r\",\"t\"],[\"y\"]], dtype=\"object\")\n\narray([list(['a', 'z', 'e']), list(['r', 't']), list(['y'])], dtype=object)\n\n\nIl existe aussi des m√©thodes pratiques pour cr√©er des array:\n\ns√©quences logiques : np.arange (suite) ou np.linspace (interpolation lin√©aire entre deux bornes)\ns√©quences ordonn√©es : array rempli de z√©ros, de 1 ou d‚Äôun nombre d√©sir√© : np.zeros, np.ones ou np.full\ns√©quences al√©atoires : fonctions de g√©n√©ration de nombres al√©atoires : np.rand.uniform, np.rand.normal, etc.\ntableau sous forme de matrice identit√©: np.eye\n\nCeci donne ainsi, pour les s√©quences logiques:\n\nnp.arange(0,10)\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\nnp.arange(0,10,3)\n\narray([0, 3, 6, 9])\n\n\n\nnp.linspace(0, 1, 5)\n\narray([0.  , 0.25, 0.5 , 0.75, 1.  ])\n\n\nPour un array initialis√© √† 0:\n\nnp.zeros(10, dtype=int)\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\nou initialis√© √† 1:\n\nnp.ones((3, 5), dtype=float)\n\narray([[1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.]])\n\n\nou encore initialis√© √† 3.14:\n\n\narray([[3.14, 3.14, 3.14, 3.14, 3.14],\n       [3.14, 3.14, 3.14, 3.14, 3.14],\n       [3.14, 3.14, 3.14, 3.14, 3.14]])\n\n\nEnfin, pour cr√©er la matrice \\(I_3\\):\n\nnp.eye(3)\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\n\n\n Exercice 1\nG√©n√©rer:\n\n\\(X\\) une variable al√©atoire, 1000 r√©p√©titions d‚Äôune loi \\(U(0,1)\\)\n\\(Y\\) une variable al√©atoire, 1000 r√©p√©titions d‚Äôune loi normale de moyenne nulle et de variance √©gale √† 2\nV√©rifier la variance de \\(Y\\) avec np.var"
  },
  {
    "objectID": "content/manipulation/01_numpy.html#indexation-et-slicing",
    "href": "content/manipulation/01_numpy.html#indexation-et-slicing",
    "title": "Numpy, la brique de base de la data science",
    "section": "Indexation et slicing",
    "text": "Indexation et slicing\n\nLogique dans le cas d‚Äôun array unidimensionnel\nLa structure la plus simple est l‚Äôarray unidimensionnel:\n\nx = np.arange(10)\nprint(x)\n\n[0 1 2 3 4 5 6 7 8 9]\n\n\nL‚Äôindexation est dans ce cas similaire √† celle d‚Äôune liste:\n\nle premier √©l√©ment est 0\nle √©ni√®me √©l√©ment est accessible √† la position \\(n-1\\)\n\nLa logique d‚Äôacc√®s aux √©l√©ments est ainsi la suivante :\nx[start:stop:step]\nAvec un array unidimensionnel, l‚Äôop√©ration de slicing (garder une coupe du array) est tr√®s simple.\nPar exemple, pour garder les K premiers √©l√©ments d‚Äôun array, on fera:\nx[:(K-1)]\nEn l‚Äôoccurrence, on s√©lectionne le K\\(^{eme}\\) √©l√©ment en utilisant\nx[K-1]\nPour s√©lectionner uniquement un √©l√©ment, on fera ainsi:\n\nx = np.arange(10)\nx[2]\n\n2\n\n\nLes syntaxes qui permettent de s√©lectionner des indices particuliers d‚Äôune liste fonctionnent √©galement\navec les arrays.\n\n\n Exercice 2\nPrenez x = np.arange(10) et‚Ä¶\n\nS√©lectionner les √©l√©ments 0,3,5 de x\nS√©lectionner les √©l√©ments pairs\nS√©lectionner tous les √©l√©ments sauf le premier\nS√©lectionner les 5 premiers √©l√©ments\n\n\n\n\n\nSur la performance\nUn √©l√©ment d√©terminant dans la performance de numpy par rapport aux listes,\nlorsqu‚Äôil est question de\nslicing est qu‚Äôun array ne renvoie pas une\ncopie de l‚Äô√©l√©ment en question (copie qui co√ªte de la m√©moire et du temps)\nmais simplement une vue de celui-ci.\nLorsqu‚Äôil est n√©cessaire d‚Äôeffectuer une copie,\npar exemple pour ne pas alt√©rer l‚Äôarray sous-jacent, on peut\nutiliser la m√©thode copy:\nx_sub_copy = x[:2, :2].copy()\n\n\n\nFiltres logiques\nIl est √©galement possible, et plus pratique, de s√©lectionner des donn√©es √† partir de conditions logiques\n(op√©ration qu‚Äôon appelle un boolean mask).\nCette fonctionalit√© servira principalement √†\neffectuer des op√©rations de filtre sur les donn√©es.\nPour des op√©rations de comparaison simples, les comparateurs logiques peuvent √™tre suffisants.\nCes comparaisons fonctionnent aussi sur les tableaux multidimensionnels gr√¢ce au\nbroadcasting sur lequel nous reviendrons :\n\nx = np.arange(10)\nx2 = np.array([[-1,1,-2],[-3,2,0]])\nprint(x)\nprint(x2)\n\n[0 1 2 3 4 5 6 7 8 9]\n[[-1  1 -2]\n [-3  2  0]]\n\n\n\nx==2\nx2&lt;0\n\narray([[ True, False,  True],\n       [ True, False, False]])\n\n\nPour s√©lectionner les observations relatives √† la condition logique,\nil suffit d‚Äôutiliser la logique de slicing de numpy qui fonctionne avec les conditions logiques\n\n\n Exercice 3\nSoit\nx = np.random.normal(size=10000)\n\nNe conserver que les valeurs dont la valeur absolue est sup√©rieure √† 1.96\nCompter le nombre de valeurs sup√©rieures √† 1.96 en valeur absolue et leur proportion dans l‚Äôensemble\nSommer les valeurs absolues de toutes les observations sup√©rieures (en valeur absolue) √† 1.96\net rapportez les √† la somme des valeurs de x (en valeur absolue)\n\n\n\nLorsque c‚Äôest possible, il est recommand√© d‚Äôutiliser les fonctions logiques de numpy (optimis√©es et\nqui g√®rent bien la dimension).\nParmi elles, on peut retrouver:\n\ncount_nonzero\nisnan\nany ; all ; notamment avec l‚Äôargument axis\nnp.array_equal pour v√©rifier, √©l√©ment par √©l√©ment, l‚Äô√©galit√©\n\nSoit\n\nx = np.random.normal(0, size=(3, 4))\n\nun array multidimensionnel et\n\ny = np.array([np.nan, 0, 1])\n\nun array unidimensionnel pr√©sentant une valeur manquante.\n\n\n Exercice 4\n\nUtiliser count_nonzero sur y\nUtiliser isnan sur y et compter le nombre de valeurs non NaN\nV√©rifier que x comporte au moins une valeur positive dans son ensemble, en parcourant les lignes puis les colonnes.\n\nNote : Jetez un oeil √† ce que correspond le param√®tre axis dans numpy en vous documentant sur internet. Par exemple ici."
  },
  {
    "objectID": "content/manipulation/01_numpy.html#manipuler-un-array",
    "href": "content/manipulation/01_numpy.html#manipuler-un-array",
    "title": "Numpy, la brique de base de la data science",
    "section": "Manipuler un array",
    "text": "Manipuler un array\nDans cette section, on utilisera un array multidimensionnel:\n\nx = np.random.normal(0, size=(3, 4))\n\n\nStatistiques sur un array\nPour les statistiques descriptives classiques,\nNumpy propose un certain nombre de fonctions d√©j√† impl√©ment√©es,\nqui peuvent √™tre combin√©es avec l‚Äôargument axis\n\n\n Exercice 5\n\nFaire la somme de tous les √©l√©ments d‚Äôun array, des √©l√©ments en ligne et des √©l√©ments en colonne. V√©rifier\nla coh√©rence\nEcrire une fonction statdesc pour renvoyer les valeurs suivantes : moyenne, m√©diane, √©cart-type, minimum et maximum.\nL‚Äôappliquer sur x en jouant avec l‚Äôargument axis\n\n\n\n\n\nFonctions de manipulation\nVoici quelques fonctions pour modifier un array,\n\n\n\n\n\n\n\nOp√©ration\nImpl√©mentation\n\n\n\n\nAplatir un array\nx.flatten() (m√©thode)\n\n\nTransposer un array\nx.T (m√©thode) ou np.transpose(x) (fonction)\n\n\nAjouter des √©l√©ments √† la fin\nnp.append(x, [1,2])\n\n\nAjouter des √©l√©ments √† un endroit donn√© (aux positions 1 et 2)\nnp.insert(x, [1,2], 3)\n\n\nSupprimer des √©l√©ments (aux positions 0 et 3)\nnp.delete(x, [0,3])\n\n\n\nPour combiner des array, on peut utiliser, selon les cas,\nles fonctions np.concatenate, np.vstack ou la m√©thode .r_ (concat√©nation rowwise).\nnp.hstack ou la m√©thode .column_stack ou .c_ (concat√©nation column-wise)\n\nx = np.random.normal(size = 10)\n\nPour ordonner un array, on utilise np.sort\n\nx = np.array([7, 2, 3, 1, 6, 5, 4])\n\nnp.sort(x)\n\narray([1, 2, 3, 4, 5, 6, 7])\n\n\nSi on d√©sire faire un r√©-ordonnement partiel pour trouver les k valeurs les plus petites d‚Äôun array sans les ordonner, on utilise partition:\n\nnp.partition(x, 3)\n\narray([2, 1, 3, 4, 6, 5, 7])"
  },
  {
    "objectID": "content/manipulation/01_numpy.html#broadcasting",
    "href": "content/manipulation/01_numpy.html#broadcasting",
    "title": "Numpy, la brique de base de la data science",
    "section": "Broadcasting",
    "text": "Broadcasting\nLe broadcasting d√©signe un ensemble de r√®gles permettant\nd‚Äôappliquer des op√©rations sur des tableaux de dimensions diff√©rentes. En pratique,\ncela consiste g√©n√©ralement √† appliquer une seule op√©ration √† l‚Äôensemble des membres d‚Äôun tableau numpy.\nLa diff√©rence peut √™tre comprise √† partir de l‚Äôexemple suivant. Le broadcasting permet\nde transformer le scalaire 5 en array de dimension 3:\n\na = np.array([0, 1, 2])\n\nb = np.array([5, 5, 5])\n\na + b\na + 5\n\narray([5, 6, 7])\n\n\nLe broadcasting peut √™tre tr√®s pratique pour effectuer de mani√®re efficace des op√©rations sur des donn√©es √†\nla structure complexe. Pour plus de d√©tails, se rendre\nici ou ici."
  },
  {
    "objectID": "content/manipulation/01_numpy.html#une-application-programmer-ses-propres-k-nearest-neighbors",
    "href": "content/manipulation/01_numpy.html#une-application-programmer-ses-propres-k-nearest-neighbors",
    "title": "Numpy, la brique de base de la data science",
    "section": "Une application: programmer ses propres k-nearest neighbors",
    "text": "Une application: programmer ses propres k-nearest neighbors\n\n\n\n Exercice (un peu plus cors√©)\n\nCr√©er X un tableau √† deux dimensions (i.e.¬†une matrice) comportant 10 lignes\net 2 colonnes. Les nombres dans le tableau sont al√©atoires.\nImporter le module matplotlib.pyplot sous le nom plt. Utiliser\nplt.scatter pour repr√©senter les donn√©es sous forme de nuage de points.\nConstuire une matrice 10x10 stockant, √† l‚Äô√©l√©ment \\((i,j)\\), la distance euclidienne entre les points \\(X[i,]\\) et \\(X[j,]\\). Pour cela, il va falloir jouer avec les dimensions en cr√©ant des tableaux embo√Æt√©s √† partir par des appels √† np.newaxis :\n\n\nEn premier lieu, utiliser X1 = X[:, np.newaxis, :] pour transformer la matrice en tableau embo√Æt√©. V√©rifier les dimensions\nCr√©er X2 de dimension (1, 10, 2) √† partir de la m√™me logique\nEn d√©duire, pour chaque point, la distance avec les autres points pour chaque coordonn√©es. Elever celle-ci au carr√©\nA ce stade, vous devriez avoir un tableau de dimension (10, 10, 2). La r√©duction √† une matrice s‚Äôobtient en sommant sur le dernier axe. Regarder dans l‚Äôaide de np.sum comme effectuer une somme sur le dernier axe.\nEnfin, appliquer la racine carr√©e pour obtenir une distance euclidienne en bonne et due forme.\n\n\nV√©rifier que les termes diagonaux sont bien nuls (distance d‚Äôun point √† lui-m√™me‚Ä¶)\nIl s‚Äôagit maintenant de classer, pour chaque point, les points dont les valeurs sont les plus similaires. Utiliser np.argsort pour obtenir, pour chaque ligne, le classement des points les plus proches\nOn va s‚Äôint√©resser aux k-plus proches voisins. Pour le moment, fixons k=2. Utiliser argpartition pour r√©ordonner chaque ligne de mani√®re √† avoir les 2 plus proches voisins de chaque point d‚Äôabord et le reste de la ligne ensuite\nUtiliser le morceau de code ci-dessous\n\n\n\n\nUn indice pour repr√©senter graphiquement les plus proches voisins\nplt.scatter(X[:, 0], X[:, 1], s=100)\n\n# draw lines from each point to its two nearest neighbors\nK = 2\n\nfor i in range(X.shape[0]):\n    for j in nearest_partition[i, :K+1]:\n        # plot a line from X[i] to X[j]\n        # use some zip magic to make it happen:\n        plt.plot(*zip(X[j], X[i]), color='black')\n\n\nPour la question 2, vous devriez obtenir un graphique ayant cet aspect :\n\n\n\n\n\nLe r√©sultat de la question 7 est le suivant :\n\n\n\n\n\nAi-je invent√© cet exercice cors√© ? Pas du tout, il vient de l‚Äôouvrage Python Data Science Handbook. Mais, si je vous l‚Äôavais indiqu√© imm√©diatement, auriez-vous cherch√© √† r√©pondre aux questions ?\nPar ailleurs, il ne serait pas une bonne id√©e de g√©n√©raliser cet algorithme √† de grosses donn√©es. La complexit√© de notre approche est \\(O(N^2)\\). L‚Äôalgorithme impl√©ment√© par Scikit-Learn est\nen \\(O[NlogN]\\).\nDe plus, le calcul de distances matricielles en utilisant la puissance des cartes graphiques serait plus rapide. A cet √©gard, la librairie faiss offre des performances beaucoup plus satisfaisantes que celles que permettraient numpy sur ce probl√®me pr√©cis."
  },
  {
    "objectID": "content/manipulation/01_numpy.html#exercices-suppl√©mentaires",
    "href": "content/manipulation/01_numpy.html#exercices-suppl√©mentaires",
    "title": "Numpy, la brique de base de la data science",
    "section": "Exercices suppl√©mentaires",
    "text": "Exercices suppl√©mentaires\n\n\n Comprendre le principe de l'algorithme PageRank\nGoogle est devenu c√©l√®bre gr√¢ce √† son algorithme PageRank. Celui-ci permet, √† partir\nde liens entre sites web, de donner un score d‚Äôimportance √† un site web qui va\n√™tre utilis√© pour √©valuer sa centralit√© dans un r√©seau.\nL‚Äôobjectif de cet exercice est d‚Äôutiliser Numpy pour mettre en oeuvre un tel\nalgorithme √† partir d‚Äôune matrice d‚Äôadjacence qui relie les sites entre eux.\n\nCr√©er la matrice suivante avec numpy. L‚Äôappeler M:\n\n\\[\n\\begin{bmatrix}\n0 & 0 & 0 & 0 & 1 \\\\\n0.5 & 0 & 0 & 0 & 0 \\\\\n0.5 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0.5 & 0 & 0 \\\\\n0 & 0 & 0.5 & 1 & 0\n\\end{bmatrix}\n\\]\n\nPour repr√©senter visuellement ce web minimaliste,\nconvertir en objet networkx (une librairie sp√©cialis√©e\ndans l‚Äôanalyse de r√©seau) et utiliser la fonction draw\nde ce package.\n\nIl s‚Äôagit de la transpos√©e de la matrice d‚Äôadjacence\nqui permet de relier les sites entre eux. Par exemple,\nle site 1 (premi√®re colonne) est r√©f√©renc√© par\nles sites 2 et 3. Celui-ci ne r√©f√©rence que le site 5.\n\nA partir de la page wikipedia anglaise de PageRank, tester\nsur votre matrice.\n\n\n\n\n\n\n\n\n\n\n\n\nLe site 1 est assez central car il est r√©f√©renc√© 2 fois. Le site\n5 est lui √©galement central puisqu‚Äôil est r√©f√©renc√© par le site 1.\n\n\narray([[0.25419178],\n       [0.13803151],\n       [0.13803151],\n       [0.20599017],\n       [0.26375504]])\n\n\nD‚Äôautres id√©es :\n\nSimulations de variables al√©atoires ;\nTCL ;"
  },
  {
    "objectID": "content/getting-started/07_rappels_classes.html#quest-ce-que-la-programmation-orient√©e-objet",
    "href": "content/getting-started/07_rappels_classes.html#quest-ce-que-la-programmation-orient√©e-objet",
    "title": "Les classes en Python",
    "section": "Qu‚Äôest-ce que la programmation orient√©e objet ?",
    "text": "Qu‚Äôest-ce que la programmation orient√©e objet ?\nLe langage Python se base sur des objets et d√©finit pour eux des actions.\nSelon le type d‚Äôobjet, les actions seront diff√©rentes.\nOn parle √† ce propos de langage orient√© objet ce qui signifie\nque la syntaxe du langage Python permet de d√©finir de mani√®re conceptuelle\ndes objets et appliquer des traitements coh√©rents avec leur structure interne.\nPar exemple,\npour manipuler des donn√©es textuelles ou num√©riques, on aura\nbesoin d‚Äôappliquer des m√©thodes diff√©rentes. Prenons l‚Äôexemple\nde l‚Äôop√©ration +. Pour des donn√©es num√©riques, il s‚Äôagit\nde l‚Äôaddition. Pour des donn√©es textuelles, l‚Äôaddition n‚Äôa pas de sens\nmais on peut envisager d‚Äôappliquer cette op√©ration pour faire de la\nconcat√©nation.\nChaque type d‚Äôobjet se verra donc appliquer des actions\nadapt√©es. Cela offre une grande flexibilit√© au langage Python car on\npeut d√©finir une m√©thode g√©n√©rique (par exemple l‚Äôaddition) et l‚Äôadapter\n√† diff√©rents types d‚Äôobjets.\nLe fait que Python soit un langage orient√© objet a une influence sur la\nsyntaxe. On retrouvera r√©guli√®re la syntaxe objet.method qui est au coeur\nde Python. Par exemple pd.DataFrame.mean se traduit par\nappliquer la m√©thode mean a un objet de type pd.DataFrame.\n\nQuand utilise-t-on cela dans le domaine de la data science ?\nLes r√©seaux de neurones programm√©s avec Keras ou PyTorch fonctionnent de\ncette mani√®re. On part d‚Äôune structure de base et modifie les attributs (par\nexemple le nombre de couches) ou les m√©thodes."
  },
  {
    "objectID": "content/getting-started/07_rappels_classes.html#la-d√©finition-dun-objet",
    "href": "content/getting-started/07_rappels_classes.html#la-d√©finition-dun-objet",
    "title": "Les classes en Python",
    "section": "La d√©finition d‚Äôun objet",
    "text": "La d√©finition d‚Äôun objet\nPour d√©finir un objet, il faut lui donner des caract√©ristiques et des actions, ce qu‚Äôil est, ce qu‚Äôil peut faire.\nAvec une liste, on peut ajouter des √©l√©ments par exemple avec l‚Äôaction .append(). On peut cr√©er autant d‚Äôobjets ‚Äúliste‚Äù qu‚Äôon le souhaite.\nUne classe regroupe des fonctions et des attributs qui d√©finissent un objet.\nUn objet est une instance d‚Äôune classe, c‚Äôest-√†-dire un exemplaire issu de la classe. L‚Äôobjet avec un comportement et un √©tat, tous deux d√©finis par la classe. On peut cr√©er autant d‚Äôobjets que l‚Äôon d√©sire avec une classe donn√©e.\nIci nous allons essayer de cr√©er une classe chat, avec des attributs pour caract√©riser le chat et des actions, pour voir ce qu‚Äôil peut faire avec un objet de la classe chat."
  },
  {
    "objectID": "content/getting-started/07_rappels_classes.html#exemple-la-classe-chat",
    "href": "content/getting-started/07_rappels_classes.html#exemple-la-classe-chat",
    "title": "Les classes en Python",
    "section": "Exemple : la Classe chat()",
    "text": "Exemple : la Classe chat()\n\nLes attributs de la classe chat\n\nClasse chat version 1 - premiers attributs\nOn veut pouvoir cr√©er un objet chat() qui nous permettra √† terme de cr√©er une colonie de chats (on sait\njamais ca peut servir ‚Ä¶).\nPour commencer, on va d√©finir un chat avec des attributs de base : une couleur et un nom.\n\nclass chat: # D√©finition de notre classe chat\n    \"\"\"Classe d√©finissant un chat caract√©ris√© par :\n    - son nom\n    - sa couleur \"\"\"\n    \n    def __init__(self): # Notre m√©thode constructeur - \n        # self c'est notre objet qu'on est en train de cr√©er\n        \"\"\"Pour l'instant, on ne va d√©finir que deux attributs - nom et couleur \"\"\"\n        self.couleur = \"Noir\"   \n        self.nom = \"Aucun nom\"\n\n\nmon_chat = chat()\n\nprint(type(mon_chat), mon_chat.couleur ,\",\", mon_chat.nom) \n\n&lt;class '__main__.chat'&gt; Noir , Aucun nom\n\n\nOn nous dit bien que Mon chat est d√©fini √† partir de la classe chat,\nc‚Äôest ce que nous apprend la fonction type.\nPour l‚Äôinstant il n‚Äôa pas de nom\n\n\nClasse chat version 2 - autres attributs\nAvec un nom et une couleur, on ne va pas loin. On peut continuer √† d√©finir des attributs pour la classe chat\nde la m√™me fa√ßon que pr√©c√©demment.\n\nclass chat: # D√©finition de notre classe chat\n    \"\"\"Classe d√©finissant un chat caract√©ris√© par :\n    - sa couleur\n    - son √¢ge\n    - son caract√®re\n    - son poids\n    - son maitre\n    - son nom \"\"\"\n\n    \n    def __init__(self): # Notre m√©thode constructeur - \n        #self c'est notre objet qu'on est en train de cr√©er\n        self.couleur = \"Noir\"    \n        self.age = 10\n        self.caractere = \"Joueur\"\n        self.poids = 3\n        self.maitre = \"Jeanne\"\n        self.nom = \"Aucun nom\"\n\n\nhelp(chat) \n# si on veut savoir ce que fait la classe \"chat\" on appelle l'aide\n\nHelp on class chat in module __main__:\n\nclass chat(builtins.object)\n |  Classe d√©finissant un chat caract√©ris√© par :\n |  - sa couleur\n |  - son √¢ge\n |  - son caract√®re\n |  - son poids\n |  - son maitre\n |  - son nom\n |  \n |  Methods defined here:\n |  \n |  __init__(self)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n\n\n\nmon_chat = chat()\nprint(\"L'√¢ge du chat est\", mon_chat.age,\"ans\") \n# on avait d√©fini l'attribut age de la classe chat comme √©tant √©gal √† 10\n#, si on demande l'attribut age de notre Martin on obtient 10\n\nL'√¢ge du chat est 10 ans\n\n\nPar d√©faut, les attributs de la classe Chat seront toujours les m√™mes √† chaque cr√©ation de chat √† partir\nde la classe Chat.\nMais une fois qu‚Äôune instance de classe est cr√©√©e (ici mon chat est une instance de classe) on peut d√©cider\nde changer la valeur de ses attributs.\n\n\nUn nouveau poids\n\nprint(mon_chat.poids)\n# si on veut changer le poids de mon chat, parce qu'il a un peu grossi apr√®s les f√™tes\nmon_chat.poids = 3.5\nprint(mon_chat.poids) # maintenant le poids est 3.5\n\n3\n3.5\n\n\n\n\nUn nouveau nom\n\n# on veut aussi lui donner un nom \nmon_chat.nom = \"Martin\"\nmon_chat.nom\n\n'Martin'\n\n\n\n\nUne autre instance de la classe Chat\nOn peut aussi cr√©er d‚Äôautres objets chat √† partir de la classe chat :\n\n# on appelle la classe\nl_autre_chat = chat()\n# on change les attributs qui nous int√©ressent\nl_autre_chat.nom = \"Ginette\"\nl_autre_chat.maitre = \"Roger\"\n# les attributs inchang√©s donnent la m√™me chose \n# que ceux d√©finis par d√©faut pour la classe\nprint(l_autre_chat.couleur)\n\nNoir\n\n\n\n\n\nLes m√©thodes de la classe chat\nLes attributs sont des variables propres √† notre objet, qui servent √† le caract√©riser.\nLes m√©thodes sont plut√¥t des actions, comme nous l‚Äôavons vu dans la partie pr√©c√©dente, agissant sur l‚Äôobjet.\nPar exemple, la m√©thode append de la classe list permet d‚Äôajouter un √©l√©ment dans l‚Äôobjet list manipul√©.\n\nClasse chat version 3 - premi√®re m√©thode\nOn peut d√©finir une premi√®re m√©thode : nourrir\n\nclass chat: # D√©finition de notre classe chat\n    \"\"\"Classe d√©finissant un chat caract√©ris√© par :\n    - sa couleur\n    - son √¢ge\n    - son caract√®re\n    - son poids\n    - son maitre\n    - son nom \n    \n    L'objet chat a une m√©thode : nourrir \"\"\"\n\n    \n    def __init__(self): # Notre m√©thode constructeur - \n        #self c'est notre objet qu'on est en train de cr√©er\n        self.couleur = \"Noir\"    \n        self.age = 10\n        self.caractere = \"Joueur\"\n        self.poids = 3\n        self.maitre = \"Jeanne\"\n        self.nom = \"Aucun nom\"\n        \n        \"\"\"Par d√©faut, notre ventre est vide\"\"\"\n        self.ventre = \"\"\n        \n    def nourrir(self, nourriture):\n        \"\"\"M√©thode permettant de donner √† manger au chat.\n        Si le ventre n'est pas vide, on met une virgule avant de rajouter\n        la nourriture\"\"\"       \n        if self.ventre != \"\":\n            self.ventre += \",\"\n        self.ventre += nourriture\n\n\nmon_chat = chat()\nmon_chat.nom = \"Martin\"\nmon_chat.ventre # On n'a rien donn√© √† Martin, son ventre est vide\n\n''\n\n\n\n# on appelle la m√©thode \"nourrir\" de la classe chat, \n# on lui donne un √©l√©ment, ici des croquettes\nmon_chat.nourrir('Croquettes')\nprint(\"Le contenu du ventre de martin : \",mon_chat.ventre)\n\nLe contenu du ventre de martin :  Croquettes\n\n\n\nmon_chat.nourrir('Saumon')\nprint(\"Le contenu du ventre de martin : \",mon_chat.ventre)\n\nLe contenu du ventre de martin :  Croquettes,Saumon\n\n\n\n\nClasse chat version 4 - autre m√©thode\nAvec un chat, on peut imaginer plein de m√©thodes. Ici on va d√©finir une action ‚Äúnourrir‚Äù et une autre action\n‚Äúlitiere‚Äù, qui consiste √† vider l‚Äôestomac du chat.\n\nclass chat: # D√©finition de notre classe Personne\n    \"\"\"Classe d√©finissant un chat caract√©ris√© par :\n    - sa couleur\n    - son √¢ge\n    - son caract√®re\n    - son poids\n    - son maitre\n    - son nom \n    \n    L'objet chat a deux m√©thodes : nourrir et litiere \"\"\"\n\n    \n    def __init__(self): # Notre m√©thode constructeur - \n        #self c'est notre objet qu'on est en train de cr√©er\n        self.nom = \"\"\n        self.couleur = \"Roux\"    \n        self.age = 10\n        self.caractere = \"Joueur\"\n        self.poids = 3\n        self.maitre = \"Jeanne\"\n        \"\"\"Par d√©faut, notre ventre est vide\"\"\"\n        self.ventre = \"\"\n        \n    def nourrir(self, nourriture):\n        \"\"\"M√©thode permettant de donner √† manger au chat.\n        Si le ventre n'est pas vide, on met une virgule avant de rajouter\n        la nourriture\"\"\"       \n        if self.ventre != \"\":\n            self.ventre += \",\"\n        self.ventre += nourriture\n\n    def litiere(self) : \n        \"\"\" M√©thode permettant au chat d'aller √† sa liti√®re : \n        en cons√©quence son ventre est vide \"\"\"       \n        self.ventre = \"\"\n        print(self.nom,\"a le ventre vide\")\n\n\n# on d√©finit Martin le chat\nmon_chat = chat()\nmon_chat.nom = \"Martin\"\n# on le nourrit avec des croquettes\nmon_chat.nourrir('croquettes')\nprint(\"Le contenu du ventre de martin\", mon_chat.ventre)\n\n\n# Il va dans sa litiere\nmon_chat.litiere()\n\nLe contenu du ventre de martin croquettes\nMartin a le ventre vide\n\n\n\nhelp(mon_chat.nourrir)\nhelp(mon_chat.litiere)\n\nHelp on method nourrir in module __main__:\n\nnourrir(nourriture) method of __main__.chat instance\n    M√©thode permettant de donner √† manger au chat.\n    Si le ventre n'est pas vide, on met une virgule avant de rajouter\n    la nourriture\n\nHelp on method litiere in module __main__:\n\nlitiere() method of __main__.chat instance\n    M√©thode permettant au chat d'aller √† sa liti√®re : \n    en cons√©quence son ventre est vide\n\n\n\n\n\nLes m√©thodes sp√©ciales (facultatif)\nSi on reprend notre classe chat, il y a en r√©alit√© des m√©thodes sp√©ciales que nous n‚Äôavons pas d√©finies mais\nqui sont implicites.\nPython comprend seul ce que doivent faire ces m√©thodes. Il a une id√©e pr√©concue de ce qu‚Äôelles doivent\neffectuer comme op√©ration. Si vous ne red√©finissez par une m√©thode sp√©ciale pour qu‚Äôelle fasse ce que vous\nsouhaitez, ca peut donner des r\u0013esultats inattendus.\nElles servent √† plusieurs choses :\n\n√† initialiser l‚Äôobjet instanci√© : __init__\n√† modifier son affichage : __repr__\n\n\n\n# pour avoir la valeur de l'attribut \"nom\"\n\nprint(mon_chat.__getattribute__(\"nom\"))\n# on aurait aussi pu faire plus simple :\nprint(mon_chat.nom)\n\nMartin\nMartin\n\n\n# si l'attribut n'existe pas : on a une erreur\n# Python recherche l'attribut et, s'il ne le trouve pas dans l'objet et si une m√©thode __getattr__ est sp√©cifi√©e, \n# il va l'appeler en lui passant en param√®tre le nom de l'attribut recherch√©, sous la forme d'une cha√Æne de caract√®res.\n\nprint(mon_chat.origine)\n## Error in py_call_impl(callable, dots$args, dots$keywords): AttributeError: 'chat' object has no attribute 'origine'\n## \n## Detailed traceback: \n##   File \"&lt;string&gt;\", line 1, in &lt;module&gt;\nMais on peut modifier les m√©thodes sp√©ciales de notre classe chat pour √©viter d‚Äôavoir des erreurs d‚Äôattributs. On va aussi en profiter pour modifier la repr√©sentation de l‚Äôinstance chat qui pour l‚Äôinstant donne &lt;_main_.chat object at 0x0000000005AB4C50&gt;\n\n\nClasse chat version 5 - m√©thode sp√©ciale\n\nclass chat: # D√©finition de notre classe Personne\n    \"\"\"Classe d√©finissant un chat caract√©ris√© par :\n    - sa couleur\n    - son √¢ge\n    - son caract√®re\n    - son poids\n    - son maitre\n    - son nom \n    \n    L'objet chat a deux m√©thodes : nourrir et litiere \"\"\"\n\n    \n    def __init__(self): # Notre m√©thode constructeur - \n        #self c'est notre objet qu'on est en train de cr√©er\n        self.nom = \"\"\n        self.couleur = \"Roux\"    \n        self.age = 10\n        self.caractere = \"Joueur\"\n        self.poids = 3\n        self.maitre = \"Jeanne\"\n        \"\"\"Par d√©faut, notre ventre est vide\"\"\"\n        self.ventre = \"\"\n        \n    def nourrir(self, nourriture):\n        \"\"\"M√©thode permettant de donner √† manger au chat.\n        Si le ventre n'est pas vide, on met une virgule avant de rajouter\n        la nourriture\"\"\"       \n        if self.ventre != \"\":\n            self.ventre += \",\"\n        self.ventre += nourriture\n\n    def litiere(self) : \n        \"\"\" M√©thode permettant au chat d'aller √† sa liti√®re : \n        en cons√©quence son ventre est vide \"\"\"       \n        self.ventre = \"\"\n        print(self.nom,\"a le ventre vide\")\n    \n    def __getattribute__(self, key):\n            return print(key,\"n'est pas un attribut de la classe chat\")   \n            \n    def __repr__(self):\n            return \"Je suis une instance de la classe chat\"\n\n\n# j'ai gard√© l'exemple chat d√©fini selon la classe version 4\n# Martin, le chat\n# on a vu pr√©c√©demment qu'il n'avait pas d'attribut origine\n# et que cela levait une erreur AttributeError\nprint(mon_chat.nom)\n\n\n# on va d√©finir un nouveau chat avec la version 5\n# on appelle √† nouveau un attribut qui n'existe pas \"origine\"\n# on a bien le message d√©fini par la m√©thode sp√©ciale _gettattribute\n\nmon_chat_nouvelle_version = chat()\nmon_chat_nouvelle_version.origine\n\n# Maintenant on a aussi une d√©finition de l'objet plus clair\nprint(mon_chat)\nprint(mon_chat_nouvelle_version)\n\nMartin\norigine n'est pas un attribut de la classe chat\n&lt;__main__.chat object at 0x7f61fc41b7c0&gt;\nJe suis une instance de la classe chat\n\n\n\n\n\n\nConclusion sur les classes : ce qu‚Äôon retient\n\nLes m√©thodes se d√©finissent comme des fonctions, sauf qu‚Äôelles se trouvent dans le corps de la classe.\nOn d√©finit les attributs d‚Äôune instance dans le constructeur de sa classe, en suivant cette syntaxe : self.nom_attribut = valeur.\nfacultatif : Les m√©thodes d‚Äôinstance prennent en premier param√®tre ‚Äúself‚Äù, l‚Äôinstance de l‚Äôobjet manipul√©.\nfacultatif : On construit une instance de classe en appelant son constructeur, une m√©thode d‚Äôinstance appel√©e init."
  },
  {
    "objectID": "content/getting-started/05_rappels_types.html",
    "href": "content/getting-started/05_rappels_types.html",
    "title": "Quelques rappels sur les principes de base de Python",
    "section": "",
    "text": "pandas et numpy sont\nessentiels pour manipuler les donn√©es.\nN√©anmoins, il est n√©cessaire de ne pas faire l‚Äôimpasse sur les fondements\ndu langage Python. Une bonne compr√©hension des √©l√©ments structurants le\nlangage entra√Æne une plus grande productivit√© et libert√©.\nCe chapitre est inspir√© du mat√©riel qui √©tait propos√©\npar Xavier Dupr√©,\nle pr√©c√©dent professeur de ce cours.\nVoir aussi Essential Cheat Sheets for Machine Learning and Deep Learning Engineers."
  },
  {
    "objectID": "content/getting-started/05_rappels_types.html#les-quelques-r√®gles-de-python",
    "href": "content/getting-started/05_rappels_types.html#les-quelques-r√®gles-de-python",
    "title": "Quelques rappels sur les principes de base de Python",
    "section": "Les quelques r√®gles de Python",
    "text": "Les quelques r√®gles de Python\nPython est un peu susceptible et protocolaire, plus formaliste que ne l‚Äôest R.\nIl y a ainsi quelques r√®gles √† respecter :\nR√®gle 1: L‚Äôindentation est primordiale : un code mal indent√© provoque une erreur.\nL‚Äôindentation indique √† l‚Äôinterpr√©teur o√π se trouvent les\ns√©parations entre des blocs d‚Äôinstructions. Un peu comme des points dans un\ntexte.\nSi les lignes ne sont pas bien align√©es, l‚Äôinterpr√©teur ne sait plus √† quel\nbloc associer la ligne. Par exemple, le corps d‚Äôune fonction doit √™tre indent√©\nd‚Äôun niveau ; les √©l√©ments dans une clause logique (if, else, etc.) √©galement.\nR√®gle 2: On commence √† compter √† 0, comme dans beaucoup de langages\n(C++, java‚Ä¶). Python diff√®re dans ce domaine de R o√π on commence\n√† compter √† 1.\nLe premier √©l√©ment d‚Äôune liste est ainsi, en Python, le 0-√®me.\nR√®gle 3: Comme dans une langue naturelle, les marques de\nponctuation sont importantes :\n\nPour une liste : []\nPour un dictionnaire : {}\nPour un tuple : ()\nPour s√©parer des √©l√©ments : ,\nPour commenter un bout de code : #\nPour aller √† la ligne dans un bloc d‚Äôinstructions : \\\nLes majuscules et minuscules sont importantes\nPar contre l‚Äôusage des ' ou des \" est indiff√©rent.\nIl faut juste avoir les m√™mes d√©but et fin.\nPour documenter une fonction ou une classe ‚Äú‚Äú‚Äù mon texte de documentation ‚Äú‚Äú‚Äú"
  },
  {
    "objectID": "content/getting-started/05_rappels_types.html#les-outputs-de-python-lop√©ration-le-print-et-le-return",
    "href": "content/getting-started/05_rappels_types.html#les-outputs-de-python-lop√©ration-le-print-et-le-return",
    "title": "Quelques rappels sur les principes de base de Python",
    "section": "Les outputs de Python : l‚Äôop√©ration, le print et le return",
    "text": "Les outputs de Python : l‚Äôop√©ration, le print et le return\nQuand Python r√©alise des op√©rations, il faut lui pr√©ciser ce qu‚Äôil doit en faire :\n\nest-ce qu‚Äôil doit juste faire l‚Äôop√©ration,\nafficher le r√©sultat de l‚Äôop√©ration,\ncr√©er un objet avec le r√©sultat de l‚Äôop√©ration ?\n\n\n\n Note\nDans l‚Äôenvironnement Jupyter Notebook, le dernier √©lement d‚Äôune cellule\nest automatiquement affich√© (print), qu‚Äôon lui demande ou non de le faire.\nCe comportement est particuli√®rement pratique pour afficher des figures\ng√©n√©r√©es via matplotlib ou seaborn.\nCe comportement\nn‚Äôest pas le cas dans un √©diteur classique comme VisualStudio,\nSpyder ou PyCharm. Pour afficher un r√©sultat dans la console,\nil faut utiliser\nprint ou la commande consacr√©e (par exemple plt.show()\npour afficher la derni√®re figure g√©n√©r√©e par matplotlib)\n\n\n\nLe print\n\n# on calcule : dans le cas d'une op√©ration par exemple une somme\n2+3 # Python calcule le r√©sultat mais n'affiche rien dans la sortie\n\n# le print : on affiche\n\nprint(2+3) # Python calcule et on lui demande juste de l'afficher\n# le r√©sultat est en dessous du code\n\n5\n\n\n\n# le print dans une fonction \n\ndef addition_v1(a,b) : \n    print(a+b)\n\nresultat_print = addition_v1(2,0) \nprint(type(resultat_print))\n\n# dans la sortie on a l'affichage du r√©sultat, car la sortie de la fonction est un print \n# en plus on lui demande quel est le type du r√©sultat. Un print ne renvoie aucun type, ce n'est ni un num√©rique,\n# ni une chaine de charact√®res, le r√©sultat d'un print n'est pas un format utilisable\n\n2\n&lt;class 'NoneType'&gt;\n\n\nLe r√©sultat de l‚Äôaddition est affich√©\ncar la fonction addition_v1 effectue un print\nPar contre, l‚Äôobjet cr√©√© n‚Äôa pas de type, il n‚Äôest pas un chiffre,\nce n‚Äôest qu‚Äôun affichage.\n\n\nLe return\nPour cr√©er un objet avec le r√©sultat de la fonction, il faut utiliser return\n\n# le return dans une fonction\ndef addition_v2(a,b) : \n    return a+b\n\nresultat_return = addition_v2(2,5) # \nprint(type(resultat_return))\n## l√† on a bien un r√©sultat qui est du type \"entier\"\n\n&lt;class 'int'&gt;\n\n\nLe r√©sultat de addition_v2 n‚Äôest pas affich√© comme dans addition_v1\nPar contre, la fonction addition_v2 permet d‚Äôavoir un objet de type int,\nun entier donc."
  },
  {
    "objectID": "content/getting-started/05_rappels_types.html#les-types-de-base-variables-listes-dictionnaires",
    "href": "content/getting-started/05_rappels_types.html#les-types-de-base-variables-listes-dictionnaires",
    "title": "Quelques rappels sur les principes de base de Python",
    "section": "Les types de base : variables, listes, dictionnaires‚Ä¶",
    "text": "Les types de base : variables, listes, dictionnaires‚Ä¶\nPython permet de manipuler diff√©rents types de base. Nous en\nverrons des extensions dans la suite du cours (np.array par exemple)\nqui, d‚Äôune mani√®re ou d‚Äôune autre, s‚Äôappuient sur ces types de base.\nOn distingue deux types de variables : les immuables (immutables)\nqui ne peuvent √™tre\nmodifi√©s et les modifiables (mutables)\n\nLes variables immuables\nLes variables immuables ne peuvent √™tre modifi√©es\n\nNone : ce type est une convention de programmation pour dire que la valeur n‚Äôest pas calcul√©e\nbool : un bool√©en\nint : un entier\nfloat : un r√©el\nstr : une chaine de caract√®res\ntuple : un vecteur\n\n\ni = 3         # entier = type num√©rique (type int)\nr = 3.3       # r√©el   = type num√©rique (type float)\ns = \"exemple\" # cha√Æne de caract√®res = type str \nn = None      # None signifie que la variable existe mais qu'elle ne contient rien\n              # elle est souvent utilis√©e pour signifier qu'il n'y a pas de r√©sultat\na = (1,2)     # tuple\n\nprint(i,r,s,n,a)         \n\n3 3.3 exemple None (1, 2)\n\n\nSi on essaie de changer le premier √©l√©ment de la chaine de caract√®res s on va avoir un peu de mal.\nPar exemple si on voulait mettre une majuscule √† ‚Äúexemple‚Äù,\non aurait envie d‚Äô√©crire que le premier √©l√©ment de la chaine s est ‚ÄúE‚Äù majuscule\nMais Python ne va pas nous laisser faire, il nous dit que les objets ‚Äúchaine de caract√®re‚Äù ne peuvent √™tre modifi√©s\n\ns[0] = \"E\"  # d√©clenche une exception\n\nTypeError: 'str' object does not support item assignment\n\n\nTout ce qu‚Äôon peut faire avec une variable immuable,\nc‚Äôest la r√©affecter √† une autre valeur : elle ne peut pas √™tre modifi√©e.\nPour s‚Äôen convaincre, utilisons la fonction id() qui donne un identifiant √† chaque objet.\n\nprint(s)\nid(s)\n\nexemple\n\n\n140566062835376\n\n\n\ns = \"autre_mot\"\nid(s)\n\n140566028547184\n\n\nOn voit bien que s a chang√© d‚Äôidentifiant : il peut avoir le m√™me nom, ce n‚Äôest plus le m√™me objet\n\n\nLes types modifiable : listes et dictionnaires\nHeureusement, il existe des variables modifiables comme les listes et les dictionnaires.\n\nLes listes - elles s‚Äô√©crivent entre [ ]\nLes listes sont des √©lements tr√®s utiles, notamment quand vous souhaitez faire des boucles.\nPour faire appel aux √©lements d‚Äôune liste, on donne leur position dans la liste : le 1er est le 0, le 2√®me est le 1 ‚Ä¶\n\nma_liste = [1,2,3,4]\n\n\nprint(\"La longueur de ma liste est de\", len(ma_liste))\nprint(\"Le premier √©l√©ment de ma liste est :\", ma_liste[0])\nprint(\"Le dernier √©l√©ment de ma liste est :\", ma_liste[3])\nprint(\"Le dernier √©l√©ment de ma liste est :\", ma_liste[-1])\n\nLa longueur de ma liste est de 4\nLe premier √©l√©ment de ma liste est : 1\nLe dernier √©l√©ment de ma liste est : 4\nLe dernier √©l√©ment de ma liste est : 4\n\n\nPour effectuer des boucles sur les listes, la m√©thode la plus lisible\nest d‚Äôutiliser les list comprehension. Cette approche consiste\n√† it√©rer les √©l√©ments d‚Äôune liste √† la vol√©e.\nPar exemple, si on reprend cet exemple,\nun code qui repose sur les list comprehension sera le suivant :\n\nfruits = [\"apple\", \"banana\", \"cherry\", \"kiwi\", \"mango\"]\nnewlist = [x for x in fruits if \"a\" in x]\nprint(newlist)\n\n['apple', 'banana', 'mango']\n\n\nLe m√™me code, ne reposant pas sur les compr√©hensions de liste, sera beaucoup\nmoins concis et ainsi inutilement verbeux:\n\nfruits = [\"apple\", \"banana\", \"cherry\", \"kiwi\", \"mango\"]\nnewlist = []\n\nfor x in fruits:\n  if \"a\" in x:\n    newlist.append(x)\n\nprint(newlist) \n\n['apple', 'banana', 'mango']\n\n\n\n\nLes dictionnaires - ils s‚Äô√©crivent entre accolades {}\nUn dictionnaire associe √† une cl√© un autre √©l√©ment, appel√© une valeur : un chiffre, un nom, une liste, un autre dictionnaire etc.\nLe format d‚Äôun dictionnaire est le suivant : {Cl√© : valeur}. Il s‚Äôagit\nd‚Äôun objet tr√®s pratique pour la recherche, beaucoup plus que les listes\nqui ne permettent pas de stocker de l‚Äôinformation diverse de mani√®re\nhi√©rarchis√©e.\n\n\nDictionnaire avec des valeurs int\nOn peut par exemple associer √† un nom, un nombre\n\nmon_dictionnaire_notes = { 'Nicolas' : 18 , 'Pimprenelle' : 15} \n# un dictionnaire qui √† chaque nom associe un nombre\n# √† Nicolas, on associe 18\n\nprint(mon_dictionnaire_notes) \n\n{'Nicolas': 18, 'Pimprenelle': 15}\n\n\n\n\nDictionnaire avec des valeurs qui sont des listes\nPour chaque cl√© d‚Äôun dictionnaire, il ne faut pas forc√©ment garder la m√™me forme de valeur\nDans l‚Äôexemple, la valeur de la cl√© ‚ÄúNicolas‚Äù est une liste, alors que celle de ‚ÄúPhilou‚Äù est une liste de liste\n\nmon_dictionnaire_loisirs =  \\\n{ 'Nicolas' : ['Rugby','Pastis','Belote'] , \n  'Pimprenelle' : ['Gin Rami','Tisane','Tara Jarmon','Barcelone','Mickey Mouse'],\n  'Philou' : [['Maths','Jeux'],['Guillaume','Jeanne','Thimoth√©e','Adrien']]}\n\nPour acc√©der √† un √©l√©ment du dictionnaire, on fait appel √† la cl√© et non plus √† la position, comme c‚Äô√©tait le cas dans les listes.\nC‚Äôest beaucoup plus pratique pour rechercher de l‚Äôinformation:\n\nprint(mon_dictionnaire_loisirs['Nicolas']) # on affiche une liste\n\n['Rugby', 'Pastis', 'Belote']\n\n\n\nprint(mon_dictionnaire_loisirs['Philou']) # on affiche une liste de listes\n\n[['Maths', 'Jeux'], ['Guillaume', 'Jeanne', 'Thimoth√©e', 'Adrien']]\n\n\nSi on ne veut avoir que la premi√®re liste des loisirs de Philou, on demande le premier √©l√©ment de la liste\n\nprint(mon_dictionnaire_loisirs['Philou'][0]) # on affiche alors juste la premi√®re liste\n\n['Maths', 'Jeux']\n\n\nOn peut aussi avoir des valeurs qui sont des int et des list\n\nmon_dictionnaire_patchwork_good = \\\n{ 'Nicolas' : ['Rugby','Pastis','Belote'] ,\n  'Pimprenelle' : 18 }\n\n\n\n\nA retenir\n\nL‚Äôindentation du code est importante (4 espaces et pas une tabulation)\nUne liste est entre [] et on peut appeler les positions par leur place\nUn dictionnaire, cl√© x valeur, s‚Äô√©crit entre {} et on appelle un √©l√©ment en fonction de la cl√©"
  },
  {
    "objectID": "content/getting-started/05_rappels_types.html#questions-pratiques",
    "href": "content/getting-started/05_rappels_types.html#questions-pratiques",
    "title": "Quelques rappels sur les principes de base de Python",
    "section": "Questions pratiques :",
    "text": "Questions pratiques :\n\n\n\n Exercice\n\nQuelle est la position de 7 dans la liste suivante\n\n\nliste_nombres = [1,2,7,5,3]\n\n\nCombien de cl√©s a ce dictionnaire ?\n\n\ndictionnaire_evangile = {\"Marc\" : \"Lion\", \"Matthieu\" : [\"Ange\",\"Homme ail√©\"] , \n                          \"Jean\" : \"Aigle\" , \"Luc\" : \"Taureau\"}\n\n\nQue faut-il √©crire pour obtenir ‚ÄúAnge‚Äù en r√©sultat √† partir du dictionnaire_evangile ?"
  },
  {
    "objectID": "content/getting-started/05_rappels_types.html#effectuer-des-op√©rations-sur-les-objets-de-base-python",
    "href": "content/getting-started/05_rappels_types.html#effectuer-des-op√©rations-sur-les-objets-de-base-python",
    "title": "Quelques rappels sur les principes de base de Python",
    "section": "Effectuer des op√©rations sur les objets de base Python",
    "text": "Effectuer des op√©rations sur les objets de base Python\nMaintenant qu‚Äôon a vu quels objets existent en Python,\nnous allons\nvoir comment nous en servir.\nPour comprendre comment modifier un objet, il convient\nde distinguer deux concepts, d√©velopp√©s plus amplement\ndans le chapitre d√©di√©: les attributs et les m√©thodes :\n\nLes attributs d√©crivent la structure interne d‚Äôun objet. Par exemple,\nla taille d‚Äôun objet, sa langue, etc.\nNous n‚Äôallons pas trop d√©velopper ce concept ici. Le chapitre d√©di√© au sujet\npermettra de plus d√©velopper ce concept.\nLes m√©thodes correspondent √† des actions qui s‚Äôappliqueront √† l‚Äôobjet et s‚Äôadaptent √† sa structure.\nLa m√™me m√©thode (par exemple append) fonctionnera ainsi de mani√®re diff√©rente selon le type d‚Äôobjet.\n\n\nPremiers exemples de m√©thodes\nAvec les √©l√©ments d√©finis dans la partie 1\n(les listes, les dictionnaires) on peut faire appel √† des m√©thodes qui sont directement li√©es √† ces objets.\n\nUne m√©thode pour les listes\nPour ajouter un √©l√©ment (item) dans une liste : on va utiliser la m√©thode .append()\n\nma_liste = [\"Nicolas\",\"Michel\",\"Bernard\"]\n\nma_liste.append(\"Philippe\")\n\nprint(ma_liste)\n\n['Nicolas', 'Michel', 'Bernard', 'Philippe']\n\n\n\n\nUne m√©thode pour les dictionnaires\nPour connaitre l‚Äôensemble des cl√©s d‚Äôun dictionnaire, on appelle la m√©thode .keys()\n\nmon_dictionnaire = {\"Marc\" : \"Lion\", \"Matthieu\" : [\"Ange\",\"Homme ail√©\"] , \n                          \"Jean\" : \"Aigle\" , \"Luc\" : \"Taureau\"}\n\nprint(mon_dictionnaire.keys())\n\ndict_keys(['Marc', 'Matthieu', 'Jean', 'Luc'])\n\n\n\n\n\nConnaitre les m√©thodes d‚Äôun objet\nPour savoir quelles sont les m√©thodes d‚Äôun objet vous pouvez :\n\ntaper help(mon_objet) ou mon_objet? dans la console Python\ntaper mon_objet. + touche tabulation dans la console Python ou dans le Notebook.\nPython permet la compl√©tion, c‚Äôest-√†-dire que vous pouvez faire appa√Ætre la liste\ndes m√©thodes possibles."
  },
  {
    "objectID": "content/getting-started/05_rappels_types.html#les-op√©rations-et-m√©thodes-classiques-des-listes",
    "href": "content/getting-started/05_rappels_types.html#les-op√©rations-et-m√©thodes-classiques-des-listes",
    "title": "Quelques rappels sur les principes de base de Python",
    "section": "Les op√©rations et m√©thodes classiques des listes",
    "text": "Les op√©rations et m√©thodes classiques des listes\n\nCr√©er une liste\nPour cr√©er un objet de la classe list, il suffit de le d√©clarer. Ici on affecte √† x une liste\n\nx = [4, 5] # cr√©ation d‚Äôune liste compos√©e de deux entiers\nx = [\"un\", 1, \"deux\", 2] # cr√©ation d‚Äôune liste compos√©e de 2 cha√Ænes de caract√®res\n# et de deux entiers, l‚Äôordre d‚Äô√©criture est important\nx = [3] # cr√©ation d‚Äôune liste d‚Äôun √©l√©ment, sans la virgule,\nx = [ ] # cr√©e une liste vide\nx = list () # cr√©e une liste vide\n\n\n\nUn premier test sur les listes\nSi on veut tester la pr√©sence d‚Äôun √©l√©ment dans une liste, on l‚Äô√©crit de la mani√®re suivante :\n\n# Exemple \n\nx = \"Marcel\"\n\nl = [\"Marcel\",\"Edith\",\"Maurice\",\"Jean\"]\n\nprint(x in l)\n\n#vrai si x est un des √©l√©ments de l\n\nTrue\n\n\n\n\n+: une m√©thode pour concat√©ner deux listes\nOn utilise le symbole +\n\nt = [\"Antoine\",\"David\"]\nprint(l + t) #concat√©nation de l et t\n\n['Marcel', 'Edith', 'Maurice', 'Jean', 'Antoine', 'David']\n\n\n\n\nPour trouver certains √©l√©ments d‚Äôune liste\nPour chercher des √©lements dans une liste, on utilise la position dans la liste.\n\nl[1] # donne l'√©l√©ment qui est en 2√®me position de la liste\n\n'Edith'\n\n\n\nl[1:3] # donne les √©l√©ments de la 2√®me position de la liste √† la 4√®me exclue\n\n['Edith', 'Maurice']\n\n\n\n\nQuelques fonctions des listes\nLes listes embarquent ainsi nativement un certain nombre de m√©thodes\nqui sont pratiques. Cependant, pour avoir certaines informations\nsur une liste, il faut parfois plut√¥t passer par\ndes fonctions natives comme les suivantes :\n\nlongueur = len(l) # nombre d‚Äô√©l√©ments de l\nminimum = min(l) # plus petit √©l√©ment de l, ici par ordre alphab√©tique\nmaximum = max(l) # plus grand √©l√©ment de l, ici par ordre alphab√©tique\nprint(longueur,minimum,maximum)\n\n4 Edith Maurice\n\n\n\ndel l[0 : 2] # supprime les √©l√©ments entre la position 0 et 2 exclue\nprint(l)\n\n['Maurice', 'Jean']\n\n\n\n\nLes m√©thodes des listes\nOn les trouve dans l‚Äôaide de la liste.\nOn distingue les m√©thodes et les m√©thodes sp√©ciales : visuellement,\nles m√©thodes sp√©ciales sont celles qui pr√©c√©d√©es et suivis de deux caract√®res de soulignement,\nles autres sont des m√©thodes classiques.\n\nhelp(l)\n\nHelp on list object:\n\nclass list(object)\n |  list(iterable=(), /)\n |  \n |  Built-in mutable sequence.\n |  \n |  If no argument is given, the constructor creates a new empty list.\n |  The argument must be an iterable if specified.\n |  \n |  Methods defined here:\n |  \n |  __add__(self, value, /)\n |      Return self+value.\n |  \n |  __contains__(self, key, /)\n |      Return key in self.\n |  \n |  __delitem__(self, key, /)\n |      Delete self[key].\n |  \n |  __eq__(self, value, /)\n |      Return self==value.\n |  \n |  __ge__(self, value, /)\n |      Return self&gt;=value.\n |  \n |  __getattribute__(self, name, /)\n |      Return getattr(self, name).\n |  \n |  __getitem__(...)\n |      x.__getitem__(y) &lt;==&gt; x[y]\n |  \n |  __gt__(self, value, /)\n |      Return self&gt;value.\n |  \n |  __iadd__(self, value, /)\n |      Implement self+=value.\n |  \n |  __imul__(self, value, /)\n |      Implement self*=value.\n |  \n |  __init__(self, /, *args, **kwargs)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __iter__(self, /)\n |      Implement iter(self).\n |  \n |  __le__(self, value, /)\n |      Return self&lt;=value.\n |  \n |  __len__(self, /)\n |      Return len(self).\n |  \n |  __lt__(self, value, /)\n |      Return self&lt;value.\n |  \n |  __mul__(self, value, /)\n |      Return self*value.\n |  \n |  __ne__(self, value, /)\n |      Return self!=value.\n |  \n |  __repr__(self, /)\n |      Return repr(self).\n |  \n |  __reversed__(self, /)\n |      Return a reverse iterator over the list.\n |  \n |  __rmul__(self, value, /)\n |      Return value*self.\n |  \n |  __setitem__(self, key, value, /)\n |      Set self[key] to value.\n |  \n |  __sizeof__(self, /)\n |      Return the size of the list in memory, in bytes.\n |  \n |  append(self, object, /)\n |      Append object to the end of the list.\n |  \n |  clear(self, /)\n |      Remove all items from list.\n |  \n |  copy(self, /)\n |      Return a shallow copy of the list.\n |  \n |  count(self, value, /)\n |      Return number of occurrences of value.\n |  \n |  extend(self, iterable, /)\n |      Extend list by appending elements from the iterable.\n |  \n |  index(self, value, start=0, stop=9223372036854775807, /)\n |      Return first index of value.\n |      \n |      Raises ValueError if the value is not present.\n |  \n |  insert(self, index, object, /)\n |      Insert object before index.\n |  \n |  pop(self, index=-1, /)\n |      Remove and return item at index (default last).\n |      \n |      Raises IndexError if list is empty or index is out of range.\n |  \n |  remove(self, value, /)\n |      Remove first occurrence of value.\n |      \n |      Raises ValueError if the value is not present.\n |  \n |  reverse(self, /)\n |      Reverse *IN PLACE*.\n |  \n |  sort(self, /, *, key=None, reverse=False)\n |      Sort the list in ascending order and return None.\n |      \n |      The sort is in-place (i.e. the list itself is modified) and stable (i.e. the\n |      order of two equal elements is maintained).\n |      \n |      If a key function is given, apply it once to each list item and sort them,\n |      ascending or descending, according to their function values.\n |      \n |      The reverse flag can be set to sort in descending order.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods defined here:\n |  \n |  __class_getitem__(...) from builtins.type\n |      See PEP 585\n |  \n |  ----------------------------------------------------------------------\n |  Static methods defined here:\n |  \n |  __new__(*args, **kwargs) from builtins.type\n |      Create and return a new object.  See help(type) for accurate signature.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __hash__ = None\n\n\n\n\n\nA retenir et questions\nA retenir :\n\nChaque objet Python a des attributs et des m√©thodes\nVous pouvez cr√©er des classes avec des attributs et des m√©thodes\nLes m√©thodes des listes et des dictionnaires qui sont les plus utilis√©es :\n\nlist.count()\nlist.sort()\nlist.append()\ndict.keys()\ndict.items()\ndict.values()\n\n\n\n\n Exercice 2\n\nD√©finir la liste allant de 1 √† 10, puis effectuez les actions suivantes :\n\n\ntriez et affichez la liste\najoutez l‚Äô√©l√©ment 11 √† la liste et affichez la liste\nrenversez et affichez la liste\naffichez l‚Äô√©l√©ment d‚Äôindice 7\nenlevez l‚Äô√©l√©ment 9 et affichez la liste\naffichez la sous-liste du 2e au 3e √©l√©ments inclus ;\naffichez la sous-liste du d√©but au 2e √©l√©ment inclus ;\naffichez la sous-liste du 3e √©l√©ment √† la fin de la liste ;\n\n\nConstruire le dictionnaire des 6 premiers mois de l‚Äôann√©e avec comme valeurs le nombre de jours respectif.\n\n\nRenvoyer la liste des mois\nRenvoyer la liste des jours\nAjoutez la cl√© du mois de Juillet"
  },
  {
    "objectID": "content/getting-started/05_rappels_types.html#passer-des-listes-dictionnaires-√†-pandas",
    "href": "content/getting-started/05_rappels_types.html#passer-des-listes-dictionnaires-√†-pandas",
    "title": "Quelques rappels sur les principes de base de Python",
    "section": "Passer des listes, dictionnaires √† pandas",
    "text": "Passer des listes, dictionnaires √† pandas\nSupposons que la variable ‚Äòdata‚Äô est une liste qui contient nos donn√©es.\nUne observation correspond √† un dictionnaire qui contient le nom, le type, l‚Äôambiance et la note d‚Äôun restaurant.\nIl est ais√© de transformer cette liste en dataframe gr√¢ce √† la fonction ‚ÄòDataFrame‚Äô.\n\nimport pandas \n\ndata = [{\"nom\": \"Little Pub\", \"type\" : \"Bar\", \"ambiance\": 9, \"note\": 7},\n     {\"nom\": \"Le Corse\", \"type\" : \"Sandwicherie\", \"ambiance\": 2, \"note\": 8},\n     {\"nom\": \"Caf√© Caumartin\", \"type\" : \"Bar\", \"ambiance\": 1}]\n\ndf = pandas.DataFrame(data)\n\nprint(data)\ndf\n\n[{'nom': 'Little Pub', 'type': 'Bar', 'ambiance': 9, 'note': 7}, {'nom': 'Le Corse', 'type': 'Sandwicherie', 'ambiance': 2, 'note': 8}, {'nom': 'Caf√© Caumartin', 'type': 'Bar', 'ambiance': 1}]\n\n\n\n\n\n\n\n\n\nnom\ntype\nambiance\nnote\n\n\n\n\n0\nLittle Pub\nBar\n9\n7.0\n\n\n1\nLe Corse\nSandwicherie\n2\n8.0\n\n\n2\nCaf√© Caumartin\nBar\n1\nNaN"
  },
  {
    "objectID": "content/getting-started/03_data_analysis.html",
    "href": "content/getting-started/03_data_analysis.html",
    "title": "Comment aborder un jeu de donn√©es",
    "section": "",
    "text": "Pour bien d√©buter des travaux sur une base de donn√©es,\nil est n√©cessaire de se poser quelques questions de bon sens\net de suivre une d√©marche scientifique dont un certain\nnombre de gestes sont assez simple.\nDans un projet sur des jeux de donn√©es, on peut sch√©matiquement\ns√©parer les √©tapes en quatre grandes parties :\n\nLa r√©cup√©ration et structuration des donn√©es ;\nL‚Äôanalyse de celle-ci, notamment la production de statistiques descriptives indispensables pour orienter les exploitations ult√©rieures ;\nLa mod√©lisation ;\nLa valorisation finale des √©tapes pr√©c√©dentes et la communication de r√©sultats ou la mise en oeuvre d‚Äôune chaine de production.\n\nCe cours explore ces diff√©rentes √©tapes de mani√®re progressive gr√¢ce √†\nl‚Äô√©cosyst√®me Python qui est tr√®s complet. Chaque chapitre du cours\npeut √™tre vu comme une mani√®re de progresser dans ce fil conducteur.\nDans ce chapitre, nous allons plut√¥t mettre en avant quelques r√©flexions\n√† avoir avant de se lancer dans chaque √©tape.\n\n\n\n\nLa phase de constitution de son jeu de donn√©es sous-tend tout le projet qui suit.\nLa premi√®re question √† se poser est\n‚Äúde quelles donn√©es ai-je besoin pour r√©pondre √† ma probl√©matique ?‚Äù.\nCette probl√©matique pourra √©ventuellement\n√™tre affin√©e en fonction des besoins mais les travaux sont g√©n√©ralement\nde meilleure qualit√© lorsque la probl√©matique am√®ne √† la r√©flexion sur les donn√©es\ndisponibles plut√¥t que l‚Äôinverse.\nEnsuite, ‚Äúqui produit et met √† disposition ces donn√©es‚Äù ?\nLes sources disponibles sur internet sont-elles fiables ?\nPar exemple, les sites d‚Äôopen data gouvernementaux sont par exemple assez fiables mais autorisent parfois l‚Äôarchivage de donn√©es restructur√©es par des tiers et non des producteurs officiels. A l‚Äôinverse, sur Kaggle ou sur Github la source de certains jeux de donn√©es n‚Äôest pas trac√©e ce qui rend compliqu√©e la confiance sur la qualit√© de la donn√©e\nUne fois identifi√© une ou plusieurs sources de donn√©es,\nest-ce que je peux les compl√©ter avec d‚Äôautres donn√©es ?\n(dans ce cas, faire attention √† avoir des niveaux de granularit√© ad√©quats).\n\n\n\nVient ensuite la phase de mise en forme et nettoyage des jeux de donn√©es r√©cup√©r√©s.\nCette √©tape est primordiale et est g√©n√©ralement celle qui mobilise le plus\nde temps. Pendant quelques ann√©es, on parlait de data cleaning. Cependant,\ncela a pu, implicitement, laisser penser qu‚Äôil s‚Äôagissait d‚Äôune t√¢che\nsubalterne. On commence √† lui pr√©f√©rer le concept de feature engineering\nqui souligne bien qu‚Äôil s‚Äôagit d‚Äôune comp√©tence qui n√©cessite beaucoup\nde comp√©tences.\nUn jeu de donn√©es propre est un jeu de donn√©es dont la structure est\nad√©quate et n‚Äôentra√Ænera pas d‚Äôerreur, visible ou non,\nlors de la phase d‚Äôanalyse. Voici quelques √©l√©ments structurants\nd‚Äôun jeu de donn√©es propre :\n\nles informations manquantes sont bien comprises et trait√©es. numpy et\npandas proposent un certain formalisme sur le sujet qu‚Äôil est utile\nd‚Äôadopter en rempla√ßant par NaN les observations manquantes. Cela\nimplique de faire attention √† la mani√®re dont certains producteurs\ncodent les valeurs manquantes : certains ont la facheuse tendance √†\n√™tre imaginatifs sur les codes pour valeurs manquantes : ‚Äú-999‚Äù, ‚ÄúXXX‚Äù, ‚ÄúNA‚Äù\nles variables servant d‚Äôidentifiants sont bien les m√™mes d‚Äôune table √† l‚Äôautre (notamment dans le cas de jointure) : m√™me format, m√™me modalit√©s\npour des variables textuelles, qui peuvent etre mal saisies, avoir corrig√© les √©ventuelles fautes (ex ‚ÄúRolland Garros‚Äù -&gt; ‚ÄúRoland Garros‚Äù)\ncr√©er des variables qui synth√©tisent l‚Äôinformation dont vous avez besoin\nsupprimer les √©l√©ments inutiles (colonne ou ligne vide)\nrenommer les colonnes avec des noms compr√©hensibles\n\n\n\n\n\nUne fois les jeux de donn√©es nettoy√©s, vous pouvez plus sereinement\n√©tudier l‚Äôinformation pr√©sente dans les donn√©es.\nCette phase et celle du nettoyage ne sont pas s√©quentielles,\nen r√©alit√© vous devrez r√©guli√®rement passer de votre nettoyage √† quelques statistiques descriptives qui vous montreront un probl√®me, retourner au nettoyage etc.\nLes questions √† se poser pour ‚Äúchallenger‚Äù le jeu de donn√©es :\n\nEst-ce que mon √©chantillon est bien repr√©sentatif de ce qui m‚Äôint√©resse ? N‚Äôavoir que 2000 communes sur les 35000 n‚Äôest pas n√©cessairement un probl√®me mais il est bon de s‚Äô√™tre pos√© la question.\nEst-ce que les ordres de grandeur sont bons ? Pour cela, confronter vos premieres stats desc √† vos recherches internet. Par exemple trouver que les maisons vendues en France en 2020 font en moyenne 400 m¬≤ n‚Äôest pas un ordre de grandeur r√©aliste.\nEst-ce que je comprends toutes les variables de mon jeu de donn√©es ? Est-ce qu‚Äôelles se ‚Äúcomportent‚Äù de la bonne fa√ßon ? A ce stade, il est parfois utile de se faire un dictionnaire de variables (qui explique comment elles sont construites ou calcul√©es). On peut √©galement mener des √©tudes de corr√©lation entre nos variables.\nEst-ce que j‚Äôai des outliers, i.e.¬†des valeurs aberrantes pour certains individus ? Dans ce cas, il faut d√©cider quel traitement on leur apporte (les supprimer, appliquer une transformation logarithmique, les laisser tel quel) et surtout bien le justifier.\nEst-ce que j‚Äôai des premiers grands messages sortis de mon jeu de donn√©es ? Est-ce que j‚Äôai des r√©sultats surprenants ? Si oui, les ai-je creus√© suffisamment pour voir si les r√©sultats tiennent toujours ou si c‚Äôest √† cause d‚Äôun souci dans la construction du jeu de donn√©es (mal nettoy√©es, mauvaise variable‚Ä¶)\n\n\n\n\nA cette √©tape, l‚Äôanalyse descriptive doit voir avoir donn√© quelques premi√®res pistes pour savoir dans quelle direction vous voulez mener votre mod√®le.\nUne erreur de d√©butant est de se lancer directement dans la mod√©lisation parce\nqu‚Äôil s‚Äôagirait d‚Äôune comp√©tence plus pouss√©e. Cela am√®ne g√©n√©ralement\n√† des analyses de pauvre qualit√© : la mod√©lisation tend g√©n√©ralement √† confirmer\nles intuitions issues de l‚Äôanalyse descriptive. Sans cette derni√®re,\nl‚Äôinterpr√©tation des r√©sultats d‚Äôun mod√®le peu s‚Äôav√©rer inutilement complexe.\nVous devrez plonger dans vos autres cours (Econom√©trie 1, Series Temporelles, Sondages, Analyse des donn√©es etc.) pour trouver le mod√®le le plus adapt√© √† votre question.\nLa m√©thode sera guid√©e par l‚Äôobjectif.\n\nEst-ce que vous voulez expliquer ou pr√©dire ? https://hal-cnam.archives-ouvertes.fr/hal-02507348/document\nEst-ce que vous voulez classer un √©l√©ment dans une cat√©gorie (classification ou clustering) ou pr√©dire une valeur num√©rique (r√©gression) ?\n\nEn fonction des mod√®les que vous aurez d√©j√† vu en cours et des questions que vous souhaiterez r√©soudre sur votre jeu de donn√©es, le choix du mod√®le sera souvent assez direct.\nVous pouvez √©galement vous r√©f√©rez √† la d√©marche propos√©e par Xavier Dupr√©\nhttp://www.xavierdupre.fr/app/ensae_teaching_cs/helpsphinx3/debutermlprojet.html#l-debutermlprojet\nPour aller plus loin (mais de mani√®re simplifi√©e) sur les algorithmes de Machine Learning :\nhttps://datakeen.co/8-machine-learning-algorithms-explained-in-human-language/\n\n\nLa mise √† disposition de code sur Github ou Gitlab est une incitation\ntr√®s forte pour produire du code de qualit√©. Il est ainsi recommand√© de\nsyst√©matiquement utiliser ces plateformes pour la mise √† disposition de\ncode. Cependant, il ne s‚Äôagit que d‚Äôune petite partie des gains √†\nl‚Äôutiliser.\nLe cours que je donne avec Romain Avouac en troisi√®me ann√©e d‚ÄôENSAE\n(ensae-reproductibilite.github.io/website/) √©voque\nl‚Äôun des principaux gains √† utiliser ces plateformes, √† savoir\nla possibilit√© de mettre √† disposition automatiquement diff√©rents livrables\npour valoriser son travail aupr√®s de diff√©rents publics.\nSelon le public vis√©, la communication ne sera pas identique. Le code peut\nint√©resser les personnes d√©sirant avoir des d√©tails sur la m√©thodologie mise\nen oeuvre en pratique mais il peut s‚Äôagir d‚Äôun format rebutant pour d‚Äôautres\npublics. Une visualisation de donn√©es dynamiques parlera √† des publics\nmoins experts de la donn√©e mais est plus dure √† mettre en oeuvre\nqu‚Äôun graphique standard.\n\n\n\nLes Notebooks Jupyter ont eu beaucoup de succ√®s dans le monde de\nla data science pour valoriser des travaux. Pourtant il ne s‚Äôagit\npas forc√©ment toujours du meilleur format. En effet, beaucoup\nde notebooks tentent √† empiler des pav√©s de code et du texte, ce\nqui les rend difficilement lisibles.\nSur un projet cons√©quent, il vaut mieux reporter le plus de code\npossible dans des scripts bien structur√©s et avoir un notebook\nqui appelle ces scripts pour produire des outputs. Ou alors ne\npas utiliser un notebook et privil√©gier un autre format (un\ntableau de bord, un site web, une appli r√©active‚Ä¶).\nDans le cours de derni√®re ann√©e de\nl‚ÄôENSAE, Mise en production de projets data science, Romain\nAvouac et moi revenons sur les moyens de communication et de partage de code alternatifs au notebook.\n\n\n\n\n\n\n\n\nLes donn√©es sont une repr√©sentation synth√©tique de la r√©alit√© et les\nconclusions de certaines analyses peuvent avoir un vrai impact sur\nla vie des citoyens. Les chiffres erron√©s de\nReinhart et Rogoff ont ainsi pu servir de justification th√©orique √† des\npolitiques d‚Äôaust√©rit√© qui ont pu avoir des cons√©quences violentes\npour certains citoyens de\npays en crise1. En Grande-Bretagne, le recensement des personnes\ncontamin√©es par le Covid en 2020, et donc de leurs proches pour le\nsuivi de l‚Äô√©pid√©mie,\na √©t√© incomplet √† cause de\ntroncatures dues √† l‚Äôutilisation d‚Äôun format non adapt√© de stockage\ndes donn√©es (tableur Excel)2.\nDernier exemple avec le credit scoring mis en oeuvre aux Etats-Unis.\nLa citation ci-dessous, issue de l‚Äôarticle de Hurley and Adebayo (2016),\nillustre tr√®s bien les cons√©quences et les aspects probl√©matiques\nd‚Äôun syst√®me de construction automatis√©e d‚Äôun score de cr√©dit :\n\nConsumers have limited ability to identify and contest unfair credit\ndecisions, and little chance to understand what steps they\nshould take to improve their credit. Recent studies have also\nquestioned the accuracy of the data used by these tools, in some\ncases identifying serious flaws that have a substantial bearing\non lending decisions. Big-data tools may also risk creating a\nsystem of ‚Äúcreditworthinessby association‚Äù in which consumers‚Äô\nfamilial, religious, social, and other affiliations determine their\neligibility for an affordable loan.\nHurley and Adebayo (2016)\n\n\n\n\nLa transparence sur les int√©r√™ts et limites d‚Äôune m√©thode mise en oeuvre\nest donc importante.\nCette exigence de la recherche, parfois oubli√©e √† cause de la course\naux r√©sultats novateurs, m√©rite √©galement d‚Äô√™tre appliqu√©e\nen entreprise ou administration.\nM√™me sans intention manifeste de la part de la personne qui analyse des donn√©es,\nune mauvaise interpr√©tation est toujours possible. Tout en valorisant un\nr√©sultat, il est possible d‚Äôalerter sur certaines limites. Il est important,\ndans ses recherches comme dans les discussions avec d‚Äôautres interlocuteurs,\nde faire attention au biais de confirmation qui consiste\n√† ne retenir que l‚Äôinformation qui correspond √† nos conceptions a priori et\n√† ne pas consid√©rer celles qui pourraient aller √† l‚Äôencontre de celles-ci:\n\n\n\n\n\nCertaines repr√©sentations de donn√©es sont √† exclure car des biais cognitifs\npeuvent amener √† des interpr√©tations erron√©es3. Dans le domaine de la\nvisualisation de donn√©es, les camemberts (pie chart) ou les diagrammes\nradar sont par exemple\n√† exclure car l‚Äôoeil humain per√ßoit mal ces formes circulaires. Pour une raison\nsimilaire, les cartes avec aplat de couleur (cartes\nchoropl√®thes) sont trompeuses.\nLes posts de blog pour datawrapper\nde Lisa Charlotte Muth ou ceux d‚ÄôEric Mauvi√®re sont d‚Äôexcellentes ressources\npour apprendre les bonnes et mauvaises pratiques de\nvisualisation (voir la partie visualisation de ce cours\npour plus de d√©tails).\n\n\n\nLe cadre r√©glementaire de protection des donn√©es a √©volu√© ces derni√®res\nann√©es avec le RGPD. Cette r√©glementation a permis de mieux faire\nsaisir le fait que la collecte de donn√©es se justifie au nom\nde finalit√©s plus ou moins bien identifi√©es. Prendre conscience que\nla confidentialit√© des donn√©es se justifie pour √©viter la diss√©mination\nnon contr√¥l√©e d‚Äôinformations sur une personne est important.\nDes donn√©es particuli√®rement sensibles, notamment les donn√©es de sant√©,\npeuvent √™tre plus contraignantes √† traiter que des donn√©es peu sensibles.\nEn Europe, par exemple, les agents du service statistique public\n(Insee ou services statistiques minist√©riels) sont tenus au secret professionnel\n(article L121-6 du Code g√©n√©ral de la fonction publique),\nqui leur interdit la communication des informations confidentielles\ndont ils sont d√©positaires au titre de leurs missions ou fonctions,\nsous peine des sanctions pr√©vues par l‚Äôarticle 226-13 du Code p√©nal\n(jusqu‚Äô√† un an d‚Äôemprisonnement et 15 000 ‚Ç¨ d‚Äôamende).\nLe secret statistique, d√©fini dans une loi de 1951,\nrenforce cette obligation dans le cas de donn√©es d√©tenues pour des usages statistiques.\nIl interdit strictement la communication de donn√©es individuelles\nou susceptibles d‚Äôidentifier les personnes,\nissues de traitements √† finalit√©s statistiques,\nque ces traitements proviennent d‚Äôenqu√™tes ou de bases de donn√©es.\nLe secret statistique exclut par principe de diffuser des donn√©es\nqui permettraient l‚Äôidentification des personnes concern√©es,\npersonnes physiques comme personnes morales.\nCette obligation limite la finesse des informations disponibles en diffusion\nCe cadre contraignant s‚Äôexplique par l‚Äôh√©ritage de la Seconde Guerre Mondiale\net le d√©sir de ne plus revivre une situation o√π la collecte d‚Äôinformation\nsert une action publique bas√©e sur la discrimination entre cat√©gories\nde la population.\n\n\n\nUn article r√©cent de Nature,\nqui reprend les travaux d‚Äôune √©quipe d‚Äô√©pid√©miologistes (Gabelica, Bojƒçiƒá, and Puljak 2022)\n√©voque le probl√®me de l‚Äôacc√®s aux donn√©es pour des chercheurs d√©sirant reproduire\nune √©tude. M√™me dans les articles scientifiques o√π il est mentionn√© que les\ndonn√©es peuvent √™tre mises √† disposition d‚Äôautres chercheurs, le partage\nde celles-ci est rare :\n\nGraphique issu de l‚Äôarticle de Nature\nCe constat, quelque peu inqui√©tant, est confirm√© par une √©tude r√©cente\nde Samuel and Mietchen (2023) qui a tent√© d‚Äôex√©cuter un peu moins de\n30 000 notebooks associ√©s √† des √©tudes scientifiques. Seuls 3%\ndes notebooks reproduisent les r√©sultats esp√©r√©s.\nAfin de partager les moyens de reproduire des publications sans diffuser des\ndonn√©es potentiellement confidentielles, les jeux de donn√©es synth√©tiques\nsont de plus en plus utilis√©s. Par le biais de mod√®les de deep learning,\nil est ainsi possible de g√©n√©rer des jeux de donn√©es synth√©tiques complexes\nqui permettent de reproduire les principales caract√©ristiques d‚Äôun jeu de donn√©es\ntout en √©vitant, si le mod√®le a √©t√© bien calibr√©, de diffuser une information\nindividuelle.\nDans l‚Äôadministration fran√ßaise, les codes sources sont\nconsid√©r√©s comme des documents administratifs et peuvent\ndonc √™tre mis √† disposition de tout citoyen sur demande √† la\nCommission d‚Äôacc√®s aux documents administratifs (CADA):\n\n¬´ Sont consid√©r√©s comme documents administratifs, au sens des titres Ier, III et IV du pr√©sent livre, quels que soient leur date, leur lieu de conservation, leur forme et leur support, les documents produits ou re√ßus, dans le cadre de leur mission de service public, par l‚Äô√âtat, les collectivit√©s territoriales ainsi que par les autres personnes de droit public ou les personnes de droit priv√© charg√©es d‚Äôune telle mission. Constituent de tels documents notamment les dossiers, rapports, √©tudes, comptes rendus, proc√®s-verbaux, statistiques, instructions, circulaires, notes et r√©ponses minist√©rielles, correspondances, avis, pr√©visions, codes sources et d√©cisions. ¬ª\nAvis 20230314 - S√©ance du 30/03/2023 de la Commission d‚Äôacc√®s aux documents administratifs\n\nEn revanche, les poids des mod√®les utilis√©s par l‚Äôadministration, notamment ceux\ndes mod√®les de machine learning ne sont pas r√©glement√©s de la m√™me\nmani√®re (Avis 20230314 de la CADA).\nEn effet, comme il existe toujours\nun risque de r√©tro-ing√©nierie amenant √† une r√©v√©lation partielle\ndes donn√©es\nd‚Äôentra√Ænement lors d‚Äôun partage de mod√®le, les mod√®les\nentra√Æn√©s sur des donn√©es\nsensibles (comme les d√©cisions de justice √©tudi√©es\npar (l‚Äôavis 20230314 de la CADA))\nn‚Äôont pas vocation √† √™tre partag√©s.\n\n\n\nLe num√©rique constitue une part croissante des\n√©missions de gaz √† effet de serre.\nRepr√©sentant aujourd‚Äôhui 4 % des √©missions mondiales\nde CO2, cette part devrait encore cro√Ætre (Arcep 2019).\nLe monde de la data science est √©galement\nconcern√©.\nL‚Äôutilisation de donn√©es de plus en\nplus massives, notamment la constitution\nde corpus monumentaux de textes,\nr√©cup√©r√©s par scraping, est une premi√®re source\nde d√©pense d‚Äô√©nergie. De m√™me, la r√©cup√©ration\nen continu de nouvelles traces num√©riques\nn√©cessite d‚Äôavoir des serveurs fonctionnels\nen continu. A cette premi√®re source de\nd√©pense d‚Äô√©nergie, s‚Äôajoute l‚Äôentra√Ænement\ndes mod√®les qui peut prendre des jours,\ny compris sur des architectures tr√®s\npuissantes. Strubell, Ganesh, and McCallum (2019)\nestime que l‚Äôentra√Ænement d‚Äôun mod√®le √†\nl‚Äô√©tat de l‚Äôart dans le domaine du\nNLP n√©cessite autant d‚Äô√©nergie que ce que\nconsommeraient cinq voitures, en moyenne,\nau cours de l‚Äôensemble de leur\ncycle de vie.\nL‚Äôutilisation accrue de l‚Äôint√©gration\ncontinue, qui permet de mettre en oeuvre de mani√®re\nautomatis√©e l‚Äôex√©cution de certains scripts ou\nla production de livrables en continu,\nam√®ne √©galement √† une d√©pense d‚Äô√©nergie importante.\nIl convient donc d‚Äôessayer de limiter l‚Äôint√©gration\ncontinue √† la production d‚Äôoutput vraiment nouveaux.\n\n\n\nPar exemple, cet ouvrage utilise de mani√®re intensive\ncette approche. N√©anmoins, pour essayer de limiter\nles effets pervers de la production en continu d‚Äôun\nouvrage extensif, seuls les chapitres modifi√©s\nsont produits lors des pr√©visualisations mises en\noeuvre √† chaque pull request sur le d√©p√¥t\nGithub.\n\n\nLes data scientists doivent √™tre conscients\ndes implications de leur usage intensif de\nressources et essayer de minimiser leur\nimpact. Par exemple, plut√¥t que r√©-estimer\nun mod√®le de NLP,\nla m√©thode de l‚Äôapprentissage par transfert,\nqui permet de transf√©rer les poids d‚Äôapprentissage\nd‚Äôun mod√®le √† une nouvelle source, permet\nde r√©duire les besoins computationnels.\nDe m√™me, il peut √™tre utile, pour prendre\nconscience de l‚Äôeffet d‚Äôun code trop long,\nde convertir le temps de calcul en\n√©missions de gaz √† effet de serre.\nLe package codecarbon\npropose cette solution en adaptant l‚Äôestimation\nen fonction du mix √©nerg√©tique du pays\nen question. Mesurer √©tant le\npr√©requis pour prendre conscience puis comprendre,\nce type d‚Äôinitiatives peut amener √† responsabiliser\nles data scientists et ainsi permettre un\nmeilleur partage des ressources.\n\n\n\n\n\n\nArcep. 2019. ‚ÄúL‚Äôempreinte Carbone Du Num√©rique.‚Äù Rapport de l‚ÄôArcep.\n\n\nGabelica, Mirko, Ru≈æica Bojƒçiƒá, and Livia Puljak. 2022. ‚ÄúMany Researchers Were Not Compliant with Their Published Data Sharing Statement: Mixed-Methods Study.‚Äù Journal of Clinical Epidemiology.\n\n\nHurley, Mikella, and Julius Adebayo. 2016. ‚ÄúCredit Scoring in the Era of Big Data.‚Äù Yale JL & Tech. 18: 148.\n\n\nSamuel, Sheeba, and Daniel Mietchen. 2023. ‚ÄúComputational Reproducibility of Jupyter Notebooks from Biomedical Publications.‚Äù https://arxiv.org/abs/2308.07333.\n\n\nStrubell, Emma, Ananya Ganesh, and Andrew McCallum. 2019. ‚ÄúEnergy and Policy Considerations for Deep Learning in NLP.‚Äù https://arxiv.org/abs/1906.02243."
  },
  {
    "objectID": "content/getting-started/03_data_analysis.html#lors-de-la-r√©cup√©ration-des-donn√©es",
    "href": "content/getting-started/03_data_analysis.html#lors-de-la-r√©cup√©ration-des-donn√©es",
    "title": "Comment aborder un jeu de donn√©es",
    "section": "",
    "text": "La phase de constitution de son jeu de donn√©es sous-tend tout le projet qui suit.\nLa premi√®re question √† se poser est\n‚Äúde quelles donn√©es ai-je besoin pour r√©pondre √† ma probl√©matique ?‚Äù.\nCette probl√©matique pourra √©ventuellement\n√™tre affin√©e en fonction des besoins mais les travaux sont g√©n√©ralement\nde meilleure qualit√© lorsque la probl√©matique am√®ne √† la r√©flexion sur les donn√©es\ndisponibles plut√¥t que l‚Äôinverse.\nEnsuite, ‚Äúqui produit et met √† disposition ces donn√©es‚Äù ?\nLes sources disponibles sur internet sont-elles fiables ?\nPar exemple, les sites d‚Äôopen data gouvernementaux sont par exemple assez fiables mais autorisent parfois l‚Äôarchivage de donn√©es restructur√©es par des tiers et non des producteurs officiels. A l‚Äôinverse, sur Kaggle ou sur Github la source de certains jeux de donn√©es n‚Äôest pas trac√©e ce qui rend compliqu√©e la confiance sur la qualit√© de la donn√©e\nUne fois identifi√© une ou plusieurs sources de donn√©es,\nest-ce que je peux les compl√©ter avec d‚Äôautres donn√©es ?\n(dans ce cas, faire attention √† avoir des niveaux de granularit√© ad√©quats).\n\n\n\nVient ensuite la phase de mise en forme et nettoyage des jeux de donn√©es r√©cup√©r√©s.\nCette √©tape est primordiale et est g√©n√©ralement celle qui mobilise le plus\nde temps. Pendant quelques ann√©es, on parlait de data cleaning. Cependant,\ncela a pu, implicitement, laisser penser qu‚Äôil s‚Äôagissait d‚Äôune t√¢che\nsubalterne. On commence √† lui pr√©f√©rer le concept de feature engineering\nqui souligne bien qu‚Äôil s‚Äôagit d‚Äôune comp√©tence qui n√©cessite beaucoup\nde comp√©tences.\nUn jeu de donn√©es propre est un jeu de donn√©es dont la structure est\nad√©quate et n‚Äôentra√Ænera pas d‚Äôerreur, visible ou non,\nlors de la phase d‚Äôanalyse. Voici quelques √©l√©ments structurants\nd‚Äôun jeu de donn√©es propre :\n\nles informations manquantes sont bien comprises et trait√©es. numpy et\npandas proposent un certain formalisme sur le sujet qu‚Äôil est utile\nd‚Äôadopter en rempla√ßant par NaN les observations manquantes. Cela\nimplique de faire attention √† la mani√®re dont certains producteurs\ncodent les valeurs manquantes : certains ont la facheuse tendance √†\n√™tre imaginatifs sur les codes pour valeurs manquantes : ‚Äú-999‚Äù, ‚ÄúXXX‚Äù, ‚ÄúNA‚Äù\nles variables servant d‚Äôidentifiants sont bien les m√™mes d‚Äôune table √† l‚Äôautre (notamment dans le cas de jointure) : m√™me format, m√™me modalit√©s\npour des variables textuelles, qui peuvent etre mal saisies, avoir corrig√© les √©ventuelles fautes (ex ‚ÄúRolland Garros‚Äù -&gt; ‚ÄúRoland Garros‚Äù)\ncr√©er des variables qui synth√©tisent l‚Äôinformation dont vous avez besoin\nsupprimer les √©l√©ments inutiles (colonne ou ligne vide)\nrenommer les colonnes avec des noms compr√©hensibles"
  },
  {
    "objectID": "content/getting-started/03_data_analysis.html#lors-de-lanalyse-descriptive",
    "href": "content/getting-started/03_data_analysis.html#lors-de-lanalyse-descriptive",
    "title": "Comment aborder un jeu de donn√©es",
    "section": "",
    "text": "Une fois les jeux de donn√©es nettoy√©s, vous pouvez plus sereinement\n√©tudier l‚Äôinformation pr√©sente dans les donn√©es.\nCette phase et celle du nettoyage ne sont pas s√©quentielles,\nen r√©alit√© vous devrez r√©guli√®rement passer de votre nettoyage √† quelques statistiques descriptives qui vous montreront un probl√®me, retourner au nettoyage etc.\nLes questions √† se poser pour ‚Äúchallenger‚Äù le jeu de donn√©es :\n\nEst-ce que mon √©chantillon est bien repr√©sentatif de ce qui m‚Äôint√©resse ? N‚Äôavoir que 2000 communes sur les 35000 n‚Äôest pas n√©cessairement un probl√®me mais il est bon de s‚Äô√™tre pos√© la question.\nEst-ce que les ordres de grandeur sont bons ? Pour cela, confronter vos premieres stats desc √† vos recherches internet. Par exemple trouver que les maisons vendues en France en 2020 font en moyenne 400 m¬≤ n‚Äôest pas un ordre de grandeur r√©aliste.\nEst-ce que je comprends toutes les variables de mon jeu de donn√©es ? Est-ce qu‚Äôelles se ‚Äúcomportent‚Äù de la bonne fa√ßon ? A ce stade, il est parfois utile de se faire un dictionnaire de variables (qui explique comment elles sont construites ou calcul√©es). On peut √©galement mener des √©tudes de corr√©lation entre nos variables.\nEst-ce que j‚Äôai des outliers, i.e.¬†des valeurs aberrantes pour certains individus ? Dans ce cas, il faut d√©cider quel traitement on leur apporte (les supprimer, appliquer une transformation logarithmique, les laisser tel quel) et surtout bien le justifier.\nEst-ce que j‚Äôai des premiers grands messages sortis de mon jeu de donn√©es ? Est-ce que j‚Äôai des r√©sultats surprenants ? Si oui, les ai-je creus√© suffisamment pour voir si les r√©sultats tiennent toujours ou si c‚Äôest √† cause d‚Äôun souci dans la construction du jeu de donn√©es (mal nettoy√©es, mauvaise variable‚Ä¶)"
  },
  {
    "objectID": "content/getting-started/03_data_analysis.html#lors-de-la-mod√©lisation",
    "href": "content/getting-started/03_data_analysis.html#lors-de-la-mod√©lisation",
    "title": "Comment aborder un jeu de donn√©es",
    "section": "",
    "text": "A cette √©tape, l‚Äôanalyse descriptive doit voir avoir donn√© quelques premi√®res pistes pour savoir dans quelle direction vous voulez mener votre mod√®le.\nUne erreur de d√©butant est de se lancer directement dans la mod√©lisation parce\nqu‚Äôil s‚Äôagirait d‚Äôune comp√©tence plus pouss√©e. Cela am√®ne g√©n√©ralement\n√† des analyses de pauvre qualit√© : la mod√©lisation tend g√©n√©ralement √† confirmer\nles intuitions issues de l‚Äôanalyse descriptive. Sans cette derni√®re,\nl‚Äôinterpr√©tation des r√©sultats d‚Äôun mod√®le peu s‚Äôav√©rer inutilement complexe.\nVous devrez plonger dans vos autres cours (Econom√©trie 1, Series Temporelles, Sondages, Analyse des donn√©es etc.) pour trouver le mod√®le le plus adapt√© √† votre question.\nLa m√©thode sera guid√©e par l‚Äôobjectif.\n\nEst-ce que vous voulez expliquer ou pr√©dire ? https://hal-cnam.archives-ouvertes.fr/hal-02507348/document\nEst-ce que vous voulez classer un √©l√©ment dans une cat√©gorie (classification ou clustering) ou pr√©dire une valeur num√©rique (r√©gression) ?\n\nEn fonction des mod√®les que vous aurez d√©j√† vu en cours et des questions que vous souhaiterez r√©soudre sur votre jeu de donn√©es, le choix du mod√®le sera souvent assez direct.\nVous pouvez √©galement vous r√©f√©rez √† la d√©marche propos√©e par Xavier Dupr√©\nhttp://www.xavierdupre.fr/app/ensae_teaching_cs/helpsphinx3/debutermlprojet.html#l-debutermlprojet\nPour aller plus loin (mais de mani√®re simplifi√©e) sur les algorithmes de Machine Learning :\nhttps://datakeen.co/8-machine-learning-algorithms-explained-in-human-language/\n\n\nLa mise √† disposition de code sur Github ou Gitlab est une incitation\ntr√®s forte pour produire du code de qualit√©. Il est ainsi recommand√© de\nsyst√©matiquement utiliser ces plateformes pour la mise √† disposition de\ncode. Cependant, il ne s‚Äôagit que d‚Äôune petite partie des gains √†\nl‚Äôutiliser.\nLe cours que je donne avec Romain Avouac en troisi√®me ann√©e d‚ÄôENSAE\n(ensae-reproductibilite.github.io/website/) √©voque\nl‚Äôun des principaux gains √† utiliser ces plateformes, √† savoir\nla possibilit√© de mettre √† disposition automatiquement diff√©rents livrables\npour valoriser son travail aupr√®s de diff√©rents publics.\nSelon le public vis√©, la communication ne sera pas identique. Le code peut\nint√©resser les personnes d√©sirant avoir des d√©tails sur la m√©thodologie mise\nen oeuvre en pratique mais il peut s‚Äôagir d‚Äôun format rebutant pour d‚Äôautres\npublics. Une visualisation de donn√©es dynamiques parlera √† des publics\nmoins experts de la donn√©e mais est plus dure √† mettre en oeuvre\nqu‚Äôun graphique standard.\n\n\n\nLes Notebooks Jupyter ont eu beaucoup de succ√®s dans le monde de\nla data science pour valoriser des travaux. Pourtant il ne s‚Äôagit\npas forc√©ment toujours du meilleur format. En effet, beaucoup\nde notebooks tentent √† empiler des pav√©s de code et du texte, ce\nqui les rend difficilement lisibles.\nSur un projet cons√©quent, il vaut mieux reporter le plus de code\npossible dans des scripts bien structur√©s et avoir un notebook\nqui appelle ces scripts pour produire des outputs. Ou alors ne\npas utiliser un notebook et privil√©gier un autre format (un\ntableau de bord, un site web, une appli r√©active‚Ä¶).\nDans le cours de derni√®re ann√©e de\nl‚ÄôENSAE, Mise en production de projets data science, Romain\nAvouac et moi revenons sur les moyens de communication et de partage de code alternatifs au notebook."
  },
  {
    "objectID": "content/getting-started/03_data_analysis.html#ethique-et-responsabilit√©-du-data-scientist",
    "href": "content/getting-started/03_data_analysis.html#ethique-et-responsabilit√©-du-data-scientist",
    "title": "Comment aborder un jeu de donn√©es",
    "section": "",
    "text": "Les donn√©es sont une repr√©sentation synth√©tique de la r√©alit√© et les\nconclusions de certaines analyses peuvent avoir un vrai impact sur\nla vie des citoyens. Les chiffres erron√©s de\nReinhart et Rogoff ont ainsi pu servir de justification th√©orique √† des\npolitiques d‚Äôaust√©rit√© qui ont pu avoir des cons√©quences violentes\npour certains citoyens de\npays en crise1. En Grande-Bretagne, le recensement des personnes\ncontamin√©es par le Covid en 2020, et donc de leurs proches pour le\nsuivi de l‚Äô√©pid√©mie,\na √©t√© incomplet √† cause de\ntroncatures dues √† l‚Äôutilisation d‚Äôun format non adapt√© de stockage\ndes donn√©es (tableur Excel)2.\nDernier exemple avec le credit scoring mis en oeuvre aux Etats-Unis.\nLa citation ci-dessous, issue de l‚Äôarticle de Hurley and Adebayo (2016),\nillustre tr√®s bien les cons√©quences et les aspects probl√©matiques\nd‚Äôun syst√®me de construction automatis√©e d‚Äôun score de cr√©dit :\n\nConsumers have limited ability to identify and contest unfair credit\ndecisions, and little chance to understand what steps they\nshould take to improve their credit. Recent studies have also\nquestioned the accuracy of the data used by these tools, in some\ncases identifying serious flaws that have a substantial bearing\non lending decisions. Big-data tools may also risk creating a\nsystem of ‚Äúcreditworthinessby association‚Äù in which consumers‚Äô\nfamilial, religious, social, and other affiliations determine their\neligibility for an affordable loan.\nHurley and Adebayo (2016)\n\n\n\n\nLa transparence sur les int√©r√™ts et limites d‚Äôune m√©thode mise en oeuvre\nest donc importante.\nCette exigence de la recherche, parfois oubli√©e √† cause de la course\naux r√©sultats novateurs, m√©rite √©galement d‚Äô√™tre appliqu√©e\nen entreprise ou administration.\nM√™me sans intention manifeste de la part de la personne qui analyse des donn√©es,\nune mauvaise interpr√©tation est toujours possible. Tout en valorisant un\nr√©sultat, il est possible d‚Äôalerter sur certaines limites. Il est important,\ndans ses recherches comme dans les discussions avec d‚Äôautres interlocuteurs,\nde faire attention au biais de confirmation qui consiste\n√† ne retenir que l‚Äôinformation qui correspond √† nos conceptions a priori et\n√† ne pas consid√©rer celles qui pourraient aller √† l‚Äôencontre de celles-ci:\n\n\n\n\n\nCertaines repr√©sentations de donn√©es sont √† exclure car des biais cognitifs\npeuvent amener √† des interpr√©tations erron√©es3. Dans le domaine de la\nvisualisation de donn√©es, les camemberts (pie chart) ou les diagrammes\nradar sont par exemple\n√† exclure car l‚Äôoeil humain per√ßoit mal ces formes circulaires. Pour une raison\nsimilaire, les cartes avec aplat de couleur (cartes\nchoropl√®thes) sont trompeuses.\nLes posts de blog pour datawrapper\nde Lisa Charlotte Muth ou ceux d‚ÄôEric Mauvi√®re sont d‚Äôexcellentes ressources\npour apprendre les bonnes et mauvaises pratiques de\nvisualisation (voir la partie visualisation de ce cours\npour plus de d√©tails).\n\n\n\nLe cadre r√©glementaire de protection des donn√©es a √©volu√© ces derni√®res\nann√©es avec le RGPD. Cette r√©glementation a permis de mieux faire\nsaisir le fait que la collecte de donn√©es se justifie au nom\nde finalit√©s plus ou moins bien identifi√©es. Prendre conscience que\nla confidentialit√© des donn√©es se justifie pour √©viter la diss√©mination\nnon contr√¥l√©e d‚Äôinformations sur une personne est important.\nDes donn√©es particuli√®rement sensibles, notamment les donn√©es de sant√©,\npeuvent √™tre plus contraignantes √† traiter que des donn√©es peu sensibles.\nEn Europe, par exemple, les agents du service statistique public\n(Insee ou services statistiques minist√©riels) sont tenus au secret professionnel\n(article L121-6 du Code g√©n√©ral de la fonction publique),\nqui leur interdit la communication des informations confidentielles\ndont ils sont d√©positaires au titre de leurs missions ou fonctions,\nsous peine des sanctions pr√©vues par l‚Äôarticle 226-13 du Code p√©nal\n(jusqu‚Äô√† un an d‚Äôemprisonnement et 15 000 ‚Ç¨ d‚Äôamende).\nLe secret statistique, d√©fini dans une loi de 1951,\nrenforce cette obligation dans le cas de donn√©es d√©tenues pour des usages statistiques.\nIl interdit strictement la communication de donn√©es individuelles\nou susceptibles d‚Äôidentifier les personnes,\nissues de traitements √† finalit√©s statistiques,\nque ces traitements proviennent d‚Äôenqu√™tes ou de bases de donn√©es.\nLe secret statistique exclut par principe de diffuser des donn√©es\nqui permettraient l‚Äôidentification des personnes concern√©es,\npersonnes physiques comme personnes morales.\nCette obligation limite la finesse des informations disponibles en diffusion\nCe cadre contraignant s‚Äôexplique par l‚Äôh√©ritage de la Seconde Guerre Mondiale\net le d√©sir de ne plus revivre une situation o√π la collecte d‚Äôinformation\nsert une action publique bas√©e sur la discrimination entre cat√©gories\nde la population.\n\n\n\nUn article r√©cent de Nature,\nqui reprend les travaux d‚Äôune √©quipe d‚Äô√©pid√©miologistes (Gabelica, Bojƒçiƒá, and Puljak 2022)\n√©voque le probl√®me de l‚Äôacc√®s aux donn√©es pour des chercheurs d√©sirant reproduire\nune √©tude. M√™me dans les articles scientifiques o√π il est mentionn√© que les\ndonn√©es peuvent √™tre mises √† disposition d‚Äôautres chercheurs, le partage\nde celles-ci est rare :\n\nGraphique issu de l‚Äôarticle de Nature\nCe constat, quelque peu inqui√©tant, est confirm√© par une √©tude r√©cente\nde Samuel and Mietchen (2023) qui a tent√© d‚Äôex√©cuter un peu moins de\n30 000 notebooks associ√©s √† des √©tudes scientifiques. Seuls 3%\ndes notebooks reproduisent les r√©sultats esp√©r√©s.\nAfin de partager les moyens de reproduire des publications sans diffuser des\ndonn√©es potentiellement confidentielles, les jeux de donn√©es synth√©tiques\nsont de plus en plus utilis√©s. Par le biais de mod√®les de deep learning,\nil est ainsi possible de g√©n√©rer des jeux de donn√©es synth√©tiques complexes\nqui permettent de reproduire les principales caract√©ristiques d‚Äôun jeu de donn√©es\ntout en √©vitant, si le mod√®le a √©t√© bien calibr√©, de diffuser une information\nindividuelle.\nDans l‚Äôadministration fran√ßaise, les codes sources sont\nconsid√©r√©s comme des documents administratifs et peuvent\ndonc √™tre mis √† disposition de tout citoyen sur demande √† la\nCommission d‚Äôacc√®s aux documents administratifs (CADA):\n\n¬´ Sont consid√©r√©s comme documents administratifs, au sens des titres Ier, III et IV du pr√©sent livre, quels que soient leur date, leur lieu de conservation, leur forme et leur support, les documents produits ou re√ßus, dans le cadre de leur mission de service public, par l‚Äô√âtat, les collectivit√©s territoriales ainsi que par les autres personnes de droit public ou les personnes de droit priv√© charg√©es d‚Äôune telle mission. Constituent de tels documents notamment les dossiers, rapports, √©tudes, comptes rendus, proc√®s-verbaux, statistiques, instructions, circulaires, notes et r√©ponses minist√©rielles, correspondances, avis, pr√©visions, codes sources et d√©cisions. ¬ª\nAvis 20230314 - S√©ance du 30/03/2023 de la Commission d‚Äôacc√®s aux documents administratifs\n\nEn revanche, les poids des mod√®les utilis√©s par l‚Äôadministration, notamment ceux\ndes mod√®les de machine learning ne sont pas r√©glement√©s de la m√™me\nmani√®re (Avis 20230314 de la CADA).\nEn effet, comme il existe toujours\nun risque de r√©tro-ing√©nierie amenant √† une r√©v√©lation partielle\ndes donn√©es\nd‚Äôentra√Ænement lors d‚Äôun partage de mod√®le, les mod√®les\nentra√Æn√©s sur des donn√©es\nsensibles (comme les d√©cisions de justice √©tudi√©es\npar (l‚Äôavis 20230314 de la CADA))\nn‚Äôont pas vocation √† √™tre partag√©s.\n\n\n\nLe num√©rique constitue une part croissante des\n√©missions de gaz √† effet de serre.\nRepr√©sentant aujourd‚Äôhui 4 % des √©missions mondiales\nde CO2, cette part devrait encore cro√Ætre (Arcep 2019).\nLe monde de la data science est √©galement\nconcern√©.\nL‚Äôutilisation de donn√©es de plus en\nplus massives, notamment la constitution\nde corpus monumentaux de textes,\nr√©cup√©r√©s par scraping, est une premi√®re source\nde d√©pense d‚Äô√©nergie. De m√™me, la r√©cup√©ration\nen continu de nouvelles traces num√©riques\nn√©cessite d‚Äôavoir des serveurs fonctionnels\nen continu. A cette premi√®re source de\nd√©pense d‚Äô√©nergie, s‚Äôajoute l‚Äôentra√Ænement\ndes mod√®les qui peut prendre des jours,\ny compris sur des architectures tr√®s\npuissantes. Strubell, Ganesh, and McCallum (2019)\nestime que l‚Äôentra√Ænement d‚Äôun mod√®le √†\nl‚Äô√©tat de l‚Äôart dans le domaine du\nNLP n√©cessite autant d‚Äô√©nergie que ce que\nconsommeraient cinq voitures, en moyenne,\nau cours de l‚Äôensemble de leur\ncycle de vie.\nL‚Äôutilisation accrue de l‚Äôint√©gration\ncontinue, qui permet de mettre en oeuvre de mani√®re\nautomatis√©e l‚Äôex√©cution de certains scripts ou\nla production de livrables en continu,\nam√®ne √©galement √† une d√©pense d‚Äô√©nergie importante.\nIl convient donc d‚Äôessayer de limiter l‚Äôint√©gration\ncontinue √† la production d‚Äôoutput vraiment nouveaux.\n\n\n\nPar exemple, cet ouvrage utilise de mani√®re intensive\ncette approche. N√©anmoins, pour essayer de limiter\nles effets pervers de la production en continu d‚Äôun\nouvrage extensif, seuls les chapitres modifi√©s\nsont produits lors des pr√©visualisations mises en\noeuvre √† chaque pull request sur le d√©p√¥t\nGithub.\n\n\nLes data scientists doivent √™tre conscients\ndes implications de leur usage intensif de\nressources et essayer de minimiser leur\nimpact. Par exemple, plut√¥t que r√©-estimer\nun mod√®le de NLP,\nla m√©thode de l‚Äôapprentissage par transfert,\nqui permet de transf√©rer les poids d‚Äôapprentissage\nd‚Äôun mod√®le √† une nouvelle source, permet\nde r√©duire les besoins computationnels.\nDe m√™me, il peut √™tre utile, pour prendre\nconscience de l‚Äôeffet d‚Äôun code trop long,\nde convertir le temps de calcul en\n√©missions de gaz √† effet de serre.\nLe package codecarbon\npropose cette solution en adaptant l‚Äôestimation\nen fonction du mix √©nerg√©tique du pays\nen question. Mesurer √©tant le\npr√©requis pour prendre conscience puis comprendre,\nce type d‚Äôinitiatives peut amener √† responsabiliser\nles data scientists et ainsi permettre un\nmeilleur partage des ressources."
  },
  {
    "objectID": "content/getting-started/03_data_analysis.html#r√©f√©rences",
    "href": "content/getting-started/03_data_analysis.html#r√©f√©rences",
    "title": "Comment aborder un jeu de donn√©es",
    "section": "",
    "text": "Arcep. 2019. ‚ÄúL‚Äôempreinte Carbone Du Num√©rique.‚Äù Rapport de l‚ÄôArcep.\n\n\nGabelica, Mirko, Ru≈æica Bojƒçiƒá, and Livia Puljak. 2022. ‚ÄúMany Researchers Were Not Compliant with Their Published Data Sharing Statement: Mixed-Methods Study.‚Äù Journal of Clinical Epidemiology.\n\n\nHurley, Mikella, and Julius Adebayo. 2016. ‚ÄúCredit Scoring in the Era of Big Data.‚Äù Yale JL & Tech. 18: 148.\n\n\nSamuel, Sheeba, and Daniel Mietchen. 2023. ‚ÄúComputational Reproducibility of Jupyter Notebooks from Biomedical Publications.‚Äù https://arxiv.org/abs/2308.07333.\n\n\nStrubell, Emma, Ananya Ganesh, and Andrew McCallum. 2019. ‚ÄúEnergy and Policy Considerations for Deep Learning in NLP.‚Äù https://arxiv.org/abs/1906.02243."
  },
  {
    "objectID": "content/getting-started/03_data_analysis.html#footnotes",
    "href": "content/getting-started/03_data_analysis.html#footnotes",
    "title": "Comment aborder un jeu de donn√©es",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLe livre de Reinhart et Rogoff, This time is different, s‚Äôappuyait\nsur un Excel constitu√© √† la main. Un doctorant s‚Äôest aper√ßu d‚Äôerreurs\ndans celui-ci et a remarqu√© que lorsqu‚Äôon\nsubstituait les chiffres officiels, les r√©sultats n‚Äô√©taient plus valides.‚Ü©Ô∏é\nOn suppose ici que le message erron√© est transmis sans volont√© de\nmanipulation. La manipulation manifeste est un probl√®me encore plus grave.‚Ü©Ô∏é\nOn suppose ici que le message erron√© est transmis sans volont√© de\nmanipulation. La manipulation manifeste est un probl√®me encore plus grave.‚Ü©Ô∏é"
  },
  {
    "objectID": "content/getting-started/01_installation.html",
    "href": "content/getting-started/01_installation.html",
    "title": "Configuration de Python",
    "section": "",
    "text": "Les exercices sont pr√©sent√©s sous la\nforme de notebook jupyter. Ils peuvent √™tre ex√©cut√©s\ndans plusieurs environnement, au gr√© des pr√©f√©rences et des connaissances\nde chacun :\nConcernant la premi√®re m√©thode, qui est celle recommand√©e,\nchaque\nchapitre pr√©sente les badges suivants qui permettent d‚Äôouvrir\nla page web en question dans l‚Äôenvironnement de pr√©dilection.\nPar exemple, pour ouvrir le chapitre relatif √†\nnumpy dans l‚Äôun des environnements temporaires propos√©s,\nles badges suivants sont propos√©s:\nQuel que soit l‚Äôenvironnement d‚Äôex√©cution des scripts, l‚Äôun des objectifs\nde ce cours est d‚Äôadopter un environnement favorable √† la reproductibilit√©\ndes traitements. Ils devraient donc fonctionner, d√®s lors que l‚Äôenvironnement\nest bien configur√©, d‚Äôune mani√®re similaire quel que soit\nla machine qui ex√©cute le code.\nComme la reproductibilit√© est une notion centrale dans une d√©marche\nscientifique mais √©galement importante dans le monde\nde l‚Äôentreprise ou de l‚Äôadministration, en suppl√©ment des notions relatives\n√† Python, ce cours montrera comment utiliser Git avec Python et\n√©voquera un\ncertain nombre de crit√®res de qualit√© du code qui sont devenus\ndes standards dans la communaut√© open-source, dans l‚Äôindustrie et dans\nl‚Äôadministration. Ces comp√©tences ne sont pas\npropres √† Python et seront\nutiles pour tout projet ult√©rieur. Un cours d√©di√© √† cette question\nest propos√© par Romain Avouac et moi en derni√®re ann√©e de l‚ÄôENSAE. Son\ncontenu est disponible sur https://ensae-reproductibilite.github.io/website/.\nLe projet final devra imp√©rativement\n√™tre associ√© √† un d√©p√¥t\nsur Github (nous reviendrons dessus) et r√©pondre √†\nces crit√®res de qualit√©, qui serviront toute la vie.\nCe cours vise √† acculturer √† la conduite de projets de data-science avec\nPython. L‚Äôenvironnement foisonnant de la data-science n√©cessite un\ncertain nombre d‚Äô√©l√©ments suppl√©mentaires √† Python. La suite\nde ce chapitre permettra de d√©crire les configurations √† mettre\nen oeuvre pour √™tre en mesure d‚Äôexploiter la richesse de l‚Äô√©cosyst√®me Python."
  },
  {
    "objectID": "content/getting-started/01_installation.html#local",
    "href": "content/getting-started/01_installation.html#local",
    "title": "Configuration de Python",
    "section": "Installer un environnement adapt√© √† la data-science sur son ordinateur personnel",
    "text": "Installer un environnement adapt√© √† la data-science sur son ordinateur personnel\nCette partie pr√©sente plusieurs √©l√©ments de configuration d‚Äôun environnement\nen local. Cependant, cette approche est de moins en moins fr√©quente. En effet,\nplusieurs facteurs conjoints ont amen√© √† privil√©gier des\nserveurs plut√¥t que des installations locales (√©volutions dans les technologies cloud,\nbesoins accrus de ressources, besoins de plus de contr√¥le sur la confidentialit√©\ndes donn√©es en limitant leur prolif√©ration‚Ä¶). Au sein des administrations et\ndes entreprises, les approches cloud, o√π l‚Äôutilisateur se voit mis √† disposition\nune interface graphique alors que les calculs sont d√©port√©s sur un serveur\ndistant, est de plus en plus fr√©quent.\n\nInstaller Python en local\nPour installer Python, il est recommand√© d‚Äôutiliser\nla distribution Anaconda\nqui permet d‚Äôinstaller une distribution minimale de Python ainsi qu‚Äô√©ventuellement\nun environnement plus complet :\n\nSous Windows, il suffit de t√©l√©charger l‚Äôex√©cutable puis\nl‚Äôex√©cuter (cf.¬†la doc officielle ;\nSous Mac, se reporter √† la doc officielle ;\nSous Linux, suivre les instructions de la doc officielle selon sa distribution\n\nPasser par Anaconda permet:\n\nd‚Äôinstaller Python ;\nd‚Äôinstaller par d√©faut une multitude de packages utiles ;\nde pouvoir utiliser un gestionnaire de package nomm√© conda.\n\nAnaconda permet de cr√©er des environnements isol√©s et facilite l‚Äôinstallation\nde certaines librairies qui n√©cessitent l‚Äôusage de langages externes (par exemple\ndu C++).\n\n\nInstaller un environnement de d√©veloppement\nLes notebooks Jupyter (extension .ipynb)\nsont tr√®s utilis√©s en data science. Ils sont en\nparticulier tr√®s adapt√©s √† la r√©alisation d‚Äôanalyses exploratoires.\nLes notebooks permettent de m√™ler du code, du texte et des sorties\ngraphiques ou des tableaux. L‚Äôint√©r√™t principal des notebooks est qu‚Äôils\npermettent d‚Äôex√©cuter du code tr√®s facilement dans un environnement\nPython donn√© (le kernel Jupyter). Ils sont particuli√®rement pratiques\npour ajouter du code ou du texte √† un document d√©j√† existant, d‚Äôo√π le\nterme de notebook.\nN√©anmoins, pass√©e l‚Äô√©tape d‚Äôexploration, il est recommand√© de plut√¥t recourir √† des\nscripts au format .py. L‚Äôutilisation du format .py est l‚Äôun des premiers\ngestes pour favoriser la reproductibilit√© des analyses.\nCes scripts peuvent √™tre √©dit√©s √† l‚Äôaide d‚Äô√©diteurs de texte adapt√©s au code, comme\nVisual Studio\n(mon pr√©f√©r√©),\nSublime Text,\nou PyCharm (privil√©gier Pycharm Community Edition)\nentre autres.\nCes √©diteurs\noffrent des fonctionalit√©s suppl√©mentaires pratiques :\n\nnombreux plugins pour une pleine utilisation de l‚Äô√©cosyst√®me Python: √©diteur de Markdown,\ninterface Git, etc.\nfonctionalit√©s classiques d‚Äôun IDE dont manque Jupyter: autocompl√©tion, diagnostic du code, etc.\nint√©gration avec les environnements Conda\n\n\n\nInstallation de Git\nLe principe de Git ainsi que son usage avec Python sont pr√©sent√©s dans\nune partie d√©di√©e. Cette partie se concentre ainsi sur la question\nde la configuration de Git.\nGit est un langage dont la fonction est de tracer l‚Äôhistorique de modification\nd‚Äôun fichier. Pour disposer de ce langage, il est n√©cessaire d‚Äôinstaller\nle logiciel Git Bash. Gr√¢ce √† lui, Git sera disponible et des outils\nexternes, notamment les interfaces de d√©veloppement comme\nVisual Studio, pourront l‚Äôutiliser."
  },
  {
    "objectID": "content/getting-started/01_installation.html#ex√©cution-dans-un-environnement-temporaire-sur-un-serveur-distant",
    "href": "content/getting-started/01_installation.html#ex√©cution-dans-un-environnement-temporaire-sur-un-serveur-distant",
    "title": "Configuration de Python",
    "section": "Ex√©cution dans un environnement temporaire sur un serveur distant",
    "text": "Ex√©cution dans un environnement temporaire sur un serveur distant\nComme √©voqu√© pr√©c√©demment, les technologies dominantes dans\nle domaine du traitement des donn√©es ont amen√© √† une √©volution des pratiques\ndepuis quelques ann√©es.\nLa multiplication de donn√©es volumineuses qui d√©passent les capacit√©s en RAM\nvoire en stockage des machines personnelles,\nles progr√®s dans les technologies de stockage type cloud,\nl‚Äôadh√©sion de la communaut√© aux outils de versioning\n(le plus connu √©tant Git) sont autant de facteurs\nayant amen√© √† repenser la mani√®re de traiter des donn√©es.\nLes infrastructures √† l‚Äô√©tat de l‚Äôart permettent ainsi de d√©coupler stockage\ndes donn√©es, stockage du code et ex√©cution des traitements sur les donn√©es.\nL‚Äôex√©cution des traitements s‚Äôeffectue ainsi sur des machines √† la dur√©e de vie\ncourte qui stockent temporairement donn√©es et code ensembles pour tester\nles traitements.\nAvec les d√©p√¥ts sur Github ou Gitlab,\non dissocie environnement de stockage des codes et\nd‚Äôex√©cution de ceux-ci. Un syst√®me de stockage S3, pr√©sent√© dans un\nchapitre ult√©rieur, permet en suppl√©ment de dissocier l‚Äôenvironnement\nde stockage des donn√©es de ces deux premiers environnements.\nSur le\nd√©p√¥t Github de ce cours , on peut\nnaviguer dans les fichiers\n(et voir tout l‚Äôhistorique de modification de ceux-ci). Mais,\ncomment ex√©cuter les scripts sans passer par un poste local ?\nDepuis quelques ann√©es, des services en ligne permettant de\nlancer une instance Jupyter √† distance (analogue √† celle que vous pouvez\nlancer en local en utilisant Anaconda) ont √©merg√©. Parmi celles-ci :\n\nLe SSP Cloud , plateforme d√©velopp√©e par l‚ÄôInsee qui fournit des environnements bac √† sable bas√©s sur des technologie de conteneurisation\nGoogle colaboratory\n\n;\nGithub Visual Studio Editor  ;\nBinder  ;\n\nIl est √©galement possible d‚Äôex√©cuter des codes sur les services d‚Äôint√©gration continue de\nGitlab (service Gitlab CI)\nou de Github (via Github Actions). Il s‚Äôagit d‚Äôune approche\nbash, c‚Äôest-√†-dire que les scripts sont ex√©cut√©s par une console √† chaque interaction avec le d√©p√¥t\ndistant Gitlab/Github, sans session ouverte pour les √©diter.\nCette approche est tr√®s appropri√©e\npour assurer la reproductibilit√© d‚Äôune cha√Æne de traitement (on peut aller\njusqu‚Äôau\nd√©ploiement de visualisations automatiques1) mais n‚Äôest pas tr√®s pratique pour\nle griffonnage.\n2 A cet √©gard, il est recommand√© de consulter le cours de derni√®re ann√©e\nde l‚ÄôENSAE d√©j√† cit√©: https://ensae-reproductibilite.github.io/website/\nKaggle \npropose des comp√©titions de code mais\ndonne √©galement la possibilit√© d‚Äôex√©cuter des notebooks,\ncomme les solutions pr√©c√©dentes.\nIl existe une API Kaggle pour\nacc√©der √† des donn√©es Kaggle hors du syst√®me Kaggle\n\n\n Warning\nLes performances de ces solutions peuvent √™tre variables.\nLes serveurs publics mis √† disposition\nne sont pas forc√©ment des foudres de guerre. Avec ceux-ci,\non v√©rifie plut√¥t la reproductibilit√© des scripts avec des jeux d‚Äôexemples.\nIl est bien s√ªr interdit de mettre des donn√©es confidentielles dessus: ces\nderni√®res doivent rester dans des infrastructures o√π elles sont autoris√©es.\nQuand on est dans une entreprise ou administration,\nqui dispose de serveurs propres,\non peut aller plus loin en utilisant ces outils\npour automatiser l‚Äôensemble de la cha√Æne de traitement.\nAttention: il n‚Äôy a pas de garantie de perennit√© de service\n(notamment avec Binder o√π\n10 minutes d‚Äôinactivit√© m√®nent √† l‚Äôextinction du service). Il s‚Äôagit plus d‚Äôun service pour griffoner\ndans le m√™me environnement que celui du d√©p√¥t Git que de solutions durables.\nLes sessions sur l‚Äôenvironnement SSPCloud sont plus durables mais il convient\nde garder √† l‚Äôesprit qu‚Äôelles sont √©galement temporaires.\n\n\n\nSSP Cloud \nOnyxia, l‚Äôautre petit nom du SSP Cloud,\nest une plateforme libre service mutualis√©e de traitement\nde donn√©es statistiques et de datascience.\nCe cloud met √† disposition aux statisticiens et aux data scientists\nde l‚Äô√âtat un catalogue de services et un environnement de travail simple, rapide et collaboratif, permettant de lancer facilement ces outils et d‚Äôy connecter ses donn√©es et son code.\nAu-del√† des ressources techniques, cette plateforme\nrepr√©sente une opportunit√© pour les statisticiens publics et les\n√©tudiants de d√©couvrir\net d‚Äôadopter de nouvelles m√©thodes de travail.\nElle est aussi utilis√© √† des fins de formations et d‚Äôauto-formations.\nDans cet environnement, Jupyter et Visual Studio sont tous deux\ndisponibles.\n\n\nGoogle colaboratory \nGoogle met √† disposition une plateforme de calculs bas√©e sur le format Jupyter Notebook.\nUn grand avantage de cette solution est la mise √† disposition gratuite de\nGPUs de qualit√© raisonnable,\noutil quasi-indispensable dans les projets bas√©s sur des m√©thodes de deep learning.\nIl est possible de connecter les notebooks ouverts √† Google Drive ou √†\nGithub. L‚Äôicone\n\nfournit un raccourci pour lancer le notebook dans un environnement d√©di√©.\n\n\nGithub Visual Studio Editor \nMicrosoft qui poss√®de √† la fois Github et Visual Studio a r√©cemment\nlanc√© une offre Github dev qui permet d‚Äôouvrir et lancer un notebook\nJupyter depuis un navigateur web.\nEn plus des fonctionalit√©s attendues du logiciel Visual Studio\nCette interface permet √©galement de g√©rer les issues et pull request\nd‚Äôun d√©p√¥t Github.\n\n\nLa technologie en arri√®re-plan : Docker \nDocker est l‚Äôoutil open-source de r√©f√©rence\nen mati√®re de cr√©ation d‚Äôenvironnements isol√©s et auto-suffisants (les conteneurs).\nEn pratique, une application cod√©e en Python ne repose que rarement seulement sur\ndu code produit par son d√©veloppeur, elle fait g√©n√©ralement intervenir des d√©pendances :\nd‚Äôautres librairies Python, ainsi que des librairies li√©es au syst√®me d‚Äôexploitation\nsur laquelle elle est d√©velopp√©e. Docker va permettre d‚Äôempaqueter l‚Äôapplication ainsi\nque toutes ses d√©pendances et rendre son ex√©cution portable, c‚Äôest √† dire ind√©pendante\ndu syst√®me sur laquelle elle est √©x√©cut√©e.\nDocker  est utilis√© dans\nle cadre de cours afin d‚Äôassurer la reproductibilit√© des exemples.\nPlus de d√©tails sont disponibles dans le cours de derni√®re ann√©e d‚ÄôENSAE\nd√©di√© √† la mise en production de projets data science\n(https://ensae-reproductibilite.github.io/website/).\nIl est possible d‚Äôutiliser les images Docker sur lesquelles reposent\nl‚Äôenvironnement de reproductibilit√© du cours. Celles-ci sont mises √†\ndisposition sur DockerHub, le principal r√©seau de mise √† disposition\nd‚Äôimages Docker. Il existe une image minimale\nqui int√®gre Python et Quarto.\nPour utiliser l‚Äôimage Visual Studio:\ndocker pull linogaliana/python-datascientist-vstudio\ndocker run --rm -p 8787:8787 -e PASSWORD=test linogaliana/python-datascientist-vstudio\nEn se rendant depuis un navigateur sur localhost:8887/, et en rentrant\nle mot de passe test (d√©fini plus haut), on peut ainsi acc√©der\n√† l‚Äôinterface d√©sir√©e (attention il s‚Äôagit d‚Äôun environnement temporaire, pas\np√©renne)."
  },
  {
    "objectID": "content/getting-started/01_installation.html#installer-des-packages-suppl√©mentaires",
    "href": "content/getting-started/01_installation.html#installer-des-packages-suppl√©mentaires",
    "title": "Configuration de Python",
    "section": "Installer des packages suppl√©mentaires",
    "text": "Installer des packages suppl√©mentaires\nUn module est un script qui a vocation √† d√©finir des objets utilis√©s\npost√©rieurement par un interpr√©teur. C‚Äôest un script .py autosuffisant,\nd√©finissant des objets et des relations entre eux et le monde ext√©rieur\n(d‚Äôautres modules). Un package est un ensemble coh√©rent de modules. Par exemple\nscikit-learn propose de nombreux modules utiles pour le machine learning.\nPython, sans ajout de briques suppl√©mentaires,\ntrouvera rapidement ses limites.\nM√™me dans les scripts les plus simples, on a g√©n√©ralement besoin de packages qui\n√©vitent de r√©inventer la roue.\nLes packages sont les √©l√©ments qui font la richesse des\nlangages open-source.\nIls sont l‚Äô√©quivalent des packages R ou Stata.\nLe monde de d√©veloppeurs Python est tr√®s prolifique :\ndes mises √† jour sont tr√®s souvent disponibles,\nles biblioth√®ques de packages sont tr√®s nombreuses. Un data scientist\nprendra l‚Äôhabitude de jongler avec des dizaines de packages dont il conna√Ætra\nquelques fonctions et o√π, surtout, il saura aller chercher de l‚Äôinformation.\nLe rythme des mises √† jour et des ajouts de fonctionalit√©s\ns‚Äôest acc√©l√©r√© ces derni√®res ann√©es. Les grandes compagnies du\nnum√©rique ont elles-m√™mes opensourc√©es des librairies\ndevenues centrales dans l‚Äô√©cosyst√®me de la data-science\n(TensorFlow par Google, PyTorch par Facebook‚Ä¶)\nLes forums, notamment StackOverflow\nregorgent de bons conseils.\nLes deux meilleurs conseils qu‚Äôon puisse donner :\n\nregarder la documentation officielle d‚Äôun package. Les bons packages sont\ng√©n√©ralement tr√®s bien document√©s et beaucoup d‚Äôerreurs peuvent √™tre √©vit√©es\nen apprenant √† chercher dans la documentation ;\nen cas d‚Äôerreur : copiez-collez l‚Äôerreur sur votre moteur de recherche pr√©f√©r√©. Quelqu‚Äôun aura d√©j√† pos√© la question, sans doute sur stackoverflow. N√©anmoins, ne copiez-collez\npas la r√©ponse sans comprendre la solution.\n\n\nLes gestionnaires de packages\nLes packages d‚Äôun langage open-source sont mis √† disposition sur\ndes d√©p√¥ts. Le CTAN est ainsi le d√©p√¥t LaTeX le plus connu, le\nCRAN celui du langage R.\nEn Python, il existe deux gestionnaires de packages qu‚Äôon utilise\nassoci√©s √† deux d√©p√¥ts diff√©rents :\n\npip associ√© au d√©p√¥t PyPi\nconda associ√© au d√©p√¥t Anaconda\n\nAnaconda a permis, il y a quelques ann√©es, de faciliter grandement\nl‚Äôinstallation de librairies d√©pendants d‚Äôautres langages\nque Python (notamment des librairies C pour am√©liorer\nla performance des calculs). Ces derni√®res sont\ncompliqu√©es √† installer, notamment sur Windows.\nLe fait de proposer des librairies pr√©-compil√©es sur une grande\nvari√©t√© de syst√®mes d‚Äôexploitation a √©t√© une avanc√©e\nd‚Äôanaconda. PyPi a adopt√© ce m√™me principe avec les\nwheels ce qui finalement, rend les installations\navec pip √† nouveau int√©ressantes (sauf pour certaines\nlibrairies en Windows).\nAnaconda a deux d√©fauts par rapport √† pip :\n\nl‚Äôinstallation de packages via pip est plus rapide que via\nconda. conda est en effet plus pr√©cautionneux sur l‚Äôinteraction\nentre les diff√©rentes versions des packages install√©s.\nmamba a r√©cemment\n√©t√© d√©velopp√© pour acc√©l√©rer l‚Äôinstallation de packages dans un\nenvironnement conda3\nles versions disponibles sur PyPi sont plus r√©centes\nque celles sur le canal par d√©faut d‚ÄôAnaconda. En effet,\npour un d√©veloppeur de packages, il est possible de publier\nun package de mani√®re automatique sur PyPi\nL‚Äôutilisation\ndu canal alternatif qu‚Äôest la conda forge permet de disposer de versions plus r√©centes des packages et limite l‚Äô√©cart avec les versions\ndisponibles sur PyPi.\n\n\n\n Warning\nLes conditions d‚Äôutilisation du canal par d√©faut d‚ÄôAnaconda sont\nassez restrictives. L‚Äôutilisation d‚ÄôAnaconda dans un cadre commercial est ainsi, depuis 2020,\nsoumis √† l‚Äôachat de licences commerciales d‚ÄôAnaconda pour r√©duire le probl√®me de\npassager clandestin.\nIl est ainsi recommand√©, notamment lorsqu‚Äôon travaille dans le\nsecteur priv√© o√π du code Python peut √™tre utilis√©,\nde ne pas ignorer ces conditions pour ne pas se mettre en faute juridiquement.\nLa conda forge n‚Äôest pas soumise √† ces conditions et est ainsi pr√©f√©rable\ndans les entreprises.\n\n\n\n\nComment installer des packages\nAvec Anaconda, il faut passer par la ligne de commande et taper\nconda install &lt;nom_module&gt;\nPar exemple conda install geopandas. Depuis une cellule de notebook\nJupyter, on ajoute un point d‚Äôexclamation pour indiquer √† Jupyter\nque la commande doit √™tre interpr√©t√©e comme une commande shell\net non une commande Python\n!conda install &lt;nom_module&gt; -y\nL‚Äôoption -y permet d‚Äô√©viter que conda nous demande confirmation\nsur l‚Äôinstallation du package. Pour mettre √† jour un package, on fera\nconda upgrade plut√¥t que conda install\nAvec pip, on va cette fois taper\npip install &lt;nom_module&gt;\npip permet √©galement d‚Äôinstaller des librairies directement depuis\nGithub √† condition que Anaconda et Git sachent\ncommuniquer (ce qui implique en g√©n√©ral que Git soit dans le PATH\ndu syst√®me d‚Äôexploitation). Par exemple, pour installer le package\npynsee\npip install git+https://github.com/InseeFrLab/Py-Insee-Data.git#egg=pynsee\nLa partie d√©di√©e aux environnement virtuels du cours de derni√®re ann√©e de\nl‚ÄôENSAE pr√©sente plus d‚Äô√©l√©ments sur les diff√©rences moins √©videntes\nentre pip et conda."
  },
  {
    "objectID": "content/getting-started/01_installation.html#footnotes",
    "href": "content/getting-started/01_installation.html#footnotes",
    "title": "Configuration de Python",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLes gains de performance peuvent √™tre assez impressionnants.\nLa cr√©ation de l‚Äôenvironnement n√©cessaire √† la construction automatis√©e\nde ce site web a ainsi √©t√© divis√©e par 12 en utilisant mamba plut√¥t\nque conda pour installer des packages dans un environnement.‚Ü©Ô∏é\nLes gains de performance peuvent √™tre assez impressionnants.\nLa cr√©ation de l‚Äôenvironnement n√©cessaire √† la construction automatis√©e\nde ce site web a ainsi √©t√© divis√©e par 12 en utilisant mamba plut√¥t\nque conda pour installer des packages dans un environnement.‚Ü©Ô∏é\nLes gains de performance peuvent √™tre assez impressionnants.\nLa cr√©ation de l‚Äôenvironnement n√©cessaire √† la construction automatis√©e\nde ce site web a ainsi √©t√© divis√©e par 12 en utilisant mamba plut√¥t\nque conda pour installer des packages dans un environnement.‚Ü©Ô∏é"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Th√®mes en vrac",
    "section": "",
    "text": "Python pour la data science \n\n\nLino Galiana\n\nStar this website on Github\n\nCe site web rend public le contenu du cours de deuxi√®me ann√©e (Master 1) de l‚ÄôENSAE :\nPython pour la data science\n\nTout est pr√©sent sur ce site web ! Des Notebooks Jupyter peuvent √™tre r√©cup√©r√©s pour s‚Äôexercer. L‚Äôensemble des codes sources est stock√© sur Github\n\n\n\n\n # Image manquante ?\n\n\n\n\nPour d√©couvrir Python  de mani√®re th√©matique\n\n\n\n\n\n\n\n\n\n\nQuelques √©l√©ments pour comprendre les enjeux du NLP\n\n\n\nNLP\n\n\nTutoriel\n\n\n\nLes corpus textuels √©tant des objets de tr√®s grande dimension\no√π le ratio signal/bruit est faible, il est n√©cessaire de mettre\nen oeuvre une s√©rie d‚Äô√©tapes de nettoyage de‚Ä¶\n\n\n\nLino Galiana\n\n\nJul 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNettoyer un texte: des exercices pour d√©couvrir l‚Äôapproche bag-of-words\n\n\n\nNLP\n\n\nExercice\n\n\n\nCe chapitre continue de pr√©senter l‚Äôapproche de nettoyage de donn√©es\ndu NLP en s‚Äôappuyant sur le corpus de trois auteurs\nanglo-saxons : Mary Shelley, Edgar Allan Poe‚Ä¶\n\n\n\nLino Galiana\n\n\nOct 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLatent Dirichlet Allocation (LDA)\n\n\n\nTutoriel\n\n\nNLP\n\n\n\nLa Latent Dirichlet Allocation (LDA)\nest un mod√®le probabiliste g√©n√©ratif qui permet\nde d√©crire des‚Ä¶\n\n\n\nLino Galiana\n\n\nOct 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nM√©thodes de vectorisation : comptages et word embeddings\n\n\n\nTutoriel\n\n\nNLP\n\n\n\nPour pouvoir utiliser des donn√©es textuelles dans des algorithmes\nde machine learning, il faut les vectoriser, c‚Äôest √† dire transformer\nle texte en donn√©es num√©riques.‚Ä¶\n\n\n\nLino Galiana\n\n\nOct 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercices suppl√©mentaires\n\n\n\nExercice\n\n\nNLP\n\n\n\nDes exercices suppl√©mentaires pour pratiquer les concepts du NLP\n\n\n\nLino Galiana\n\n\nApr 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPartie 4 : Natural Language Processing (NLP)\n\n\n\nIntroduction\n\n\nNLP\n\n\n\nL‚Äôun des grands avantages comparatifs de Python par rapport aux\nlangages concurrents (R notamment) est dans\nla richesse des librairies de Traitement du Langage Naturel‚Ä¶\n\n\n\nLino Galiana\n\n\nOct 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCorrections\n\n\nNotebooks corrig√©s des diff√©rents chapitres du cours\n\n\n\nLino Galiana\n\n\nSep 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluation\n\n\nR√©sum√© des attentes pour les projets de fin d‚Äôann√©e\n\n\n\nLino Galiana\n\n\nSep 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfiguration de Python\n\n\n\nTutoriel\n\n\nRappels\n\n\n\nL‚Äôenvironnement que propose Python pour la data science\nest tr√®s riche. Afin de b√©n√©ficier du meilleur environnement\npour tirer parti du langage, ce chapitre‚Ä¶\n\n\n\nLino Galiana\n\n\nJul 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL‚Äôenvironnement Python pour la data science\n\n\n\nTutoriel\n\n\nRappels\n\n\n\nPython propose un √©cosyst√®me tr√®s riche pour la\ndata science. Ce chapitre fait un tour\nd‚Äôhorizon de celui-ci en pr√©sentant les principaux\npackages qui seront pr√©sent√©s‚Ä¶\n\n\n\nLino Galiana\n\n\nJul 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComment aborder un jeu de donn√©es\n\n\n\nTutoriel\n\n\nRappels\n\n\n\nQuelques √©l√©ments pour adopter une d√©marche\nscientifique et √©thique face √† un\njeu de donn√©es.\n\n\n\nLino Galiana\n\n\nJul 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBonne pratique de Python\n\n\n\nTutoriel\n\n\nRappels\n\n\n\nLes normes communautaires du monde de\nl‚Äôopen-source ont permis une\nharmonisation de la structure des projets\nPython et des scripts. Ce chapitre\n√©voque quelques-unes de‚Ä¶\n\n\n\nLino Galiana\n\n\nJul 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuelques rappels sur les principes de base de Python\n\n\n\nTutoriel\n\n\nRappels\n\n\n\nRappels d‚Äô√©l√©ments essentiels en Python: les r√®gles de syntaxes, les classes,\nles m√©thodes, etc.\n\n\n\nLino Galiana\n\n\nJul 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModules, tests, boucles, fonctions\n\n\n\nTutoriel\n\n\nRappels\n\n\n\nLes fonctions permettent de g√©n√©raliser des\ninstructions. Il s‚Äôagit ainsi d‚Äôun outil privil√©gi√©\npour automatiser des t√¢ches r√©p√©titives ou r√©duire\nla complexit√© d‚Äôune cha√Æne‚Ä¶\n\n\n\nLino Galiana\n\n\nJul 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLes classes en Python\n\n\n\nTutoriel\n\n\nRappels\n\n\n\nLa programmation orient√©e objet (POO) est\nl‚Äôun des atouts de Python. Elle permet\nd‚Äôadapter des‚Ä¶\n\n\n\nLino Galiana\n\n\nJul 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\nCette introduction propose quelques √©l√©ments de\nr√©vision des concepts de base en Python et\npr√©sente l‚Äô√©cosyst√®me Python que nous allons\nd√©couvrir tout au long de ce‚Ä¶\n\n\n\nLino Galiana\n\n\nJun 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUn cadavre exquis pour d√©couvrir Git\n\n\n\nExercice\n\n\nGit\n\n\n\nCe chapitre propose une mise en application de quelques principes\ncentraux du langage Git vus pr√©c√©demment.\n\n\n\nLino Galiana\n\n\nSep 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGit: un outil n√©cessaire pour les data scientists\n\n\n\nGit\n\n\n\nUne partie annexe au cours pour d√©couvrir Git,\nun outil\ndevenu indispensable pour les data scientists\nafin de mener des projets impliquant\ndu code Python.\n\n\n\nLino Galiana\n\n\nJul 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGit : un √©l√©ment essentiel au quotidien\n\n\n\nTutoriel\n\n\nGit\n\n\n\nGit est un syst√®me de contr√¥le de version qui facilite la\nsauvegarde, la gestion des √©volutions et le partage\nd‚Äôun projet informatique. Il s‚Äôagit d‚Äôun √©l√©ment‚Ä¶\n\n\n\nLino Galiana\n\n\nSep 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumpy, la brique de base de la data science\n\n\n\nTutoriel\n\n\nManipulation\n\n\n\nNumpy constitue la brique de base de l‚Äô√©cosyst√®me de la data science en\nPython. Toutes les librairies de manipulation de donn√©es, de mod√©lisation\net de visualisation‚Ä¶\n\n\n\nLino Galiana\n\n\nJul 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction √† Pandas\n\n\n\nTutoriel\n\n\nManipulation\n\n\n\nPandas est l‚Äô√©l√©ment central de l‚Äô√©cosyst√®me Python pour la data science.\nLe succ√®s r√©cent de Python dans l‚Äôanalyse de donn√©es tient beaucoup √† Pandas qui a permis‚Ä¶\n\n\n\nLino Galiana\n\n\nJul 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPratique de pandas : un exemple complet\n\n\n\nManipulation\n\n\nExercice\n\n\n\nApr√®s avoir pr√©sent√© la logique de Pandas dans le chapitre pr√©c√©dent,\nce chapitre vise √† illustrer les fonctionalit√©s du package\n√† partir de donn√©es d‚Äô√©missions de gaz √†‚Ä¶\n\n\n\nLino Galiana\n\n\nJul 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPratique de geopandas avec les donn√©es v√©lib\n\n\n\nManipulation\n\n\nExercice\n\n\n\nCe chapitre illustre les fonctionalit√©s de GeoPandas √† partir des\nd√©comptes de v√©lo fournis par la ville de Paris\nen‚Ä¶\n\n\n\nLino Galiana\n\n\nJul 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDonn√©es spatiales : d√©couverte de geopandas\n\n\n\nTutoriel\n\n\nManipulation\n\n\n\nLes donn√©es g√©olocalis√©es se sont multipli√©es depuis quelques ann√©es, qu‚Äôil\ns‚Äôagisse de donn√©es open-data ou de traces num√©riques g√©olocalis√©es de\ntype big-data. Pour les‚Ä¶\n\n\n\nLino Galiana\n\n\nJul 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeb scraping avec Python\n\n\n\nExercice\n\n\nManipulation\n\n\n\nPython permet de facilement r√©cup√©rer une page web pour en extraire des\ndonn√©es √† restructurer. Le web scraping, que les Canadiens nomment\n‚Äúmoissonnage du web‚Äù, est‚Ä¶\n\n\n\nLino Galiana\n\n\nSep 8, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMa√Ætriser les expressions r√©guli√®res\n\n\n\nTutoriel\n\n\nManipulation\n\n\n\nLes expressions r√©guli√®res fournissent un cadre tr√®s pratique pour manipuler\nde mani√®re flexible des donn√©es textuelles. Elles sont tr√®s utiles\nnotamment pour les t√¢ches de‚Ä¶\n\n\n\nLino Galiana\n\n\nJul 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR√©cup√©rer des donn√©es avec des API depuis Python\n\n\n\nExercice\n\n\nManipulation\n\n\n\nLes API (Application Programming Interface) sont un mode d‚Äôacc√®s aux\ndonn√©es en expansion. Gr√¢ce aux API, l‚Äôautomatisation de scripts\nest facilit√©e puisqu‚Äôil n‚Äôest‚Ä¶\n\n\n\nLino Galiana\n\n\nJul 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction √† dask gr√¢ce aux donn√©es DVF\n\n\n\nTutoriel\n\n\nManipulation\n\n\n\n\n\n\n\nLino Galiana\n\n\nAug 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPartie 1: manipuler des donn√©es\n\n\n\nManipulation\n\n\nIntroduction\n\n\n\nPython s‚Äôest impos√© comme une alternative tr√®s cr√©dible √† R dans\nla manipulation de donn√©es. L‚Äô√©cosyst√®me Pandas a permis de d√©mocratiser\nl‚Äôutilisation des DataFrames‚Ä¶\n\n\n\nLino Galiana\n\n\nJul 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPr√©paration des donn√©es pour construire un mod√®le\n\n\n\nMod√©lisation\n\n\nExercice\n\n\n\nAfin d‚Äôavoir des donn√©es coh√©rentes avec les hypoth√®ses de mod√©lisation,\nil est absolument fondamental de prendre le temps de\npr√©parer les donn√©es √† fournir √† un mod√®le. La‚Ä¶\n\n\n\nLino Galiana\n\n\nJul 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluer la qualit√© d‚Äôun mod√®le\n\n\n\nMod√©lisation\n\n\nExercice\n\n\n\nFaire preuve de m√©thode pour √©valuer la qualit√© d‚Äôun mod√®le\npermet de proposer des pr√©dictions plus robustes, ayant\nde meilleures performances sur un nouveau jeu de‚Ä¶\n\n\n\nLino Galiana\n\n\nJul 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification: premier mod√®le avec les SVM\n\n\n\nMod√©lisation\n\n\nExercice\n\n\n\nLa classification permet d‚Äôattribuer une classe d‚Äôappartenance (label\ndans la terminologie du machine learning)\ndiscr√®te √† des donn√©es √† partir de certaines variables‚Ä¶\n\n\n\nLino Galiana\n\n\nJul 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR√©gression : une introduction\n\n\n\nMod√©lisation\n\n\nExercice\n\n\n\nLa r√©gression lin√©aire est la premi√®re mod√©lisation statistique\nqu‚Äôon d√©couvre dans un cursus quantitatif. Il s‚Äôagit en effet d‚Äôune\nm√©thode tr√®s intuitive et tr√®s riche. Le‚Ä¶\n\n\n\nLino Galiana\n\n\nJul 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nS√©lection de variables : une introduction\n\n\n\nMod√©lisation\n\n\nExercice\n\n\n\nL‚Äôacc√®s √† des bases de donn√©es de plus en plus riches permet\ndes mod√©lisations de plus en plus raffin√©es. Cependant,\nles mod√®les parcimonieux sont g√©n√©ralement‚Ä¶\n\n\n\nLino Galiana\n\n\nJul 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustering\n\n\n\nMod√©lisation\n\n\nExercice\n\n\n\nLe clustering consiste √† r√©partir des observations dans des groupes,\ng√©n√©ralement non observ√©s,\nen fonction de caract√©ristiques observables. Il s‚Äôagit d‚Äôune\napplication‚Ä¶\n\n\n\nLino Galiana\n\n\nJul 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPremier pas vers l‚Äôindustrialisation avec les pipelines scikit\n\n\n\nMod√©lisation\n\n\nTutoriel\n\n\n\nLes pipelines scikit permettent d‚Äôint√©grer de mani√®re tr√®s flexible\nun ensemble d‚Äôop√©rations de pre-processing et d‚Äôentra√Ænement de mod√®les\ndans une cha√Æne d‚Äôop√©rations.‚Ä¶\n\n\n\nLino Galiana\n\n\nOct 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPartie 3: mod√©liser\n\n\n\nIntroduction\n\n\nMod√©lisation\n\n\n\nLa facilit√© √† mod√©liser des processus tr√®s diverses a grandement\nparticip√© au succ√®s de Python. La librairie scikit offre une\ngrande vari√©t√© de mod√®les et permet ainsi‚Ä¶\n\n\n\nLino Galiana\n\n\nOct 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInt√©gration continue avec Python\n\n\nUn chapitre plus avanc√© sur l‚Äôint√©gration continue\n\n\n\nLino Galiana\n\n\nJul 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nG√©n√©ration d‚Äôimages avec Python, DALL-E et StableDiffusion\n\n\n\nTutoriel\n\n\nAvanc√©\n\n\n\nLa hype autour du\nmod√®le de g√©n√©ration d‚Äôimage Dall-E a amen√©\nune grande attention sur les mod√®les\nautog√©n√©ratifs de contenu. Dall-E est, √† l‚Äôheure\nactuelle, le mod√®le‚Ä¶\n\n\n\nLino Galiana\n\n\nAug 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApprofondissement ElasticSearch pour des recherches de proximit√© g√©ographique\n\n\n\nTutoriel\n\n\nAvanc√©\n\n\n\nUn chapitre plus approfondi sur ElasticSearch\n\n\n\nLino Galiana\n\n\nSep 3, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction √† ElasticSearch pour la recherche textuelle\n\n\n\nTutoriel\n\n\nAvanc√©\n\n\n\nElasticSearch est un moteur de recherche extr√™mement rapide et flexible.\nCette technologie s‚Äôest impos√©e dans le domaine du traitement des\ndonn√©es textuelles. L‚ÄôAPI‚Ä¶\n\n\n\nLino Galiana\n\n\nSep 3, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPartie 5: Introduction aux outils et m√©thodes √† l‚Äô√©tat de l‚Äôart\n\n\n\nIntroduction\n\n\nAvanc√©\n\n\n\nApr√®s avoir abord√© les diff√©rents champs de la\ndata science, nous pouvons maintenant\nintroduire √† quelques outils et m√©thodes plus avanc√©s\nqui correspondent √† des aspects‚Ä¶\n\n\n\nLino Galiana\n\n\nOct 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLes nouveaux modes d‚Äôacc√®s aux donn√©es : le format parquet et les donn√©es sur le cloud\n\n\n\nTutoriel\n\n\nAvanc√©\n\n\n\n\n\n\n\nLino Galiana\n\n\nAug 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPartie 2: visualiser les donn√©es\n\n\n\nIntroduction\n\n\nVisualisation\n\n\n\nCette partie pr√©sente les outils pour visualiser des\ndonn√©es avec Python, qu‚Äôil s‚Äôagisse de graphiques\nfig√©s (matplotlib, seaborn, geoplot‚Ä¶) ou de\nvisualisation‚Ä¶\n\n\n\nLino Galiana\n\n\nJul 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDe belles cartes avec python : mise en pratique\n\n\n\nVisualisation\n\n\nExercice\n\n\n\nLa cartographie est un excellent moyen de diffuser\nune connaissance, y compris √† des publics peu\nfamiliers de la statistique. Ce chapitre permet\nde d√©couvrir la mani√®re dont‚Ä¶\n\n\n\nLino Galiana\n\n\nJul 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDe beaux graphiques avec python : mise en pratique\n\n\n\nVisualisation\n\n\nExercice\n\n\n\nUne partie essentielle du travail du\ndata scientist est d‚Äô√™tre en mesure\nde synth√©tiser une information dans des\nrepr√©sentations graphiques percutantes. Ce\nchapitre permet‚Ä¶\n\n\n\nLino Galiana\n\n\nJul 14, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n Back to topCitationBibTeX citation:@book{galiana2023,\n  author = {Galiana, Lino},\n  title = {Python Pour La Data Science},\n  date = {2023},\n  url = {https://pythonds.linogaliana.fr/},\n  doi = {10.5281/zenodo.8229676},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGaliana, Lino. 2023. Python Pour La Data Science. https://doi.org/10.5281/zenodo.8229676."
  },
  {
    "objectID": "content/getting-started/index.html",
    "href": "content/getting-started/index.html",
    "title": "Introduction",
    "section": "",
    "text": "Ce cours rassemble l‚Äôensemble du contenu du cours\nPython  pour la data science que je donne\n√† l‚ÄôENSAE\ndepuis 2018.\nCe cours √©tait auparavant donn√© par Xavier Dupr√©.\nQuelques √©l√©ments suppl√©mentaires sont disponibles dans\nles slides d‚Äôintroduction.\nDes √©l√©ments plus avanc√©s sont pr√©sents dans un autre cours consacr√©\n√† la mise en production de projets data science\nque je donne avec Romain Avouac\n√† l‚ÄôENSAE (ensae-reproductibilite.github.io/website)\nPython est un langage qui a d√©j√† plus de trente ans\nmais qui a connu, au cours de la d√©cennie 2010, une\nnouvelle jeunesse du fait de l‚Äôengouement pour\nla data science.\nPython, plus que tout autre\nlangage informatique, r√©unit des communaut√©s aussi\ndiverses que des statisticiens, des d√©veloppeurs,\ndes gestionnaires\nd‚Äôapplications ou d‚Äôinfrastructures informatiques,\ndes lyc√©es - Python est au programme du bac fran√ßais\ndepuis quelques ann√©es - ou des chercheurs\ndans des champs √† la fois th√©oriques et appliqu√©s. Contrairement\n√† beaucoup de langages informatiques qui f√©d√®rent\nune communaut√© assez homog√®ne, Python est parvenu √† r√©unir\nlargement gr√¢ce √† quelques principes centraux : la lisibilit√©\ndu langage, la simplicit√© √† utiliser des modules,\nla simplicit√© √† l‚Äôassocier √† des langages plus performants\npour certaines t√¢ches donn√©es, l‚Äô√©norme volume de documentation\ndisponible en ligne‚Ä¶\n√ätre le deuxi√®me meilleur langage pour r√©aliser telle ou telle\nt√¢che\npeut ainsi √™tre une source de succ√®s lorsque la concurrence ne dispose\npas d‚Äôun √©ventail aussi large d‚Äôavantages.\nLe succ√®s de Python, de par sa nature de\nlangage couteau-suisse, est indissociable\nde l‚Äô√©mergence du profil du data scientist, individu\ncapable de s‚Äôint√©grer √† diff√©rents niveaux dans la valorisation\nde donn√©es.\nDavenport and Patil (2012a), dans la Harvard Business Review,\nont ainsi pu parler du ‚Äúboulot le plus sexy du 21e si√®cle‚Äù\net ont pu, dix ans plus tard, faire un panorama complet de l‚Äô√©volution\ndes comp√©tences attendues d‚Äôun data scientist dans\nla m√™me revue (Davenport and Patil 2012b).\nLa richesse de Python permet de l‚Äôutiliser dans toutes les phases\ndu traitement de la donn√©e, de sa r√©cup√©ration et structuration √† partir de\nsources diverses √† sa valorisation.\nPar le prisme de la data science, nous verrons que Python est\nun tr√®s bon candidat pour assister les data scientists dans tous\nles aspects du travail de donn√©es.\nCe cours introduit diff√©rents outils qui permettent de mettre en relation\ndes donn√©es et des th√©ories gr√¢ce √† Python. N√©anmoins, ce cours\nva au-del√† d‚Äôune simple introduction au langage et propose\ndes √©l√©ments plus approfondis, notamment sur les derni√®res\ninnovations permises par la data science dans les m√©thodes de travail.\n\n\nLe succ√®s de scikit-learn et\nde Tensorflow dans la communaut√©\nde la Data-Science ont beaucoup contribu√© √† l‚Äôadoption de Python. Cependant,\nr√©sumer Python √† ces quelques librairies serait r√©ducteur tant il s‚Äôagit\nd‚Äôun v√©ritable couteau-suisse pour les data scientists,\nles social scientists ou les √©conomistes.\nL‚Äôint√©r√™t de Python pour un data scientist ou data economist\nva au-del√† du champ du Machine Learning.\nComme pour R, l‚Äôint√©r√™t de Python est son r√¥le central dans un\n√©cosyst√®me plus large autour d‚Äôoutils puissants, flexibles et open-source.\nPython concurrence tr√®s bien R dans son domaine de pr√©dilection, √†\nsavoir l‚Äôanalyse statistique sur des bases de donn√©es structur√©es.\nComme dans R, les dataframes sont un concept central de Python.\nPython est n√©anmoins bien plus complet dans certains domaines.\nOutre le Machine Learning,\nPython est mieux adapt√© aux donn√©es volumineuses que\nR. Python est √©galement meilleur que R pour faire\ndu webscraping ou acc√©der √† des donn√©es par le biais d‚ÄôAPI.\nDans le domaine de l‚Äô√©conom√©trie, Python offre\nl‚Äôavantage de la simplicit√© avec un nombre restreint de packages (scikit et\nstatsmodels) permettant d‚Äôavoir des mod√®les tr√®s g√©n√©raux\n(les generalized estimating equations)\nalors qu‚Äôil faut\nchoisir parmi une grande vari√©t√© de packages en R pour obtenir les\nmod√®les √©quivalents. Dans le domaine du Deep Learning, Python √©crase\nla concurrence.\nAu contraire, dans certains domaines, R reste meilleur, m√™me si les\n√©volutions tr√®s r√©centes de certains outils peuvent amener √† r√©viser\nce constat. Historiquement,\nR √©tait tr√®s bien int√©gr√© au langage de publication Markdown ce qui,\npermet la construction de documents reproductibles tr√®s raffin√©s.\nL‚Äô√©mergence r√©cente de Quarto, h√©ritier de R Markdown d√©velopp√© par\nla soci√©t√© Posit permet aux utilisateur de Python de b√©n√©ficier\n√©galement de la richesse de cette approche pour leur langage de pr√©dilection.\nCe site web, √† l‚Äôarborescence relativement complexe, est ainsi\nconstruit gr√¢ce √† cet outil qui permet √† la fois de tester les blocs\nde code pr√©sent√©s mais aussi de produire de mani√®re automatis√©e les\ntableaux et graphiques pr√©sent√©s. S‚Äôil fallait trouver un point faible\n√† Python par rapport √† R dans le domaine de la data science\nc‚Äôest sur la production de graphiques. matplotlib et seaborn, qui sont\npr√©sent√©s dans la partie visualisation, sont d‚Äôexcellents outils. N√©anmoins,\nggplot2, l‚Äô√©quivalent en R est plus facile de prise en main et\npropose une syntaxe extr√™mement flexible, qu‚Äôil est difficile de ne pas\nappr√©cier. Cependant, l‚Äô√©cosyst√®me de la\nvisualisation de donn√©es est en pleine r√©volution avec le succ√®s\nd‚ÄôObservable qui\nrapproche l‚Äô√©cosyst√®me JavaScript des d√©veloppeurs web\nde la communaut√© des analystes de donn√©es.\nUn des avantages comparatifs de Python par rapport √† d‚Äôautres\nlangages (notamment R et Julia) est sa dynamique,\nce que montre l‚Äôexplosion du nombre de questions\nsur Stack Overflow.\nCependant, il ne s‚Äôagit pas b√™tement d‚Äôenterrer R.\nAu contraire, outre leur logique tr√®s proche,\nles deux langages sont dans une phase de convergence avec des initiatives comme\nreticulate,\nquarto ou\nsnakemake qui\npermettent, de mani√®re diff√©rente, de cr√©er des cha√Ænes de traitement\nm√©langeant R et Python.\nUne autre raison pour laquelle cette gu√©guerre R/Python n‚Äôa pas\nde sens est que les bonnes\npratiques peuvent √™tre transpos√©es de mani√®re presque transparente d‚Äôun\nlangage √† l‚Äôautre. Il s‚Äôagit d‚Äôun point qui est d√©velopp√© plus amplement\ndans le cours plus avanc√© que je donne avec Romain Avouac en derni√®re ann√©e\nd‚ÄôENSAE : ensae-reproductibilite.github.io/website.\nA terme, les data scientists et chercheurs en sciences sociales ou\n√©conomie utiliseront\nde mani√®re presque indiff√©rente, et en alternance, Python et R. Ce cours\npr√©sentera ainsi r√©guli√®rement des analogies avec R pour aider les\npersonnes d√©couvrant Python, mais connaissant d√©j√† bien R, √†\nmieux comprendre certains messages.\n\n\n\nLe but de ce cours est de rendre autonome sur\nl‚Äôutilisation de Python\ndans un contexte de travail de data scientist ou de\nsocial scientist (√©conomie, sociologie, g√©ographie‚Ä¶).\nAutrement dit,\nil pr√©suppose qu‚Äôon d√©sire faire un usage intense\nde donn√©es dans un cadre statistique rigoureux.\nLa data science est un ensemble de techniques\nvisant √† donner du sens √† des sources de donn√©es\ndiverses. Selon les organisations,\nles data scientists peuvent ainsi √™tre √†\nl‚Äôinterface de projets n√©cessitant un\nlarge spectre de comp√©tences\n(analyse\nde donn√©es textuelles, repr√©sentation\ngraphique interactive‚Ä¶),\navoir des interactions avec des profils\ntr√®s diff√©rents (experts m√©tiers,\nd√©veloppeurs, data architect,\ndata engineer‚Ä¶) voire adopter\nun peu tous ces r√¥les.\nLes innovations\nr√©centes de la data science ne se r√©duisent\nn√©anmoins\npas qu‚Äô√† des d√©couvertes m√©thodologiques.\nLa data science propose un ensemble de\ntechniques et de m√©thodes de travail\npour r√©duire les co√ªts de passage\nd‚Äôun protype √† une chaine\nde production p√©renne.\nCe cours introduit √† quelques notions\nsur le sujet, notamment les\npipelines scikit, pour adopter\nd√®s l‚Äôapprentissage du langage\nquelques bons r√©flexes (ensae-reproductibilite.github.io/website).\n\n\n\nCe cours ne revient que de mani√®re secondaire\nsur les fondements statistiques ou algorithmiques\nderri√®re certaines des techniques √©voqu√©es.\nNe pas conna√Ætre ces notions n‚Äôemp√™che n√©anmoins pas de comprendre\nle contenu de ce site web. En effet, la facilit√© d‚Äôusage de Python\n√©vite de devoir programmer soi-m√™me un mod√®le, ce qui rend\npossible l‚Äôapplication\nde mod√®les dont on n‚Äôest pas expert. La connaissance des mod√®les sera\nplut√¥t n√©cessaire dans l‚Äôinterpr√©tation des r√©sultats.\nCependant, la facilit√© avec laquelle il est possible de construire des mod√®les complexes\navec Python peut laisser appara√Ætre que conna√Ætre les sp√©cifit√©s de chaque\nmod√®le est inutile. Il\ns‚Äôagirait d‚Äôune grave erreur : m√™me si l‚Äôimpl√©mentation de mod√®les est ais√©e, il\nest n√©cessaire de bien comprendre la structure des donn√©es et leur ad√©quation\navec les hypoth√®ses d‚Äôun mod√®le.\n\n\n\nCe cours donne une place centrale √†\nla notion de reproductibilit√©. Cette exigence se traduit de diverses\nmani√®res dans cet enseignement, en particulier en insistant sur un\noutil indispensable pour favoriser le partage de codes informatiques,\n√† savoir Git.\nL‚Äôensemble du contenu du site web est reproductible dans des environnements\ninformatiques divers. Il est bien s√ªr possible de copier-coller les morceaux\nde code pr√©sents dans ce site. Cette m√©thode montrant rapidement ses limites,\nle site pr√©sente un certain nombre de boutons disponibles pour\nouvrir la page sous un format Jupyter Notebook sur divers\npages web:\n\nSur l‚Äôensemble du site web,\nil est possible de cliquer\nsur la petite icone \npour √™tre redirig√© vers le d√©p√¥t Github associ√© √† ce cours.\nUn certain nombre de boutons permettent de transformer chaque\npage web en Jupyter Notebooks s‚Äôil est n√©cessaire de\nvisualiser ou ex√©cuter du code Python.\n\nVoici, par exemple, ces boutons pour le tutoriel numpy :\n\n\n\n\n\n\n\n\n\n\n\nPour les agents de la fonction publique, ou\nles √©l√®ves des √©coles partenaires, il est recommand√©\nde privil√©gier le bouton SSPCloud qui est\nune infrastructure cloud moderne, puissante et flexible\nd√©velopp√©e par l‚ÄôInsee et accessible √† l‚Äôurl\nhttps://datalab.sspcloud.fr1.\nL‚Äôensemble du contenu de ce site s‚Äôappuie sur des donn√©es\nouvertes, qu‚Äôil s‚Äôagisse de donn√©es fran√ßaises (principalement\nissues de la plateforme\ncentralisatrice data.gouv ou du site\nweb de l‚ÄôInsee) ou de donn√©es\nam√©ricaines. Les r√©sultats sont donc reproductibles pour quelqu‚Äôun\ndisposant d‚Äôun environnement identique.\n\n\n\nCe cours pr√©sente\ndes tutoriels et des exercices complets.\nChaque page est structur√©e sous la forme\nd‚Äôun probl√®me concret et pr√©sente la\nd√©marche g√©n√©rique pour r√©soudre ce probl√®me g√©n√©ral.\nVous pouvez naviguer dans l‚Äôarchitecture du site via la table des mati√®res\nou par les liens vers le contenu ant√©rieur ou post√©rieur √† la fin de chaque\npage. Certaines parties, notamment celle consacr√©e √† la mod√©lisation,\nproposent des exemples fil-rouge pour illustrer la d√©marche de mani√®re\nplus extensive.\n\n\n\nLes √©l√®ves de l‚ÄôENSAE valident le cours gr√¢ce √†\nun projet approfondi.\nLes √©l√©ments relatifs √† l‚Äô√©valuation du cours, ainsi qu‚Äôune\nliste des projets d√©j√† effectu√©s, sont disponibles dans la\nSection Evaluation.\n\n\n\n\n\nDavenport, Thomas H, and DJ Patil. 2012a. ‚ÄúData Scientist, the Sexiest Job of the 21st Century.‚Äù Harvard Business Review 90 (5): 70‚Äì76. https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century.\n\n\n‚Äî‚Äî‚Äî. 2012b. ‚ÄúIs Data Scientist Still the Sexiest Job of the 21st Century?‚Äù Harvard Business Review 90 (5): 70‚Äì76. https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century."
  },
  {
    "objectID": "content/getting-started/index.html#pourquoi-faire-du-python-pour-lanalyse-de-donn√©es",
    "href": "content/getting-started/index.html#pourquoi-faire-du-python-pour-lanalyse-de-donn√©es",
    "title": "Introduction",
    "section": "",
    "text": "Le succ√®s de scikit-learn et\nde Tensorflow dans la communaut√©\nde la Data-Science ont beaucoup contribu√© √† l‚Äôadoption de Python. Cependant,\nr√©sumer Python √† ces quelques librairies serait r√©ducteur tant il s‚Äôagit\nd‚Äôun v√©ritable couteau-suisse pour les data scientists,\nles social scientists ou les √©conomistes.\nL‚Äôint√©r√™t de Python pour un data scientist ou data economist\nva au-del√† du champ du Machine Learning.\nComme pour R, l‚Äôint√©r√™t de Python est son r√¥le central dans un\n√©cosyst√®me plus large autour d‚Äôoutils puissants, flexibles et open-source.\nPython concurrence tr√®s bien R dans son domaine de pr√©dilection, √†\nsavoir l‚Äôanalyse statistique sur des bases de donn√©es structur√©es.\nComme dans R, les dataframes sont un concept central de Python.\nPython est n√©anmoins bien plus complet dans certains domaines.\nOutre le Machine Learning,\nPython est mieux adapt√© aux donn√©es volumineuses que\nR. Python est √©galement meilleur que R pour faire\ndu webscraping ou acc√©der √† des donn√©es par le biais d‚ÄôAPI.\nDans le domaine de l‚Äô√©conom√©trie, Python offre\nl‚Äôavantage de la simplicit√© avec un nombre restreint de packages (scikit et\nstatsmodels) permettant d‚Äôavoir des mod√®les tr√®s g√©n√©raux\n(les generalized estimating equations)\nalors qu‚Äôil faut\nchoisir parmi une grande vari√©t√© de packages en R pour obtenir les\nmod√®les √©quivalents. Dans le domaine du Deep Learning, Python √©crase\nla concurrence.\nAu contraire, dans certains domaines, R reste meilleur, m√™me si les\n√©volutions tr√®s r√©centes de certains outils peuvent amener √† r√©viser\nce constat. Historiquement,\nR √©tait tr√®s bien int√©gr√© au langage de publication Markdown ce qui,\npermet la construction de documents reproductibles tr√®s raffin√©s.\nL‚Äô√©mergence r√©cente de Quarto, h√©ritier de R Markdown d√©velopp√© par\nla soci√©t√© Posit permet aux utilisateur de Python de b√©n√©ficier\n√©galement de la richesse de cette approche pour leur langage de pr√©dilection.\nCe site web, √† l‚Äôarborescence relativement complexe, est ainsi\nconstruit gr√¢ce √† cet outil qui permet √† la fois de tester les blocs\nde code pr√©sent√©s mais aussi de produire de mani√®re automatis√©e les\ntableaux et graphiques pr√©sent√©s. S‚Äôil fallait trouver un point faible\n√† Python par rapport √† R dans le domaine de la data science\nc‚Äôest sur la production de graphiques. matplotlib et seaborn, qui sont\npr√©sent√©s dans la partie visualisation, sont d‚Äôexcellents outils. N√©anmoins,\nggplot2, l‚Äô√©quivalent en R est plus facile de prise en main et\npropose une syntaxe extr√™mement flexible, qu‚Äôil est difficile de ne pas\nappr√©cier. Cependant, l‚Äô√©cosyst√®me de la\nvisualisation de donn√©es est en pleine r√©volution avec le succ√®s\nd‚ÄôObservable qui\nrapproche l‚Äô√©cosyst√®me JavaScript des d√©veloppeurs web\nde la communaut√© des analystes de donn√©es.\nUn des avantages comparatifs de Python par rapport √† d‚Äôautres\nlangages (notamment R et Julia) est sa dynamique,\nce que montre l‚Äôexplosion du nombre de questions\nsur Stack Overflow.\nCependant, il ne s‚Äôagit pas b√™tement d‚Äôenterrer R.\nAu contraire, outre leur logique tr√®s proche,\nles deux langages sont dans une phase de convergence avec des initiatives comme\nreticulate,\nquarto ou\nsnakemake qui\npermettent, de mani√®re diff√©rente, de cr√©er des cha√Ænes de traitement\nm√©langeant R et Python.\nUne autre raison pour laquelle cette gu√©guerre R/Python n‚Äôa pas\nde sens est que les bonnes\npratiques peuvent √™tre transpos√©es de mani√®re presque transparente d‚Äôun\nlangage √† l‚Äôautre. Il s‚Äôagit d‚Äôun point qui est d√©velopp√© plus amplement\ndans le cours plus avanc√© que je donne avec Romain Avouac en derni√®re ann√©e\nd‚ÄôENSAE : ensae-reproductibilite.github.io/website.\nA terme, les data scientists et chercheurs en sciences sociales ou\n√©conomie utiliseront\nde mani√®re presque indiff√©rente, et en alternance, Python et R. Ce cours\npr√©sentera ainsi r√©guli√®rement des analogies avec R pour aider les\npersonnes d√©couvrant Python, mais connaissant d√©j√† bien R, √†\nmieux comprendre certains messages."
  },
  {
    "objectID": "content/getting-started/index.html#objectif-du-cours",
    "href": "content/getting-started/index.html#objectif-du-cours",
    "title": "Introduction",
    "section": "",
    "text": "Le but de ce cours est de rendre autonome sur\nl‚Äôutilisation de Python\ndans un contexte de travail de data scientist ou de\nsocial scientist (√©conomie, sociologie, g√©ographie‚Ä¶).\nAutrement dit,\nil pr√©suppose qu‚Äôon d√©sire faire un usage intense\nde donn√©es dans un cadre statistique rigoureux.\nLa data science est un ensemble de techniques\nvisant √† donner du sens √† des sources de donn√©es\ndiverses. Selon les organisations,\nles data scientists peuvent ainsi √™tre √†\nl‚Äôinterface de projets n√©cessitant un\nlarge spectre de comp√©tences\n(analyse\nde donn√©es textuelles, repr√©sentation\ngraphique interactive‚Ä¶),\navoir des interactions avec des profils\ntr√®s diff√©rents (experts m√©tiers,\nd√©veloppeurs, data architect,\ndata engineer‚Ä¶) voire adopter\nun peu tous ces r√¥les.\nLes innovations\nr√©centes de la data science ne se r√©duisent\nn√©anmoins\npas qu‚Äô√† des d√©couvertes m√©thodologiques.\nLa data science propose un ensemble de\ntechniques et de m√©thodes de travail\npour r√©duire les co√ªts de passage\nd‚Äôun protype √† une chaine\nde production p√©renne.\nCe cours introduit √† quelques notions\nsur le sujet, notamment les\npipelines scikit, pour adopter\nd√®s l‚Äôapprentissage du langage\nquelques bons r√©flexes (ensae-reproductibilite.github.io/website)."
  },
  {
    "objectID": "content/getting-started/index.html#public-cible",
    "href": "content/getting-started/index.html#public-cible",
    "title": "Introduction",
    "section": "",
    "text": "Ce cours ne revient que de mani√®re secondaire\nsur les fondements statistiques ou algorithmiques\nderri√®re certaines des techniques √©voqu√©es.\nNe pas conna√Ætre ces notions n‚Äôemp√™che n√©anmoins pas de comprendre\nle contenu de ce site web. En effet, la facilit√© d‚Äôusage de Python\n√©vite de devoir programmer soi-m√™me un mod√®le, ce qui rend\npossible l‚Äôapplication\nde mod√®les dont on n‚Äôest pas expert. La connaissance des mod√®les sera\nplut√¥t n√©cessaire dans l‚Äôinterpr√©tation des r√©sultats.\nCependant, la facilit√© avec laquelle il est possible de construire des mod√®les complexes\navec Python peut laisser appara√Ætre que conna√Ætre les sp√©cifit√©s de chaque\nmod√®le est inutile. Il\ns‚Äôagirait d‚Äôune grave erreur : m√™me si l‚Äôimpl√©mentation de mod√®les est ais√©e, il\nest n√©cessaire de bien comprendre la structure des donn√©es et leur ad√©quation\navec les hypoth√®ses d‚Äôun mod√®le."
  },
  {
    "objectID": "content/getting-started/index.html#reproductibilit√©",
    "href": "content/getting-started/index.html#reproductibilit√©",
    "title": "Introduction",
    "section": "",
    "text": "Ce cours donne une place centrale √†\nla notion de reproductibilit√©. Cette exigence se traduit de diverses\nmani√®res dans cet enseignement, en particulier en insistant sur un\noutil indispensable pour favoriser le partage de codes informatiques,\n√† savoir Git.\nL‚Äôensemble du contenu du site web est reproductible dans des environnements\ninformatiques divers. Il est bien s√ªr possible de copier-coller les morceaux\nde code pr√©sents dans ce site. Cette m√©thode montrant rapidement ses limites,\nle site pr√©sente un certain nombre de boutons disponibles pour\nouvrir la page sous un format Jupyter Notebook sur divers\npages web:\n\nSur l‚Äôensemble du site web,\nil est possible de cliquer\nsur la petite icone \npour √™tre redirig√© vers le d√©p√¥t Github associ√© √† ce cours.\nUn certain nombre de boutons permettent de transformer chaque\npage web en Jupyter Notebooks s‚Äôil est n√©cessaire de\nvisualiser ou ex√©cuter du code Python.\n\nVoici, par exemple, ces boutons pour le tutoriel numpy :\n\n\n\n\n\n\n\n\n\n\n\nPour les agents de la fonction publique, ou\nles √©l√®ves des √©coles partenaires, il est recommand√©\nde privil√©gier le bouton SSPCloud qui est\nune infrastructure cloud moderne, puissante et flexible\nd√©velopp√©e par l‚ÄôInsee et accessible √† l‚Äôurl\nhttps://datalab.sspcloud.fr1.\nL‚Äôensemble du contenu de ce site s‚Äôappuie sur des donn√©es\nouvertes, qu‚Äôil s‚Äôagisse de donn√©es fran√ßaises (principalement\nissues de la plateforme\ncentralisatrice data.gouv ou du site\nweb de l‚ÄôInsee) ou de donn√©es\nam√©ricaines. Les r√©sultats sont donc reproductibles pour quelqu‚Äôun\ndisposant d‚Äôun environnement identique."
  },
  {
    "objectID": "content/getting-started/index.html#architecture-du-site-web",
    "href": "content/getting-started/index.html#architecture-du-site-web",
    "title": "Introduction",
    "section": "",
    "text": "Ce cours pr√©sente\ndes tutoriels et des exercices complets.\nChaque page est structur√©e sous la forme\nd‚Äôun probl√®me concret et pr√©sente la\nd√©marche g√©n√©rique pour r√©soudre ce probl√®me g√©n√©ral.\nVous pouvez naviguer dans l‚Äôarchitecture du site via la table des mati√®res\nou par les liens vers le contenu ant√©rieur ou post√©rieur √† la fin de chaque\npage. Certaines parties, notamment celle consacr√©e √† la mod√©lisation,\nproposent des exemples fil-rouge pour illustrer la d√©marche de mani√®re\nplus extensive."
  },
  {
    "objectID": "content/getting-started/index.html#evaluation",
    "href": "content/getting-started/index.html#evaluation",
    "title": "Introduction",
    "section": "",
    "text": "Les √©l√®ves de l‚ÄôENSAE valident le cours gr√¢ce √†\nun projet approfondi.\nLes √©l√©ments relatifs √† l‚Äô√©valuation du cours, ainsi qu‚Äôune\nliste des projets d√©j√† effectu√©s, sont disponibles dans la\nSection Evaluation."
  },
  {
    "objectID": "content/getting-started/index.html#r√©f√©rences",
    "href": "content/getting-started/index.html#r√©f√©rences",
    "title": "Introduction",
    "section": "",
    "text": "Davenport, Thomas H, and DJ Patil. 2012a. ‚ÄúData Scientist, the Sexiest Job of the 21st Century.‚Äù Harvard Business Review 90 (5): 70‚Äì76. https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century.\n\n\n‚Äî‚Äî‚Äî. 2012b. ‚ÄúIs Data Scientist Still the Sexiest Job of the 21st Century?‚Äù Harvard Business Review 90 (5): 70‚Äì76. https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century."
  },
  {
    "objectID": "content/getting-started/index.html#footnotes",
    "href": "content/getting-started/index.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPour les utilisateurs de cette infrastructure, les notebooks\nsont √©galement list√©s, parmi de nombreuses autres\nressources de qualit√©, sur la\npage Formation‚Ü©Ô∏é"
  },
  {
    "objectID": "content/getting-started/02_DS_environment.html",
    "href": "content/getting-started/02_DS_environment.html",
    "title": "L‚Äôenvironnement Python pour la data science",
    "section": "",
    "text": "La richesse des langages open-source est la possibilit√©\nd‚Äôutiliser des packages\nd√©velopp√©s par des sp√©cialistes. Python est particuli√®rement\nbien dot√© dans le domaine. Pour caricaturer, on lit parfois\nque Python est le deuxi√®me meilleur langage pour toutes les\nt√¢ches, ce qui en fait le meilleur langage.\nEn effet, la mall√©abilit√© de Python fait qu‚Äôon peut\nl‚Äôaborder de mani√®re tr√®s diff√©rentes\nselon que l‚Äôon est plut√¥t SysAdmin, d√©veloppeur web ou\ndata scientist. C‚Äôest ce dernier profil qui va ici nous\nint√©resser.\nLe data scientist devant disposer de nombreuses cordes\n√† son arc. Cela se refl√®te sur l‚Äô√©cosyst√®me de la data science\nqui est assez √©clat√©. Cependant, ce foisonnement\nn‚Äôest pas propre √† Python puisque R propose encore plus de\npackages que Python o√π un certain nombre de framework\nnormalis√©s limitent l‚Äô√©clatement de l‚Äô√©cosyst√®me. De plus,\nle foisonnement de l‚Äôenvironnement du data scientist\nest une v√©ritable opportunit√© puisqu‚Äôelle permet\naux packages de se sp√©cialiser dans un\ndomaine, o√π ils sont plus efficaces, et aux concepteurs\nde package d‚Äôoser mettre en oeuvre de nouvelles m√©thodes,\nindispensables pour que le langage suive les √©volutions\nrapides de la recherche ou de la technologie."
  },
  {
    "objectID": "content/getting-started/02_DS_environment.html#les-packages-python-essentiels-pour-le-cours-et-la-vie-des-data-scientists",
    "href": "content/getting-started/02_DS_environment.html#les-packages-python-essentiels-pour-le-cours-et-la-vie-des-data-scientists",
    "title": "L‚Äôenvironnement Python pour la data science",
    "section": "Les packages Python essentiels pour le cours et la vie des data scientists",
    "text": "Les packages Python essentiels pour le cours et la vie des data scientists\n\n\n\n\n\nCe\npost,\ndont l‚Äôimage ci-dessus est tir√©e, r√©sume la plupart des packages utiles\npour un data scientist ou un √©conomiste/sociologue. Nous nous bornerons\nici √† √©voquer ceux utilis√©s quotidiennement.\n\nnumpy\nnumpy g√®re tout ce qui est calcul matriciel.\nLe langage Python est un des langages les plus lents qui soient1.\nTous les calculs rapides ne sont pas √©crits en Python mais en C++, voire Fortran.\nC‚Äôest le cas du package numpy. Celui-ci est incontournable\nd√®s qu‚Äôon veut √™tre rapide. Le package\nscipy est une extension o√π l‚Äôon peut trouver\ndes fonctions statistiques, d‚Äôoptimisation.\nLa Cheat Sheet de numpy est pratique:\nhttps://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf\nComme numpy est la brique de base de l‚Äôanalyse de donn√©es, un chapitre\nde ce cours lui est consacr√©.\n\n\npandas\nAvant tout, un bon data scientist doit √™tre capable de\ns‚Äôapproprier et manipuler des donn√©es rapidement. Pour cette raison,\npandas est incontournable.\nIl g√®re la plupart des formats de donn√©es. Pour √™tre efficace,\nil est lui aussi impl√©ment√© en C++.\nLe package est rapide si on utilise les m√©thodes pr√©-impl√©ment√©es sur\ndes donn√©es d‚Äôune taille raisonnable (par rapport √† la RAM disponible). Il faut\nn√©anmoins s‚Äôen m√©fier avec des donn√©es volumineuses.\nEn r√®gle g√©n√©rale, un jeu de donn√©es n√©cessite\ntrois fois plus d‚Äôespace en m√©moire que les\ndonn√©es n‚Äôen prennent sur le disque.\nLa Cheat Sheet de pandas :\nhttps://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_Pandas_Cheat_Sheet_2.pdf\npandas √©tant un √©l√©ment incontournable, deux chapitres y sont consacr√©s.\n\n\nmatplotlib et seaborn\nmatplotlib existe depuis une vingtaine d‚Äôann√©es pour doter Python de\nfonctionalit√©s graphiques. Il s‚Äôagit d‚Äôun package tr√®s flexible, offrant\nde nombreuses fonctionalit√©s. N√©anmoins, ces derni√®res ann√©es,\nseaborn a √©merg√© pour simplifier la cr√©ation de certains graphiques\nstandards de l‚Äôanalyse de donn√©es (histogrammes, diagramme en barre, etc. ).\nLe succ√®s de seaborn n‚Äô√©clipse n√©anmoins pas matplotlib puisque ce\ndernier est souvent n√©cessaire pour finaliser la customisation d‚Äôun\ngraphique produit par seaborn2\n\n\nscikit-learn\nscikit-learn est le module de machine learning le plus populaire pour\ntrois raisons:\n\nil s‚Äôappuie sur une API extr√™mement consistante (m√©thodes fit, transform\net predict, respectivement pour apprendre des donn√©es, appliquer des transformations et pr√©dire sur de nouvelles donn√©es) ;\nil permet de construire\ndes analyses reproductibles en construisant des pipelines de donn√©es ;\nsa documentation est un mod√®le √† suivre.\n\nL‚ÄôINRIA, institution fran√ßaise, est l‚Äôun des √©l√©ments moteurs dans\nla cr√©ation et la maintenance de scikit-learn\n\n\nTensorFlow, PyTorch et Keras\nLes librairies essentielles pour impl√©menter et utiliser des mod√®les\nde deep learning en Python ont √©t√© d√©velopp√©es par des acteurs du\nnum√©rique.\nTensorFlow est la librairie la plus mature, mais pas n√©cessairement la plus facile √† prendre en main. D‚Äôailleurs, Google semble l‚Äôabandonner en usage interne pour lui\npr√©f√©rer JAX.\nKeras propose une interface high-level,\ndonc plus facile d‚Äôutilisation,\nmais qui n‚Äôen reste pas moins suffisante pour une grande vari√©t√© d‚Äôusages.\nLa documentation de Keras est tr√®s bien faite.\nPyTorch est un framework plus r√©cent mais tr√®s complet,\ndont la syntaxe plaira aux amateurs de programmation orient√©-objet.\nD√©velopp√© par Facebook,\nil est tr√®s utilis√© dans certains domaines de recherche, comme le NLP.\nIl s‚Äôagit du framework dont la dynamique r√©cente a √©t√© la plus\nascensionnelle.\n\n\nstatsmodels\nstatsmodels plaira plus aux statisticiens, il impl√©mente des mod√®les\n√©conom√©triques similaires √† scikit-learn.\nPar rapport √† scikit-learn,\nstatsmodels est plus orient√© √©conom√©trie. La pr√©sentation des\nr√©sultats est tr√®s proche de ce qu‚Äôon trouve en R.\n\n\nrequests et beautifulsoup\nrequests est l‚Äôune des librairies de base de Python, d√©di√©e\n√† g√©rer la connexion avec internet. Les amateurs d‚ÄôAPI\nseront des utilisateurs fr√©quents de celle-ci. Les\npersonnes plus sp√©cialistes de web scraping l‚Äôutiliseront avec\nbeautifulsoup qui offre une syntaxe extr√™mement puissante\npour r√©cup√©rer automatiquement du contenu de pages web.\n\n\nnltk et spaCy\nDans le domaine du traitement automis√© du langage, plus connu\nsous son acronyme anglais NLP, les deux packages phares sont\nnltk et spaCy.\nnltk est le package historique. Il existe depuis les ann√©es\n1990 et propose de nombreuses ressources utiles pour l‚Äôanalyse\ntextuelle. N√©anmoins, ces derni√®res ann√©es, spaCy est venu\nmoderniser l‚Äôapproche en proposant une approche permettant\nde mieux int√©grer les diff√©rentes phases du traitement de donn√©es\ntextuelles, une excellente documentation et un meilleur support\ndes langues non anglo-saxonnes, comme le Fran√ßais.\nMais Python est √©galement un outil privil√©gi√© pour communiquer:\n\nUne bonne int√©gration de Python √† Markdown (gr√¢ce notamment √† ‚Ä¶ R Markdown) qui facilite la construction de documents HTML ou PDF (via Latex)\nSphynx et JupyterBook proposent des mod√®les de documentation\ntr√®s complets\nbokeh ou streamlit comme alternative √† shiny (R)\nDjango et Flask permettent de construire des applications web en Python\nLes librairies dynamiques, notamment\nfolium ou\nplotly, sont tr√®s appr√©ci√©es pour construire des\nvisualisations dynamiques qui sont pratiques dans une analyse exploratoire\nmais √©galement lorsqu‚Äôil faut valoriser ses travaux aupr√®s de\npublics non experts de la donn√©e.\n\nL‚Äôun des nouveaux arrivants dans cet √©cosyst√®me d√©j√† riche\nest FastAPI). Avec ce package,\nil est tr√®s facile de transformer un code Python en API ce qui facilite\nla mise √† disposition de donn√©es mais aussi de productions par Python (comme\nla mise √† disposition d‚Äôune API pour permettre √† des personnes de tester\nles r√©sultats d‚Äôun mod√®le de machine learning).\nCe n‚Äôest qu‚Äôune petite partie de l‚Äô√©cosyst√®me Python, d‚Äôune richesse rare."
  },
  {
    "objectID": "content/getting-started/02_DS_environment.html#environnement-autour-de-python",
    "href": "content/getting-started/02_DS_environment.html#environnement-autour-de-python",
    "title": "L‚Äôenvironnement Python pour la data science",
    "section": "Environnement autour de Python",
    "text": "Environnement autour de Python\nPython est un langage tr√®s riche, gr√¢ce √† sa logique open-source. Mais l‚Äôun\ndes principaux int√©r√™ts r√©side dans le riche √©cosyst√®me avec lequel Python\ns‚Äôint√®gre. On peut donner quelques √©l√©ments, dans un inventaire √† la Pr√©vert non exaustif.\nEn premier lieu, des √©l√©ments reli√©s au traitement des donn√©es :\n\nSpark,\nle framework dominant dans le domaine du traitement des big-data, tr√®s bien\ninterfac√© avec Python (gr√¢ce √† l‚ÄôAPI pyspark), qui facilite le traitement des donn√©es volumineuses. Son utilisation n√©cessite cependant d‚Äôavoir acc√®s √† une\ninfrastructure de calculs distribu√©e.\nCython permet d‚Äôint√©grer facilement du code C, tr√®s\nefficace avec Python (√©quivalent de Rcpp pour R).\nJulia est un langage r√©cent, qui propose une syntaxe famili√®re aux utilisateurs de languages scientifiques (Python, R, MATLAB), tout en permettant des performances proches du C gr√¢ce √† une compilation √† la vol√©e.\n\nEnfin, des √©l√©ments permettant un d√©ploiement de r√©sultats ou d‚Äôapplications\nen continu :\n* Les images Docker de Jupyterhub facilitent l‚Äôusage de l‚Äôint√©gration continue\npour construire des modules, les tester et d√©ployer des site web.\n* Les services type Binder, Google Colab et Kaggle proposent des kernels\nPython"
  },
  {
    "objectID": "content/getting-started/02_DS_environment.html#rester-au-courant-des-√©volutions",
    "href": "content/getting-started/02_DS_environment.html#rester-au-courant-des-√©volutions",
    "title": "L‚Äôenvironnement Python pour la data science",
    "section": "Rester au courant des √©volutions",
    "text": "Rester au courant des √©volutions\nL‚Äô√©cosyst√®me riche et foisonnant de Python a comme contrepartie\nqu‚Äôil faut rester attentif √† ses √©volutions pour ne pas\nvoir son capital humain vieillir et ainsi devenir has-been.\nAlors qu‚Äôavec des langages\nmonolithiques comme\nSAS ou Stata on pouvait se permettre de ne faire de vieille technique\nmais seulement consulter la documentation officielle, avec Python\nou R c‚Äôest impossible. Ce cours lui-m√™me est en √©volution continue, ce\nqui est assez exigeant :sweating:, pour √©pouser les √©volutions\nde l‚Äô√©cosyst√®me.\nTwitter est une excellente source d‚Äôinformation pour √™tre rapidement\nau courant des √©volutions du monde de la data science. Les agr√©gateurs\nde contenu comme medium ou towardsdatascience proposent des posts\nde qualit√© h√©t√©rog√®ne mais il peut √™tre utile de recevoir par mail\nle feed des nouveaux posts : au bout d‚Äôun certain temps, cela peut\npermettre de d√©gager les nouvelles tendances. Le site\nrealpython propose g√©n√©ralement de tr√®s bon posts, complets et\np√©dagogiques.\nEn ce qui concerne les ouvrages papiers, certains sont de tr√®s bonne qualit√©.\nCependant, il convient de faire attention √† la date de mise √† jour de ceux-ci :\nla vitesse d‚Äô√©volution de certains √©l√©ments de l‚Äô√©cosyst√®me peut les\np√©rimer tr√®s rapidement."
  },
  {
    "objectID": "content/getting-started/02_DS_environment.html#footnotes",
    "href": "content/getting-started/02_DS_environment.html#footnotes",
    "title": "L‚Äôenvironnement Python pour la data science",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPython est un langage interpr√©t√©, comme R. Cela le rend tr√®s\nintelligible, y compris par un non-expert. C‚Äôest une des raisons de son\nsucc√®s. Le cr√©ateur de Python, Guido Van Rossum,\nen a fait un des principes philosophiques\n√† l‚Äôorigine de Python: un code est plus souvent lu qu‚Äô√©crit.\nLa contrepartie est qu‚Äôil s‚Äôagit d‚Äôune surcouche √† des langages\nplus bas-niveau, notamment C. Ces derniers proposent beaucoup moins de\nsurcouches. En r√©alit√©, les fonctions Python font appel, plus ou moins\ndirectement, √† du C. Une mani√®re d‚Äôoptimiser le code est ainsi d‚Äôarriver,\navec le moins de surcouches possible, √† la fonction C sous-jacente,\nbeaucoup plus rapide.‚Ü©Ô∏é\nLa situation est diff√©rente en R o√π ggplot2 a quasiment √©clips√©\nl‚Äôoutil de graphique de base de R.‚Ü©Ô∏é"
  },
  {
    "objectID": "content/getting-started/04_python_practice.html",
    "href": "content/getting-started/04_python_practice.html",
    "title": "Bonne pratique de Python",
    "section": "",
    "text": "Une r√©f√©rence utile √† lire est le\nHitchhiker‚Äôs Guide to Python"
  },
  {
    "objectID": "content/getting-started/04_python_practice.html#structure-dun-projet-en-python",
    "href": "content/getting-started/04_python_practice.html#structure-dun-projet-en-python",
    "title": "Bonne pratique de Python",
    "section": "Structure d‚Äôun projet en python",
    "text": "Structure d‚Äôun projet en python\nLa structure basique d‚Äôun projet d√©velopp√© en Python est la suivante, qu‚Äôon peut retrouver dans\nce d√©p√¥t:\nREADME.md\nLICENSE\nsetup.py\nrequirements.txt\nmonmodule/__init__.py\nmonmodule/core.py\nmonmodule/helpers.py\ndocs/conf.py\ndocs/index.rst\ntests/context.py\ntests/test_basic.py\ntests/test_advanced.py\nQuelques explications et parall√®les avec les packages R1 :\n\nLe code Python est stock√© dans un module nomm√© monmodule. C‚Äôest le coeur du code dans le projet. Contrairement\n√† R, il est possible d‚Äôavoir une arborescence avec plusieurs modules dans un seul package. Un bon exemple\nde package dont le fonctionnement adopte une arborescence √† plusieurs niveaux est scikit\nLe fichier setup.py sert √† construire le package monmodule pour en faire un code utilisable. Il n‚Äôest pas\nobligatoire quand le projet n‚Äôa pas vocation √† √™tre sur PyPi mais il est assez facile √† cr√©er en suivant ce\ntemplate. C‚Äôest l‚Äô√©quivalent\ndu fichier Description dans un package R\n(exemple)\nLe fichier requirements.txt permet de contr√¥ler les d√©pendances du projet. Il s‚Äôagit des\nd√©pendances n√©cessaires pour faire tourner les fonctions (par exemple numpy), les tester et\nconstruire automatiquement la documentation (par exemple sphinx). Dans un package R, le fichier qui contr√¥le\nl‚Äôenvironnement est le NAMESPACE.\nLe dossier docs stocke la documentation du package. Le mieux est de le g√©n√©rer √† partir de\nsphinx et non de l‚Äô√©diter\nmanuellement. (cf.¬†plus tard).\nLes √©l√©ments qui s‚Äôen rapprochent dans un package R sont les vignettes.\nLes tests g√©n√©riques des fonctions. Ce n‚Äôest pas obligatoire mais c‚Äôest recommand√©: √ßa √©vite de d√©couvrir deux jours\navant un rendu de projet que la fonction ne produit pas le r√©sultat esp√©r√©.\nLe README.md permet de cr√©er une pr√©sentation du package qui s‚Äôaffiche automatiquement sur\ngithub/gitlab et le fichier LICENSE vise √† prot√©ger la propri√©t√© intellectuelle. Un certain nombre de licences\nstandards existent et peuvent √™tre utilis√©es comme template gr√¢ce au site https://choosealicense.com/\n\n2 La structure n√©cessaire des projets n√©cessaire pour pouvoir construire un package R est plus contrainte.\nLes packages devtools, usethis et testthat ont grandement facilit√© l‚Äô√©laboration d‚Äôun package R. A cet √©gard,\nil est recommand√© de lire l‚Äôincontournable livre d‚ÄôHadley Wickham"
  },
  {
    "objectID": "content/getting-started/04_python_practice.html#style-de-programmation-et-de-documentation",
    "href": "content/getting-started/04_python_practice.html#style-de-programmation-et-de-documentation",
    "title": "Bonne pratique de Python",
    "section": "Style de programmation et de documentation",
    "text": "Style de programmation et de documentation\n\nThe code is read much more often than it is written.\nGuido Van Rossum [cr√©ateur de Python]\n\nPython est un langage tr√®s lisible. Avec un peu d‚Äôeffort sur le nom des objets, sur la gestion\ndes d√©pendances et sur la structure du programme, on peut\ntr√®s bien comprendre un script sans avoir besoin de l‚Äôex√©cuter. La communaut√© Python a abouti √† un certain\nnombre de normes, dites PEP (Python Enhancement Proposal), qui constituent un standard\ndans l‚Äô√©cosyst√®me Python. Les deux normes les plus connues sont\nla norme PEP8 (code) et la norme PEP257 (documentation).\nLa plupart de ces recommandations ne sont pas propres √† Python, on les retrouve aussi dans R\n(cf.¬†ici).\nOn retrouve de nombreux conseils dans cet ouvrage qu‚Äôil est\nrecommand√© de suivre. La suite se concentrera sur des √©l√©ments compl√©mentaires.\n\nImport des modules\nLes √©l√©ments suivants concernent plut√¥t les scripts finaux, qui appellent de multiples fonctions, que des\nscripts qui d√©finissent des fonctions.\nUn module est un ensemble de fonctions stock√©es dans un fichier .py. Lorsqu‚Äôon √©crit dans un script\nimport modu\nPython commence par chercher le fichier modu.py dans le dossier de travail. Il n‚Äôest donc pas une bonne\nid√©e d‚Äôappeler un fichier du nom d‚Äôun module standard de python, par exemple math.py ou os.py. Si le fichier\nmodu.py n‚Äôest pas trouv√© dans le dossier de travail, Python va chercher dans le chemin et s‚Äôil ne le trouve pas\nretournera une erreur.\nUne fois que modu.py est trouv√©, il sera ex√©cut√© dans un environnement isol√© (reli√© de mani√®re coh√©rente\naux d√©pendances renseign√©es) et le r√©sultat rendu disponible √† l‚Äôinterpr√©teur Python pour un usage\ndans la session via le namespace (espace o√π Python associe les noms donn√©s aux objets).\nEn premier lieu, ne jamais utiliser la syntaxe suivante :\n# A NE PAS UTILISER\nfrom modu import *\nx = sqrt(4)  # Is sqrt part of modu? A builtin? Defined above?\nL‚Äôutilisation de la syntaxe import * cr√©√© une ambiguit√© sur les fonctions disponibles dans l‚Äôenvironnement. Le code\nest ainsi moins clair, moins compartiment√© et ainsi moins robuste. La syntaxe √† privil√©gier est la suivante :\nimport modu\nx = modu.sqrt(4)  # Is sqrt part of modu? A builtin? Defined above?"
  },
  {
    "objectID": "content/getting-started/04_python_practice.html#structuration-du-code",
    "href": "content/getting-started/04_python_practice.html#structuration-du-code",
    "title": "Bonne pratique de Python",
    "section": "Structuration du code",
    "text": "Structuration du code\nIl est commun de trouver sur internet des codes tr√®s longs, g√©n√©ralement dans un fichier __init__.py\n(m√©thode pour passer d‚Äôun module √† un package, qui est un ensemble plus structur√© de fonctions).\nContrairement √† la l√©gende, avoir des scripts longs est peu d√©sirable et est m√™me mauvais ;\ncela rend le code difficilement √† s‚Äôapproprier et √† faire √©voluer. Mieux vaut avoir des scripts relativement courts\n(sans l‚Äô√™tre √† l‚Äôexc√®s‚Ä¶) qui font √©ventuellement appels √† des fonctions d√©finies dans d‚Äôautres scripts.\nPour la m√™me raison, la multiplication de conditions logiques if‚Ä¶else if‚Ä¶else est g√©n√©ralement tr√®s mauvais\nsigne (on parle de code spaghetti) ; mieux vaut\nutiliser des m√©thodes g√©n√©riques dans ce type de circonstances."
  },
  {
    "objectID": "content/getting-started/04_python_practice.html#√©crire-des-fonctions",
    "href": "content/getting-started/04_python_practice.html#√©crire-des-fonctions",
    "title": "Bonne pratique de Python",
    "section": "√âcrire des fonctions",
    "text": "√âcrire des fonctions\nLes fonctions sont un objet central en Python.\nLa fonction id√©ale est une fonction qui agit de mani√®re compartiment√©e :\nelle prend un certain nombre d‚Äôinputs et est reli√©e au monde ext√©rieur uniquement par les d√©pendances,\nelle effectue des op√©rations sans interaction avec le monde ext√©rieur et retourne un r√©sultat.\nCette d√©finition assez consensuelle masque un certain nombre d‚Äôenjeux :\n\nUne bonne gestion des d√©pendances n√©cessite d‚Äôavoir appliqu√© les recommandations √©voqu√©es pr√©c√©demment\nIsoler du monde ext√©rieur n√©cessite de ne pas faire appel √† un objet ext√©rieur √† l‚Äôenvironnement de la fonction.\nAutrement dit, aucun objet hors de la port√©e (scope) de la fonction ne doit √™tre alt√©r√© ou utilis√©.\n\nPar exemple, le script suivant est mauvais au sens o√π il utilise un objet y hors du scope de la fonction add\ndef add(x):\n    return x + y\nIl faudrait revoir la fonction pour y ajouter un √©l√©ment y:\ndef add(x, y):\n    return x + y\nPycharm offre des outils de diagnostics tr√®s pratiques pour d√©tecter et corriger ce type d‚Äôerreur.\n\n‚ö†Ô∏è aux arguments optionnels\nLa fonction la plus lisible (mais la plus contraignante) est celle\nqui utilise exclusivement des arguments positionnels avec des noms explicites.\nDans le cadre d‚Äôune utilisation avanc√©e des fonctions (par exemple un gros mod√®le de microsimulation), il est\ndifficile d‚Äôanticiper tous les objets qui seront n√©cessaires √† l‚Äôutilisateur. Dans ce cas, on retrouve g√©n√©ralement\ndans la d√©finition d‚Äôune fonction le mot-cl√© **kwargs (√©quivalent du ... en R) qui capture les\narguments suppl√©mentaires et les stocke sous forme de dictionnaire. Il s‚Äôagit d‚Äôune technique avanc√©e de\nprogrammation qui est √† utiliser avec parcimonie."
  },
  {
    "objectID": "content/getting-started/04_python_practice.html#documenter-les-fonctions",
    "href": "content/getting-started/04_python_practice.html#documenter-les-fonctions",
    "title": "Bonne pratique de Python",
    "section": "Documenter les fonctions",
    "text": "Documenter les fonctions\nLa documentation d‚Äôune fonction s‚Äôappelle le docstring. Elle prend la forme suivante :\ndef square_and_rooter(x):\n    \"\"\"Return the square root of self times self.\"\"\"\n    ...\nAvec PyCharm, lorsqu‚Äôon utilise trois guillemets sous la d√©finition d‚Äôune fonction, un template minimal √†\ncompleter est automatiquement g√©n√©r√©. Les normes √† suivre pour que la docstrings soit reconnue par le package\nsphinx sont pr√©sent√©es dans la PEP257. N√©anmoins,\nelles ont √©t√© enrichies par le style de docstrings NumPy qui est plus riche et permet ainsi des documentations\nplus explicites\n(voir ici et\nici).\nSuivre ces canons formels permet une lecture simplifi√©e du code source de la documentation. Mais cela a surtout\nl‚Äôavantage, lors de la g√©n√©ration d‚Äôun package, de permettre une mise en forme automatique des fichiers\nhelp d‚Äôune fonction √† partir de la docstrings. L‚Äôoutil canonique pour ce type de construction automatique est\nsphinx (dont l‚Äô√©quivalent R est Roxygen)"
  },
  {
    "objectID": "content/getting-started/04_python_practice.html#les-tests",
    "href": "content/getting-started/04_python_practice.html#les-tests",
    "title": "Bonne pratique de Python",
    "section": "Les tests",
    "text": "Les tests\nTester ses fonctions peut appara√Ætre formaliste mais c‚Äôest, en fait, souvent d‚Äôun grand secours car cela permet de\nd√©tecter et corriger des bugs pr√©coces (ou au moins d‚Äô√™tre conscient de leur existence).\nAu-del√† de la correction de bug, cela permet de v√©rifier que\nla fonction produit bien un r√©sultat esp√©r√© dans une exp√©rience contr√¥l√©e.\nEn fait, il existe deux types de tests:\n\ntests unitaires : on teste seulement une fonctionalit√© ou propri√©t√©\ntests d‚Äôint√©gration : on teste l‚Äôint√©gration de la fonction dans un ensemble plus large de fonctionalit√©s\n\nIci, on va plut√¥t se focaliser sur la notion de test unitaire, la notion de\ntest d‚Äôint√©gration n√©cessitant d‚Äôavoir une cha√Æne plus compl√®te de fonctions (mais il ne faut\npas la n√©gliger).\nOn peut partir du principe suivant :\n\ntoute fonctionnalit√© non test√©e comporte un bug\n\nLe fichier tests/context.py sert √† d√©finir le contexte dans lequel le test de la fonction s‚Äôex√©cute, de mani√®re\nisol√©e. On peut adopter le mod√®le suivant, en changeant import monmodule par le nom de module ad√©quat\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nimport monmodule\nChaque fichier du dossier de test\n(par exemple test_basic.py et test_advanced.py) incorpore ensuite la ligne suivante,\nen d√©but de script\nfrom .context import sample\nPour automatiser les tests, on peut utiliser le package unittest\n(doc ici). L‚Äôid√©e est que dans un cadre contr√¥l√©\n(on conna√Æt l‚Äôinput et en tant que concepteur de la fonction on conna√Æt l‚Äôoutput ou, a minima\nles propri√©t√©s de l‚Äôoutput) on peut tester la sortie d‚Äôune fonction.\nLa structure canonique de test est la suivante3\nimport unittest\n\ndef fun(x):\n    return x + 1\n\nclass MyTest(unittest.TestCase):\n    def test(self):\n        self.assertEqual(fun(3), 4)\n4 Le code √©quivalent avec R serait testthat::expect_equal(fun(3),4)\nParler de codecov"
  },
  {
    "objectID": "content/getting-started/04_python_practice.html#partager",
    "href": "content/getting-started/04_python_practice.html#partager",
    "title": "Bonne pratique de Python",
    "section": "Partager",
    "text": "Partager\nCe point est ici √©voqu√© en dernier mais, en fait, il est essentiel et m√©rite d‚Äô√™tre une r√©flexion prioritaire.\nTout travail n‚Äôa pas vocation √† √™tre public\nou √† d√©passer le cadre d‚Äôune √©quipe. Cependant, les m√™mes exigences qui s‚Äôappliquent lorsqu‚Äôun code est public m√©ritent\nde s‚Äôappliquer avec un projet personnel. Avant de partager un code avec d‚Äôautres, on le partage avec le ‚Äúfutur moi‚Äù.\nReprendre un code √©crit il y a plusieurs semaines est co√ªteux et m√©rite d‚Äôanticiper en adoptant des bonnes pratiques qui\nrendront quasi-indolore la r√©-appropriation du code.\nL‚Äôint√©gration d‚Äôun projet avec git fiabilise grandement le processus d‚Äô√©criture du code mais aussi, gr√¢ce aux\noutils d‚Äôint√©gration continue, la production de contenu (par exemple des visualisations html ou des rapports\nfinaux √©crits avec markdown). Il est recommand√© d‚Äôimm√©diatement connecter un projet √† git, m√™me avec un\nd√©p√¥t qui aura vocation √† √™tre personnel. Les instructions d‚Äôutilisation de git sont d√©taill√©es ici."
  },
  {
    "objectID": "content/getting-started/04_python_practice.html#footnotes",
    "href": "content/getting-started/04_python_practice.html#footnotes",
    "title": "Bonne pratique de Python",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n1‚Ü©Ô∏é\n1:‚Ü©Ô∏é\n2‚Ü©Ô∏é\n2:‚Ü©Ô∏é"
  },
  {
    "objectID": "content/getting-started/06_rappels_fonctions.html#utilisation-dun-module-install√©",
    "href": "content/getting-started/06_rappels_fonctions.html#utilisation-dun-module-install√©",
    "title": "Modules, tests, boucles, fonctions",
    "section": "Utilisation d‚Äôun module install√©",
    "text": "Utilisation d‚Äôun module install√©\nPython, comme R, sont des langages\nconstruits sur le principe de briques.\nCes briques sont ce qu‚Äôon appelle des packages.\nAu contraire de Stata mais comme pour R,\nil\nfaut toujours pr√©ciser les packages que vous utilisez au d√©but du code,\nsinon Python ne reconnait pas les fonctions appel√©es.\n\nImport module\nOn charge un module gr√¢ce √† la commande import.\nPour chaque code que vous ex√©cutez,\nil faut charger les modules en introduction.\nUne fois qu‚Äôon a charg√© le module,\non peut faire appel aux commandes qui en d√©pendent en les appelant\napr√®s avoir tap√© le nom du module.\nSi vous ne pr√©cisez pas le nom du module avant celui de la fonction,\nil ne la trouvera pas forc√©ment.\nVoici un exemple avec le module numpy\nqui est tr√®s courant et permet de faire des\ncalculs matriciels sous Python.\n\nimport numpy\nprint(numpy.arange(5))\n\n[0 1 2 3 4]\n\n\n\n\nImport module as md - donner un nom au module\nOn peut aussi donner un pseudonyme au module pour\n√©viter de taper un nom trop long √† chaque fois\nqu‚Äôon utilise une fonction.\nClassiquement le nom raccourci de numpy est np,\ncelui de pandas est pd.\n\nimport pandas as pd\nimport numpy as np\nsmall_array = np.array([[1, 2], [3, 4]])\ndata = pd.DataFrame(small_array)\ndata.head()\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n1\n2\n\n\n1\n3\n4\n\n\n\n\n\n\n\n\n\nfrom Module Import fonction - seulement une partie du module\nSi on ne veut pas √™tre oblig√© de donner\nle nom du module avant d‚Äôappeler\nla fonction,\nil y a toujours la possibilit√© de n‚Äôimporter qu‚Äôune fonction du module.\nDans le cas de l‚Äôexemple, Python sait que la fonction arrange est celle de numpy.\nMais attention : si deux fonctions de modules diff√©rents\nont le m√™me nom,\nc‚Äôest toujours la derni√®re import√©e qui gagne.\nOn voit souvent from _module_ import *.\nC‚Äôest-√†-dire qu‚Äôon importe toutes\nles fonctions du module\nmais on n‚Äôa pas besoin de sp√©cifier le nom du module avant les m√©thodes.\n\n\n Warning\nLa m√©thode from _module_ import * n‚Äôest pas recommand√©e car elle rend le code moins intelligible.\nEn effet, d‚Äôo√π vient la fonction floor ? De maths ou de numpy ?\nElle risque\naussi de cr√©er des conflits de fonction, qui malgr√© un nom commun peuvent ne\npas attendre les m√™mes arguments ou objets.\n\n\n\nfrom numpy import array\nprint(array(5))\n\n5"
  },
  {
    "objectID": "content/getting-started/06_rappels_fonctions.html#les-tests",
    "href": "content/getting-started/06_rappels_fonctions.html#les-tests",
    "title": "Modules, tests, boucles, fonctions",
    "section": "Les tests",
    "text": "Les tests\nLes tests permettent d‚Äôex√©cuter telle ou telle instruction\nselon la valeur d‚Äôune condition.\nPour faire un test avec un bloc d‚Äôinstructions, il faut toujours :\n\nque l‚Äôexpression √† v√©rifier soit suivie de :\nque le bloc d‚Äôinstruction soit forc√©ment indent√©\n\n\nTest avec contrepartie : if et else\nComme dans les autres langages,\non teste une condition. Si elle est v√©rifi√©e,\nalors une instruction suit et sinon, une autre instruction est ex√©cut√©e.\nIl est conseill√© de toujours indiquer une contrepartie afin d‚Äô√©viter les surprises.\n\nTest d‚Äôune √©galit√© ou in√©galit√©\n\nx = 6\n\nif x &gt; 5 :\n    print(\"x est plus grand que 5\")\nelse : # la contrepartie si la condition if n'est pas r√©alis√©e\n    print(\"x est plus petit que 5\")\n\nx est plus grand que 5\n\n\n\n\nTest dans un intervalle\n\n# on peut avoir des intervalles directement\nx = 6\nif 5 &lt; x &lt; 10 : \n    print(\"x est entre 5 et 10\")\nelse : \n    print(\"x est plus grand que 10\")\n\nx est entre 5 et 10\n\n\n\n# tester plusieurs valeurs avec l'op√©rateur logique \"or\"\nx = 5\nif x == 5 or x == 10 : \n    print(\"x vaut 5 ou 10\")    \nelse : \n    print(\"x est diff√©rent de 5 et 10\")\n\nx vaut 5 ou 10\n\n\n\n\n\nTests successifs : if, elif et else\nAvec if et elif,\nd√®s qu‚Äôon rencontre une condition qui est r√©alis√©e,\non n‚Äôen cherche pas d‚Äôautres potentiellement v√©rifi√©es.\nPlusieurs if √† la suite peuvent quant √† eux √™tre v√©rifi√©s.\nSuivant ce que vous souhaitez faire, les op√©rateurs ne sont pas substituables.\nNotez la diff√©rence entre ces deux bouts de code :\n\n#code 1\nx = 5\n\nif x != 10 : \n    print(\"x ne vaut pas 10\")   \nelif x &gt;= 5 : \n    print(\"x est √©gal ou sup√©rieur √† 5\")\n\nx ne vaut pas 10\n\n\nDans le cas de elif, on s‚Äôarr√™te √† la premi√®re condition v√©rifi√©e et dans le cas suivant, on continue √† chaque condition v√©rifi√©e\n\n#code 2\nx = 5\n\nif x != 10 : \n    print(\"x ne vaut pas 10\")   \nif x &gt;= 5 : \n    print(\"x est √©gal ou sup√©rieur √† 5\")\n\nx ne vaut pas 10\nx est √©gal ou sup√©rieur √† 5"
  },
  {
    "objectID": "content/getting-started/06_rappels_fonctions.html#boucles",
    "href": "content/getting-started/06_rappels_fonctions.html#boucles",
    "title": "Modules, tests, boucles, fonctions",
    "section": "Boucles",
    "text": "Boucles\nIl existe deux types de boucles : les boucles for et les boucles while\nLa boucle for parcourt un ensemble, tandis que la boucle while\ncontinue tant qu‚Äôune condition est vraie.\n\nBoucle for\n\nParcourir une liste croissantes d‚Äôentiers\n\n# parcourt les entiers de 0 √† n-1 inclus\nfor i in range(0,3) : \n    print(i)\n\n0\n1\n2\n\n\n\n\nParcourir une liste d√©croissante d‚Äôentiers\n\n# parcourt les entiers de 3 √† n+1 inclus\nfor i in range(3,0,-1) : \n    print(i)\n\n3\n2\n1\n\n\n\n\nParcourir une liste de chaines de caract√®res\nOn va faire une boucle sur les √©l√©ments d‚Äôune liste\n\n\nBoucle sur les √©l√©ments d‚Äôune liste\n\nliste_elements = ['Nicolas','Romain','Florimond']\n\n# pour avoir l'ensemble des √©l√©ments de la liste\nfor item in liste_elements : \n    print(item)\n\nNicolas\nRomain\nFlorimond\n\n\n\n\nBoucle sur les √©l√©ments d‚Äôune liste dans une autre liste\n\n# pour avoir la place des √©l√©ments de la premi√®re liste dans la seconde liste  \n\nliste_globale = ['Violette','Nicolas','Mathilde','Romain','Florimond','Helene'] \n\nfor item in liste_elements : \n    print(item,liste_globale.index(item))\n\nNicolas 1\nRomain 3\nFlorimond 4\n\n\n\n\n\nBonus : les list comprehension\nAvec les listes, il existe aussi un moyen tr√®s √©l√©gant de condenser son code pour √©viter de faire apparaitre des boucles sans arr√™t. Comme les boucles doivent etre indent√©es, le code peut rapidement devenir illisible.\nGrace aux list comprehension, vous pouvez en une ligne faire ce qu‚Äôune boucle vous permettait de faire en 3 lignes.\nPar exemple, imaginez que vous vouliez faire la liste de toutes les lettres contenues dans un mot, avec un boucle vous devrez d‚Äôabord cr√©er une liste vide, puis ajouter √† cette liste toutes les lettres en question avec un .append()\n\nliste_lettres = []\n\nfor lettre in 'ENSAE':\n    liste_lettres.append(lettre)\n\nprint(liste_lettres)\n\n['E', 'N', 'S', 'A', 'E']\n\n\navec une list comprehension, on condense la syntaxe de la mani√®re suivante :\n\nh_letters = [ letter for letter in 'ENSAE' ]\nprint(h_letters)\n\n['E', 'N', 'S', 'A', 'E']\n\n\nAvec une list comprehension\n[ expression for item in list if conditional ]\nest √©quivalent √†\nfor item in list:\n    if conditional:\n        expression  \n\nMise en application\nMettez sous forme de list comprehension le bout de code suivant\n\nsquares = []\n\nfor x in range(10):\n    squares.append(x**2)\nprint(squares)\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\n\n\n\n\nBoucle while\nLe bloc d‚Äôinstruction d‚Äôune boucle while est ex√©cut√© tant que la condition est v√©rifi√©e.\nLe pi√®ge de ces boucles : la boucle while infinie ! Il faut toujours v√©rifier que votre boucle s‚Äôarr√™tera un jour, il faut qu‚Äô√† un moment ou √† un autre, il y ait un √©l√©ment qui s‚Äôincr√©mente ou qui soit modifi√©.\n\nx = 10\ny = 8\n# tant que y est plus petit que 10, je continue de lui ajouter 1\nwhile y &lt;= x : \n    print(\"y n'est pas encore plus grand que x\")\n    y += 1 # l'incr√©ment\nelse : \n    print(\"y est plus grand que x et vaut\",y)\n\ny n'est pas encore plus grand que x\ny n'est pas encore plus grand que x\ny n'est pas encore plus grand que x\ny est plus grand que x et vaut 11\n\n\n\n\nBreak et continue\nDans les boucles for ou while on peut avoir besoin d‚Äôignorer ou de ne pas effectuer certaines it√©rations. 2 instructions utiles :\n\nl‚Äôinstruction break : permet de sortir de la boucle\nl‚Äôinstruction continue : permet de passer √† l‚Äôit√©ration suivante sans ex√©cuter les instructions qui suivent\n\n\n# utilisation de break\nfor x in range(5) : \n    if x == 2 : \n        break\n    else :\n        print(x)\n\n0\n1\n\n\n\n# utilisation de continue\nfor x in range(5) : \n    if x == 2 : \n        continue\n    else :\n        print(x)\n\n0\n1\n3\n4"
  },
  {
    "objectID": "content/getting-started/06_rappels_fonctions.html#cr√©er-ses-fonctions",
    "href": "content/getting-started/06_rappels_fonctions.html#cr√©er-ses-fonctions",
    "title": "Modules, tests, boucles, fonctions",
    "section": "Cr√©er ses fonctions",
    "text": "Cr√©er ses fonctions\nLes fonctions permettent de faire la m√™me chose sans avoir √† recopier le code plusieurs fois dans le m√™me programme. D√®s que vous le pouvez, faites des fonctions : le copier-coller est trop dangereux.\n\nElles peuvent prendre plusieurs param√®tres (ou aucun) elles peuvent retourner plusieurs r√©sultats (ou aucun)\nPour mettre une aide √† la fonction, on √©crit au d√©but entre ‚Äú‚Äú‚Äù ‚Äú‚Äú‚Äù (en rouge dans l‚Äôexemple)\n\n\n# 1er exemple de fonction\n\ndef ma_fonction_increment(parametre) : \n    \"\"\"Cette fonction ajoute 1 au param√®tre qu'on lui donne\"\"\"\n    x = parametre + 1 \n    return x\n\n# pour documenter la fonction, on √©crit son aide\nhelp(ma_fonction_increment)\n\nHelp on function ma_fonction_increment in module __main__:\n\nma_fonction_increment(parametre)\n    Cette fonction ajoute 1 au param√®tre qu'on lui donne\n\n\n\nOn peut √©galement :\n\navoir des param√®tres facultatifs, mais ils doivent toujours √™tre plac√©s √† la fin des param√®tres\nen cas de param√®tre facultatif, il faut lui donner une valeur par d√©faut\nretourner plus d‚Äôun √©l√©ment √† la fin d‚Äôune fonction\navoir des param√®tres de tailles diff√©rentes\n\n\ndef ma_fonction(p,q = 2) :\n    y1 = p + q\n    y2 = y1%3 #reste de la division euclidienne\n    return y1,y2\n\nx = ma_fonction(11) \n# ici, on n'a pas de 2nd param√®tre\n#, par d√©faut, x = ma_fonction(10,2)\nprint(\"x=\", x)\n\nz = ma_fonction(10,-1)\nprint(\"z =\",z)\n\nx= (13, 1)\nz = (9, 0)\n\n\nUne fonction peut √©galement s‚Äôappeler elle m√™me : c‚Äôest ce qu‚Äôon appelle une fonction r√©cursive.\nDans cet exemple, somme_recursion() est une fonction que nous avons d√©finie de sorte √† ce qu‚Äôelle s‚Äôappelle elle-m√™me (r√©cursif).\nOn utilise l‚Äôargument k, qui d√©croit (-1) chaque fois qu‚Äôon fait appel √† la fonction.\nLa r√©cursion s‚Äôarr√™te quand k est nul.\nDans cet exemple, on va donc appeler 6 fois la fonction r√©cursive.\n\ndef somme_recursion(k):\n    if(k &gt; 0):\n        result = k + somme_recursion(k - 1)\n        print(k,result)\n    else:\n        result = 0\n    return result\nsomme_recursion(6)\n\n1 1\n2 3\n3 6\n4 10\n5 15\n6 21\n\n\n21\n\n\nLes fonctions sont tr√®s utiles et nous vous invitons √† les utiliser d√®s que vous le pouvez car elles permettent d‚Äôavoir un code clair et structur√©, plut√¥t que des bouts de code √©parpill√©s."
  },
  {
    "objectID": "content/getting-started/06_rappels_fonctions.html#lever-des-exceptions",
    "href": "content/getting-started/06_rappels_fonctions.html#lever-des-exceptions",
    "title": "Modules, tests, boucles, fonctions",
    "section": "Lever des exceptions",
    "text": "Lever des exceptions\nPython peut rencontrer des erreurs en ex√©cutant votre programme.\nCes erreurs peuvent √™tre intercept√©es tr√®s facilement et c‚Äôest m√™me, dans certains cas, indispensable. Par exemple, si vous voulez faire une boucle mais que vous savez que l‚Äôinstruction ne marchera pas toujours : au lieu de lister les cas o√π une op√©ration n‚Äôest pas possible, on peut indiquer directement quelle erreur doit √™tre ignor√©e.\nCependant, il ne faut pas tout intercepter non plus : si Python envoie une erreur, c‚Äôest qu‚Äôil y a une raison. Si vous ignorez une erreur, vous risquez d‚Äôavoir des r√©sultats tr√®s √©tranges dans votre programme.\n\n# √©viter une division par 0, c'est une bonne id√©e : \n\ndef inverse(x) :\n    '''Cette fonction renvoie l inverse de l argument'''\n    y = 1/x\n    return y\n\ndiv = inverse(0)\n\nZeroDivisionError: division by zero\n\n\nL‚Äôerreur est √©crite noir sur blanc : ZeroDivisionError\nDans l‚Äôid√©al on aimerait √©viter que notre code bloque sur ce probl√®me. On pourrait passer par un test if et v√©rifier que x est diff√©rent de 0. Mais on se rend vite compte que dans certains cas, on ne peut lister tous les tests en fonction de valeurs.\nAlors on va lui pr√©cisier ce qu‚Äôil doit faire en fonction de l‚Äôerreur retourn√©e.\nSyntaxe\nTry :\ninstruction\nexcept TypeErreur :\nautre instruction\n\n\ndef inverse(x) : \n    try :\n        y = 1/x\n    except ZeroDivisionError :\n        y = None\n    return y\n    \nprint(inverse(10))\nprint(inverse(0))\n\n0.1\nNone\n\n\nIl est recommand√© de toujours pr√©ciser le type d‚Äôerreur qu‚Äôon rencontre. Si on met uniquement ‚Äúexcept‚Äù sans pr√©ciser le type, on peut passer √† c√¥t√© d‚Äôerreurs pour lesquelles la solution n‚Äôest pas universelle."
  },
  {
    "objectID": "content/getting-started/06_rappels_fonctions.html#a-retenir-et-questions",
    "href": "content/getting-started/06_rappels_fonctions.html#a-retenir-et-questions",
    "title": "Modules, tests, boucles, fonctions",
    "section": "A retenir et questions",
    "text": "A retenir et questions\n\nA retenir\n\nToujours mettre ‚Äú:‚Äù avant un bloc d‚Äôinstructions\nIndenter avant un bloc d‚Äôinstructions (avec 4 espaces et non une tabulation !)\nIndiquer les modules n√©cessaires √† l‚Äôex√©cution en d√©but de code\nDocumenter les fonctions cr√©√©es\nPr√©ciser le type d‚Äôerreur pour les exceptions et potentiellement diff√©rencier les blocs d‚Äôinstructions en fonction de l‚Äôerreur\n\n\n\nQuestions\n\nQue fait ce programme ?\n\n\ndef inverse(x) : \n    try :\n        y = 1/x\n    except ZeroDivisionError :\n        y = None\n        return y\n\n\nEcrivez un programme qui peut trouver tous les nombres divisibles par 7 et non multiples de 5 entre 6523 et 8463 (inclus)\nEcrivez un programme qui prend une phrase en entr√©e et qui calcule le nombre de voyelles en Majuscules et de consonnes en minuscules :\n\nphrase = ‚ÄúVous savez, moi je ne crois pas qu‚Äôil y ait de bonne ou de mauvaise situation. Moi, si je devais r√©sumer ma vie aujourd‚Äôhui avec vous, je dirais que c‚Äôest d‚Äôabord des rencontres. Des gens qui m‚Äôont tendu la main, peut-√™tre √† un moment o√π je ne pouvais pas, o√π j‚Äô√©tais seul chez moi. Et c‚Äôest assez curieux de se dire que les hasards, les rencontres forgent une destin√©e‚Ä¶ Parce que quand on a le go√ªt de la chose, quand on a le go√ªt de la chose bien faite, le beau geste, parfois on ne trouve pas l‚Äôinterlocuteur en face je dirais, le miroir qui vous aide √† avancer. Alors √ßa n‚Äôest pas mon cas, comme je disais l√†, puisque moi au contraire, j‚Äôai pu : et je dis merci √† la vie, je lui dis merci, je chante la vie, je danse la vie‚Ä¶ je ne suis qu‚Äôamour ! Et finalement, quand beaucoup de gens aujourd‚Äôhui me disent ‚ÄòMais comment fais-tu pour avoir cette humanit√© ?‚Äô, et bien je leur r√©ponds tr√®s simplement, je leur dis que c‚Äôest ce go√ªt de l‚Äôamour ce go√ªt donc qui m‚Äôa pouss√© aujourd‚Äôhui √† entreprendre une construction m√©canique, mais demain qui sait ? Peut-√™tre simplement √† me mettre au service de la communaut√©, √† faire le don, le don de soi‚Ä¶‚Äù"
  },
  {
    "objectID": "content/manipulation/index.html",
    "href": "content/manipulation/index.html",
    "title": "Partie 1: manipuler des donn√©es",
    "section": "",
    "text": "Si on associe souvent les data scientists √† la mise en oeuvre\nde mod√®les d‚Äôintelligence artificielle, il est important\nde ne pas oublier que l‚Äôentra√Ænement et l‚Äôutilisation\nde ces mod√®les ne repr√©sente pas\nforc√©ment le quotidien des data scientists.\nEn pratique,\nla r√©cup√©ration de sources de donn√©es h√©t√©rog√®nes, la structuration\net harmonisation de celles-ci en vue d‚Äôune analyse exploratoire\npr√©alable √† la mod√©lisation ou la visualisation\nrepr√©sente une part importante du travail des data scientists.\nDans de nombreux environnements c‚Äôest m√™me l‚Äôessence du travail\ndu data scientist.\nL‚Äô√©laboration de mod√®les pertinents requiert en effet une r√©flexion approfondie sur les donn√©es ;\nune √©tape que l‚Äôon ne saurait n√©gliger.\nCe cours,\ncomme de nombreuses ressources introductives sur\nla data science (Wickham, √áetinkaya-Rundel, and Grolemund 2023; VanderPlas 2016; McKinney 2012),\nproposera donc beaucoup d‚Äô√©l√©ments sur la manipulation de donn√©es, comp√©tence\nessentielle pour les data scientists.\nLes logiciels de programmation\norient√©s autour du concept de base de donn√©es\nsont devenus les outils principaux des data scientists.\nLe fait de pouvoir appliquer un certain nombre d‚Äôop√©rations\nstandards sur des bases de donn√©es, quelle que soit leur nature,\npermet aux programmeurs d‚Äô√™tre plus efficaces que s‚Äôils devaient\nr√©p√©ter ces op√©rations √† la main, comme dans Excel.\nTous les langages de programmation dominants dans l‚Äô√©cosyst√®me\nde la data science reposent sur le principe du dataframe.\nIl s‚Äôagit m√™me d‚Äôun objet central dans certains logiciels,\nnotamment R.\nLa logique SQL,\nun langage de d√©claration d‚Äôop√©rations sur des donn√©es qui a d√©j√† plus de cinquante ans,\noffre un cadre pertinent pour effectuer des op√©rations standardis√©es\nsur les colonnes (cr√©ation de nouvelles colonnes, s√©lection de sous-ensemble de lignes‚Ä¶).\nN√©anmoins, le dataframe ne s‚Äôest impos√© que r√©cemment en Python,\ngr√¢ce au package Pandas cr√©√©\npar Wes McKinney.\nL‚Äôessor de la librairie Pandas (t√©l√©charg√©e plus de 5 millions de fois\npar jour en 2023) est pour beaucoup dans le succ√®s de Python\ndans l‚Äô√©cosyst√®me de la data science et a amen√©, en quelques ann√©es,\na un renouvellement complet de la mani√®re de coder en Python, ce\nlangage si mall√©able, autour de l‚Äôanalyse de donn√©es.\nCette partie du cours est une introduction\ng√©n√©rale √† l‚Äô√©cosyst√®me tr√®s riche de\nla manipulation de donn√©es avec Python.\nCes chapitres √©voquent aussi bien la r√©cup√©ration de donn√©es\nque la restructuration et la production d‚Äôanalyse\n√† partir de celles-ci."
  },
  {
    "objectID": "content/manipulation/index.html#r√©sum√©-de-cette-partie",
    "href": "content/manipulation/index.html#r√©sum√©-de-cette-partie",
    "title": "Partie 1: manipuler des donn√©es",
    "section": "R√©sum√© de cette partie",
    "text": "R√©sum√© de cette partie\nPandas est devenu incontournable dans l‚Äô√©cosyst√®me Python pour la data science.\nPandas est lui-m√™me construit √† partir du package Numpy, qu‚Äôil est utile de comprendre\npour √™tre √† l‚Äôaise avec Pandas. Numpy est une librairie bas-niveau\npour stocker et manipuler des donn√©es.\nNumpy est au coeur de l‚Äô√©cosyst√®me de la data science car la plupart des librairies, m√™me celles\nqui manient des objets destructur√©s,\nutilisent des objets construits √† partir de Numpy1.\nL‚Äôapproche Pandas, qui offre un point d‚Äôentr√©e harmonis√© pour manipuler\ndes jeux de donn√©es de nature tr√®s diff√©rente,\na √©t√© √©tendue aux objets g√©ographiques avec Geopandas.\nIl est ainsi possible de manipuler des donn√©es g√©ographiques comme s‚Äôil\ns‚Äôagissait de donn√©es structur√©es classiques. Les donn√©es g√©ographiques et\nla repr√©sentation cartographique deviennent de plus en plus commun avec\nla multiplication de donn√©es ouvertes localis√©es et de big-data g√©olocalis√©es.\nCependant, les donn√©es structur√©es, import√©es depuis des fichiers plats\nne repr√©sentent pas l‚Äôunique source de donn√©es. Les API et le webscraping\npermettent de t√©l√©charger ou d‚Äôextraire\ndes donn√©es de mani√®re tr√®s flexible depuis des pages web ou des guichets\nsp√©cialis√©s. Ces donn√©es, notamment\ncelles obtenues par webscraping n√©cessitent souvent un peu plus de travail\nde nettoyage de donn√©es, notamment des cha√Ænes de caract√®re.\nL‚Äô√©cosyst√®me Pandas repr√©sente donc un couteau-suisse\npour l‚Äôanalyse de donn√©es. C‚Äôest pour cette raison que ce cours\nd√©veloppera beaucoup de contenu dessus.\nAvant d‚Äôessayer de mettre en oeuvre une solution ad hoc, il est\nsouvent utile de se poser la question suivante : ‚Äúne pourrais-je pas le faire\navec les fonctionalit√©s de base de Pandas ?‚Äù Se poser cette question peut\n√©viter des chemins ardus et faire √©conomiser beaucoup de temps.\nN√©anmoins, Pandas n‚Äôest pas\nadapt√© √† des donn√©es ayant une volum√©trie\nimportante. Pour traiter de telles\ndonn√©es, il est plut√¥t recommander\nde privil√©gier Polars ou Dask qui reprennent la logique de Pandas mais\noptimisent son fonctionnement, Spark si on a une infrastructure adapt√©e, g√©n√©ralement dans\ndes environnements big data, ou\nDuckDB si on est pr√™t √† utiliser des requ√™tes SQL plut√¥t qu‚Äôune librairie haut-niveau."
  },
  {
    "objectID": "content/manipulation/index.html#exercices",
    "href": "content/manipulation/index.html#exercices",
    "title": "Partie 1: manipuler des donn√©es",
    "section": "Exercices",
    "text": "Exercices\nCette partie pr√©sente √† la fois des tutoriels d√©taill√©s\net des exercices guid√©s.\nIl est\npossible de les consulter sur ce site ou d‚Äôutiliser l‚Äôun des\nbadges pr√©sents en d√©but de chapitre, par exemple\nceux-ci pour ouvrir\nle chapitre d‚Äôexercices sur Pandas:"
  },
  {
    "objectID": "content/manipulation/index.html#pour-aller-plus-loin",
    "href": "content/manipulation/index.html#pour-aller-plus-loin",
    "title": "Partie 1: manipuler des donn√©es",
    "section": "Pour aller plus loin",
    "text": "Pour aller plus loin\nCe cours n‚Äôaborde pas vraiment les questions de volum√©trie ou de vitesse de\ncalcul.\nPandas peut montrer ses limites dans ce domaine sur des jeux de donn√©es\nd‚Äôune volum√©trie cons√©quente (plusieurs Gigas).\nIl est ainsi int√©ressant de porter attention √†:\n\nLe livre Modern Pandas\npour obtenir des √©l√©ments suppl√©mentaires sur la question de la performance\navec Pandas ;\nLa question des\nobjets sparse ;\nLes packages Dask ou Polars pour acc√©l√©rer les calculs ;\nDuckDB pour effectuer de mani√®re tr√®s efficace des requ√™tes SQL ;\nPySpark pour des donn√©es tr√®s volumineuses.\n\n\nR√©f√©rences\nVoici une bibliographie s√©lective des ouvrages\nint√©ressants en compl√©ment des chapitres de la partie ‚ÄúManipulation‚Äù de ce cours :\n\n\nMcKinney, Wes. 2012. Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython. \" O‚ÄôReilly Media, Inc.\".\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook: Essential Tools for Working with Data. \" O‚ÄôReilly Media, Inc.\".\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O‚ÄôReilly Media, Inc.\"."
  },
  {
    "objectID": "content/manipulation/index.html#footnotes",
    "href": "content/manipulation/index.html#footnotes",
    "title": "Partie 1: manipuler des donn√©es",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCertaines librairies commencent, petit √† petit, √† s‚Äô√©manciper\nde Numpy qui n‚Äôest pas toujours le plus adapt√© pour la gestion\nde certains types de donn√©es. Le framework Arrow tend √† devenir\nla couche basse utilis√©e par de plus en plus de librairies de data science.\nCe post de blog approfondit\nde mani√®re tr√®s p√©dagogique ce sujet.‚Ü©Ô∏é"
  },
  {
    "objectID": "content/manipulation/02a_pandas_tutorial.html",
    "href": "content/manipulation/02a_pandas_tutorial.html",
    "title": "Introduction √† Pandas",
    "section": "",
    "text": "Pour essayer les exemples pr√©sents dans ce tutoriel :\nLe package Pandas est l‚Äôune des briques centrales de l‚Äô√©cosyst√®me de\nla data science. Le DataFrame,\nobjet central dans des langages comme R\nou Stata, a longtemps √©tait un grand absent dans l‚Äô√©cosyst√®me Python.\nPourtant, gr√¢ce √† Numpy, toutes les briques de base √©taient pr√©sentes.\nWes McKinney, lorsqu‚Äôil a cr√©√© Pandas en s‚Äôappuyant sur Numpy\na ainsi introduit cet objet devenu banal qu‚Äôest le DataFrame.\nPandas est rapidement\ndevenu un incontournable de la data-science. L‚Äôouvrage\nde r√©f√©rence de McKinney (2012) pr√©sente de mani√®re plus\nample ce package.\nCe tutoriel vise √† introduire aux concepts\nde base de ce package par l‚Äôexemple et √† introduire √† certaines\ndes t√¢ches les plus fr√©quentes de (re)structuration\ndes donn√©es du data scientist. Il ne s‚Äôagit pas d‚Äôun ensemble\nexhaustif de commandes : Pandas est un package tentaculaire\nqui permet de r√©aliser la m√™me op√©ration de nombreuses mani√®res.\nNous nous concentrerons ainsi sur les √©l√©ments les plus pertinents\ndans le cadre d‚Äôune introduction √† la data science et laisserons\nles utilisateurs int√©ress√©s approfondir leurs connaissances\ndans les ressources foisonnantes qu‚Äôil existe sur le sujet.\nDans ce tutoriel Pandas, nous allons utiliser :\nLe chapitre suivant permettra de mettre en application des √©l√©ments pr√©sents dans ce chapitre avec\nles donn√©es ci-dessus associ√©es √† des donn√©es de contexte au niveau communal.\nNous suivrons les conventions habituelles dans l‚Äôimport des packages :\n!pip install pynsee\n\nRequirement already satisfied: pynsee in /opt/mamba/lib/python3.9/site-packages (0.1.4)\nRequirement already satisfied: pandas&gt;=0.24.2 in /opt/mamba/lib/python3.9/site-packages (from pynsee) (2.1.1)\nRequirement already satisfied: tqdm&gt;=4.56.0 in /opt/mamba/lib/python3.9/site-packages (from pynsee) (4.66.1)\nRequirement already satisfied: requests&gt;=2.23 in /opt/mamba/lib/python3.9/site-packages (from pynsee) (2.31.0)\nRequirement already satisfied: appdirs&gt;=1.4.4 in /opt/mamba/lib/python3.9/site-packages (from pynsee) (1.4.4)\nRequirement already satisfied: unidecode&gt;=1.1.0 in /opt/mamba/lib/python3.9/site-packages (from pynsee) (1.3.6)\nRequirement already satisfied: shapely&gt;=2.0.0 in /opt/mamba/lib/python3.9/site-packages (from pynsee) (2.0.1)\nRequirement already satisfied: urllib3 in /opt/mamba/lib/python3.9/site-packages (from pynsee) (1.26.11)\nRequirement already satisfied: numpy&gt;=1.22.4 in /opt/mamba/lib/python3.9/site-packages (from pandas&gt;=0.24.2-&gt;pynsee) (1.26.0)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /opt/mamba/lib/python3.9/site-packages (from pandas&gt;=0.24.2-&gt;pynsee) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /opt/mamba/lib/python3.9/site-packages (from pandas&gt;=0.24.2-&gt;pynsee) (2022.5)\nRequirement already satisfied: tzdata&gt;=2022.1 in /opt/mamba/lib/python3.9/site-packages (from pandas&gt;=0.24.2-&gt;pynsee) (2023.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/mamba/lib/python3.9/site-packages (from requests&gt;=2.23-&gt;pynsee) (2.1.1)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/mamba/lib/python3.9/site-packages (from requests&gt;=2.23-&gt;pynsee) (3.3)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /opt/mamba/lib/python3.9/site-packages (from requests&gt;=2.23-&gt;pynsee) (2022.9.24)\nRequirement already satisfied: six&gt;=1.5 in /opt/mamba/lib/python3.9/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas&gt;=0.24.2-&gt;pynsee) (1.16.0)\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pynsee.download\nPour obtenir des r√©sultats reproductibles, on peut fixer la racine du g√©n√©rateur\npseudo-al√©atoire.\nnp.random.seed(123)\nAu cours de cette d√©monstration des principales fonctionalit√©s de Pandas, et\nlors du chapitre suivant,\nje recommande de se r√©f√©rer r√©guli√®rement aux ressources suivantes :"
  },
  {
    "objectID": "content/manipulation/02a_pandas_tutorial.html#logique-de-pandas",
    "href": "content/manipulation/02a_pandas_tutorial.html#logique-de-pandas",
    "title": "Introduction √† Pandas",
    "section": "Logique de Pandas",
    "text": "Logique de Pandas\nL‚Äôobjet central dans la logique Pandas est le DataFrame.\nIl s‚Äôagit d‚Äôune structure particuli√®re de donn√©es\n√† deux dimensions, structur√©es en alignant des lignes et colonnes.\nContrairement √† une matrice, les colonnes\npeuvent √™tre de types diff√©rents.\nUn DataFrame est compos√© des √©l√©ments suivants :\n\nl‚Äôindice de la ligne ;\nle nom de la colonne ;\nla valeur de la donn√©e ;\n\n\n\n\nStructuration d‚Äôun DataFrame Pandas,\nemprunt√©e √† https://medium.com/epfl-extension-school/selecting-data-from-a-pandas-dataframe-53917dc39953\n\n\nLe concept de tidy data, popularis√© par Hadley Wickham via ses packages R,\nest parfaitement pertinent pour d√©crire la structure d‚Äôun DataFrame pandas.\nLes trois r√®gles sont les suivantes :\n\nChaque variable poss√®de sa propre colonne ;\nChaque observation poss√®de sa propre ligne ;\nUne valeur, mat√©rialisant une observation d‚Äôune variable,\nse trouve sur une unique cellule.\n\n\n\n\nConcept de tidy data (emprunt√© √† H. Wickham)\n\n\nConcernant la syntaxe, une partie des commandes Python est inspir√©e par la logique SQL.\nOn retrouvera ainsi une philosophie proche de celle du SQL o√π on fait des op√©rations\nde s√©lection de ligne ou de colonne. Voici une illustration de quelques manipulations de donn√©es\nque nous mettrons en oeuvre par la suite :\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR√©ordonner le DataFrame\n\n\n\n\nPandas propose √©norm√©ment de fonctionnalit√©s pr√©-impl√©ment√©es.\nIl est vivement recommand√©, avant de se lancer dans l‚Äô√©criture d‚Äôune\nfonction, de se poser la question de son impl√©mentation native dans Numpy, Pandas, etc.\nLa plupart du temps, s‚Äôil existe une solution impl√©ment√©e dans une librairie, il convient\nde l‚Äôutiliser car elle sera plus efficace que celle que vous mettrez en oeuvre."
  },
  {
    "objectID": "content/manipulation/02a_pandas_tutorial.html#les-series",
    "href": "content/manipulation/02a_pandas_tutorial.html#les-series",
    "title": "Introduction √† Pandas",
    "section": "Les Series",
    "text": "Les Series\nEn fait, un DataFrame est une collection d‚Äôobjets appel√©s pandas.Series.\nCes Series sont des objets d‚Äôune dimension qui sont des extensions des\narray-unidimensionnels numpy. En particulier, pour faciliter le traitement\nde donn√©es cat√©gorielles ou temporelles, des types de variables\nsuppl√©mentaires sont disponibles dans pandas par rapport √†\nnumpy (categorical, datetime64 et timedelta64). Ces\ntypes sont associ√©s √† des m√©thodes optimis√©es pour faciliter le traitement\nde ces donn√©es.\nIl ne faut pas n√©gliger l‚Äôattribut dtype d‚Äôun objet\npandas.Series car cela a une influence d√©terminante sur les m√©thodes\net fonctions pouvant √™tre utilis√©es (on ne fait pas les m√™mes op√©rations\nsur une donn√©e temporelle et une donn√©e cat√©gorielle) et le volume en\nm√©moire d‚Äôune variable (le type de la variable d√©termine le volume\nd‚Äôinformation stock√©e pour chaque √©l√©ment ; √™tre trop pr√©cis est parfois\nn√©faste).\nIl existe plusieurs types possibles pour un pandas.Series.\nLe type object correspond aux types Python str ou mixed.\nIl existe un type particulier pour les variables dont le nombre de valeurs\nest une liste finie et relativement courte, le type category.\nIl faut bien examiner les types de son DataFrame, et convertir √©ventuellement\nles types lors de l‚Äô√©tape de data cleaning.\n\nIndexation\nLa diff√©rence essentielle entre une Series et un objet numpy est l‚Äôindexation.\nDans numpy,\nl‚Äôindexation est implicite ; elle permet d‚Äôacc√©der √† une donn√©e (celle √†\nl‚Äôindex situ√© √† la position i).\nAvec une Series, on peut bien s√ªr utiliser un indice de position mais on peut\nsurtout faire appel √† des indices plus explicites.\nPar exemple,\n\ntaille = pd.Series(\n    [1.,1.5,1],\n    index = ['chat', 'chien', 'koala']\n)\n\ntaille.head()\n\nchat     1.0\nchien    1.5\nkoala    1.0\ndtype: float64\n\n\nCette indexation permet d‚Äôacc√©der √† des valeurs de la Series\nvia une valeur de l‚Äôindice. Par\nexemple, taille['koala']:\n\ntaille['koala']\n\n1.0\n\n\nL‚Äôexistence d‚Äôindice rend le subsetting particuli√®rement ais√©, ce que vous\npouvez exp√©rimenter dans les TP :\n\n\n\n\n\n\n\n\n\n\n\n\n\nPour transformer un objet pandas.Series en array numpy,\non utilise la m√©thode values. Par exemple, taille.values:\n\ntaille.values\n\narray([1. , 1.5, 1. ])\n\n\nUn avantage des Series par rapport √† un array numpy est que\nles op√©rations sur les Series alignent\nautomatiquement les donn√©es √† partir des labels.\nAvec des Series lab√©lis√©es, il n‚Äôest ainsi pas n√©cessaire\nde se poser la question de l‚Äôordre des lignes.\nL‚Äôexemple dans la partie suivante permettra de s‚Äôen assurer.\n\n\nValeurs manquantes\nPar d√©faut, les valeurs manquantes sont affich√©es NaN et sont de type np.nan (pour\nles valeurs temporelles, i.e.¬†de type datatime64, les valeurs manquantes sont\nNaT).\nOn a un comportement coh√©rent d‚Äôagr√©gation lorsqu‚Äôon combine deux DataFrames (ou deux colonnes).\nPar exemple,\n\nx = pd.DataFrame(\n    {'prix': np.random.uniform(size = 5),\n     'quantite': [i+1 for i in range(5)]\n    },\n    index = ['yaourt','pates','riz','tomates','gateaux']\n)\nx\n\n\n\n\n\n\n\n\nprix\nquantite\n\n\n\n\nyaourt\n0.696469\n1\n\n\npates\n0.286139\n2\n\n\nriz\n0.226851\n3\n\n\ntomates\n0.551315\n4\n\n\ngateaux\n0.719469\n5\n\n\n\n\n\n\n\n\ny = pd.DataFrame(\n    {'prix': [np.nan, 0, 1, 2, 3],\n     'quantite': [i+1 for i in range(5)]\n    },\n    index = ['tomates','yaourt','gateaux','pates','riz']\n)\ny\n\n\n\n\n\n\n\n\nprix\nquantite\n\n\n\n\ntomates\nNaN\n1\n\n\nyaourt\n0.0\n2\n\n\ngateaux\n1.0\n3\n\n\npates\n2.0\n4\n\n\nriz\n3.0\n5\n\n\n\n\n\n\n\n\nx + y\n\n\n\n\n\n\n\n\nprix\nquantite\n\n\n\n\ngateaux\n1.719469\n8\n\n\npates\n2.286139\n6\n\n\nriz\n3.226851\n8\n\n\ntomates\nNaN\n5\n\n\nyaourt\n0.696469\n3\n\n\n\n\n\n\n\ndonne bien une valeur manquante pour la ligne tomates. Au passage, on peut remarquer que l‚Äôagr√©gation\na tenu compte des index.\nIl est possible de supprimer les valeurs manquantes gr√¢ce √† dropna().\nCette m√©thode va supprimer toutes les lignes o√π il y a au moins une valeur manquante.\nIl est aussi possible de supprimer seulement les colonnes o√π il y a des valeurs manquantes\ndans un DataFrame avec dropna() avec le param√®tre axis=1 (par d√©faut √©gal √† 0).\nIl est √©galement possible de remplir les valeurs manquantes gr√¢ce √† la m√©thode fillna()."
  },
  {
    "objectID": "content/manipulation/02a_pandas_tutorial.html#le-dataframe-pandas",
    "href": "content/manipulation/02a_pandas_tutorial.html#le-dataframe-pandas",
    "title": "Introduction √† Pandas",
    "section": "Le DataFrame Pandas",
    "text": "Le DataFrame Pandas\nLe DataFrame est l‚Äôobjet central de la librairie pandas.\nIl s‚Äôagit d‚Äôune collection de pandas.Series (colonnes) align√©es par les index.\nLes types des variables peuvent diff√©rer.\nUn DataFrame non-index√© a la structure suivante :\n\ndf = pd.DataFrame(\n    {'taille': [1.,1.5,1],\n    'poids' : [3, 5, 2.5]\n    },\n    index = ['chat', 'chien', 'koala']\n)\ndf.reset_index()\n\n\n\n\n\n\n\n\nindex\ntaille\npoids\n\n\n\n\n0\nchat\n1.0\n3.0\n\n\n1\nchien\n1.5\n5.0\n\n\n2\nkoala\n1.0\n2.5\n\n\n\n\n\n\n\nAlors que le m√™me DataFrame index√© aura la structure suivante :\n\ndf = pd.DataFrame(\n    {'taille': [1.,1.5,1],\n    'poids' : [3, 5, 2.5]\n    },\n    index = ['chat', 'chien', 'koala']\n)\ndf.head()\n\n\n\n\n\n\n\n\ntaille\npoids\n\n\n\n\nchat\n1.0\n3.0\n\n\nchien\n1.5\n5.0\n\n\nkoala\n1.0\n2.5\n\n\n\n\n\n\n\n\nLes attributs et m√©thodes utiles\nPour pr√©senter les m√©thodes les plus pratiques pour l‚Äôanalyse de donn√©es,\non peut partir de l‚Äôexemple des consommations de CO2 communales issues\ndes donn√©es de l‚ÄôAdeme. Cette base de donn√©es est exploit√©e plus intens√©ment\ndans le TP.\n\n\n\n\n\n\n\n\n\n\n\n\n\nL‚Äôimport de donn√©es depuis un fichier plat se fait avec la fonction read_csv:\n\ndf = pd.read_csv(\"https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert\")\ndf\n\n\n\n\n\n\n\n\nINSEE commune\nCommune\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nD√©chets\nEnergie\nIndustrie hors-√©nergie\nR√©sidentiel\nRoutier\nTertiaire\n\n\n\n\n0\n01001\nL'ABERGEMENT-CLEMENCIAT\n3711.425991\nNaN\nNaN\n432.751835\n101.430476\n2.354558\n6.911213\n309.358195\n793.156501\n367.036172\n\n\n1\n01002\nL'ABERGEMENT-DE-VAREY\n475.330205\nNaN\nNaN\n140.741660\n140.675439\n2.354558\n6.911213\n104.866444\n348.997893\n112.934207\n\n\n2\n01004\nAMBERIEU-EN-BUGEY\n499.043526\n212.577908\nNaN\n10313.446515\n5314.314445\n998.332482\n2930.354461\n16616.822534\n15642.420313\n10732.376934\n\n\n3\n01005\nAMBERIEUX-EN-DOMBES\n1859.160954\nNaN\nNaN\n1144.429311\n216.217508\n94.182310\n276.448534\n663.683146\n1756.341319\n782.404357\n\n\n4\n01006\nAMBLEON\n448.966808\nNaN\nNaN\n77.033834\n48.401549\nNaN\nNaN\n43.714019\n398.786800\n51.681756\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n35793\n95676\nVILLERS-EN-ARTHIES\n1628.065094\nNaN\nNaN\n165.045396\n65.063617\n11.772789\n34.556067\n176.098160\n309.627908\n235.439109\n\n\n35794\n95678\nVILLIERS-ADAM\n698.630772\nNaN\nNaN\n1331.126598\n111.480954\n2.354558\n6.911213\n1395.529811\n18759.370071\n403.404815\n\n\n35795\n95680\nVILLIERS-LE-BEL\n107.564967\nNaN\nNaN\n8367.174532\n225.622903\n534.484607\n1568.845431\n22613.830247\n12217.122402\n13849.512001\n\n\n35796\n95682\nVILLIERS-LE-SEC\n1090.890170\nNaN\nNaN\n326.748418\n108.969749\n2.354558\n6.911213\n67.235487\n4663.232127\n85.657725\n\n\n35797\n95690\nWY-DIT-JOLI-VILLAGE\n1495.103542\nNaN\nNaN\n125.236417\n97.728612\n4.709115\n13.822427\n117.450851\n504.400972\n147.867245\n\n\n\n\n35798 rows √ó 12 columns\n\n\n\n\n\n Note\nDans un processus de production, o√π normalement on connait les types des variables du DataFrame qu‚Äôon va importer,\nil convient de pr√©ciser les types avec lesquels on souhaite importer les donn√©es\n(argument dtype, sous la forme d‚Äôun dictionnaire).\nCela est particuli√®rement important lorsqu‚Äôon d√©sire utiliser une colonne\ncomme une variable textuelle mais qu‚Äôelle comporte des attributs proches d‚Äôun nombre\nqui vont inciter pandas √† l‚Äôimporter sous forme de variable num√©rique.\nPar exemple, une colonne [00001,00002,...] risque d‚Äô√™tre import√©e comme une variable num√©rique, ignorant l‚Äôinformation des premiers 0 (qui peuvent pourtant la distinguer de la s√©quence 1, 2, etc.). Pour s‚Äôassurer que pandas importe sous forme textuelle la variable, on peut utiliser dtype = {\"code\": \"str\"}\nSinon, on peut importer le csv, et modifier les types avec astype().\nAvec astype, on peut g√©rer les erreurs de conversion avec le param√®tre errors.\n\n\nL‚Äôaffichage des DataFrames est tr√®s ergonomique. On obtiendrait le m√™me output\navec display(df). Les premi√®res et derni√®res lignes s‚Äôaffichent\nautomatiquement. Autrement, on peut aussi faire:\n\nhead qui permet, comme son\nnom l‚Äôindique, de n‚Äôafficher que les premi√®res lignes ;\ntail qui permet, comme son\nnom l‚Äôindique, de n‚Äôafficher que les derni√®res lignes\nsample qui permet d‚Äôafficher un √©chantillon al√©atoire de n lignes.\nCette m√©thode propose de nombreuses options.\n\n\n\n Warning\nIl faut faire attention au display et aux\ncommandes qui r√©v√®lent des donn√©es (head, tail, etc.)\ndans un Notebook ou un Markdown qui exploite\ndes donn√©es confidentielles lorsqu‚Äôon utilise Git.\nEn effet, on peut se\nretrouver √† partager des donn√©es, involontairement, dans l‚Äôhistorique\nGit. Avec un R Markdown, il suffit d‚Äôajouter les sorties au fichier\n.gitignore (par exemple avec une balise de type *.html). Avec un\nNotebook Jupyter, la d√©marche est plus compliqu√©e car les fichiers\n.ipynb int√®grent dans le m√™me document, texte, sorties et mise en forme.\nTechniquement, il est possible d‚Äôappliquer des filtres avec Git\n(voir\nici)\nmais c‚Äôest une d√©marche tr√®s complexe.\nCe post de l‚Äô√©quipe √† l‚Äôorigine de nbdev2\nr√©sume bien le probl√®me du contr√¥le de version avec Git et des solutions qui\npeuvent y √™tre apport√©es.\nUne solution est d‚Äôutiliser Quarto qui permet de g√©n√©rer les\n.ipynb en output d‚Äôun document texte, ce qui facilite le contr√¥le sur les\n√©l√©ments pr√©sents dans le document.\n\n\n\n\nDimensions et structure du DataFrame\nLes premi√®res m√©thodes utiles permettent d‚Äôafficher quelques\nattributs d‚Äôun DataFrame.\n\ndf.axes\n\n[RangeIndex(start=0, stop=35798, step=1),\n Index(['INSEE commune', 'Commune', 'Agriculture', 'Autres transports',\n        'Autres transports international', 'CO2 biomasse hors-total', 'D√©chets',\n        'Energie', 'Industrie hors-√©nergie', 'R√©sidentiel', 'Routier',\n        'Tertiaire'],\n       dtype='object')]\n\n\n\ndf.columns\n\nIndex(['INSEE commune', 'Commune', 'Agriculture', 'Autres transports',\n       'Autres transports international', 'CO2 biomasse hors-total', 'D√©chets',\n       'Energie', 'Industrie hors-√©nergie', 'R√©sidentiel', 'Routier',\n       'Tertiaire'],\n      dtype='object')\n\n\n\ndf.index\n\nRangeIndex(start=0, stop=35798, step=1)\n\n\nPour conna√Ætre les dimensions d‚Äôun DataFrame, on peut utiliser quelques m√©thodes\npratiques :\n\ndf.ndim\n\n2\n\n\n\ndf.shape\n\n(35798, 12)\n\n\n\ndf.size\n\n429576\n\n\nPour d√©terminer le nombre de valeurs uniques d‚Äôune variable, plut√¥t que chercher √† √©crire soi-m√™me une fonction,\non utilise la\nm√©thode nunique. Par exemple,\n\ndf['Commune'].nunique()\n\n33338\n\n\npandas propose √©norm√©ment de m√©thodes utiles.\nVoici un premier r√©sum√©, accompagn√© d‚Äôun comparatif avec R :\n\n\n\n\n\n\n\n\n\nOp√©ration\npandas\ndplyr (R)\ndata.table (R)\n\n\n\n\nR√©cup√©rer le nom des colonnes\ndf.columns\ncolnames(df)\ncolnames(df)\n\n\nR√©cup√©rer les indices\ndf.index\n\nunique(df[,get(key(df))])\n\n\nR√©cup√©rer les dimensions\ndf.shape\ndim(df)\ndim(df)\n\n\nR√©cup√©rer le nombre de valeurs uniques d‚Äôune variable\ndf['myvar'].nunique()\ndf %&gt;%  summarise(distinct(myvar))\ndf[,uniqueN(myvar)]\n\n\n\n\n\nStatistiques agr√©g√©es\npandas propose une s√©rie de m√©thodes pour faire des statistiques\nagr√©g√©es de mani√®re efficace.\nOn peut, par exemple, appliquer des m√©thodes pour compter le nombre de lignes,\nfaire une moyenne ou une somme de l‚Äôensemble des lignes\n\ndf.count()\n\nINSEE commune                      35798\nCommune                            35798\nAgriculture                        35736\nAutres transports                   9979\nAutres transports international     2891\nCO2 biomasse hors-total            35798\nD√©chets                            35792\nEnergie                            34490\nIndustrie hors-√©nergie             34490\nR√©sidentiel                        35792\nRoutier                            35778\nTertiaire                          35798\ndtype: int64\n\n\n\ndf.mean(numeric_only = True)\n\nAgriculture                        2459.975760\nAutres transports                   654.919940\nAutres transports international    7692.344960\nCO2 biomasse hors-total            1774.381550\nD√©chets                             410.806329\nEnergie                             662.569846\nIndustrie hors-√©nergie             2423.127789\nR√©sidentiel                        1783.677872\nRoutier                            3535.501245\nTertiaire                          1105.165915\ndtype: float64\n\n\n\ndf.sum(numeric_only = True)\n\nAgriculture                        8.790969e+07\nAutres transports                  6.535446e+06\nAutres transports international    2.223857e+07\nCO2 biomasse hors-total            6.351931e+07\nD√©chets                            1.470358e+07\nEnergie                            2.285203e+07\nIndustrie hors-√©nergie             8.357368e+07\nR√©sidentiel                        6.384140e+07\nRoutier                            1.264932e+08\nTertiaire                          3.956273e+07\ndtype: float64\n\n\n\ndf.nunique()\n\nINSEE commune                      35798\nCommune                            33338\nAgriculture                        35576\nAutres transports                   9963\nAutres transports international     2883\nCO2 biomasse hors-total            35798\nD√©chets                            11016\nEnergie                             1453\nIndustrie hors-√©nergie              1889\nR√©sidentiel                        35791\nRoutier                            35749\nTertiaire                           8663\ndtype: int64\n\n\n\ndf.quantile(q = [0.1,0.25,0.5,0.75,0.9], numeric_only = True)\n\n\n\n\n\n\n\n\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nD√©chets\nEnergie\nIndustrie hors-√©nergie\nR√©sidentiel\nRoutier\nTertiaire\n\n\n\n\n0.10\n382.620882\n25.034578\n4.430792\n109.152816\n14.811230\n2.354558\n6.911213\n50.180933\n199.765410\n49.289082\n\n\n0.25\n797.682631\n52.560412\n10.050967\n197.951108\n25.655166\n2.354558\n6.911213\n96.052911\n419.700460\n94.749885\n\n\n0.50\n1559.381285\n106.795928\n19.924343\n424.849988\n54.748653\n4.709115\n13.822427\n227.091193\n1070.895593\n216.297718\n\n\n0.75\n3007.883903\n237.341501\n32.983111\n1094.749825\n110.820941\n51.800270\n152.046694\n749.469293\n3098.612157\n576.155869\n\n\n0.90\n5442.727470\n528.349529\n59.999169\n3143.759029\n190.695774\n367.311008\n1154.172630\n2937.699671\n8151.047248\n1897.732565\n\n\n\n\n\n\n\n\n\n Warning\nLa version 2.0 de Pandas a introduit un changement\nde comportement dans les m√©thodes d‚Äôagr√©gation.\nIl est dor√©navant n√©cessaire de pr√©ciser quand on d√©sire\neffectuer des op√©rations si on d√©sire ou non le faire\nexclusivement sur les colonnes num√©riques. C‚Äôest pour cette\nraison qu‚Äôon exlicite ici l‚Äôargument numeric_only = True.\nCe comportement\n√©tait par le pass√© implicite.\n\n\nIl faut toujours regarder les options de ces fonctions en termes de valeurs manquantes, car\nces options sont d√©terminantes dans le r√©sultat obtenu.\nLes exercices de TD visent √† d√©montrer l‚Äôint√©r√™t de ces m√©thodes dans quelques cas pr√©cis.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLe tableau suivant r√©capitule le code √©quivalent pour avoir des\nstatistiques sur toutes les colonnes d‚Äôun dataframe en R.\n\n\n\n\n\n\n\n\n\nOp√©ration\npandas\ndplyr (R)\ndata.table (R)\n\n\n\n\nNombre de valeurs non manquantes\ndf.count()\ndf %&gt;% summarise_each(funs(sum(!is.na(.))))\ndf[, lapply(.SD, function(x) sum(!is.na(x)))]\n\n\nMoyenne de toutes les variables\ndf.mean()\ndf %&gt;% summarise_each(funs(mean((., na.rm = TRUE))))\ndf[,lapply(.SD, function(x) mean(x, na.rm = TRUE))]\n\n\n\nLa m√©thode describe permet de sortir un tableau de statistiques\nagr√©g√©es :\n\ndf.describe()\n\n\n\n\n\n\n\n\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nD√©chets\nEnergie\nIndustrie hors-√©nergie\nR√©sidentiel\nRoutier\nTertiaire\n\n\n\n\ncount\n35736.000000\n9979.000000\n2.891000e+03\n35798.000000\n35792.000000\n3.449000e+04\n3.449000e+04\n35792.000000\n35778.000000\n35798.000000\n\n\nmean\n2459.975760\n654.919940\n7.692345e+03\n1774.381550\n410.806329\n6.625698e+02\n2.423128e+03\n1783.677872\n3535.501245\n1105.165915\n\n\nstd\n2926.957701\n9232.816833\n1.137643e+05\n7871.341922\n4122.472608\n2.645571e+04\n5.670374e+04\n8915.902379\n9663.156628\n5164.182507\n\n\nmin\n0.003432\n0.000204\n3.972950e-04\n3.758088\n0.132243\n2.354558e+00\n1.052998e+00\n1.027266\n0.555092\n0.000000\n\n\n25%\n797.682631\n52.560412\n1.005097e+01\n197.951108\n25.655166\n2.354558e+00\n6.911213e+00\n96.052911\n419.700460\n94.749885\n\n\n50%\n1559.381285\n106.795928\n1.992434e+01\n424.849988\n54.748653\n4.709115e+00\n1.382243e+01\n227.091193\n1070.895593\n216.297718\n\n\n75%\n3007.883903\n237.341501\n3.298311e+01\n1094.749825\n110.820941\n5.180027e+01\n1.520467e+02\n749.469293\n3098.612157\n576.155869\n\n\nmax\n98949.317760\n513140.971691\n3.303394e+06\n576394.181208\n275500.374439\n2.535858e+06\n6.765119e+06\n410675.902028\n586054.672836\n288175.400126\n\n\n\n\n\n\n\n\n\nM√©thodes relatives aux valeurs manquantes\nLes m√©thodes relatives aux valeurs manquantes peuvent √™tre mobilis√©es\nen conjonction des m√©thodes de statistiques agr√©g√©es. C‚Äôest utile lorsqu‚Äôon\nd√©sire obtenir une id√©e de la part de valeurs manquantes dans un jeu de\ndonn√©es\n\ndf.isnull().sum()\n\nINSEE commune                          0\nCommune                                0\nAgriculture                           62\nAutres transports                  25819\nAutres transports international    32907\nCO2 biomasse hors-total                0\nD√©chets                                6\nEnergie                             1308\nIndustrie hors-√©nergie              1308\nR√©sidentiel                            6\nRoutier                               20\nTertiaire                              0\ndtype: int64\n\n\nOn trouvera aussi la r√©f√©rence √† isna() qui est la m√™me m√©thode que isnull()."
  },
  {
    "objectID": "content/manipulation/02a_pandas_tutorial.html#graphiques-rapides",
    "href": "content/manipulation/02a_pandas_tutorial.html#graphiques-rapides",
    "title": "Introduction √† Pandas",
    "section": "Graphiques rapides",
    "text": "Graphiques rapides\nLes m√©thodes par d√©faut de graphiques\n(approfondies dans la partie visualisation)\nsont pratiques pour\nproduire rapidement un graphique, notamment apr√®s des op√©rations\ncomplexes de maniement de donn√©es.\nEn effet, on peut appliquer la m√©thode plot() directement √† une pandas.Series :\n\ndf['D√©chets'].plot()\ndf['D√©chets'].hist()\ndf['D√©chets'].plot(kind = 'hist', logy = True)\n\n\nplt.figure()\nfig = df['D√©chets'].plot()\nfig\n#plt.savefig('plot_base.png', bbox_inches='tight')\n\nplt.figure()\nfig = df['D√©chets'].hist()\nfig\n#plt.savefig('plot_hist.png', bbox_inches='tight')\n\nplt.figure()\nfig = df['D√©chets'].plot(kind = 'hist', logy = True)\nfig\n#plt.show()\n#plt.savefig('plot_hist_log.png', bbox_inches='tight')\n\n&lt;Axes: ylabel='Frequency'&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa sortie est un objet matplotlib. La customisation de ces\nfigures est ainsi\npossible (et m√™me d√©sirable car les graphiques matplotlib\nsont, par d√©faut, assez rudimentaires), nous en verrons quelques exemples."
  },
  {
    "objectID": "content/manipulation/02a_pandas_tutorial.html#acc√©der-√†-des-√©l√©ments-dun-dataframe",
    "href": "content/manipulation/02a_pandas_tutorial.html#acc√©der-√†-des-√©l√©ments-dun-dataframe",
    "title": "Introduction √† Pandas",
    "section": "Acc√©der √† des √©l√©ments d‚Äôun DataFrame",
    "text": "Acc√©der √† des √©l√©ments d‚Äôun DataFrame\n\nS√©lectionner des colonnes\nEn SQL, effectuer des op√©rations sur les colonnes se fait avec la commande\nSELECT. Avec pandas,\npour acc√©der √† une colonne dans son ensemble on peut\nutiliser plusieurs approches :\n\ndataframe.variable, par exemple df.Energie.\nCette m√©thode requiert n√©anmoins d‚Äôavoir des\nnoms de colonnes sans espace.\ndataframe[['variable']] pour renvoyer la variable sous\nforme de DataFrame ou dataframe['variable'] pour\nla renvoyer sous forme de Series. Par exemple, df[['Autres transports']]\nou df['Autres transports']. C‚Äôest une mani√®re pr√©f√©rable de proc√©der.\n\n\n\nAcc√©der √† des lignes\nPour acc√©der √† une ou plusieurs valeurs d‚Äôun DataFrame,\nil existe deux mani√®res conseill√©es de proc√©der, selon la\nforme des indices de lignes ou colonnes utilis√©s:\n\ndf.loc : utilise les labels\ndf.iloc : utilise les indices\n\n\n\n Warning\nLes bouts de code utilisant la structure df.ix\nsont √† bannir car la fonction est deprecated et peut\nainsi dispara√Ætre √† tout moment.\n\n\niloc va se r√©f√©rer √† l‚Äôindexation de 0 √† N o√π N est √©gal √† df.shape[0] d‚Äôun\npandas.DataFrame. loc va se r√©f√©rer aux valeurs de l‚Äôindex\nde df.\nPar exemple, avec le pandas.DataFrame df_example:\n\ndf_example = pd.DataFrame(\n    {'month': [1, 4, 7, 10], 'year': [2012, 2014, 2013, 2014], 'sale': [55, 40, 84, 31]})\ndf_example = df_example.set_index('month')\ndf_example\n\n\n\n\n\n\n\n\nyear\nsale\n\n\nmonth\n\n\n\n\n\n\n1\n2012\n55\n\n\n4\n2014\n40\n\n\n7\n2013\n84\n\n\n10\n2014\n31\n\n\n\n\n\n\n\n\ndf_example.loc[1, :] donnera la premi√®re ligne de df (ligne o√π l‚Äôindice month est √©gal √† 1) ;\ndf_example.iloc[1, :] donnera la deuxi√®me ligne (puisque l‚Äôindexation en Python commence √† 0) ;\ndf_example.iloc[:, 1] donnera la deuxi√®me colonne, suivant le m√™me principe."
  },
  {
    "objectID": "content/manipulation/02a_pandas_tutorial.html#principales-manipulation-de-donn√©es",
    "href": "content/manipulation/02a_pandas_tutorial.html#principales-manipulation-de-donn√©es",
    "title": "Introduction √† Pandas",
    "section": "Principales manipulation de donn√©es",
    "text": "Principales manipulation de donn√©es\nL‚Äôobjectif du TP pandas est de se familiariser plus avec ces\ncommandes √† travers l‚Äôexemple des donn√©es des √©missions de C02.\nLes op√©rations les plus fr√©quentes en SQL sont r√©sum√©es par le tableau suivant.\nIl est utile de les conna√Ætre (beaucoup de syntaxes de maniement de donn√©es\nreprennent ces termes) car, d‚Äôune\nmani√®re ou d‚Äôune autre, elles couvrent la plupart\ndes usages de manipulation des donn√©es\n\n\n\n\n\n\n\n\n\n\nOp√©ration\nSQL\npandas\ndplyr (R)\ndata.table (R)\n\n\n\n\nS√©lectionner des variables par leur nom\nSELECT\ndf[['Autres transports','Energie']]\ndf %&gt;% select(Autres transports, Energie)\ndf[, c('Autres transports','Energie')]\n\n\nS√©lectionner des observations selon une ou plusieurs conditions;\nFILTER\ndf[df['Agriculture']&gt;2000]\ndf %&gt;% filter(Agriculture&gt;2000)\ndf[Agriculture&gt;2000]\n\n\nTrier la table selon une ou plusieurs variables\nSORT BY\ndf.sort_values(['Commune','Agriculture'])\ndf %&gt;% arrange(Commune, Agriculture)\ndf[order(Commune, Agriculture)]\n\n\nAjouter des variables qui sont fonction d‚Äôautres variables;\nSELECT *, LOG(Agriculture) AS x FROM df\ndf['x'] = np.log(df['Agriculture'])\ndf %&gt;% mutate(x = log(Agriculture))\ndf[,x := log(Agriculture)]\n\n\nEffectuer une op√©ration par groupe\nGROUP BY\ndf.groupby('Commune').mean()\ndf %&gt;% group_by(Commune) %&gt;% summarise(m = mean)\ndf[,mean(Commune), by = Commune]\n\n\nJoindre deux bases de donn√©es (inner join)\nSELECT * FROM table1 INNER JOIN table2 ON table1.id = table2.x\ntable1.merge(table2, left_on = 'id', right_on = 'x')\ntable1 %&gt;% inner_join(table2, by = c('id'='x'))\nmerge(table1, table2, by.x = 'id', by.y = 'x')\n\n\n\n\nOp√©rations sur les colonnes : select, mutate, drop\nLes DataFrames pandas sont des objets mutables en langage Python,\nc‚Äôest-√†-dire qu‚Äôil est possible de faire √©voluer le DataFrame au gr√®s\ndes op√©rations. L‚Äôop√©ration la plus classique consiste √† ajouter ou retirer\ndes variables √† la table de donn√©es.\n\ndf_new = df.copy()\n\n\n\n Warning\nAttention au comportement de pandas lorsqu‚Äôon cr√©e une duplication\nd‚Äôun DataFrame.\nPar d√©faut, pandas effectue une copie par r√©f√©rence. Dans ce\ncas, les deux objets (la copie et l‚Äôobjet copi√©) restent reli√©s. Les colonnes\ncr√©√©es sur l‚Äôun vont √™tre r√©percut√©es sur l‚Äôautre. Ce comportement permet de\nlimiter l‚Äôinflation en m√©moire de Python. En faisant √ßa, le deuxi√®me\nobjet prend le m√™me espace m√©moire que le premier. Le package data.table\nen R adopte le m√™me comportement, contrairement √† dplyr.\nCela peut amener √† quelques surprises si ce comportement d‚Äôoptimisation\nn‚Äôest pas anticip√©. Si vous voulez, par s√©curit√©, conserver intact le\npremier DataFrame, faites appel √† une copie profonde (deep copy) en\nutilisant la m√©thode copy, comme ci-dessus.\nAttention toutefois, cela a un co√ªt m√©moire.\nAvec des donn√©es volumineuses, c‚Äôest une pratique √† utiliser avec pr√©caution.\n\n\nLa mani√®re la plus simple d‚Äôop√©rer pour ajouter des colonnes est\nd‚Äôutiliser la r√©assignation. Par exemple, pour cr√©er une variable\nx qui est le log de la\nvariable Agriculture:\n\ndf_new['x'] = np.log(df_new['Agriculture'])\n\nIl est possible d‚Äôappliquer cette approche sur plusieurs colonnes. Un des\nint√©r√™ts de cette approche est qu‚Äôelle permet de recycler le nom de colonnes.\n\nvars = ['Agriculture', 'D√©chets', 'Energie']\n\ndf_new[[v + \"_log\" for v in vars]] = np.log(df_new[vars])\ndf_new\n\n\n\n\n\n\n\n\nINSEE commune\nCommune\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nD√©chets\nEnergie\nIndustrie hors-√©nergie\nR√©sidentiel\nRoutier\nTertiaire\nx\nAgriculture_log\nD√©chets_log\nEnergie_log\n\n\n\n\n0\n01001\nL'ABERGEMENT-CLEMENCIAT\n3711.425991\nNaN\nNaN\n432.751835\n101.430476\n2.354558\n6.911213\n309.358195\n793.156501\n367.036172\n8.219171\n8.219171\n4.619374\n0.856353\n\n\n1\n01002\nL'ABERGEMENT-DE-VAREY\n475.330205\nNaN\nNaN\n140.741660\n140.675439\n2.354558\n6.911213\n104.866444\n348.997893\n112.934207\n6.164010\n6.164010\n4.946455\n0.856353\n\n\n2\n01004\nAMBERIEU-EN-BUGEY\n499.043526\n212.577908\nNaN\n10313.446515\n5314.314445\n998.332482\n2930.354461\n16616.822534\n15642.420313\n10732.376934\n6.212693\n6.212693\n8.578159\n6.906086\n\n\n3\n01005\nAMBERIEUX-EN-DOMBES\n1859.160954\nNaN\nNaN\n1144.429311\n216.217508\n94.182310\n276.448534\n663.683146\n1756.341319\n782.404357\n7.527881\n7.527881\n5.376285\n4.545232\n\n\n4\n01006\nAMBLEON\n448.966808\nNaN\nNaN\n77.033834\n48.401549\nNaN\nNaN\n43.714019\n398.786800\n51.681756\n6.106949\n6.106949\n3.879532\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n35793\n95676\nVILLERS-EN-ARTHIES\n1628.065094\nNaN\nNaN\n165.045396\n65.063617\n11.772789\n34.556067\n176.098160\n309.627908\n235.439109\n7.395148\n7.395148\n4.175366\n2.465791\n\n\n35794\n95678\nVILLIERS-ADAM\n698.630772\nNaN\nNaN\n1331.126598\n111.480954\n2.354558\n6.911213\n1395.529811\n18759.370071\n403.404815\n6.549122\n6.549122\n4.713854\n0.856353\n\n\n35795\n95680\nVILLIERS-LE-BEL\n107.564967\nNaN\nNaN\n8367.174532\n225.622903\n534.484607\n1568.845431\n22613.830247\n12217.122402\n13849.512001\n4.678095\n4.678095\n5.418865\n6.281303\n\n\n35796\n95682\nVILLIERS-LE-SEC\n1090.890170\nNaN\nNaN\n326.748418\n108.969749\n2.354558\n6.911213\n67.235487\n4663.232127\n85.657725\n6.994749\n6.994749\n4.691070\n0.856353\n\n\n35797\n95690\nWY-DIT-JOLI-VILLAGE\n1495.103542\nNaN\nNaN\n125.236417\n97.728612\n4.709115\n13.822427\n117.450851\n504.400972\n147.867245\n7.309951\n7.309951\n4.582194\n1.549500\n\n\n\n\n35798 rows √ó 16 columns\n\n\n\nIl est √©galement possible d‚Äôutiliser la m√©thode assign. Pour des op√©rations\nvectoris√©es, comme le sont les op√©rateurs de numpy, cela n‚Äôa pas d‚Äôint√©r√™t.\nCela permet notamment d‚Äôenchainer les op√©rations sur un m√™me DataFrame (notamment gr√¢ce au pipe que\nnous verrons plus loin).\nCette approche utilise g√©n√©ralement\ndes lambda functions. Par exemple le code pr√©c√©dent (celui concernant une\nseule variable) prendrait la forme:\n\ndf_new.assign(Energie_log = lambda x: np.log(x['Energie']))\n\n\n\n\n\n\n\n\nINSEE commune\nCommune\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nD√©chets\nEnergie\nIndustrie hors-√©nergie\nR√©sidentiel\nRoutier\nTertiaire\nx\nAgriculture_log\nD√©chets_log\nEnergie_log\n\n\n\n\n0\n01001\nL'ABERGEMENT-CLEMENCIAT\n3711.425991\nNaN\nNaN\n432.751835\n101.430476\n2.354558\n6.911213\n309.358195\n793.156501\n367.036172\n8.219171\n8.219171\n4.619374\n0.856353\n\n\n1\n01002\nL'ABERGEMENT-DE-VAREY\n475.330205\nNaN\nNaN\n140.741660\n140.675439\n2.354558\n6.911213\n104.866444\n348.997893\n112.934207\n6.164010\n6.164010\n4.946455\n0.856353\n\n\n2\n01004\nAMBERIEU-EN-BUGEY\n499.043526\n212.577908\nNaN\n10313.446515\n5314.314445\n998.332482\n2930.354461\n16616.822534\n15642.420313\n10732.376934\n6.212693\n6.212693\n8.578159\n6.906086\n\n\n3\n01005\nAMBERIEUX-EN-DOMBES\n1859.160954\nNaN\nNaN\n1144.429311\n216.217508\n94.182310\n276.448534\n663.683146\n1756.341319\n782.404357\n7.527881\n7.527881\n5.376285\n4.545232\n\n\n4\n01006\nAMBLEON\n448.966808\nNaN\nNaN\n77.033834\n48.401549\nNaN\nNaN\n43.714019\n398.786800\n51.681756\n6.106949\n6.106949\n3.879532\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n35793\n95676\nVILLERS-EN-ARTHIES\n1628.065094\nNaN\nNaN\n165.045396\n65.063617\n11.772789\n34.556067\n176.098160\n309.627908\n235.439109\n7.395148\n7.395148\n4.175366\n2.465791\n\n\n35794\n95678\nVILLIERS-ADAM\n698.630772\nNaN\nNaN\n1331.126598\n111.480954\n2.354558\n6.911213\n1395.529811\n18759.370071\n403.404815\n6.549122\n6.549122\n4.713854\n0.856353\n\n\n35795\n95680\nVILLIERS-LE-BEL\n107.564967\nNaN\nNaN\n8367.174532\n225.622903\n534.484607\n1568.845431\n22613.830247\n12217.122402\n13849.512001\n4.678095\n4.678095\n5.418865\n6.281303\n\n\n35796\n95682\nVILLIERS-LE-SEC\n1090.890170\nNaN\nNaN\n326.748418\n108.969749\n2.354558\n6.911213\n67.235487\n4663.232127\n85.657725\n6.994749\n6.994749\n4.691070\n0.856353\n\n\n35797\n95690\nWY-DIT-JOLI-VILLAGE\n1495.103542\nNaN\nNaN\n125.236417\n97.728612\n4.709115\n13.822427\n117.450851\n504.400972\n147.867245\n7.309951\n7.309951\n4.582194\n1.549500\n\n\n\n\n35798 rows √ó 16 columns\n\n\n\nDans les m√©thodes suivantes, il est possible de modifier le pandas.DataFrame\nen place, c‚Äôest √† dire en ne le r√©assignant pas, avec le param√®tre inplace = True.\nPar d√©faut, inplace est √©gal √† False et pour modifier le pandas.DataFrame,\nil convient de le r√©assigner.\nOn peut facilement renommer des variables avec la m√©thode rename qui\nfonctionne bien avec des dictionnaires (pour renommer des colonnes il faut\npr√©ciser le param√®tre axis = 1):\n\ndf_new = df_new.rename({\"Energie\": \"eneg\", \"Agriculture\": \"agr\"}, axis=1)\n\nEnfin, pour effacer des colonnes, on utilise la m√©thode drop avec l‚Äôargument\ncolumns:\n\ndf_new = df_new.drop(columns = [\"eneg\", \"agr\"])\n\n\n\nR√©ordonner\nLa m√©thode sort_values permet de r√©ordonner un DataFrame. Par exemple,\nsi on d√©sire classer par ordre d√©croissant de consommation de CO2 du secteur\nr√©sidentiel, on fera\n\ndf = df.sort_values(\"R√©sidentiel\", ascending = False)\n\nAinsi, en une ligne de code, on identifie les villes o√π le secteur\nr√©sidentiel consomme le plus.\n\n\nFiltrer\nL‚Äôop√©ration de s√©lection de lignes s‚Äôappelle FILTER en SQL. Elle s‚Äôutilise\nen fonction d‚Äôune condition logique (clause WHERE). On s√©lectionne les\ndonn√©es sur une condition logique. Il existe plusieurs m√©thodes en pandas.\nLa plus simple est d‚Äôutiliser les boolean mask, d√©j√† vus dans le chapitre\nnumpy.\nPar exemple, pour s√©lectionner les communes dans les Hauts-de-Seine, on\npeut utiliser le r√©sultat de la m√©thode str.startswith (qui renvoie\nTrue ou False) directement dans les crochets:\n\ndf[df['INSEE commune'].str.startswith(\"92\")].head(2)\n\n\n\n\n\n\n\n\nINSEE commune\nCommune\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nD√©chets\nEnergie\nIndustrie hors-√©nergie\nR√©sidentiel\nRoutier\nTertiaire\n\n\n\n\n35494\n92012\nBOULOGNE-BILLANCOURT\nNaN\n1250.483441\n34.234669\n51730.704250\n964.828694\n8817.818741\n25882.493998\n92216.971456\n64985.280901\n60349.109482\n\n\n35501\n92025\nCOLOMBES\nNaN\n411.371588\n14.220061\n53923.847088\n698.685861\n12855.885267\n50244.664227\n87469.549463\n52070.927943\n41526.600867\n\n\n\n\n\n\n\nPour remplacer des valeurs sp√©cifiques, on utilise la m√©thode where ou une\nr√©assignation coupl√©e √† la m√©thode pr√©c√©dente.\nPar exemple, pour assigner des valeurs manquantes aux d√©partements du 92,\non peut faire cela\n\ndf_copy = df.copy()\ndf_copy = df_copy.where(~df['INSEE commune'].str.startswith(\"92\"))\n\net v√©rifier les r√©sultats:\n\ndf_copy[df['INSEE commune'].str.startswith(\"92\")].head(2)\ndf_copy[~df['INSEE commune'].str.startswith(\"92\")].head(2)\n\n\n\n\n\n\n\n\nINSEE commune\nCommune\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nD√©chets\nEnergie\nIndustrie hors-√©nergie\nR√©sidentiel\nRoutier\nTertiaire\n\n\n\n\n12167\n31555\nTOULOUSE\n1434.045233\n4482.980062\n130.792683\n576394.181208\n88863.732538\n91549.914092\n277062.573234\n410675.902028\n586054.672836\n288175.400126\n\n\n16774\n44109\nNANTES\n248.019465\n138738.544337\n250814.701179\n193478.248177\n18162.261628\n17461.400209\n77897.138554\n354259.013785\n221068.632724\n173447.582779\n\n\n\n\n\n\n\nou alors utiliser une r√©assignation plus classique:\n\ndf_copy = df.copy()\ndf_copy[df_copy['INSEE commune'].str.startswith(\"92\")] = np.nan\n\nIl est conseill√© de filtrer avec loc en utilisant un masque.\nEn effet, contrairement √† df[mask], df.loc[mask, :] permet d‚Äôindiquer clairement\n√† Python que l‚Äôon souhaite appliquer le masque aux labels de l‚Äôindex.\nCe n‚Äôest pas le cas avec df[mask].\nD‚Äôailleurs, lorsqu‚Äôon utilise la syntaxe df[mask], pandas renvoie g√©n√©ralement un warning\n\n\nOp√©rations par groupe\nEn SQL, il est tr√®s simple de d√©couper des donn√©es pour\neffectuer des op√©rations sur des blocs coh√©rents et recollecter des r√©sultats\ndans la dimension appropri√©e.\nLa logique sous-jacente est celle du split-apply-combine qui est repris\npar les langages de manipulation de donn√©es, auxquels pandas\nne fait pas exception.\nL‚Äôimage suivante, issue de\nce site\nrepr√©sente bien la mani√®re dont fonctionne l‚Äôapproche\nsplit-apply-combine\n\n\n\nSplit-apply-combine\n\n\nCe tutoriel sur le sujet\nest particuli√®rement utile.\nPour donner quelques exemples, on peut cr√©er une variable d√©partementale qui\nservira de crit√®re de groupe.\n\ndf['dep'] = df['INSEE commune'].str[:2]\n\nEn pandas, on utilise groupby pour d√©couper les donn√©es selon un ou\nplusieurs axes. Techniquement, cette op√©ration consiste √† cr√©er une association\nentre des labels (valeurs des variables de groupe) et des\nobservations.\nPar exemple, pour compter le nombre de communes par d√©partement en SQL, on\nutiliserait la requ√™te suivante :\nSELECT dep, count(INSEE commune)\nFROM df\nGROUP BY dep;\nCe qui, en pandas, donne:\n\ndf.groupby('dep')[\"INSEE commune\"].count()\n\ndep\n01    410\n02    805\n03    318\n04    199\n05    168\n     ... \n91    196\n92     36\n93     40\n94     47\n95    185\nName: INSEE commune, Length: 96, dtype: int64\n\n\nLa syntaxe est quasiment transparente. On peut bien s√ªr effectuer des op√©rations\npar groupe sur plusieurs colonnes. Par exemple,\n\ndf.groupby('dep').mean(numeric_only = True)\n\n\n\n\n\n\n\n\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nD√©chets\nEnergie\nIndustrie hors-√©nergie\nR√©sidentiel\nRoutier\nTertiaire\n\n\ndep\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n01\n1974.535382\n100.307344\n8.900375\n1736.353087\n671.743966\n280.485435\n1744.567552\n1346.982227\n3988.658995\n1021.089078\n\n\n02\n1585.417729\n202.878748\n17.390638\n767.072924\n223.907551\n76.316247\n932.135611\n793.615867\n1722.240298\n403.744266\n\n\n03\n6132.029417\n240.076499\n45.429978\n1779.630883\n349.746819\n326.904841\n1452.423506\n1401.650215\n3662.773062\n705.937016\n\n\n04\n1825.455590\n177.321816\nNaN\n583.198128\n253.975910\n62.808435\n313.913553\n587.116013\n1962.654370\n493.609329\n\n\n05\n1847.508592\n141.272766\nNaN\n502.012857\n132.548068\n34.971220\n102.649239\n728.734494\n2071.010178\n463.604908\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n91\n802.793163\n10114.998156\n73976.107892\n3716.906101\n1496.516194\n538.761253\n1880.810170\n6532.123033\n10578.452789\n3866.757200\n\n\n92\n8.309835\n362.964554\n13.132461\n29663.579634\n7347.163353\n6745.611611\n19627.706224\n40744.279029\n33289.456629\n23222.587595\n\n\n93\n50.461775\n1753.443710\n61188.896632\n18148.789684\n6304.173594\n2570.941598\n10830.409025\n32911.305703\n35818.236459\n21575.444794\n\n\n94\n48.072971\n5474.808839\n16559.384091\n14710.744314\n4545.099181\n1624.281505\n9940.192318\n28444.561597\n24881.531613\n16247.876321\n\n\n95\n609.172047\n682.143912\n37984.576873\n3408.871963\n1334.032970\n463.860672\n1729.692179\n6684.181989\n8325.948748\n4014.985843\n\n\n\n\n96 rows √ó 10 columns\n\n\n\nA noter que la variable de groupe, ici dep, devient, par d√©faut, l‚Äôindex\ndu DataFrame de sortie. Si on avait utilis√© plusieurs variables de groupe,\non obtiendrait un objet multi-index√©. Sur la gestion des multi-index, on\npourra se r√©f√©rer √† l‚Äôouvrage Modern Pandas dont la r√©f√©rence est\ndonn√©e en fin de cours.\nTant qu‚Äôon n‚Äôappelle pas une action sur un DataFrame par groupe, du type\nhead ou display, pandas n‚Äôeffectue aucune op√©ration. On parle de\nlazy evaluation. Par exemple, le r√©sultat de df.groupby('dep') est\nune transformation qui n‚Äôest pas encore √©valu√©e:\n\ndf.groupby('dep')\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x7fc2357358e0&gt;\n\n\nIl est possible d‚Äôappliquer plus d‚Äôune op√©ration √† la fois gr√¢ce √† la m√©thode\nagg. Par exemple, pour obtenir √† la fois le minimum, la m√©diane et le maximum\nde chaque d√©partement, on peut faire:\n\nnumeric_columns = df.select_dtypes(['number']).columns\ndf.loc[:, numeric_columns.tolist() + [\"dep\"] ].groupby('dep').agg(['min',\"median\",\"max\"], numeric_only = True)\n\n\n\n\n\n\n\n\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\n...\nIndustrie hors-√©nergie\nR√©sidentiel\nRoutier\nTertiaire\n\n\n\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\n...\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\n\n\ndep\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n01\n0.003432\n1304.519570\n14402.057335\n3.307596\n75.686090\n617.281080\n0.297256\n6.985161\n2.209492e+01\n30.571400\n...\n175185.892467\n9.607822\n351.182294\n57689.832901\n20.848982\n1598.934149\n45258.256406\n10.049230\n401.490676\n30847.366865\n\n\n02\n0.391926\n1205.725078\n13257.716591\n0.326963\n130.054615\n1126.961565\n0.517437\n15.492120\n5.799402e+01\n28.294993\n...\n220963.067245\n7.849347\n138.819865\n99038.124236\n22.936184\n700.826152\n49245.101730\n6.220952\n130.639994\n34159.345750\n\n\n03\n5.041238\n5382.194339\n24912.249269\n24.158870\n144.403590\n1433.217868\n29.958027\n42.762328\n8.269019e+01\n44.825515\n...\n154061.446374\n19.441088\n217.959697\n75793.882483\n120.667614\n1426.905646\n40957.845304\n17.705787\n191.892445\n31099.772884\n\n\n04\n30.985972\n1404.752852\n11423.535554\n33.513854\n158.780500\n362.637639\nNaN\nNaN\nNaN\n7.162928\n...\n16889.531061\n1.708652\n133.130946\n18088.189529\n30.206298\n687.390045\n31438.078325\n0.957070\n122.504902\n16478.024806\n\n\n05\n38.651727\n1520.896526\n13143.465812\n0.299734\n139.754980\n456.042002\nNaN\nNaN\nNaN\n20.931602\n...\n4271.129851\n6.871678\n211.945147\n46486.555748\n57.132270\n958.506314\n37846.651181\n4.785348\n151.695524\n23666.235898\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n91\n0.400740\n516.908303\n5965.349174\n25.785594\n177.177127\n513140.971691\n1.651873\n14.762210\n7.858782e+05\n41.661474\n...\n50288.560827\n15.886514\n2580.902085\n48464.979708\n20.260110\n3610.009634\n72288.020125\n36.368643\n1428.426303\n38296.204729\n\n\n92\n0.073468\n6.505185\n32.986132\n7.468879\n297.529178\n1250.483441\n1.104401\n11.482381\n3.423467e+01\n2173.614704\n...\n95840.512400\n4122.277198\n33667.904692\n92216.971456\n4968.382962\n23516.458236\n113716.853033\n800.588678\n18086.633085\n65043.364499\n\n\n93\n3.308495\n3.308495\n1362.351634\n24.188172\n320.755486\n45251.869710\n0.171075\n12.449476\n1.101146e+06\n899.762120\n...\n89135.302368\n4364.038661\n31428.227282\n87927.730552\n1632.496185\n22506.758771\n193039.792609\n2257.370945\n20864.923339\n71918.163984\n\n\n94\n1.781885\n1.781885\n556.939161\n6.249609\n294.204166\n103252.271268\n0.390223\n14.944807\n1.571965e+05\n928.232154\n...\n96716.055178\n2668.358896\n24372.900300\n100948.169898\n1266.101605\n19088.651049\n97625.957714\n1190.115985\n14054.223449\n58528.623477\n\n\n95\n8.779506\n445.279844\n2987.287417\n1.749091\n80.838639\n44883.982753\n0.201508\n13.149987\n1.101131e+06\n13.490977\n...\n66216.914749\n11.585833\n1434.343631\n104543.465908\n2.619451\n3417.197938\n147040.904455\n11.484835\n725.467969\n61497.821477\n\n\n\n\n96 rows √ó 30 columns\n\n\n\nLa premi√®re ligne est pr√©sente pour nous faciliter la r√©cup√©ration des noms de colonnes des variables\nnum√©riques\n\n\nAppliquer des fonctions\npandas est, comme on a pu le voir, un package tr√®s flexible, qui\npropose une grande vari√©t√© de m√©thodes optimis√©es. Cependant, il est fr√©quent\nd‚Äôavoir besoin de m√©thodes non impl√©ment√©es.\nDans ce cas, on recourt souvent aux lambda functions. Par exemple, si\non d√©sire conna√Ætre les communes dont le nom fait plus de 40 caract√®res,\non peut appliquer la fonction len de mani√®re it√©rative:\n\n# Noms de communes superieurs √† 40 caracteres\ndf[df['Commune'].apply(lambda s: len(s)&gt;40)]\n\n\n\n\n\n\n\n\nINSEE commune\nCommune\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nD√©chets\nEnergie\nIndustrie hors-√©nergie\nR√©sidentiel\nRoutier\nTertiaire\ndep\n\n\n\n\n28082\n70058\nBEAUJEU-SAINT-VALLIER-PIERREJUX-ET-QUITTEUR\n4024.909815\n736.948351\n41.943384\n1253.135313\n125.101996\n2.354558\n6.911213\n549.734302\n1288.215480\n452.693897\n70\n\n\n4984\n14621\nSAINT-MARTIN-DE-BIENFAITE-LA-CRESSONNIERE\n1213.333523\nNaN\nNaN\n677.571743\n72.072503\n63.573059\n186.602760\n298.261044\n1396.353375\n260.801452\n14\n\n\n19276\n51513\nSAINT-REMY-EN-BOUZEMONT-SAINT-GENEST-ET-ISSON\n1927.401921\nNaN\nNaN\n595.583152\n71.675773\n4.709115\n13.822427\n273.826687\n521.864748\n259.365848\n51\n\n\n5402\n16053\nBORS (CANTON DE BAIGNES-SAINTE-RADEGONDE)\n1919.249545\nNaN\nNaN\n165.443226\n16.265904\n2.354558\n6.911213\n54.561623\n719.293151\n58.859777\n16\n\n\n\n\n\n\n\nCependant, toutes les lambda functions ne se justifient pas.\nPar exemple, prenons\nle r√©sultat d‚Äôagr√©gation pr√©c√©dent. Imaginons qu‚Äôon d√©sire avoir les r√©sultats\nen milliers de tonnes. Dans ce cas, le premier r√©flexe est d‚Äôutiliser\nla lambda function suivante :\n\nnumeric_columns = df.select_dtypes(['number']).columns\n(df\n    .loc[:, numeric_columns.tolist() + [\"dep\"] ]\n    .groupby('dep')\n    .agg(['min',\"median\",\"max\"])\n    .apply(lambda s: s/1000)\n)\n\n\n\n\n\n\n\n\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\n...\nIndustrie hors-√©nergie\nR√©sidentiel\nRoutier\nTertiaire\n\n\n\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\n...\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\n\n\ndep\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n01\n0.000003\n1.304520\n14.402057\n0.003308\n0.075686\n0.617281\n0.000297\n0.006985\n0.022095\n0.030571\n...\n175.185892\n0.009608\n0.351182\n57.689833\n0.020849\n1.598934\n45.258256\n0.010049\n0.401491\n30.847367\n\n\n02\n0.000392\n1.205725\n13.257717\n0.000327\n0.130055\n1.126962\n0.000517\n0.015492\n0.057994\n0.028295\n...\n220.963067\n0.007849\n0.138820\n99.038124\n0.022936\n0.700826\n49.245102\n0.006221\n0.130640\n34.159346\n\n\n03\n0.005041\n5.382194\n24.912249\n0.024159\n0.144404\n1.433218\n0.029958\n0.042762\n0.082690\n0.044826\n...\n154.061446\n0.019441\n0.217960\n75.793882\n0.120668\n1.426906\n40.957845\n0.017706\n0.191892\n31.099773\n\n\n04\n0.030986\n1.404753\n11.423536\n0.033514\n0.158781\n0.362638\nNaN\nNaN\nNaN\n0.007163\n...\n16.889531\n0.001709\n0.133131\n18.088190\n0.030206\n0.687390\n31.438078\n0.000957\n0.122505\n16.478025\n\n\n05\n0.038652\n1.520897\n13.143466\n0.000300\n0.139755\n0.456042\nNaN\nNaN\nNaN\n0.020932\n...\n4.271130\n0.006872\n0.211945\n46.486556\n0.057132\n0.958506\n37.846651\n0.004785\n0.151696\n23.666236\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n91\n0.000401\n0.516908\n5.965349\n0.025786\n0.177177\n513.140972\n0.001652\n0.014762\n785.878155\n0.041661\n...\n50.288561\n0.015887\n2.580902\n48.464980\n0.020260\n3.610010\n72.288020\n0.036369\n1.428426\n38.296205\n\n\n92\n0.000073\n0.006505\n0.032986\n0.007469\n0.297529\n1.250483\n0.001104\n0.011482\n0.034235\n2.173615\n...\n95.840512\n4.122277\n33.667905\n92.216971\n4.968383\n23.516458\n113.716853\n0.800589\n18.086633\n65.043364\n\n\n93\n0.003308\n0.003308\n1.362352\n0.024188\n0.320755\n45.251870\n0.000171\n0.012449\n1101.145545\n0.899762\n...\n89.135302\n4.364039\n31.428227\n87.927731\n1.632496\n22.506759\n193.039793\n2.257371\n20.864923\n71.918164\n\n\n94\n0.001782\n0.001782\n0.556939\n0.006250\n0.294204\n103.252271\n0.000390\n0.014945\n157.196520\n0.928232\n...\n96.716055\n2.668359\n24.372900\n100.948170\n1.266102\n19.088651\n97.625958\n1.190116\n14.054223\n58.528623\n\n\n95\n0.008780\n0.445280\n2.987287\n0.001749\n0.080839\n44.883983\n0.000202\n0.013150\n1101.131222\n0.013491\n...\n66.216915\n0.011586\n1.434344\n104.543466\n0.002619\n3.417198\n147.040904\n0.011485\n0.725468\n61.497821\n\n\n\n\n96 rows √ó 30 columns\n\n\n\nEn effet, cela effectue le r√©sultat d√©sir√©. Cependant, il y a mieux : utiliser\nla m√©thode div:\n\nimport timeit\ndf_numeric = df.loc[:, numeric_columns.tolist() + [\"dep\"] ]\n%timeit df_numeric.groupby('dep').agg(['min',\"median\",\"max\"]).div(1000)\n%timeit df_numeric.groupby('dep').agg(['min',\"median\",\"max\"]).apply(lambda s: s/1000)\n\nLa m√©thode div est en moyenne plus rapide et a un temps d‚Äôex√©cution\nmoins variable. Dans ce cas, on pourrait m√™me utiliser le principe\ndu broadcasting de numpy (cf.¬†chapitre numpy) qui offre\ndes performances √©quivalentes:\n\n%timeit df_numeric.groupby('dep').agg(['min',\"median\",\"max\"])/1000\n\napply est plus rapide qu‚Äôune boucle (en interne, apply utilise Cython\npour it√©rer) mais reste moins rapide qu‚Äôune solution vectoris√©e quand\nelle existe. Ce site\npropose des solutions, par exemple les m√©thodes isin ou digitize, pour\n√©viter de manuellement cr√©er des boucles lentes.\nEn particulier, il faut noter que apply avec le param√®tre axis=1 est en g√©n√©rale lente.\n\n\nJoindre des donn√©es\nIl est commun de devoir combiner des donn√©es issues de sources diff√©rentes.\nNous allons ici nous focaliser sur le cas le plus favorable qui est la situation\no√π une information permet d‚Äôapparier de mani√®re exacte deux bases de donn√©es (autrement nous\nserions dans une situation, beaucoup plus complexe, d‚Äôappariement flou1).\nLa situation typique est l‚Äôappariement entre deux sources de donn√©es selon un identifiant\nindividuel. Ici, il s‚Äôagit d‚Äôun identifiant de code commune.\nIl est recommand√© de lire ce guide assez complet sur la question des jointures avec R\nqui donne des recommandations √©galement utiles pour un utilisateur de Python.\n\n\n\n\n\nOn utilise de mani√®re indiff√©rente les termes merge ou join.\nLe deuxi√®me terme provient de la syntaxe SQL.\nEn Pandas, dans la plupart des cas, on peut utiliser indiff√©remment df.join et df.merge\n\n\n\n\n\nIl est aussi possible de r√©aliser un merge en utilisant la fonction pandas.concat() avec axis=1.\nSe r√©f√©rer √† la documentation de concat pour voir les options possibles.\n\n\nRestructurer des donn√©es (reshape)\nOn pr√©sente g√©n√©ralement deux types de donn√©es :\n\nformat wide : les donn√©es comportent des observations r√©p√©t√©es, pour un m√™me individu (ou groupe), dans des colonnes diff√©rentes\nformat long : les donn√©es comportent des observations r√©p√©t√©es, pour un m√™me individu, dans des lignes diff√©rentes avec une colonne permettant de distinguer les niveaux d‚Äôobservations\n\nUn exemple de la distinction entre les deux peut √™tre emprunt√© √† l‚Äôouvrage de r√©f√©rence d‚ÄôHadley Wickham, R for Data Science:\n\n\n\n\n\nL‚Äôaide m√©moire suivante aidera √† se rappeler les fonctions √† appliquer si besoin :\n\n\n\n\n\nLe fait de passer d‚Äôun format wide au format long (ou vice-versa) peut √™tre extr√™mement pratique car\ncertaines fonctions sont plus ad√©quates sur une forme de donn√©es ou sur l‚Äôautre.\nEn r√®gle g√©n√©rale, avec Python comme avec R, les formats long sont souvent pr√©f√©rables.\nLe chapitre suivant, qui fait office de TP, proposera des applications de ces principes :\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLes pipe\nEn g√©n√©ral, dans un projet, le nettoyage de donn√©es va consister en un ensemble de\nm√©thodes appliqu√©es √† un pandas.DataFrame.\nOn a vu que assign permettait de cr√©er une variable dans un DataFrame.\nIl est √©galement possible d‚Äôappliquer une fonction, appel√©e par exemple my_udf au\nDataFrame gr√¢ce √† pipe:\ndf = (pd.read_csv(path2data)\n            .pipe(my_udf))\nL‚Äôutilisation des pipe rend le code tr√®s lisible et peut √™tre tr√®s\npratique lorsqu‚Äôon enchaine des op√©rations sur le m√™me\ndataset."
  },
  {
    "objectID": "content/manipulation/02a_pandas_tutorial.html#quelques-enjeux-de-performance",
    "href": "content/manipulation/02a_pandas_tutorial.html#quelques-enjeux-de-performance",
    "title": "Introduction √† Pandas",
    "section": "Quelques enjeux de performance",
    "text": "Quelques enjeux de performance\nLa librairie Dask int√®gre la structure de numpy, pandas et sklearn.\nElle a vocation √† traiter de donn√©es en grande dimension, ainsi elle ne sera pas\noptimale pour des donn√©es qui tiennent tr√®s bien en RAM.\nIl s‚Äôagit d‚Äôune librairie construite sur la parall√©lisation.\nUn chapitre dans ce cours lui est consacr√©.\nPour aller plus loin, se r√©f√©rer √† la documentation de Dask."
  },
  {
    "objectID": "content/manipulation/02a_pandas_tutorial.html#r√©f√©rences",
    "href": "content/manipulation/02a_pandas_tutorial.html#r√©f√©rences",
    "title": "Introduction √† Pandas",
    "section": "R√©f√©rences",
    "text": "R√©f√©rences\n\nLe site\npandas.pydata\nfait office de r√©f√©rence\nLe livre Modern Pandas de Tom Augspurger : https://tomaugspurger.github.io/modern-1-intro.html\n\n\n\nMcKinney, Wes. 2012. Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython. \" O‚ÄôReilly Media, Inc.\"."
  },
  {
    "objectID": "content/manipulation/02a_pandas_tutorial.html#footnotes",
    "href": "content/manipulation/02a_pandas_tutorial.html#footnotes",
    "title": "Introduction √† Pandas",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSur l‚Äôappariement flou, se reporter aux chapitres pr√©sentant ElasticSearch.‚Ü©Ô∏é"
  },
  {
    "objectID": "content/manipulation/03_geopandas_tutorial.html",
    "href": "content/manipulation/03_geopandas_tutorial.html",
    "title": "Donn√©es spatiales : d√©couverte de geopandas",
    "section": "",
    "text": "Dans ce tutoriel, nous allons utiliser les donn√©es suivantes :\nLa repr√©sentation des donn√©es, notamment la cartographie, est pr√©sent√©e plus\namplement dans la partie visualiser. Quelques m√©thodes\npour faire rapidement des cartes seront pr√©sent√©es ici, mais\nl‚Äôobjet de ce chapitre porte davantage sur la manipulation des donn√©es g√©ographiques.\nCe tutoriel s‚Äôinspire beaucoup d‚Äôun autre tutoriel que j‚Äôai fait pour\nR disponible\ndans la documentation utilitr.\nIl peut servir de pendant √† celui-ci pour l‚Äôutilisateur de R.\nQuelques installations pr√©alables sont n√©cessaires :\n!pip install pandas fiona shapely pyproj rtree # √† faire obligatoirement en premier pour utiliser rtree ou pygeos pour les jointures spatiales\n!pip install contextily\n!pip install geopandas\n!pip install topojson\nPour √™tre en mesure d‚Äôex√©cuter ce tutoriel, les imports suivants\nseront utiles.\nimport geopandas as gpd\nimport contextily as ctx\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "content/manipulation/03_geopandas_tutorial.html#donn√©es-spatiales",
    "href": "content/manipulation/03_geopandas_tutorial.html#donn√©es-spatiales",
    "title": "Donn√©es spatiales : d√©couverte de geopandas",
    "section": "Donn√©es spatiales",
    "text": "Donn√©es spatiales\n\nQuelle diff√©rence avec des donn√©es traditionnelles ?\nLe terme ‚Äúdonn√©es spatiales‚Äù d√©signe les donn√©es qui portent sur les caract√©ristiques g√©ographiques des objets (localisation, contours, liens).\nLes caract√©ristiques g√©ographiques des objets sont d√©crites √† l‚Äôaide d‚Äôun syst√®me de coordonn√©es\nqui permettent une repr√©sentation dans un espace euclidien \\((x,y)\\).\nLe passage de l‚Äôespace r√©el (la Terre, qui est une sph√®re) √† l‚Äôespace plan\nse fait gr√¢ce √† un syst√®me de projection. Voici quelques exemples\nde donn√©es spatiales :\n\nUne table d√©crivant des b√¢timents, avec les coordonn√©es g√©ographiques de chaque b√¢timent ;\nLe d√©coupage communal du territoire, avec le contour du territoire de chaque commune ;\nLes routes terrestres, avec les coordonn√©es d√©crivant leur parcours en 3 dimensions (longitude, latitude, altitude).\n\nLes donn√©es spatiales rassemblent classiquement deux types de donn√©es :\n\ndes donn√©es g√©ographiques (ou g√©om√©tries) : objets g√©om√©triques tels que des points, des vecteurs, des polygones, ou des maillages (raster). Exemple: la forme de chaque commune, les coordonn√©es d‚Äôun b√¢timent;\ndes donn√©es attributaires (ou attributs) : des mesures et des caract√©ristiques associ√©es aux objets g√©om√©triques. Exemple: la population de chaque commune, le nombre de fen√™tres et le nombre d‚Äô√©tages d‚Äôun b√¢timent.\n\nLes donn√©es spatiales sont fr√©quemment trait√©es √† l‚Äôaide d‚Äôun syst√®me d‚Äôinformation g√©ographique (SIG), c‚Äôest-√†-dire un syst√®me d‚Äôinformation capable de stocker, d‚Äôorganiser et de pr√©senter des donn√©es alphanum√©riques spatialement r√©f√©renc√©es par des coordonn√©es dans un syst√®me de r√©f√©rence (CRS). Python dispose de fonctionnalit√©s lui permettant de r√©aliser les m√™mes t√¢ches qu‚Äôun SIG (traitement de donn√©es spatiales, repr√©sentations cartographiques).\n\n\nDe Pandas √† Geopandas\nLe package Geopandas est une bo√Æte √† outils con√ßue pour faciliter la manipulation de donn√©es spatiales. La grande force de Geopandas est qu‚Äôil permet de manipuler des donn√©es spatiales comme s‚Äôil s‚Äôagissait de donn√©es traditionnelles, car il repose sur le standard ISO 19125 simple feature access d√©fini conjointement par l‚ÄôOpen Geospatial Consortium (OGC) et l‚ÄôInternational Organization for Standardization (ISO).\nPar rapport √† un DataFrame standard, un objet Geopandas comporte\nune colonne suppl√©mentaire: geometry. Elle stocke les coordonn√©es des\nobjets g√©ographiques (ou ensemble de coordonn√©es s‚Äôagissant de contours). Un objet Geopandas h√©rite des propri√©t√©s d‚Äôun\nDataFrame Pandas mais propose des m√©thodes adapt√©es au traitement des donn√©es spatiales.\nAinsi, gr√¢ce √† Geopandas, on pourra effectuer des manipulations sur les attributs des donn√©es comme avec pandas mais on pourra √©galement faire des manipulations sur la dimension spatiale des donn√©es. En particulier,\n\nCalculer des distances et des surfaces ;\nAgr√©ger rapidement des zonages (regrouper les communes en d√©partement par exemple) ;\nTrouver dans quelle commune se trouve un b√¢timent √† partir de ses coordonn√©es g√©ographiques ;\nRecalculer des coordonn√©es dans un autre syst√®me de projection ;\nFaire une carte, rapidement et simplement.\n\n\n\n Hint\nLes manipulations de donn√©es sur un objet Geopandas sont nettement plus lentes que sur\nun DataFrame traditionnel (car Python doit g√©rer les informations g√©ographiques pendant la manipulation des donn√©es).\nLorsque vous manipulez des donn√©es de grandes dimensions,\nil peut √™tre pr√©f√©rable d‚Äôeffectuer les op√©rations sur les donn√©es avant de joindre une g√©om√©trie √† celles-ci.\n\n\nPar rapport √† un logiciel sp√©cialis√© comme QGIS, Python permettra\nd‚Äôautomatiser le traitement et la repr√©sentation des donn√©es. D‚Äôailleurs,\nQGIS utilise lui-m√™me Python‚Ä¶\n\n\nR√©sum√©\nEn r√©sum√©, un objet GeoPandas comporte les √©l√©ments suivantes :\n\n\n\n\n\n\nLes attributs. Ce sont les valeurs associ√©es √† chaque niveau g√©ographique.\nIl s‚Äôagit de la dimension tabulaire usuelle, dont le traitement est similaire\n√† celui d‚Äôun objet Pandas classique.\nLes g√©om√©tries. Ce sont les valeurs num√©riques interpr√©t√©es pour repr√©senter la dimension g√©ographique. Elles permettent de repr√©senter dans un certain\nr√©f√©rentiel (le syst√®me de r√©f√©rence) la dimension g√©ographique.\nLe syst√®me de r√©f√©rence. Il s‚Äôagit du syst√®me permettant de transformer les positions sur\nle globe (3 dimensions avec une boule asym√©trique) en un plan en deux dimensions.\nIl en existe une multitude, identifiables √† partir d‚Äôun code EPSG (4326, 2154‚Ä¶).\nLeur manipulation est facilit√©e par Geopandas qui s‚Äôappuie sur Shapely, de la m√™me\nmani√®re que Pandas s‚Äôappuie sur Numpy ou Arrow."
  },
  {
    "objectID": "content/manipulation/03_geopandas_tutorial.html#le-syst√®me-de-projection-cartographique",
    "href": "content/manipulation/03_geopandas_tutorial.html#le-syst√®me-de-projection-cartographique",
    "title": "Donn√©es spatiales : d√©couverte de geopandas",
    "section": "Le syst√®me de projection cartographique",
    "text": "Le syst√®me de projection cartographique\n\nPrincipe\nLes donn√©es spatiales sont\nplus riches que les donn√©es traditionnelles car elles\nincluent, habituellement, des √©l√©ments suppl√©mentaires pour placer dans\nun espace cart√©sien les objets. Cette dimension suppl√©mentaire peut √™tre simple\n(un point comporte deux informations suppl√©mentaire: \\(x\\) et \\(y\\)) ou\nassez complexe (polygones, lignes avec direction, etc.).\nL‚Äôanalyse cartographique emprunte d√®s lors √† la g√©om√©trie\ndes concepts\npour repr√©senter des objets dans l‚Äôespace. Les projections\nsont au coeur de la gestion des donn√©es spatiales.\nCes derni√®res consistent √† transformer une position dans l‚Äôespace\nterrestre √† une position sur un plan. Il s‚Äôagit donc d‚Äôune op√©ration\nde projection d‚Äôun espace tri-dimensionnel dans un espace\n√† deux dimensions.\nCe post propose de riches √©l√©ments sur le\nsujet, notamment l‚Äôimage suivante qui montre bien le principe d‚Äôune projection :\n\n\n\nLes diff√©rents types de projection\n\n\nCette op√©ration n‚Äôest pas neutre. L‚Äôune des cons√©quences du\nth√©or√®me remarquable de Gauss\nest que la surface de la Terre ne peut √™tre cartographi√©e sans distortion.\nUne projection ne peut simultan√©ment conserver intactes les distances et les\nangles (i.e.¬†les positions).\nIl n‚Äôexiste ainsi pas de projection universellement meilleure, ce qui ouvre\nla porte √† la coexistence de nombreuses projections diff√©rentes, pens√©es\npour des t√¢ches diff√©rentes.\nUn mauvais syst√®me de repr√©sentation\nfausse l‚Äôappr√©ciation visuelle mais peut aussi entra√Æner des erreurs dans\nles calculs sur la dimension spatiale.\nLes syst√®mes de projection font l‚Äôobjet de standards internationaux et sont souvent d√©sign√©s par des codes dits codes EPSG. Ce site est un bon aide-m√©moire. Les plus fr√©quents, pour les utilisateurs fran√ßais, sont les suivants (plus d‚Äôinfos ici) :\n\n2154 : syst√®me de projection Lambert 93. Il s‚Äôagit du syst√®me de projection officiel. La plupart des donn√©es diffus√©es par l‚Äôadministration pour la m√©tropole sont disponibles dans ce syst√®me de projection.\n27572 : Lambert II √©tendu. Il s‚Äôagit de l‚Äôancien syst√®me de projection officiel. Les donn√©es spatiales anciennes peuvent √™tre dans ce format.\n4326 : WGS 84 ou syst√®me de pseudo-Mercator ou encore Web Mercator. Ce n‚Äôest en r√©alit√© pas un syst√®me de projection mais un syst√®me de coordonn√©es (longitude / latitude) qui permet simplement un rep√©rage angulaire sur l‚Äôellipso√Øde. Il est utilis√© pour les donn√©es GPS. Il s‚Äôagit du syst√®me le plus\nusuel, notamment quand on travaille avec des fonds de carte web.\n\nComme √©voqu√© plus haut, l‚Äôune des projections les plus connues est la\nprojection Web Mercator dite WGS84 (code EPSG 4326). Il\ns‚Äôagit d‚Äôune projection conservant intacte les angles, ce\nqui implique qu‚Äôelle alt√®re les distances. Celle-ci a en effet √©t√©\npens√©e, √† l‚Äôorigine, pour repr√©senter l‚Äôh√©misph√®re Nord. Plus\non s‚Äô√©loigne de celui-ci, plus les distances sont distordues. Cela\nam√®ne √† des distorsions bien\nconnues (le Groenland hypertrophi√©, l‚ÄôAfrique de taille r√©duite, l‚ÄôAntarctique d√©mesur√©‚Ä¶).\nEn revanche, la projection Mercator conserve intacte les positions.\nC‚Äôest cette propri√©t√© qui explique son utilisation dans les syst√®mes\nGPS et ainsi dans les fonds de carte de navigation du type Google Maps.\n\n\n\nExemple de reprojection de pays depuis le site thetruesize.com\n\n\nObservez les variations significatives\nde proportions pour certains pays selon les projections\nchoisies:\n\nhtml`&lt;div&gt;${container_projection}&lt;/div&gt;`\n\n\n\n\n\n\n\ncontainer_projection = html`&lt;div class=\"container\"&gt;\n  &lt;div class=\"row\"&gt;\n    &lt;div class=\"projection\"&gt;\n      &lt;div class=\"projection-label\"&gt;Choisir une projection&lt;/div&gt;\n      ${viewof projection}\n    &lt;/div&gt;\n  &lt;/div&gt;\n  &lt;div class=\"row\"&gt;\n    &lt;div class=\"projectedMap\"&gt;\n      ${projectedMap}\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/div&gt;`\n\n\n\n\n\n\n\nviewof projection = projectionInput({\n  name: \"\",\n  value: \"Mercator\"\n})\n\n\n\n\n\n\n\nimport {projectionInput} from \"@fil/d3-projections\"\nimport {map} from \"@linogaliana/base-map\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprojectedMap = map(projection,\n                   {\n                     //svg: true,\n                     value: projection.options,\n                     width: width_projected_map,\n                     //height: 300,\n                     //rotate: [0, -90],\n                     //inertia: true,\n                     show_equator: true,\n                     background: \"#f1f0eb\"\n                     \n                     //show_structure: true\n                   })\n\n\n\n\n\n\n\nwidth_projected_map = screen.width/2\n\n\n\n\n\n\nPour aller plus loin, la carte interactive\nsuivante, construite par Nicolas Lambert, issue de\nce notebook Observable, illustre l‚Äôeffet\nd√©formant de la projection Mercator, et de quelques-unes autres,\nsur notre perception de la taille des pays.\n\n\nVoir la carte interactive\n\n\nhtml`&lt;div class=\"grid-container\"&gt;\n  &lt;div class=\"viewof-projection\"&gt;${viewof projectionBertin}&lt;/div&gt;\n  &lt;div class=\"viewof-mycountry\"&gt;${viewof mycountry}&lt;/div&gt;\n  &lt;div class=\"map-bertin\"&gt;${mapBertin}&lt;/div&gt;\n&lt;/div&gt;`\n\n\n\n\n\n\n\n\nimport {map as mapBertin, viewof projection as projectionBertin, viewof mycountry} from \"@neocartocnrs/impact-of-projections-on-areas\"\n\n\n\n\n\n\nIl existe en fait de nombreuses repr√©sentations possibles du monde, plus ou moins\nalambiqu√©es. Les projections sont tr√®s nombreuses et certaines peuvent avoir une forme suprenante.\nPar exemple,\nla projection de Spillhaus\npropose de centrer la vue sur les oc√©ans et non une terre. C‚Äôest pour\ncette raison qu‚Äôon parle parfois de monde tel que vu par les poissons\n√† son propos.\n\nhtml`&lt;div class=\"centered\"&gt;${spilhaus}&lt;/div&gt;`\n\n\n\n\n\n\n\nspilhaus = {\n  const width = 600;\n  const height = width;\n\n  const context = DOM.context2d(width, height);\n  const projection = d3.geoStereographic()\n    .rotate([95, 45])\n    .translate([width / 2, height / 2])\n    .scale(width / 10.1)\n    .center([30, -5])\n    .clipAngle(166);\n  const path = d3.geoPath(projection, context);\n\n  const land = topojson.feature(world, world.objects.land);\n\n  context.lineJoin = \"round\";\n  context.lineCap = \"round\";\n  context.fillStyle = \"#f2f1ed\";\n  context.fillRect(0, 0, width, height);\n\n  context.beginPath();\n  path({type: \"Sphere\"});\n  path(land);\n  context.lineWidth = 0.5;\n  context.stroke();\n  context.clip(\"evenodd\");\n\n  context.save();\n  context.beginPath();\n  path(land);\n  context.filter = \"blur(12px)\";\n  context.fillStyle = \"#006994\";\n  context.fill(\"evenodd\");\n  context.restore();\n  \n  context.beginPath();\n  path(d3.geoGraticule10());\n  context.globalAlpha = 0.2;\n  context.strokeStyle = \"#000\";\n  context.stroke();\n\n  return context.canvas;\n}\n\n\n\n\n\n\n\n//import {map as spilhausmap} with {height, width} from \"@d3/spilhaus-shoreline-map\"\nimport { world } from \"@d3/spilhaus-shoreline-map\"\n\n\n\n\n\n\n\n\n Astuce pour la France\nPour la France, dans le syst√®me WGS84 (4326) :\n\nLongitude (\\(x\\)) tourne autour de 0¬∞ (de -5.2 √† +9.6 pour √™tre plus pr√©cis)\nLa latitude (\\(y\\)) autour de 45 (entre +41.3 √† +51.1)\n\nDans le syst√®me Lambert 93 (2154) :\n\nCoordonn√©es \\(x\\): entre 100 000 et 1 300 000\nLa latitude (\\(y\\)): entre 6 000 000 et 7 200 000\n\nPlus de d√©tails"
  },
  {
    "objectID": "content/manipulation/03_geopandas_tutorial.html#importer-des-donn√©es-spatiales",
    "href": "content/manipulation/03_geopandas_tutorial.html#importer-des-donn√©es-spatiales",
    "title": "Donn√©es spatiales : d√©couverte de geopandas",
    "section": "Importer des donn√©es spatiales",
    "text": "Importer des donn√©es spatiales\nLes formats les plus communs de donn√©es spatiales sont les suivants :\n\nshapefile (.shp) : format (propri√©taire) le plus commun de donn√©es g√©ographiques.\nLa table de donn√©es (attributs) est stock√©e dans un fichier s√©par√© des\ndonn√©es spatiales. En faisant geopandas.read_file(\"monfichier.shp\"), le\npackage fait lui-m√™me le lien entre les observations et leur repr√©sentation spatiale.\ngeopackage (.gpkg) : ce (relativement) nouveau format libre en un seul fichier √©galement (lui recommand√© par l‚ÄôOGC) vise progressivement √† se substituer au shapefile. Il est par exemple le format par d√©faut dans QGIS.\ngeojson (.json) : ce format, non pr√©conis√© par l‚ÄôOGC, est largement utilis√© pour le d√©veloppement web\ncomme dans la librairie leaflet.js.\nLa dimension spatiale est stock√©e dans le m√™me fichier que les attributs.\nCes fichiers sont g√©n√©ralement beaucoup plus l√©gers que les shapefiles mais poss√®dent des limites s‚Äôagissant de gros jeux de donn√©es.\ntopojson (.json) : une variante du geojson qui se d√©veloppe progressivement pour assister les visualisations web. Au lieu de stocker l‚Äôensemble des points permettant de repr√©senter une\ng√©om√©trie, seuls les arcs sont conserv√©s. Cela all√®ge substantiellement le poids du fichier et\npermet, avec une librairie adapt√©e, de reconstruire l‚Äôensemble des contours g√©ographiques.\n\nCette page compare plus en d√©tail les principes formats de donn√©es g√©ographiques.\nL‚Äôaide de Geopandas propose des bouts de code en fonction des diff√©rentes situations dans lesquelles on se trouve.\n\nExemple : r√©cup√©rer les d√©coupages territoriaux\nL‚Äôun des fonds de carte les plus fr√©quents qu‚Äôon utilise est celui des\nlimites administratives des communes.\nCelui-ci peut √™tre r√©cup√©r√© de plusieurs mani√®res.\nEn premier lieu, pour r√©cup√©rer\nle fond de carte officiel, produit par l‚ÄôIGN, sous\nle nom d‚ÄôAdminExpress[^1],\nil est possible de se rendre sur le site de l‚ÄôIGN et de le t√©l√©charger.\nIl est √©galement possible d‚Äôutiliser l‚Äôune des API de l‚ÄôIGN\nmais ces derni√®res ne sont pas encore tr√®s document√©es pour des utilisateurs\nde Python.\nLe package cartiflette, issu\nd‚Äôun projet interminist√©riel, propose\nune r√©cup√©ration\nfacilit√©e de fonds de carte officiels de l‚ÄôIGN.\nCe projet vise √† faciliter la r√©cup√©ration des sources officielles, notamment\ncelles de l‚ÄôIGN, et leur association √† des jeux de donn√©es g√©ographiques.\n\n\n Note\nLe package cartiflette est exp√©rimental\net n‚Äôest disponible que sur\nGithub, pas sur PyPi.\nIl est amen√© √† √©voluer rapidement et cette page sera mise √† jour\nquand de nouvelles fonctionalit√©s (notamment l‚Äôutilisation d‚ÄôAPI)\nseront disponibles pour encore simplifier la r√©cup√©ration de\ncontours g√©ographiques.\nPour installer cartiflette, il est n√©cessaire d‚Äôutiliser les commandes suivantes\ndepuis un Jupyter Notebook (si vous utilisez la ligne de commande directement,\nvous pouvez retirer les ! en d√©but de ligne):\n\n!pip install requests py7zr geopandas openpyxl tqdm s3fs PyYAML xlrd\n!pip install git+https://github.com/inseefrlab/cartiflette@80b8a5a28371feb6df31d55bcc2617948a5f9b1a\n\nCes commandes permettent de r√©cup√©rer l‚Äôensemble du code\nsource depuis Github\n\n\nIci, nous sommes int√©ress√©s par les contours des communes\nde la petite couronne. On pourrait d√©sirer r√©cup√©rer\nl‚Äôensemble de la r√©gion Ile-de-France mais nous\nallons nous contenter de l‚Äôanalyse de Paris intra-muros\net des d√©partements limitrophes.\nC‚Äôest l‚Äôun des avantage de cartiflette que de faciliter\nla r√©cup√©ration de fonds de carte sur un ensemble de d√©partement.\nCela √©vite la r√©cup√©ration d‚Äôun fond de carte tr√®s\nvolumineux (plus de 500Mo) pour une analyse restreinte (quelques d√©partements).\nUn autre avantage de cartiflette est de faciliter la r√©cup√©ration de fonds\nde carte consolid√©s comme celui dont on a besoin ici : arrondissements\ndans Paris, communes ailleurs. Comme cela est expliqu√© dans un encadr√© √† part,\nil s‚Äôagirait d‚Äôune op√©ration p√©nible √† mettre en oeuvre sans cartiflette.\nLes contours de cet espace peuvent √™tre r√©cup√©r√©s de la mani√®re suivante :\n\nimport cartiflette.s3 as s3\n\nshp_communes = s3.download_vectorfile_url_all(\n    crs = 4326,\n    values = [\"75\", \"92\", \"93\", \"94\"],\n    borders=\"COMMUNE_ARRONDISSEMENT\",\n    vectorfile_format=\"topojson\",\n    filter_by=\"DEPARTEMENT\",\n    source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n    year=2022)\n\n\nshp_communes.head(3)\n\n\n\n\n\n\n\n\nid\nID\nNOM\nNOM_M\nINSEE_COM\nSTATUT\nPOPULATION\nINSEE_CAN\nINSEE_ARR\nINSEE_DEP\nINSEE_REG\nSIREN_EPCI\nsource\nINSEE_COG\ngeometry\n\n\n\n\n0\nARR_MUNI0000000009736045\nNaN\nParis 3e Arrondissement\nPARIS 3E ARRONDISSEMENT\n75056\nCapitale d'√©tat\n34025\nNR\n1\n75\n11\n200054781\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n75103\nPOLYGON ((2.35016 48.86199, 2.35019 48.86203, ...\n\n\n1\nARR_MUNI0000000009736046\nNaN\nParis 2e Arrondissement\nPARIS 2E ARRONDISSEMENT\n75056\nCapitale d'√©tat\n21595\nNR\n1\n75\n11\n200054781\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n75102\nPOLYGON ((2.34792 48.87069, 2.34827 48.87062, ...\n\n\n2\nARR_MUNI0000000009736545\nNaN\nParis 4e Arrondissement\nPARIS 4E ARRONDISSEMENT\n75056\nCapitale d'√©tat\n29131\nNR\n1\n75\n11\n200054781\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n75104\nPOLYGON ((2.36849 48.85580, 2.36873 48.85482, ...\n\n\n\n\n\n\n\nOn reconna√Æt la structure d‚Äôun DataFrame Pandas. A cette structure s‚Äôajoute\nune colonne geometry qui enregistre la position des limites des polygones de chaque\nobservation.\nComme vu pr√©c√©demment, le syst√®me de projection est un √©l√©ment important. Il permet √† Python\nd‚Äôinterpr√©ter les valeurs des points (deux dimensions) en position sur\nla terre, qui n‚Äôest pas un espace plan.\n\nshp_communes.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nIci, les donn√©es sont dans le syst√®me WGS84 (code EPSG 4326).\nCe n‚Äôest pas le\nLambert-93 comme on pourrait s‚Äôy attendre, ce dernier\n√©tant le syst√®me l√©gal de projection pour la France\nm√©tropolitaine.\nPour s‚Äôassurer qu‚Äôon a bien r√©cup√©r√© les contours voulus,\non peut repr√©senter graphiquement\nles contours gr√¢ce √† la m√©thode plot sur laquelle nous\nreviendrons :\n\nax = shp_communes.boundary.plot()\nax.set_axis_off()\n\n\n\n\n\n\n\n\n\n\n Note\nSi on ne d√©sire pas utiliser le niveau COMMUNE_ARRONDISSEMENT,\nil est n√©cessaire de mettre en oeuvre une construction du fond de\ncarte en plusieurs phases.\nEn premier lieu, il est n√©cessaire de r√©cup√©rer le niveau des communes.\n\nimport cartiflette.s3 as s3\n\nshp_communes = s3.download_vectorfile_url_all(\n    crs = 4326,\n    values = [\"75\", \"92\", \"93\", \"94\"],\n    borders=\"COMMUNE\",\n    vectorfile_format=\"topojson\",\n    filter_by=\"DEPARTEMENT\",\n    source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n    year=2022)\n\n\nshp_communes.head(4)\n\n\n\n\n\n\n\n\nid\nID\nNOM\nNOM_M\nINSEE_COM\nSTATUT\nPOPULATION\nINSEE_CAN\nINSEE_ARR\nINSEE_DEP\nINSEE_REG\nSIREN_EPCI\nsource\ngeometry\n\n\n\n\n0\nCOMMUNE_0000000009736048\nNaN\nParis\nPARIS\n75056\nCapitale d'√©tat\n2165423\nNR\n1\n75\n11\n200054781\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.36421 48.81640, 2.36333 48.81615, ...\n\n\n0\nCOMMUNE_0000000009736037\nNaN\nLevallois-Perret\nLEVALLOIS-PERRET\n92044\nCommune simple\n66082\n16\n2\n92\n11\n200054781/200057982\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.28739 48.90364, 2.28846 48.90302, ...\n\n\n1\nCOMMUNE_0000000009736055\nNaN\nBois-Colombes\nBOIS-COLOMBES\n92009\nCommune simple\n28841\n11\n2\n92\n11\n200054781/200057990\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.26639 48.90629, 2.26645 48.90615, ...\n\n\n2\nCOMMUNE_0000000009736538\nNaN\nMalakoff\nMALAKOFF\n92046\nCommune simple\n30950\n18\n1\n92\n11\n200054781/200057966\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.27818 48.81425, 2.28066 48.81469, ...\n\n\n\n\n\n\n\n\nax = shp_communes.boundary.plot()\nax.set_axis_off()\n\n\n\n\n\n\n\n\nOn peut remarquer que la ville de Paris ne comporte pas d‚Äôarrondissements\nsur cette carte. Pour vous en convaincre, vous pouvez ex√©cuter la\ncommande :\n\nax = shp_communes.loc[shp_communes['INSEE_DEP']==\"75\"].boundary.plot()\nax.set_axis_off()\n\n\n\n\n\n\n\n\nIl faut donc utiliser une source compl√©mentaire.\nLe contour officiel des arrondissements est\nproduit par l‚ÄôIGN s√©paremment des contours de communes.\nLes contours d‚Äôarrondissements sont √©galement\ndisponibles\ngr√¢ce √† cartiflette:\n\narrondissements = s3.download_vectorfile_url_all(\n    crs = 4326,\n    values = [\"75\"],\n    borders=\"COMMUNE_ARRONDISSEMENT\",\n    vectorfile_format=\"topojson\",\n    filter_by=\"DEPARTEMENT\",\n    source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n    year=2022)\n\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE_ARRONDISSEMENT/crs=4326/DEPARTEMENT=75/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\n\n\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 28.0kiB [00:00, 167kiB/s]Downloading: : 40.1kiB [00:00, 213kiB/s]\n\n\n\nax = arrondissements.boundary.plot(alpha = 0.8, edgecolor = \"k\")\nax.set_axis_off()\n\n\n\n\n\n\n\n\nIl ne reste plus qu‚Äô√† remplacer Paris par\nses arrondissements dans shp_communes.\nPour cela, on peut utiliser les m√©thodes\nvues dans le chapitre Pandas relatives\naux filtres et √† la concat√©nation\nde plusieurs DataFrames:\n\nimport pandas as pd\n\nshp_communes = pd.concat(\n  [\n    shp_communes.loc[shp_communes['INSEE_DEP'] != \"75\"].to_crs(2154),\n    arrondissements.to_crs(2154)\n  ])\n\n\nax = shp_communes.boundary.plot(alpha = 0.8, edgecolor = \"k\")\nax.set_axis_off()\n\n\n\n\n\n\n\n\nCette approche fonctionne mais elle n√©cessite un certain nombre\nde gestes, qui sont autant de risques d‚Äôerreurs. Il est\ndonc recommand√© de privil√©gier le niveau COMMUNE_ARRONDISSEMENT\nqui fait exactement ceci mais de mani√®re fiable."
  },
  {
    "objectID": "content/manipulation/03_geopandas_tutorial.html#op√©rations-sur-les-attributs-et-les-g√©om√©tries",
    "href": "content/manipulation/03_geopandas_tutorial.html#op√©rations-sur-les-attributs-et-les-g√©om√©tries",
    "title": "Donn√©es spatiales : d√©couverte de geopandas",
    "section": "Op√©rations sur les attributs et les g√©om√©tries",
    "text": "Op√©rations sur les attributs et les g√©om√©tries\n\nImport des donn√©es v√©lib\nSouvent, le d√©coupage communal ne sert qu‚Äôen fond de cartes, pour donner des\nrep√®res. En compl√©ment de celui-ci, on peut d√©sirer exploiter\nun autre jeu de donn√©es. On va partir des donn√©es de localisation des\nstations velib,\ndisponibles sur le site d‚Äôopen data de la ville de Paris et\nrequ√™tables directement par l‚Äôurl\nhttps://opendata.paris.fr/explore/dataset/velib-emplacement-des-stations/download/?format=geojson&timezone=Europe/Berlin&lang=fr\n\nvelib_data = 'https://opendata.paris.fr/explore/dataset/velib-emplacement-des-stations/download/?format=geojson&timezone=Europe/Berlin&lang=fr'\nstations = gpd.read_file(velib_data)\nstations.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nLes donn√©es sont dans le syst√®me de projection WGS84 qui est celui du\nsyst√®me GPS. Celui-ci s‚Äôint√®gre bien avec les fonds de carte\nOpenStreetMap ou Google Maps. En toute rigueur, si on\nd√©sire effectuer certains calculs g√©om√©triques (mesurer des surfaces‚Ä¶), il est\nn√©cessaire de re-projeter les donn√©es dans un syst√®me qui pr√©serve la g√©om√©trie\n(c‚Äôest le cas du Lambert 93).\nPour avoir une intuition de la localisation des stations, et notamment de la\ndensit√© h√©t√©rog√®ne de celles-ci,\non peut afficher les donn√©es sur la carte des communes\nde la petite couronne. Il s‚Äôagit donc d‚Äôenrichir la carte\npr√©c√©dente d‚Äôune couche suppl√©mentaire, √† savoir la localisation\ndes stations. Au passage, on va utiliser un fond de carte\nplus esth√©tique:\n\nfig,ax = plt.subplots(figsize=(10, 10))\nstations.sample(200).to_crs(3857).plot(ax = ax, color = 'red', alpha = 0.4, zorder=2)\nshp_communes.to_crs(3857).plot(ax = ax, zorder=1, edgecolor = \"black\", facecolor=\"none\",\n                                                           color = None)\nctx.add_basemap(ax, source = ctx.providers.OpenStreetMap.Mapnik)\nax.set_axis_off()\n\n\n\n\n\n\n\n\nD√©couvrez ci-dessous par √©tape les diff√©rentes lignes de commandes permettant d‚Äôafficher cette carte compl√®te,\n√©tape par √©tape :\n1Ô∏è‚É£ Afficher le nuage de points de 200 stations v√©libs prises au hasard\n\nfig, ax = plt.subplots(figsize=(10, 10))\nstations.sample(200).to_crs(3857).plot(ax = ax, color = 'red', alpha = 0.4, zorder=2)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\n2Ô∏è‚É£ Ajouter √† cette couche, en-dessous, les contours des communes\n\nfig, ax = plt.subplots(figsize=(10, 10))\nstations.sample(200).to_crs(3857).plot(ax = ax, color = 'red', alpha = 0.4, zorder=2)\nshp_communes.to_crs(3857).plot(ax = ax, zorder=1, edgecolor = \"black\", facecolor=\"none\",\n                                                           color = None)\n\n\n\n\n\n\n\n\n\n\n3Ô∏è‚É£ Ajouter un fond de carte de type open street map gr√¢ce au package\ncontextily\n\nfig,ax = plt.subplots(figsize=(10, 10))\nstations.sample(200).to_crs(3857).plot(ax = ax, color = 'red', alpha = 0.4, zorder=2)\nshp_communes.to_crs(3857).plot(ax = ax, zorder=1, edgecolor = \"black\", facecolor=\"none\",\n                                                           color = None)\nctx.add_basemap(ax, source = ctx.providers.OpenStreetMap.Mapnik)\n\n\n\n\n\n\n\n\n\n\n4Ô∏è‚É£\nIl ne reste plus qu‚Äô√† retirer l‚Äôaxe des coordonn√©es, qui n‚Äôest pas tr√®s\nesth√©tique.\n\nfig,ax = plt.subplots(figsize=(10, 10))\nstations.sample(200).to_crs(3857).plot(ax = ax, color = 'red', alpha = 0.4, zorder=2)\nshp_communes.to_crs(3857).plot(ax = ax, zorder=1, edgecolor = \"black\", facecolor=\"none\",\n                                                           color = None)\nctx.add_basemap(ax, source = ctx.providers.OpenStreetMap.Mapnik)\nax.set_axis_off()\nax\n\n\n\n\n\n\n\n\n\n\nIn fine, on obtient la carte d√©sir√©e.\n\n\nOp√©rations sur les attributs\nToutes les op√©rations possibles sur un objet Pandas le sont √©galement\nsur un objet GeoPandas. Pour manipuler les donn√©es, et non la g√©om√©trie,\non parlera d‚Äôop√©rations sur les attributs.\nPar exemple, si on d√©sire\nconna√Ætre quelques statistiques sur la taille des stations, l‚Äôapproche\nest identique √† si on avait un objet Pandas classique :\n\nstations.describe()\n\n\n\n\n\n\n\n\ncapacity\n\n\n\n\ncount\n1470.000000\n\n\nmean\n31.171429\n\n\nstd\n12.784457\n\n\nmin\n0.000000\n\n\n25%\n23.000000\n\n\n50%\n29.000000\n\n\n75%\n37.000000\n\n\nmax\n200.000000\n\n\n\n\n\n\n\nPour classer les d√©partements de la petite couronne, du plus grand au plus petit,\nproc√©dons en deux √©tapes:\n\nR√©cup√©rons le contour des communes\ngr√¢ce √† cartiflette.\nNotons qu‚Äôon pourrait r√©cup√©rer directement les contours d√©partementaux mais\npour l‚Äôexercice, nous allons le cr√©er nous-m√™mes comme agr√©gation\ndes contours communaux\n(voir plus bas ainsi que ce notebook Observable pour la m√©thode plus\nl√©g√®re qui utilise pleinement les fonctionnalit√©s de cartiflette).\nCalculons la surface totale de ce territoire (m√©thode area sur un objet GeoPandas.GeoDataFrame ramen√©e en km¬≤, attention n√©amoins au syst√®me de projection comme cela est expliqu√© plus bas)\n\n\nshp_communes['surface'] = shp_communes.area.div(10**6)\n\nLes plus grands d√©partements s‚Äôobtiennent par une agr√©gation des\nsurfaces communales :\n\nshp_communes.groupby('INSEE_DEP').sum(numeric_only = True).sort_values('surface', ascending = False)\n\n\n\n\n\n\n\n\nID\nPOPULATION\nsurface\n\n\nINSEE_DEP\n\n\n\n\n\n\n\n94\n0.0\n1407124\n244.811517\n\n\n93\n0.0\n1644903\n236.868125\n\n\n92\n0.0\n1624357\n175.570765\n\n\n75\n0.0\n2165423\n105.431335\n\n\n\n\n\n\n\nSi on veut directement les plus\ngrandes communes de la petite couronne parisienne :\n\nshp_communes.sort_values('surface', ascending = False).head(10)\n\n\n\n\n\n\n\n\nid\nID\nNOM\nNOM_M\nINSEE_COM\nSTATUT\nPOPULATION\nINSEE_CAN\nINSEE_ARR\nINSEE_DEP\nINSEE_REG\nSIREN_EPCI\nsource\ngeometry\nINSEE_COG\nsurface\n\n\n\n\n23\nCOMMUNE_0000000009735015\nNaN\nTremblay-en-France\nTREMBLAY-EN-FRANCE\n93073\nCommune simple\n36461\n20\n2\n93\n11\n200054781/200058097\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((663432.643 6875170.586, 663414.781 6...\nNaN\n22.680693\n\n\n12\nARR_MUNI0000000009736553\nNaN\nParis 16e Arrondissement\nPARIS 16E ARRONDISSEMENT\n75056\nCapitale d'√©tat\n165523\nNR\n1\n75\n11\n200054781\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((647190.220 6864524.360, 647201.518 6...\n75116\n16.409713\n\n\n19\nARR_MUNI0000000009736532\nNaN\nParis 12e Arrondissement\nPARIS 12E ARRONDISSEMENT\n75056\nCapitale d'√©tat\n139297\nNR\n1\n75\n11\n200054781\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((655221.113 6858576.460, 655149.729 6...\n75112\n16.380842\n\n\n7\nCOMMUNE_0000000009735500\nNaN\nAulnay-sous-Bois\nAULNAY-SOUS-BOIS\n93005\nCommune simple\n86969\n02\n2\n93\n11\n200054781/200058097\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((660415.819 6872923.194, 660423.603 6...\nNaN\n16.167599\n\n\n31\nCOMMUNE_0000000009736056\nNaN\nRueil-Malmaison\nRUEIL-MALMAISON\n92063\nCommune simple\n78317\n22\n2\n92\n11\n200054781/200057982\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((639099.023 6866521.194, 639135.281 6...\nNaN\n14.540955\n\n\n37\nCOMMUNE_0000000009736517\nNaN\nNoisy-le-Grand\nNOISY-LE-GRAND\n93051\nCommune simple\n67871\n14\n2\n93\n11\n200054781/200058790\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((664453.846 6861358.828, 664461.038 6...\nNaN\n13.138755\n\n\n0\nCOMMUNE_0000000009735515\nNaN\nSaint-Denis\nSAINT-DENIS\n93066\nSous-pr√©fecture\n112852\n99\n3\n93\n11\n200054781/200057867\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((652098.246 6872022.511, 652102.652 6...\nNaN\n12.372213\n\n\n4\nCOMMUNE_0000000009736052\nNaN\nNanterre\nNANTERRE\n92050\nPr√©fecture\n96277\n99\n2\n92\n11\n200054781/200057982\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((643490.675 6867612.283, 643581.446 6...\nNaN\n12.231274\n\n\n37\nCOMMUNE_0000000009737009\nNaN\nVitry-sur-Seine\nVITRY-SUR-SEINE\n94081\nCommune simple\n95510\n99\n3\n94\n11\n200054781/200058014\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((653536.980 6852608.424, 653536.652 6...\nNaN\n11.665507\n\n\n20\nCOMMUNE_0000000009735517\nNaN\nGennevilliers\nGENNEVILLIERS\n92036\nCommune simple\n48530\n14\n2\n92\n11\n200054781/200057990\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((649583.723 6872218.087, 649556.138 6...\nNaN\n11.632021\n\n\n\n\n\n\n\nLors des √©tapes d‚Äôagr√©gation, groupby ne conserve pas les g√©om√©tries. Autrement\ndit, si on effectue, par exemple, une somme en fonction d‚Äôune variable de groupe avec\nle combo groupby(...).sum(...) , on perd\nla dimension g√©ographique.\nIl est n√©anmoins possible d‚Äôaggr√©ger √† la fois les g√©om√©tries et les\nattribus avec la m√©thode dissolve:\n\nfig,ax = plt.subplots(figsize=(10, 10))\nshp_communes.dissolve(by='INSEE_DEP', aggfunc='sum').plot(ax = ax, column = \"surface\")\nax.set_axis_off()\nax\n\n/opt/mamba/lib/python3.9/site-packages/geopandas/plotting.py:730: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nPour produire l‚Äô√©quivalent de cette carte √† un niveau France enti√®re, il est n√©anmoins plus simple de directement\nr√©cup√©rer les fonds officiels des d√©partements plut√¥t que d‚Äôagr√©ger les\ncontours des communes:\n\ndep = s3.download_vectorfile_url_all(\n    values = \"metropole\",\n    crs = 4326,\n    borders = \"DEPARTEMENT\",\n    vectorfile_format=\"topojson\",\n    filter_by=\"FRANCE_ENTIERE\",\n    source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n    year=2022)\n\ndep[\"area\"] = dep.to_crs(2154).area\n\nAvant de calculer les surfaces des d√©partements, pour √©viter les d√©formations li√©es au\nsyst√®me Mercator, nous faisons une reprojection des donn√©es √† la vol√©e. Plus de d√©tails\npar la suite.\n\ndep.sort_values('area', ascending = False).head(3)\n\n\n\n\n\n\n\n\nid\nID\nNOM_M\nNOM\nINSEE_DEP\nINSEE_REG\nsource\nterritoire\ngeometry\narea\n\n\n\n\n33\nDEPARTEM_FXX_00000000034\nNaN\nGIRONDE\nGironde\n33\n75\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nmetropole\nMULTIPOLYGON (((-1.15275 45.60453, -1.15084 45...\n1.036783e+10\n\n\n40\nDEPARTEM_FXX_00000000041\nNaN\nLANDES\nLandes\n40\n75\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nmetropole\nPOLYGON ((-1.25361 44.46752, -1.25317 44.46762...\n9.354177e+09\n\n\n24\nDEPARTEM_FXX_00000000025\nNaN\nDORDOGNE\nDordogne\n24\n75\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nmetropole\nPOLYGON ((-0.04044 45.10261, -0.04073 45.10309...\n9.210826e+09\n\n\n\n\n\n\n\n\nax = dep.plot(column = \"area\")\nax.set_axis_off()\n\n/opt/mamba/lib/python3.9/site-packages/geopandas/plotting.py:730: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n\n\n\n\n\n\n\n\n\nOp√©rations sur les g√©om√©tries\nOutre la repr√©sentation graphique simplifi√©e,\nsur laquelle nous reviendrons ult√©rieurement, l‚Äôint√©r√™t principal d‚Äôutiliser\nGeoPandas est l‚Äôexistence de m√©thodes efficaces pour\nmanipuler la dimension spatiale. Un certain nombre proviennent du\npackage\nShapely.\n\n\n Warning\nLes donn√©es sont en syst√®me de coordonn√©es WGS 84 ou pseudo-Mercator (epsg: 4326) et ne sont pas projet√©es.\nC‚Äôest un format appropri√© lorsqu‚Äôil s‚Äôagit d‚Äôutiliser un fonds\nde carte OpenStreetMap, Stamen, Google Maps, etc.\nMais ce n‚Äôest pas un\nformat sur lequel on d√©sire faire des calculs car les distances sont fauss√©es sans utiliser de projection. D‚Äôailleurs, geopandas refusera certaines op√©rations\nsur des donn√©es dont le crs est 4326. On reprojette ainsi les donn√©es\ndans la projection officielle pour la m√©tropole, le Lambert 93\n(epsg: 2154).\n\n\nComme indiqu√© ci-dessus, nous reprojetons les donn√©es\ndans le syst√®me Lambert 93 qui ne fausse pas les\ncalculs de distance et d‚Äôaires.\n\ncommunes = shp_communes.to_crs(2154)\nstations = stations.to_crs(2154)\n\nPar exemple, on peut recalculer la taille d‚Äôune commune ou d‚Äôarrondissement\navec la m√©thode area (et diviser par \\(10^6\\) pour avoir des \\(km^2\\) au lieu\ndes \\(m^2\\)):\n\ncommunes['superficie'] = communes.area.div(10**6)\ncommunes.head(3)\n\n\n\n\n\n\n\n\nid\nID\nNOM\nNOM_M\nINSEE_COM\nSTATUT\nPOPULATION\nINSEE_CAN\nINSEE_ARR\nINSEE_DEP\nINSEE_REG\nSIREN_EPCI\nsource\ngeometry\nINSEE_COG\nsurface\nsuperficie\n\n\n\n\n0\nCOMMUNE_0000000009736037\nNaN\nLevallois-Perret\nLEVALLOIS-PERRET\n92044\nCommune simple\n66082\n16\n2\n92\n11\n200054781/200057982\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((647761.341 6867306.988, 647839.223 6...\nNaN\n2.417491\n2.417491\n\n\n1\nCOMMUNE_0000000009736055\nNaN\nBois-Colombes\nBOIS-COLOMBES\n92009\nCommune simple\n28841\n11\n2\n92\n11\n200054781/200057990\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((646224.655 6867615.810, 646228.990 6...\nNaN\n1.926619\n1.926619\n\n\n2\nCOMMUNE_0000000009736538\nNaN\nMalakoff\nMALAKOFF\n92046\nCommune simple\n30950\n18\n1\n92\n11\n200054781/200057966\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((646995.323 6857373.499, 647177.485 6...\nNaN\n2.070953\n2.070953\n\n\n\n\n\n\n\nUne m√©thode qu‚Äôon utilise r√©guli√®rement est centroid qui, comme son nom l‚Äôindique,\nrecherche le centro√Øde de chaque polygone et transforme ainsi des donn√©es\nsurfaciques en donn√©es ponctuelles. Par exemple, pour\nrepr√©senter approximativement les centres des villages de la\nHaute-Garonne (31), apr√®s avoir t√©l√©charg√© le fonds de carte adapt√©,\nfera\n\ncommunes_31 = s3.download_vectorfile_url_all(\n      crs = 4326,\n      values = \"31\",\n      borders=\"COMMUNE\",\n      vectorfile_format=\"topojson\",\n      filter_by=\"DEPARTEMENT\",\n      source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n      year=2022)\n\n# on reprojete en 3857 pour le fond de carte\ncommunes_31 = communes_31.to_crs(3857)\n\n# on calcule le centroide\ndep_31 = communes_31.copy()\ncommunes_31['geometry'] = communes_31['geometry'].centroid\n\nax = communes_31.plot(figsize = (10,10), color = 'red', alpha = 0.4, zorder=2)\ndep_31.to_crs(3857).plot(ax = ax, zorder=1, edgecolor = \"black\", facecolor=\"none\",\n                                                           color = None)\n#ctx.add_basemap(ax, source = ctx.providers.Stamen.Toner)\nax.set_axis_off()\nax\n\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=31/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\n\n\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 12.0kiB [00:00, 119kiB/s]Downloading: : 44.0kiB [00:00, 234kiB/s]Downloading: : 96.0kiB [00:00, 359kiB/s]Downloading: : 200kiB [00:00, 620kiB/s] Downloading: : 404kiB [00:00, 1.12MiB/s]Downloading: : 816kiB [00:00, 2.11MiB/s]Downloading: : 1.59MiB [00:00, 4.05MiB/s]Downloading: : 1.60MiB [00:00, 2.29MiB/s]\n\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "content/manipulation/03_geopandas_tutorial.html#g√©rer-le-syst√®me-de-projection",
    "href": "content/manipulation/03_geopandas_tutorial.html#g√©rer-le-syst√®me-de-projection",
    "title": "Donn√©es spatiales : d√©couverte de geopandas",
    "section": "G√©rer le syst√®me de projection",
    "text": "G√©rer le syst√®me de projection\nPr√©c√©demment, nous avons appliqu√© une m√©thode to_crs pour reprojeter\nles donn√©es dans un syst√®me de projection diff√©rent de celui du fichier\nd‚Äôorigine :\n\ncommunes = communes.to_crs(2154)\nstations = stations.to_crs(2154)\n\nConcernant la gestion des projections avec GeoPandas,\nla documentation officielle est tr√®s bien\nfaite. Elle fournit notamment l‚Äôavertissement suivant qu‚Äôil est\nbon d‚Äôavoir en t√™te :\n\nBe aware that most of the time you don‚Äôt have to set a projection. Data loaded from a reputable source (using the geopandas.read_file() command) should always include projection information. You can see an objects current CRS through the GeoSeries.crs attribute.\nFrom time to time, however, you may get data that does not include a projection. In this situation, you have to set the CRS so geopandas knows how to interpret the coordinates.\n\n\n\n\nImage emprunt√©e √† XKCD https://xkcd.com/2256/ qu‚Äôon peut √©galement trouver sur https://blog.chrislansdown.com/2020/01/17/a-great-map-projection-joke/\n\n\nPour d√©terminer le syst√®me de projection d‚Äôune base de donn√©es, on peut v√©rifier l‚Äôattribut crs :\n\ncommunes.crs\n\n&lt;Projected CRS: EPSG:2154&gt;\nName: RGF93 v1 / Lambert-93\nAxis Info [cartesian]:\n- X[east]: Easting (metre)\n- Y[north]: Northing (metre)\nArea of Use:\n- name: France - onshore and offshore, mainland and Corsica (France m√©tropolitaine including Corsica).\n- bounds: (-9.86, 41.15, 10.38, 51.56)\nCoordinate Operation:\n- name: Lambert-93\n- method: Lambert Conic Conformal (2SP)\nDatum: Reseau Geodesique Francais 1993 v1\n- Ellipsoid: GRS 1980\n- Prime Meridian: Greenwich\n\n\nLes deux principales m√©thodes pour d√©finir le syst√®me de projection utilis√© sont :\n\ndf.set_crs : cette commande sert √† pr√©ciser quel est le syst√®me de projection utilis√©, c‚Äôest-√†-dire comment les coordonn√©es (x,y) sont reli√©es √† la surface terrestre. Cette commande ne doit pas √™tre utilis√©e pour transformer le syst√®me de coordonn√©es, seulement pour le d√©finir.\ndf.to_crs : cette commande sert √† projeter les points d‚Äôune g√©om√©trie dans une autre, c‚Äôest-√†-dire √† recalculer les coordonn√©es selon un autre syst√®me de projection.\n\nDans le cas particulier de production de carte avec un fond OpenStreetMaps ou une carte dynamique leaflet, il est n√©cessaire de d√©-projeter les donn√©es (par exemple √† partir du Lambert-93) pour atterrir dans le syst√®me non-projet√© WGS 84 (code EPSG 4326). Ce site d√©di√© aux projections g√©ographiques peut √™tre utile pour retrouver le syst√®me de projection d‚Äôun fichier o√π il n‚Äôest pas indiqu√©.\nLa d√©finition du syst√®me de projection se fait de la mani√®re suivante (:warning: avant de le faire, se souvenir de l‚Äôavertissement !) :\ncommunes = communes.set_crs(2154)\nAlors que la reprojection (projection Albers : 5070) s‚Äôobtient de la mani√®re suivante :\n\nshp_region = s3.download_vectorfile_url_all(\n    values = \"metropole\",\n    crs = 4326,\n    borders = \"REGION\",\n    vectorfile_format=\"topojson\",\n    filter_by=\"FRANCE_ENTIERE\",\n    source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n    year=2022)\n    \nfig,ax = plt.subplots(figsize=(10, 10))\nshp_region.to_crs(5070).plot(ax = ax)\nax\n\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=REGION/crs=4326/FRANCE_ENTIERE=metropole/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\n\n\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 29.0kiB [00:00, 178kiB/s]Downloading: : 93.0kiB [00:00, 368kiB/s]Downloading: : 289kiB [00:00, 886kiB/s] Downloading: : 629kiB [00:00, 1.59MiB/s]Downloading: : 893kiB [00:00, 1.60MiB/s]Downloading: : 2.65MiB [00:00, 6.09MiB/s]Downloading: : 3.61MiB [00:00, 7.15MiB/s]Downloading: : 4.55MiB [00:01, 7.91MiB/s]Downloading: : 4.76MiB [00:01, 4.87MiB/s]\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nOn le voit, cela modifie totalement la repr√©sentation de l‚Äôobjet dans l‚Äôespace.\nClairement, cette projection n‚Äôest pas adapt√©e aux longitudes et latitudes fran√ßaises.\nC‚Äôest normal, il s‚Äôagit d‚Äôune projection adapt√©e au continent\nnord-am√©ricain (et encore, pas dans son ensemble !).\n\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n\nfig,ax = plt.subplots(figsize=(10, 10))\nworld[world.continent == \"North America\"].to_crs(5070).plot(alpha = 0.2, edgecolor = \"k\", ax = ax)\nax\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "content/manipulation/03_geopandas_tutorial.html#joindre-des-donn√©es",
    "href": "content/manipulation/03_geopandas_tutorial.html#joindre-des-donn√©es",
    "title": "Donn√©es spatiales : d√©couverte de geopandas",
    "section": "Joindre des donn√©es",
    "text": "Joindre des donn√©es\n\nJoindre des donn√©es sur des attributs\nCe type de jointure se fait entre un objet g√©ographique et un\ndeuxi√®me objet, g√©ographique ou non. A l‚Äôexception de la question\ndes g√©om√©tries, il n‚Äôy a pas de diff√©rence par rapport √† Pandas.\nLa seule diff√©rence avec Pandas est dans la dimension g√©ographique.\nSi on d√©sire conserver la dimension g√©ographique, il faut faire\nattention √† faire :\ngeopandas_object.merge(pandas_object)\nSi on utilise deux objets g√©ographiques mais ne d√©sire conserver qu‚Äôune seule\ndimension g√©ographique1, on fera\ngeopandas_object1.merge(geopandas_object2)\nSeule la g√©om√©trie de l‚Äôobjet de gauche\nsera conserv√©e, m√™me si on fait un right join.\n\n\nProlongation possible : joindre des donn√©es sur dimension g√©ographique\nLe chapitre suivant permettra de mettre en oeuvre des\njointures g√©ographiques.\n\n\n Hint\nLes jointures spatiales peuvent √™tre tr√®s gourmandes en ressources (car il peut √™tre n√©cessaire de croiser toutes les g√©om√©tries de x avec toutes les g√©om√©tries de y). Voici deux conseils qui peuvent vous aider :\n\nIl est pr√©f√©rable de tester les jointures g√©ographiques sur un petit √©chantillon de donn√©es, pour estimer le temps et les ressources n√©cessaires √† la r√©alisation de la jointure.\nIl est parfois possible d‚Äô√©crire une fonction qui r√©duit la taille du probl√®me. Exemple: vous voulez d√©terminer dans quelle commune se situe un logement dont vous connaissez les coordonn√©es et le d√©partement; vous pouvez √©crire une fonction qui r√©alise pour chaque d√©partement une jointure spatiale entre les logements situ√©s dans ce d√©partement et les communes de ce d√©partement, puis empiler les 101 tables de sorties."
  },
  {
    "objectID": "content/manipulation/03_geopandas_tutorial.html#footnotes",
    "href": "content/manipulation/03_geopandas_tutorial.html#footnotes",
    "title": "Donn√©es spatiales : d√©couverte de geopandas",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIl est techniquement possible d‚Äôavoir un DataFrame comportant plusieurs\ng√©ographies. Par exemple une g√©om√©trie polygone et une g√©om√©trie point\n(le centroid). C‚Äôest n√©anmoins souvent compliqu√© √† g√©rer et donc peu\nrecommandable.‚Ü©Ô∏é"
  },
  {
    "objectID": "content/manipulation/04a_webscraping_TP.html",
    "href": "content/manipulation/04a_webscraping_TP.html",
    "title": "Web scraping avec Python",
    "section": "",
    "text": "Le web scraping d√©signe les techniques d‚Äôextraction du contenu des sites internet.\nC‚Äôest une pratique tr√®s utile pour toute personne souhaitant travailler sur des informations disponibles en ligne, mais n‚Äôexistant pas forc√©ment sous la forme d‚Äôun tableau Excel.\nCe TP vous pr√©sente comment cr√©er et ex√©cuter des robots afin de recup√©rer rapidement des informations utiles √† vos projets actuels ou futurs.\nIl part de quelques cas d‚Äôusages concret.\nCe chapitre est tr√®s fortement inspir√© et r√©adapt√© √† partir de celui de Xavier Dupr√©, l‚Äôancien professeur de la mati√®re."
  },
  {
    "objectID": "content/manipulation/04a_webscraping_TP.html#enjeux",
    "href": "content/manipulation/04a_webscraping_TP.html#enjeux",
    "title": "Web scraping avec Python",
    "section": "Enjeux",
    "text": "Enjeux\nUn certain nombre d‚Äôenjeux du web scraping ne seront √©voqu√©s\nque superficiellement dans le cadre de ce chapitre.\n\nLa zone grise de la l√©galit√© du web scraping\nEn premier lieu, en ce qui concerne la question de la l√©galit√©\nde la r√©cup√©ration d‚Äôinformation par scraping, il existe\nune zone grise. Ce n‚Äôest pas parce qu‚Äôune information est\ndisponible sur internet, directement ou avec un peu de recherche,\nqu‚Äôelle peut √™tre r√©cup√©r√©e et r√©utilis√©e.\nL‚Äôexcellent cours d‚ÄôAntoine Palazzolo √©voque un certain nombre de cas\nm√©diatiques et judiciaires sur cette question.\nDans le champ fran√ßais, la CNIL a publi√© en 2020\nde nouvelles directives sur le web scraping repr√©cisant\nque toute donn√©e ne peut √™tre r√©utilis√©e √† l‚Äôinsu de la personne\n√† laquelle ces donn√©es appartiennent. Autrement dit, en principe,\nles donn√©es collect√©es par web scraping sont soumises au\nRGPD, c‚Äôest-√†-dire n√©cessitent le consentement des personnes\n√† partir desquelles la r√©utilisation des donn√©es est faite.\nIl est donc recommand√© d‚Äô√™tre vigilant avec les donn√©es r√©cup√©r√©es\npar web scraping pour ne pas se mettre en faute l√©galement.\n\n\nStabilit√© et fiabilit√© des informations re√ßues\nLa r√©cup√©ration de donn√©es par web scraping\nest certes pratique mais elle ne correspond pas n√©cessairement\n√† un usage pens√©, ou d√©sir√©, par un fournisseur de donn√©es.\nLes donn√©es √©tant co√ªteuses √† collecter et √† mettre √† disposition,\ncertains sites ne d√©sirent pas n√©cessairement que celles-ci soient\nextraites gratuitement et facilement. A fortiori lorsque la donn√©e\npeut permettre √† un concurrent de disposer d‚Äôune information\nutile d‚Äôun point de vue commercial (prix d‚Äôun produit concurrent, etc.).\nLes acteurs mettent donc souvent en oeuvre des strat√©gies pour bloquer ou\nlimiter la quantit√© de donn√©es scrap√©es. La m√©thode la plus\nclassique est la d√©tection et le blocage\ndes requ√™tes faites par des robots plut√¥t que par des humains.\nPour des acteurs sp√©cialis√©s, cette d√©tection est tr√®s facile car\nde nombreuses preuves permettent d‚Äôidentifier si une visite du site web\nprovient d‚Äôun utilisateur\nhumain derri√®re un navigateur ou d‚Äôun robot. Pour ne citer que quelques indices :\nvitesse de la navigation entre pages, rapidit√© √† extraire la donn√©e,\nempreinte digitale du navigateur utilis√©, capacit√© √† r√©pondre √† des\nquestions al√©atoires (captcha)‚Ä¶\nLes bonnes pratiques, √©voqu√©es par la suite, ont pour objectif de faire\nen sorte qu‚Äôun robot se comporte de mani√®re civile en adoptant un comportement\nproche de celui de l‚Äôhumain mais sans contrefaire le fait qu‚Äôil ne s‚Äôagit\npas d‚Äôun humain.\nIl convient d‚Äôailleurs\nd‚Äô√™tre prudent quant aux informations re√ßues par web scraping.\nLa donn√©e √©tant au coeur du mod√®le √©conomique de certains acteurs, certains\nn‚Äôh√©sitent pas √† renvoyer des donn√©es fausses aux robots\nplut√¥t que les bloquer. C‚Äôest de bonne guerre !\nUne autre technique pi√®ge s‚Äôappelle le honey pot. Il s‚Äôagit de pages qu‚Äôun humain\nn‚Äôirait jamais visiter - par exemple parce qu‚Äôelles n‚Äôapparaissent pas dans\nl‚Äôinterface graphique - mais sur lesquelles un robot, en recherche automatique\nde contenu, va rester bloquer.\nSans aller jusqu‚Äô√† la strat√©gie de blocage du web scraping, d‚Äôautres raisons\npeuvent expliquer qu‚Äôune r√©cup√©ration de donn√©es ait fonctionn√© par\nle pass√© mais ne fonctionne plus. La plus fr√©quente est un changement dans la structure\nd‚Äôun site web. Le web scraping pr√©sente en effet l‚Äôinconv√©nient d‚Äôaller chercher\nde l‚Äôinformation dans une structure tr√®s hi√©rarchis√©e. Un changement dans cette structure\npeut suffire √† rendre un robot incapable de r√©cup√©rer du contenu. Or, pour rester\nattractifs, les sites web changent fr√©quemment ce qui peut facilement\nrendre inop√©rant un robot.\nDe mani√®re g√©n√©rale, l‚Äôun des principaux messages de ce\nchapitre, √† retenir, est que le\nweb scraping est une solution de dernier ressort, pour des r√©cup√©rations ponctuelles de donn√©es sans garantie de fonctionnement ult√©rieur. Il est pr√©f√©rable de privil√©gier les API lorsque celles-ci sont disponibles.\nCes derni√®res ressemblent √† un contrat (formel ou non) entre un fournisseur de donn√©es\net un utilisateur o√π sont d√©finis des besoins (les donn√©es) mais aussi des\nconditions d‚Äôacc√®s (nombre de requ√™tes, volum√©trie, authentification‚Ä¶) l√†\no√π le web scraping est plus proche du comportement dans le Far West.\n\n\nLes bonnes pratiques\nLa possibilit√© de r√©cup√©rer des donn√©es par l‚Äôinterm√©diaire\nd‚Äôun robot ne signifie pas qu‚Äôon peut se permettre de ne pas √™tre\ncivilis√©. En effet, lorsqu‚Äôil est non-ma√Ætris√©, le\nweb scraping peut ressembler √† une attaque informatique\nclassique pour faire sauter un site web : le d√©ni de service.\nLe cours d‚ÄôAntoine Palazzolo revient\nsur certaines bonnes pratiques qui ont √©merg√© dans la communaut√©\ndes scrapeurs. Il est recommand√© de lire cette ressource\npour en apprendre plus sur ce sujet. Y sont √©voqu√©es\nplusieurs conventions, parmi lesquelles :\n\nSe rendre, depuis la racine du site,\nsur le fichier robots.txt pour v√©rifier les consignes\npropos√©es par les d√©veloppeurs du site web pour\ncadrer le comportement des robots ;\nEspacer chaque requ√™tes de plusieurs secondes, comme le ferait\nun humain, afin d‚Äô√©viter de surcharger le site web et de le\nfaire sauter par d√©ni de service ;\nFaire les requ√™tes dans les heures creuses de fr√©quentation du\nsite web s‚Äôil ne s‚Äôagit pas d‚Äôun site consult√© internationalement.\nPar exemple, pour un site en fran√ßais, lancer le robot\npendant la nuit en France m√©tropolitaine, est une bonne pratique.\nPour lancer un robot depuis Python √† une heure programm√©e\n√† l‚Äôavance, il existe les cronjobs."
  },
  {
    "objectID": "content/manipulation/04a_webscraping_TP.html#un-d√©tour-par-le-web-comment-fonctionne-un-site",
    "href": "content/manipulation/04a_webscraping_TP.html#un-d√©tour-par-le-web-comment-fonctionne-un-site",
    "title": "Web scraping avec Python",
    "section": "Un d√©tour par le Web : comment fonctionne un site ?",
    "text": "Un d√©tour par le Web : comment fonctionne un site ?\nM√™me si ce TP ne vise pas √† faire un cours de web, il vous faut n√©anmoins certaines bases sur la mani√®re dont un site internet fonctionne afin de comprendre comment sont structur√©es les informations sur une page.\nUn site Web est un ensemble de pages cod√©es en HTML qui permet de d√©crire √† la fois le contenu et la forme d‚Äôune page Web.\nPour voir cela, ouvrez n‚Äôimporte quelle page web et faites un clic-droit dessus.\n- Sous Chrome  : Cliquez ensuite sur ‚ÄúAffichez le code source de la page‚Äù (CTRL+U) ;\n- Sous Firefox  : ‚ÄúCode source de la page‚Äù (CTRL+MAJ+K) ;\n- Sous Edge  : ‚ÄúAffichez la page source‚Äù (CTRL+U) ;\n- Sous Safari  : voir comment faire ici\nSi vous savez quel √©l√©ment vous int√©resse, vous pouvez √©galement ouvrir l‚Äôinspecteur du navigateur (clic droit sur l‚Äô√©l√©ment + ‚ÄúInspecter‚Äù),\npour afficher les balises encadrant votre √©l√©ment de fa√ßon plus ergonomique, un peu comme un zoom.\n\nLes balises\nSur une page web, vous trouverez toujours √† coup s√ªr des √©l√©ments comme &lt;head&gt;, &lt;title&gt;, etc. Il s‚Äôagit des codes qui vous permettent de structurer le contenu d‚Äôune page HTML et qui s‚Äôappellent des balises.\nCitons, par exemple, les balises &lt;p&gt;, &lt;h1&gt;, &lt;h2&gt;, &lt;h3&gt;, &lt;strong&gt; ou &lt;em&gt;.\nLe symbole &lt; &gt; est une balise : il sert √† indiquer le d√©but d‚Äôune partie. Le symbole &lt;/ &gt; indique la fin de cette partie. La plupart des balises vont par paires, avec une balise ouvrante et une balise fermante (par exemple &lt;p&gt; et &lt;/p&gt;).\nPar exemple, les principales balises\nd√©finissant la structure d‚Äôun tableau sont les suivantes:\n\n\n\nBalise\nDescription\n\n\n\n\n&lt;table&gt;\nTableau\n\n\n&lt;caption&gt;\nTitre du tableau\n\n\n&lt;tr&gt;\nLigne de tableau\n\n\n&lt;th&gt;\nCellule d‚Äôen-t√™te\n\n\n&lt;td&gt;\nCellule\n\n\n&lt;thead&gt;\nSection de l‚Äôen-t√™te du tableau\n\n\n&lt;tbody&gt;\nSection du corps du tableau\n\n\n&lt;tfoot&gt;\nSection du pied du tableau\n\n\n\nApplication : un tableau en HTML\nLe code HTML du tableau suivant:\n&lt;table&gt;\n&lt;caption&gt; Le Titre de mon tableau &lt;/caption&gt;\n\n   &lt;tr&gt;\n      &lt;th&gt;Pr√©nom&lt;/th&gt;\n      &lt;th&gt;Nom&lt;/th&gt;\n      &lt;th&gt;Profession&lt;/th&gt;\n   &lt;/tr&gt;\n   &lt;tr&gt;\n      &lt;td&gt;Mike &lt;/td&gt;\n      &lt;td&gt;Stuntman&lt;/td&gt;\n      &lt;td&gt;Cascadeur&lt;/td&gt;\n   &lt;/tr&gt;\n   &lt;tr&gt;\n      &lt;td&gt;Mister&lt;/td&gt;\n      &lt;td&gt;Pink&lt;/td&gt;\n      &lt;td&gt;Gangster&lt;/td&gt;\n   &lt;/tr&gt;\n&lt;/table&gt;\nDonnera dans le navigateur :\n\n\nLe Titre de mon tableau\n\n\nPr√©nom\nNom\nProfession\n\n\nMike\nStuntman\nCascadeur\n\n\nMister\nPink\nGangster\n\n\n\n\n\nParent et enfant\nDans le cadre du langage HTML, les termes de parent (parent) et enfant (child) servent √† d√©signer des √©lements embo√Æt√©s les uns dans les autres. Dans la construction suivante, par exemple :\n&lt;div&gt; \n    &lt;p&gt;\n       bla,bla\n    &lt;/p&gt;\n&lt;/div&gt;\nSur la page web, cela apparaitra de la mani√®re suivante :\n\n \n    \n       bla,bla\n    \n\n\nOn dira que l‚Äô√©l√©ment &lt;div&gt; est le parent de l‚Äô√©l√©ment &lt;p&gt; tandis que l‚Äô√©l√©ment &lt;p&gt; est l‚Äôenfant de l‚Äô√©l√©ment &lt;div&gt;.\n\nMais pourquoi apprendre √ßa pour ‚Äúscraper‚Äù ?\n\nParce que, pour bien r√©cup√©rer les informations d‚Äôun site internet, il faut pouvoir comprendre sa structure et donc son code HTML. Les fonctions Python qui servent au scraping sont principalement construites pour vous permettre de naviguer entre les balises.\nAvec Python, vous allez en fait reproduire votre comportement manuel de recherche de mani√®re\n√† l‚Äôautomatiser."
  },
  {
    "objectID": "content/manipulation/04a_webscraping_TP.html#scraper-avec-python-le-package-beautifulsoup",
    "href": "content/manipulation/04a_webscraping_TP.html#scraper-avec-python-le-package-beautifulsoup",
    "title": "Web scraping avec Python",
    "section": "Scraper avec Python: le package BeautifulSoup",
    "text": "Scraper avec Python: le package BeautifulSoup\n\nLes packages disponibles\nDans la premi√®re partie de ce chapitre,\nnous allons essentiellement utiliser le package BeautifulSoup4,\nen conjonction avec urllib\nou requests. Ces deux derniers packages permettent de r√©cup√©rer le texte\nbrut d‚Äôune page qui sera ensuite\ninspect√© via BeautifulSoup4.\nBeautifulSoup sera suffisant quand vous voudrez travailler sur des pages HTML statiques. D√®s que les informations que vous recherchez sont g√©n√©r√©es via l‚Äôex√©cution de scripts Javascript, il vous faudra passer par des outils comme Selenium.\nDe m√™me, si vous ne connaissez pas l‚ÄôURL, il faudra passer par un framework comme Scrapy, qui passe facilement d‚Äôune page √† une autre. On appelle\ncette technique le ‚Äúweb crawling‚Äù. Scrapy est plus complexe √† manipuler que BeautifulSoup : si vous voulez plus de d√©tails, rendez-vous sur la page du tutoriel Scrapy.\nLe web scraping est un domaine o√π la reproductibilit√© est compliqu√©e √† mettre en oeuvre.\nUne page web √©volue\npotentiellement r√©guli√®rement et d‚Äôune page web √† l‚Äôautre, la structure peut\n√™tre tr√®s diff√©rente ce qui rend certains codes difficilement exportables.\nPar cons√©quent, la meilleure mani√®re d‚Äôavoir un programme fonctionnel est\nde comprendre la structure d‚Äôune page web et dissocier les √©l√©ments exportables\n√† d‚Äôautres cas d‚Äôusages des requ√™tes ad hoc.\n\n!pip install -q lxml\n\nimport bs4\nimport lxml\nimport pandas\nimport urllib\n\nfrom urllib import request\n\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\n\n Note\nPour √™tre en mesure d‚Äôutiliser Selenium, il est n√©cessaire\nde faire communiquer Python avec un navigateur web (Firefox ou Chromium).\nLe package webdriver-manager permet de faire savoir √† Python o√π\nse trouve ce navigateur s‚Äôil est d√©j√† install√© dans un chemin standard.\nPour l‚Äôinstaller, le code de la cellule ci-dessous peut √™tre utilis√©.\n\n\nPour faire fonctionner Selenium, il faut utiliser un package\nnomm√© webdriver-manager:\n\n!pip install webdriver-manager\n\nCollecting webdriver-manager\n  Obtaining dependency information for webdriver-manager from https://files.pythonhosted.org/packages/b1/51/b5c11cf739ac4eecde611794a0ec9df420d0239d51e73bc19eb44f02b48b/webdriver_manager-4.0.1-py2.py3-none-any.whl.metadata\n  Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: requests in /opt/mamba/lib/python3.9/site-packages (from webdriver-manager) (2.31.0)\nCollecting python-dotenv (from webdriver-manager)\n  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\nRequirement already satisfied: packaging in /opt/mamba/lib/python3.9/site-packages (from webdriver-manager) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /opt/mamba/lib/python3.9/site-packages (from packaging-&gt;webdriver-manager) (3.0.9)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/mamba/lib/python3.9/site-packages (from requests-&gt;webdriver-manager) (2.1.1)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/mamba/lib/python3.9/site-packages (from requests-&gt;webdriver-manager) (3.3)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /opt/mamba/lib/python3.9/site-packages (from requests-&gt;webdriver-manager) (1.26.11)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /opt/mamba/lib/python3.9/site-packages (from requests-&gt;webdriver-manager) (2022.9.24)\nDownloading webdriver_manager-4.0.1-py2.py3-none-any.whl (27 kB)\nInstalling collected packages: python-dotenv, webdriver-manager\nSuccessfully installed python-dotenv-1.0.0 webdriver-manager-4.0.1\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\n\nR√©cup√©rer le contenu d‚Äôune page HTML\nOn va commencer doucement. Prenons une page wikipedia,\npar exemple celle de la Ligue 1 de football, mill√©sime 2019-2020 : Championnat de France de football 2019-2020. On va souhaiter r√©cup√©rer la liste des √©quipes, ainsi que les url des pages Wikipedia de ces √©quipes.\nEtape 1Ô∏è‚É£ : se connecter √† la page wikipedia et obtenir le code source.\nPour cela, le plus simple est d‚Äôutiliser le package urllib ou, mieux, requests.\nNous allons ici utiliser la fonction request du package urllib:\n\nurl_ligue_1 = \"https://fr.wikipedia.org/wiki/Championnat_de_France_de_football_2019-2020\"\n    \nrequest_text = request.urlopen(url_ligue_1).read()\n# print(request_text[:1000])    \n\n\ntype(request_text)\n\nbytes\n\n\nEtape 2Ô∏è‚É£ : utiliser le package BeautifulSoup\nqui permet de rechercher efficacement\nles balises contenues dans la chaine de caract√®res\nrenvoy√©e par la fonction request:\n\npage = bs4.BeautifulSoup(request_text, \"lxml\")\n\nSi on print l‚Äôobjet page cr√©√©e avec BeautifulSoup,\non voit que ce n‚Äôest plus une chaine de caract√®res mais bien une page HTML avec des balises.\nOn peut √† pr√©sent chercher des √©lements √† l‚Äôint√©rieur de ces balises.\n\n\nLa m√©thode find\nPar exemple, si on veut conna√Ætre le titre de la page, on utilise la m√©thode .find et on lui demande ‚Äútitle‚Äù\n\nprint(page.find(\"title\"))\n\n&lt;title&gt;Championnat de France de football 2019-2020 ‚Äî Wikip√©dia&lt;/title&gt;\n\n\nLa methode .find ne renvoie que la premi√®re occurence de l‚Äô√©l√©ment.\nPour vous en assurer vous pouvez :\n\ncopier le bout de code source obtenu lorsque vous chercher une table,\nle coller dans une cellule de votre notebook\net passer la cellule en ‚ÄúMarkdown‚Äù\n\nLa cellule avec le copier-coller du code source donne :\n\nprint(page.find(\"table\"))\n\n&lt;table&gt;&lt;caption style=\"background-color:#99cc99;color:#000000;\"&gt;G√©n√©ralit√©s&lt;/caption&gt;&lt;tbody&gt;&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Sport&lt;/th&gt;\n&lt;td&gt;\n&lt;a href=\"/wiki/Football\" title=\"Football\"&gt;Football&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Organisateur(s)&lt;/th&gt;\n&lt;td&gt;\n&lt;a href=\"/wiki/Ligue_de_football_professionnel_(France)\" title=\"Ligue de football professionnel (France)\"&gt;LFP&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;√âdition&lt;/th&gt;\n&lt;td&gt;\n&lt;abbr class=\"abbr\" title=\"Quatre-vingt-deuxi√®me (huitante-deuxi√®me / octante-deuxi√®me)\"&gt;82&lt;sup&gt;e&lt;/sup&gt;&lt;/abbr&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Lieu(x)&lt;/th&gt;\n&lt;td&gt;\n&lt;span class=\"datasortkey\" data-sort-value=\"France\"&gt;&lt;span class=\"flagicon\"&gt;&lt;span class=\"mw-image-border noviewer\" typeof=\"mw:File\"&gt;&lt;a class=\"mw-file-description\" href=\"/wiki/Fichier:Flag_of_France.svg\" title=\"Drapeau de la France\"&gt;&lt;img alt=\"Drapeau de la France\" class=\"mw-file-element\" data-file-height=\"600\" data-file-width=\"900\" decoding=\"async\" height=\"13\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Flag_of_France.svg/20px-Flag_of_France.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Flag_of_France.svg/30px-Flag_of_France.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Flag_of_France.svg/40px-Flag_of_France.svg.png 2x\" width=\"20\"/&gt;&lt;/a&gt;&lt;/span&gt; &lt;/span&gt;&lt;a href=\"/wiki/France\" title=\"France\"&gt;France&lt;/a&gt;&lt;/span&gt; et &lt;br/&gt;&lt;span class=\"datasortkey\" data-sort-value=\"Monaco\"&gt;&lt;span class=\"flagicon\"&gt;&lt;span class=\"mw-image-border noviewer\" typeof=\"mw:File\"&gt;&lt;a class=\"mw-file-description\" href=\"/wiki/Fichier:Flag_of_Monaco.svg\" title=\"Drapeau de Monaco\"&gt;&lt;img alt=\"Drapeau de Monaco\" class=\"mw-file-element\" data-file-height=\"800\" data-file-width=\"1000\" decoding=\"async\" height=\"16\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Flag_of_Monaco.svg/20px-Flag_of_Monaco.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Flag_of_Monaco.svg/30px-Flag_of_Monaco.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Flag_of_Monaco.svg/40px-Flag_of_Monaco.svg.png 2x\" width=\"20\"/&gt;&lt;/a&gt;&lt;/span&gt; &lt;/span&gt;&lt;a href=\"/wiki/Monaco\" title=\"Monaco\"&gt;Monaco&lt;/a&gt;&lt;/span&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Date&lt;/th&gt;\n&lt;td&gt;\nDu &lt;time class=\"nowrap date-lien\" data-sort-value=\"2019-08-09\" datetime=\"2019-08-09\"&gt;&lt;a href=\"/wiki/9_ao%C3%BBt_en_sport\" title=\"9 ao√ªt en sport\"&gt;9&lt;/a&gt; &lt;a class=\"mw-redirect\" href=\"/wiki/Ao%C3%BBt_2019_en_sport\" title=\"Ao√ªt 2019 en sport\"&gt;ao√ªt&lt;/a&gt; &lt;a href=\"/wiki/2019_en_football\" title=\"2019 en football\"&gt;2019&lt;/a&gt;&lt;/time&gt;&lt;br/&gt;au &lt;time class=\"nowrap date-lien\" data-sort-value=\"2020-03-08\" datetime=\"2020-03-08\"&gt;&lt;a href=\"/wiki/8_mars_en_sport\" title=\"8 mars en sport\"&gt;8 mars&lt;/a&gt; &lt;a href=\"/wiki/2020_en_football\" title=\"2020 en football\"&gt;2020&lt;/a&gt;&lt;/time&gt; &lt;small&gt;(arr√™t d√©finitif)&lt;/small&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Participants&lt;/th&gt;\n&lt;td&gt;\n20 √©quipes&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Matchs jou√©s&lt;/th&gt;\n&lt;td&gt;\n279 (sur 380 pr√©vus)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Site web officiel&lt;/th&gt;\n&lt;td&gt;\n&lt;a class=\"external text\" href=\"https://www.ligue1.fr/\" rel=\"nofollow\"&gt;Site officiel&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;\n\n\nce qui est le texte source permettant de g√©n√©rer le tableau suivant :\n\n\n\n\nG√©n√©ralit√©s\n\n\n\n\nSport\n\n\nFootball\n\n\n\n\nOrganisateur(s)\n\n\nLFP\n\n\n\n\n√âdition\n\n\n82e\n\n\n\n\nLieu(x)\n\n\n France et  Monaco\n\n\n\n\nDate\n\n\nDu 9 ao√ªt 2019au 8 mars 2020 (arr√™t d√©finitif)\n\n\n\n\nParticipants\n\n\n20 √©quipes\n\n\n\n\nMatchs jou√©s\n\n\n279 (sur 380 pr√©vus)\n\n\n\n\nSite web officiel\n\n\nSite officiel\n\n\n\n\n\n\n\n\nLa m√©thode findAll\nPour trouver toutes les occurences, on utilise .findAll().\n\nprint(\"Il y a\", len(page.findAll(\"table\")), \"√©l√©ments dans la page qui sont des &lt;table&gt;\")\n\nIl y a 34 √©l√©ments dans la page qui sont des &lt;table&gt;"
  },
  {
    "objectID": "content/manipulation/04a_webscraping_TP.html#exercice-guid√©-obtenir-la-liste-des-√©quipes-de-ligue-1",
    "href": "content/manipulation/04a_webscraping_TP.html#exercice-guid√©-obtenir-la-liste-des-√©quipes-de-ligue-1",
    "title": "Web scraping avec Python",
    "section": "Exercice guid√© : obtenir la liste des √©quipes de Ligue 1",
    "text": "Exercice guid√© : obtenir la liste des √©quipes de Ligue 1\nDans le premier paragraphe de la page ‚ÄúParticipants‚Äù,\non a le tableau avec les r√©sultats de l‚Äôann√©e.\n\n\n Exercice 1 : R√©cup√©rer les participants de la Ligue 1\nPour cela, nous allons proc√©der en 6 √©tapes:\n\nTrouver le tableau\nR√©cup√©rer chaque ligne du table\nNettoyer les sorties en ne gardant que le texte sur une ligne\nG√©n√©raliser sur toutes les lignes\nR√©cup√©rer les ent√™tes du tableau\nFinalisation du tableau\n\n\n\n1Ô∏è‚É£ Trouver le tableau\n\n# on identifie le tableau en question : c'est le premier qui a cette classe \"wikitable sortable\"\ntableau_participants = page.find('table', {'class' : 'wikitable sortable'})\n\n\nprint(tableau_participants)\n\n\n\n\n\nClub\n\n\nDerni√®remont√©e\n\n\nBudget[3]en M‚Ç¨\n\n\nClassement2018-2019\n\n\nEntra√Æneur\n\n\nDepuis\n\n\nStade\n\n\nCapacit√©en L1[4]\n\n\nNombrede saisonsen L1\n\n\n\n\nParis Saint-Germain\n\n\n1974\n\n\n637\n\n\n1er\n\n\n Thomas Tuchel\n\n\n2018\n\n\nParc des Princes\n\n\n47¬†929\n\n\n46\n\n\n\n\nLOSC Lille\n\n\n2000\n\n\n120\n\n\n2e\n\n\n Christophe Galtier\n\n\n2017\n\n\nStade Pierre-Mauroy\n\n\n49¬†712\n\n\n59\n\n\n\n\nOlympique lyonnais\n\n\n1989\n\n\n310\n\n\n3e\n\n\n Rudi Garcia\n\n\n2019\n\n\nGroupama Stadium\n\n\n57¬†206\n\n\n60\n\n\n\n\nAS Saint-√âtienne\n\n\n2004\n\n\n100\n\n\n4e\n\n\n Claude Puel\n\n\n2019\n\n\nStade Geoffroy-Guichard\n\n\n41¬†965\n\n\n66\n\n\n\n\nOlympique de Marseille\n\n\n1996\n\n\n110\n\n\n5e\n\n\n Andr√© Villas-Boas\n\n\n2019\n\n\nOrange V√©lodrome\n\n\n66¬†226\n\n\n69\n\n\n\n\nMontpellier HSC\n\n\n2009\n\n\n40\n\n\n6e\n\n\n Michel Der Zakarian\n\n\n2017\n\n\nStade de la Mosson\n\n\n22¬†000\n\n\n27\n\n\n\n\nOGC Nice\n\n\n2002\n\n\n50\n\n\n7e\n\n\n Patrick Vieira\n\n\n2018\n\n\nAllianz Riviera\n\n\n35¬†596\n\n\n60\n\n\n\n\nStade de Reims\n\n\n2018\n\n\n45\n\n\n8e\n\n\n David Guion\n\n\n2017\n\n\nStade Auguste-Delaune\n\n\n20¬†546\n\n\n35\n\n\n\n\nN√Æmes Olympique\n\n\n2018\n\n\n27\n\n\n9e\n\n\n Bernard Blaquart\n\n\n2015\n\n\nStade des Costi√®res\n\n\n15¬†788\n\n\n35\n\n\n\n\nStade rennais FC\n\n\n1994\n\n\n65\n\n\n10e\n\n\n Julien St√©phan\n\n\n2018\n\n\nRoazhon Park\n\n\n29¬†194\n\n\n62\n\n\n\n\nRC Strasbourg Alsace\n\n\n2017\n\n\n43\n\n\n11e\n\n\n Thierry Laurey\n\n\n2016\n\n\nStade de la Meinau\n\n\n26¬†109\n\n\n58\n\n\n\n\nFC Nantes\n\n\n2013\n\n\n70\n\n\n12e\n\n\n Christian Gourcuff\n\n\n2019\n\n\nStade de la Beaujoire - Louis Fonteneau\n\n\n35¬†322\n\n\n51\n\n\n\n\nSCO d‚ÄôAngers\n\n\n2015\n\n\n32\n\n\n13e\n\n\n St√©phane Moulin\n\n\n2011\n\n\nStade Raymond-Kopa\n\n\n14¬†582\n\n\n27\n\n\n\n\nGirondins de Bordeaux\n\n\n1992\n\n\n70\n\n\n14e\n\n\n Paulo Sousa\n\n\n2019\n\n\nMatmut Atlantique\n\n\n42¬†115\n\n\n66\n\n\n\n\nAmiens SC\n\n\n2017\n\n\n30\n\n\n15e\n\n\n Luka Elsner\n\n\n2019\n\n\nStade Cr√©dit Agricole la Licorne\n\n\n12¬†999\n\n\n2\n\n\n\n\nToulouse FC\n\n\n2003\n\n\n35\n\n\n16e\n\n\n Denis Zanko\n\n\n2020\n\n\nStadium de Toulouse\n\n\n33¬†033\n\n\n32\n\n\n\n\nAS Monaco\n\n\n2013\n\n\n220\n\n\n17e\n\n\n Robert Moreno\n\n\n2019\n\n\nStade Louis-II\n\n\n16¬†500\n\n\n60\n\n\n\n\nDijon FCO\n\n\n2016\n\n\n38\n\n\n18e\n\n\n St√©phane Jobard\n\n\n2019\n\n\nParc des Sports Gaston-G√©rard\n\n\n15¬†459\n\n\n4\n\n\n\n\nFC Metz\n\n\n2019\n\n\n40\n\n\n1er (Ligue 2)\n\n\n Vincent Hognon\n\n\n2019\n\n\nStade Saint-Symphorien\n\n\n25¬†865\n\n\n61\n\n\n\n\nStade brestois 29\n\n\n2019\n\n\n30\n\n\n2e (Ligue 2)\n\n\n Olivier Dall‚ÄôOglio\n\n\n2019\n\n\nStade Francis-Le Bl√©\n\n\n14¬†920\n\n\n13\n\n\n\n\n\n\n2Ô∏è‚É£ R√©cup√©rer chaque ligne du tableau\nOn recherche d‚Äôabord toutes les lignes du tableau avec la balise tr\n\ntable_body = tableau_participants.find('tbody')\nrows = table_body.find_all('tr')\n\nOn obtient une liste o√π chaque √©l√©ment est une des lignes du tableau\nPour illustrer cela, on va d‚Äôabord afficher la premi√®re ligne.\nCelle-ci correspont aux ent√™tes de colonne:\n\nprint(rows[0])\n\n&lt;tr&gt;\n&lt;th scope=\"col\"&gt;Club\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Derni√®re&lt;br/&gt;mont√©e\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Budget&lt;sup class=\"reference\" id=\"cite_ref-3\"&gt;&lt;a href=\"#cite_note-3\"&gt;&lt;span class=\"cite_crochet\"&gt;[&lt;/span&gt;3&lt;span class=\"cite_crochet\"&gt;]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;&lt;br/&gt;en M&lt;a href=\"/wiki/Euro\" title=\"Euro\"&gt;‚Ç¨&lt;/a&gt;\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Classement&lt;br/&gt;&lt;a href=\"/wiki/Championnat_de_France_de_football_2018-2019\" title=\"Championnat de France de football 2018-2019\"&gt;2018-2019&lt;/a&gt;\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Entra√Æneur\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Depuis\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Stade\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Capacit√©&lt;br/&gt;en L1&lt;sup class=\"reference\" id=\"cite_ref-4\"&gt;&lt;a href=\"#cite_note-4\"&gt;&lt;span class=\"cite_crochet\"&gt;[&lt;/span&gt;4&lt;span class=\"cite_crochet\"&gt;]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Nombre&lt;br/&gt;de saisons&lt;br/&gt;en L1\n&lt;/th&gt;&lt;/tr&gt;\n\n\nLa seconde ligne va correspondre √† la ligne du premier club pr√©sent dans le tableau:\n\nprint(rows[1])\n\n&lt;tr bgcolor=\"#97DEFF\"&gt;\n&lt;td&gt;&lt;a href=\"/wiki/Paris_Saint-Germain_Football_Club\" title=\"Paris Saint-Germain Football Club\"&gt;Paris Saint-Germain&lt;/a&gt;\n&lt;/td&gt;\n&lt;td&gt;1974\n&lt;/td&gt;\n&lt;td&gt;637\n&lt;/td&gt;\n&lt;td&gt;&lt;span data-sort-value=\"101 !\"&gt;&lt;/span&gt;&lt;abbr class=\"abbr\" title=\"Premier\"&gt;1&lt;sup&gt;er&lt;/sup&gt;&lt;/abbr&gt;\n&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;span class=\"flagicon\"&gt;&lt;span class=\"mw-image-border noviewer\" typeof=\"mw:File\"&gt;&lt;a class=\"mw-file-description\" href=\"/wiki/Fichier:Flag_of_Germany.svg\" title=\"Drapeau : Allemagne\"&gt;&lt;img alt=\"\" class=\"mw-file-element\" data-file-height=\"600\" data-file-width=\"1000\" decoding=\"async\" height=\"12\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Flag_of_Germany.svg/20px-Flag_of_Germany.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Flag_of_Germany.svg/30px-Flag_of_Germany.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Flag_of_Germany.svg/40px-Flag_of_Germany.svg.png 2x\" width=\"20\"/&gt;&lt;/a&gt;&lt;/span&gt;&lt;/span&gt; &lt;a href=\"/wiki/Thomas_Tuchel\" title=\"Thomas Tuchel\"&gt;Thomas Tuchel&lt;/a&gt;\n&lt;/td&gt;\n&lt;td&gt;2018\n&lt;/td&gt;\n&lt;td&gt;&lt;a href=\"/wiki/Parc_des_Princes\" title=\"Parc des Princes\"&gt;Parc des Princes&lt;/a&gt;\n&lt;/td&gt;\n&lt;td&gt;47¬†929\n&lt;/td&gt;\n&lt;td&gt;46\n&lt;/td&gt;&lt;/tr&gt;\n\n\n3Ô∏è‚É£ Nettoyer les sorties en ne gardant que le texte sur une ligne\nOn va utiliser l‚Äôattribut text afin de se d√©barasser de toute la couche de HTML qu‚Äôon obtient √† l‚Äô√©tape 2.\nUn exemple sur la ligne du premier club :\n- on commence par prendre toutes les cellules de cette ligne, avec la balise td.\n- on fait ensuite une boucle sur chacune des cellules et on ne garde que le texte de la cellule avec l‚Äôattribut text.\n- enfin, on applique la m√©thode strip() pour que le texte soit bien mis en forme (sans espace inutile etc).\n\ncols = rows[1].find_all('td')\nprint(cols[0])\nprint(cols[0].text.strip())\n\n&lt;td&gt;&lt;a href=\"/wiki/Paris_Saint-Germain_Football_Club\" title=\"Paris Saint-Germain Football Club\"&gt;Paris Saint-Germain&lt;/a&gt;\n&lt;/td&gt;\nParis Saint-Germain\n\n\n\nfor ele in cols : \n    print(ele.text.strip())\n\nParis Saint-Germain\n1974\n637\n1er\nThomas Tuchel\n2018\nParc des Princes\n47¬†929\n46\n\n\n4Ô∏è‚É£ G√©n√©raliser sur toutes les lignes :\n\nfor row in rows:\n    cols = row.find_all('td')\n    cols = [ele.text.strip() for ele in cols]\n    print(cols)\n\n[]\n['Paris Saint-Germain', '1974', '637', '1er', 'Thomas Tuchel', '2018', 'Parc des Princes', '47\\xa0929', '46']\n['LOSC Lille', '2000', '120', '2e', 'Christophe Galtier', '2017', 'Stade Pierre-Mauroy', '49\\xa0712', '59']\n['Olympique lyonnais', '1989', '310', '3e', 'Rudi Garcia', '2019', 'Groupama Stadium', '57\\xa0206', '60']\n['AS Saint-√âtienne', '2004', '100', '4e', 'Claude Puel', '2019', 'Stade Geoffroy-Guichard', '41\\xa0965', '66']\n['Olympique de Marseille', '1996', '110', '5e', 'Andr√© Villas-Boas', '2019', 'Orange V√©lodrome', '66\\xa0226', '69']\n['Montpellier HSC', '2009', '40', '6e', 'Michel Der Zakarian', '2017', 'Stade de la Mosson', '22\\xa0000', '27']\n['OGC Nice', '2002', '50', '7e', 'Patrick Vieira', '2018', 'Allianz Riviera', '35\\xa0596', '60']\n['Stade de Reims', '2018', '45', '8e', 'David Guion', '2017', 'Stade Auguste-Delaune', '20\\xa0546', '35']\n['N√Æmes Olympique', '2018', '27', '9e', 'Bernard Blaquart', '2015', 'Stade des Costi√®res', '15\\xa0788', '35']\n['Stade rennais FC', '1994', '65', '10e', 'Julien St√©phan', '2018', 'Roazhon Park', '29\\xa0194', '62']\n['RC Strasbourg Alsace', '2017', '43', '11e', 'Thierry Laurey', '2016', 'Stade de la Meinau', '26\\xa0109', '58']\n['FC Nantes', '2013', '70', '12e', 'Christian Gourcuff', '2019', 'Stade de la Beaujoire - Louis Fonteneau', '35\\xa0322', '51']\n['SCO d‚ÄôAngers', '2015', '32', '13e', 'St√©phane Moulin', '2011', 'Stade Raymond-Kopa', '14\\xa0582', '27']\n['Girondins de Bordeaux', '1992', '70', '14e', 'Paulo Sousa', '2019', 'Matmut Atlantique', '42\\xa0115', '66']\n['Amiens SC', '2017', '30', '15e', 'Luka Elsner', '2019', 'Stade Cr√©dit Agricole la Licorne', '12\\xa0999', '2']\n['Toulouse FC', '2003', '35', '16e', 'Denis Zanko', '2020', 'Stadium de Toulouse', '33\\xa0033', '32']\n['AS Monaco', '2013', '220', '17e', 'Robert Moreno', '2019', 'Stade Louis-II', '16\\xa0500', '60']\n['Dijon FCO', '2016', '38', '18e', 'St√©phane Jobard', '2019', 'Parc des Sports Gaston-G√©rard', '15\\xa0459', '4']\n['FC Metz', '2019', '40', '1er (Ligue 2)', 'Vincent Hognon', '2019', 'Stade Saint-Symphorien', '25\\xa0865', '61']\n['Stade brestois 29', '2019', '30', '2e (Ligue 2)', \"Olivier Dall'Oglio\", '2019', 'Stade Francis-Le Bl√©', '14\\xa0920', '13']\n\n\nOn a bien r√©ussi √† avoir les informations contenues dans le tableau des participants du championnat.\nMais la premi√®re ligne est √©trange : c‚Äôest une liste vide ‚Ä¶\nIl s‚Äôagit des en-t√™tes : elles sont reconnues par la balise th et non td.\nOn va mettre tout le contenu dans un dictionnaire, pour le transformer ensuite en DataFrame pandas :\n\ndico_participants = dict()\nfor row in rows:\n    cols = row.find_all('td')\n    cols = [ele.text.strip() for ele in cols]\n    if len(cols) &gt; 0 : \n        dico_participants[cols[0]] = cols[1:]\ndico_participants\n\n{'Paris Saint-Germain': ['1974',\n  '637',\n  '1er',\n  'Thomas Tuchel',\n  '2018',\n  'Parc des Princes',\n  '47\\xa0929',\n  '46'],\n 'LOSC Lille': ['2000',\n  '120',\n  '2e',\n  'Christophe Galtier',\n  '2017',\n  'Stade Pierre-Mauroy',\n  '49\\xa0712',\n  '59'],\n 'Olympique lyonnais': ['1989',\n  '310',\n  '3e',\n  'Rudi Garcia',\n  '2019',\n  'Groupama Stadium',\n  '57\\xa0206',\n  '60'],\n 'AS Saint-√âtienne': ['2004',\n  '100',\n  '4e',\n  'Claude Puel',\n  '2019',\n  'Stade Geoffroy-Guichard',\n  '41\\xa0965',\n  '66'],\n 'Olympique de Marseille': ['1996',\n  '110',\n  '5e',\n  'Andr√© Villas-Boas',\n  '2019',\n  'Orange V√©lodrome',\n  '66\\xa0226',\n  '69'],\n 'Montpellier HSC': ['2009',\n  '40',\n  '6e',\n  'Michel Der Zakarian',\n  '2017',\n  'Stade de la Mosson',\n  '22\\xa0000',\n  '27'],\n 'OGC Nice': ['2002',\n  '50',\n  '7e',\n  'Patrick Vieira',\n  '2018',\n  'Allianz Riviera',\n  '35\\xa0596',\n  '60'],\n 'Stade de Reims': ['2018',\n  '45',\n  '8e',\n  'David Guion',\n  '2017',\n  'Stade Auguste-Delaune',\n  '20\\xa0546',\n  '35'],\n 'N√Æmes Olympique': ['2018',\n  '27',\n  '9e',\n  'Bernard Blaquart',\n  '2015',\n  'Stade des Costi√®res',\n  '15\\xa0788',\n  '35'],\n 'Stade rennais FC': ['1994',\n  '65',\n  '10e',\n  'Julien St√©phan',\n  '2018',\n  'Roazhon Park',\n  '29\\xa0194',\n  '62'],\n 'RC Strasbourg Alsace': ['2017',\n  '43',\n  '11e',\n  'Thierry Laurey',\n  '2016',\n  'Stade de la Meinau',\n  '26\\xa0109',\n  '58'],\n 'FC Nantes': ['2013',\n  '70',\n  '12e',\n  'Christian Gourcuff',\n  '2019',\n  'Stade de la Beaujoire - Louis Fonteneau',\n  '35\\xa0322',\n  '51'],\n 'SCO d‚ÄôAngers': ['2015',\n  '32',\n  '13e',\n  'St√©phane Moulin',\n  '2011',\n  'Stade Raymond-Kopa',\n  '14\\xa0582',\n  '27'],\n 'Girondins de Bordeaux': ['1992',\n  '70',\n  '14e',\n  'Paulo Sousa',\n  '2019',\n  'Matmut Atlantique',\n  '42\\xa0115',\n  '66'],\n 'Amiens SC': ['2017',\n  '30',\n  '15e',\n  'Luka Elsner',\n  '2019',\n  'Stade Cr√©dit Agricole la Licorne',\n  '12\\xa0999',\n  '2'],\n 'Toulouse FC': ['2003',\n  '35',\n  '16e',\n  'Denis Zanko',\n  '2020',\n  'Stadium de Toulouse',\n  '33\\xa0033',\n  '32'],\n 'AS Monaco': ['2013',\n  '220',\n  '17e',\n  'Robert Moreno',\n  '2019',\n  'Stade Louis-II',\n  '16\\xa0500',\n  '60'],\n 'Dijon FCO': ['2016',\n  '38',\n  '18e',\n  'St√©phane Jobard',\n  '2019',\n  'Parc des Sports Gaston-G√©rard',\n  '15\\xa0459',\n  '4'],\n 'FC Metz': ['2019',\n  '40',\n  '1er (Ligue 2)',\n  'Vincent Hognon',\n  '2019',\n  'Stade Saint-Symphorien',\n  '25\\xa0865',\n  '61'],\n 'Stade brestois 29': ['2019',\n  '30',\n  '2e (Ligue 2)',\n  \"Olivier Dall'Oglio\",\n  '2019',\n  'Stade Francis-Le Bl√©',\n  '14\\xa0920',\n  '13']}\n\n\n\ndata_participants = pandas.DataFrame.from_dict(dico_participants,orient='index')\ndata_participants.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\nParis Saint-Germain\n1974\n637\n1er\nThomas Tuchel\n2018\nParc des Princes\n47¬†929\n46\n\n\nLOSC Lille\n2000\n120\n2e\nChristophe Galtier\n2017\nStade Pierre-Mauroy\n49¬†712\n59\n\n\nOlympique lyonnais\n1989\n310\n3e\nRudi Garcia\n2019\nGroupama Stadium\n57¬†206\n60\n\n\nAS Saint-√âtienne\n2004\n100\n4e\nClaude Puel\n2019\nStade Geoffroy-Guichard\n41¬†965\n66\n\n\nOlympique de Marseille\n1996\n110\n5e\nAndr√© Villas-Boas\n2019\nOrange V√©lodrome\n66¬†226\n69\n\n\n\n\n\n\n\n5Ô∏è‚É£ R√©cup√©rer les en-t√™tes du tableau:\n\nfor row in rows:\n    cols = row.find_all('th')\n    print(cols)\n    if len(cols) &gt; 0 : \n        cols = [ele.get_text(separator=' ').strip().title() for ele in cols]\n        columns_participants = cols\n\n[&lt;th scope=\"col\"&gt;Club\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Derni√®re&lt;br/&gt;mont√©e\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Budget&lt;sup class=\"reference\" id=\"cite_ref-3\"&gt;&lt;a href=\"#cite_note-3\"&gt;&lt;span class=\"cite_crochet\"&gt;[&lt;/span&gt;3&lt;span class=\"cite_crochet\"&gt;]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;&lt;br/&gt;en M&lt;a href=\"/wiki/Euro\" title=\"Euro\"&gt;‚Ç¨&lt;/a&gt;\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Classement&lt;br/&gt;&lt;a href=\"/wiki/Championnat_de_France_de_football_2018-2019\" title=\"Championnat de France de football 2018-2019\"&gt;2018-2019&lt;/a&gt;\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Entra√Æneur\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Depuis\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Stade\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Capacit√©&lt;br/&gt;en L1&lt;sup class=\"reference\" id=\"cite_ref-4\"&gt;&lt;a href=\"#cite_note-4\"&gt;&lt;span class=\"cite_crochet\"&gt;[&lt;/span&gt;4&lt;span class=\"cite_crochet\"&gt;]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Nombre&lt;br/&gt;de saisons&lt;br/&gt;en L1\n&lt;/th&gt;]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n\n\n\ncolumns_participants\n\n['Club',\n 'Derni√®re Mont√©e',\n 'Budget [ 3 ] En M ‚Ç¨',\n 'Classement 2018-2019',\n 'Entra√Æneur',\n 'Depuis',\n 'Stade',\n 'Capacit√© En L1 [ 4 ]',\n 'Nombre De Saisons En L1']\n\n\n6Ô∏è‚É£ Finalisation du tableau\n\ndata_participants.columns = columns_participants[1:]\n\n\ndata_participants.head()\n\n\n\n\n\n\n\n\nDerni√®re Mont√©e\nBudget [ 3 ] En M ‚Ç¨\nClassement 2018-2019\nEntra√Æneur\nDepuis\nStade\nCapacit√© En L1 [ 4 ]\nNombre De Saisons En L1\n\n\n\n\nParis Saint-Germain\n1974\n637\n1er\nThomas Tuchel\n2018\nParc des Princes\n47¬†929\n46\n\n\nLOSC Lille\n2000\n120\n2e\nChristophe Galtier\n2017\nStade Pierre-Mauroy\n49¬†712\n59\n\n\nOlympique lyonnais\n1989\n310\n3e\nRudi Garcia\n2019\nGroupama Stadium\n57¬†206\n60\n\n\nAS Saint-√âtienne\n2004\n100\n4e\nClaude Puel\n2019\nStade Geoffroy-Guichard\n41¬†965\n66\n\n\nOlympique de Marseille\n1996\n110\n5e\nAndr√© Villas-Boas\n2019\nOrange V√©lodrome\n66¬†226\n69"
  },
  {
    "objectID": "content/manipulation/04a_webscraping_TP.html#pour-aller-plus-loin",
    "href": "content/manipulation/04a_webscraping_TP.html#pour-aller-plus-loin",
    "title": "Web scraping avec Python",
    "section": "Pour aller plus loin",
    "text": "Pour aller plus loin\n\nR√©cup√©ration des localisations des stades\nEssayez de comprendre pas √† pas ce qui est fait dans les √©tapes qui suivent (la r√©cup√©ration d‚Äôinformations suppl√©mentaires en naviguant dans les pages des diff√©rents clubs).\n\nimport urllib\nimport pandas as pd\nimport bs4 \n\ndivision=[]\nequipe=[]\nstade=[]\nlatitude_stade=[]        \nlongitude_stade=[]     \n\nurl_list=[\"http://fr.wikipedia.org/wiki/Championnat_de_France_de_football_2019-2020\", \"http://fr.wikipedia.org/wiki/Championnat_de_France_de_football_de_Ligue_2_2019-2020\"]\n\nfor url_ligue in url_list :\n       \n    print(url_ligue)\n    sock = urllib.request.urlopen(url_ligue).read() \n    page=bs4.BeautifulSoup(sock)\n\n# Rechercher les liens des √©quipes dans la liste disponible sur wikipedia \n\n    for team in page.findAll('span' , {'class' : 'toponyme'}) :  \n        \n        # Indiquer si c'est de la ligue 1 ou de la ligue 2\n        \n        if url_ligue==url_list[0] :\n            division.append(\"L1\")\n        else :\n            division.append(\"L2\")\n\n       # Trouver le nom et le lien de l'√©quipe\n            \n        if team.find('a')!=None :\n            team_url=team.find('a').get('href')\n            name_team=team.find('a').get('title')\n            equipe.append(name_team)\n            url_get_info = \"http://fr.wikipedia.org\"+team_url\n            print(url_get_info)\n \n       # aller sur la page de l'√©quipe\n           \n            search = urllib.request.urlopen(url_get_info).read()\n            search_team=bs4.BeautifulSoup(search)\n\n       # trouver le stade             \n            compteur = 0\n            for stadium in search_team.findAll('tr'):\n                for x in stadium.findAll('th' , {'scope' : 'row'} ) :\n                    if x.contents[0].string==\"Stade\" and compteur == 0:\n                        compteur = 1\n                        # trouver le lien du stade et son nom\n                        url_stade=stadium.findAll('a')[1].get('href')\n                        name_stadium=stadium.findAll('a')[1].get('title')\n                        stade.append(name_stadium)\n                        url_get_stade = \"http://fr.wikipedia.org\"+url_stade\n                        print(url_get_stade)\n                        \n                        # Aller sur la page du stade et trouver ses coodronn√©es g√©ographiques\n                        \n                        search_stade = urllib.request.urlopen(url_get_stade).read()\n                        soup_stade=bs4.BeautifulSoup(search_stade) \n                        kartographer = soup_stade.find('a',{'class': \"mw-kartographer-maplink\"})\n                        if kartographer == None :\n                          latitude_stade.append(None)\n                          longitude_stade.append(None) \n                        else :\n                            for coordinates in kartographer :\n                                print(coordinates)\n                                liste =   coordinates.split(\",\")          \n                                latitude_stade.append(str(liste[0]).replace(\" \", \"\") + \"'\")\n                                longitude_stade.append(str(liste[1]).replace(\" \", \"\") + \"'\")\n                            \n\ndict = {'division' : division , 'equipe': equipe, 'stade': stade, 'latitude': latitude_stade, 'longitude' : longitude_stade}\ndata = pd.DataFrame(dict)\ndata = data.dropna()\n\n\ndata.head(5)\n\n\n\n\n\n\n\n\ndivision\nequipe\nstade\nlatitude\nlongitude\n\n\n\n\n0\nL1\nParis Saint-Germain Football Club\nParc des Princes\n48¬∞¬†50‚Ä≤¬†29‚Ä≥¬†N'\n2¬∞¬†15‚Ä≤¬†11‚Ä≥¬†E'\n\n\n1\nL1\nLOSC Lille\nStade Pierre-Mauroy\n50¬∞¬†36‚Ä≤¬†43‚Ä≥¬†N'\n3¬∞¬†07‚Ä≤¬†50‚Ä≥¬†E'\n\n\n2\nL1\nOlympique lyonnais\nParc Olympique lyonnais\n45¬∞¬†45‚Ä≤¬†55‚Ä≥¬†N'\n4¬∞¬†58‚Ä≤¬†55‚Ä≥¬†E'\n\n\n3\nL1\nAssociation sportive de Saint-√âtienne\nStade Geoffroy-Guichard\n45¬∞¬†27‚Ä≤¬†39‚Ä≥¬†N'\n4¬∞¬†23‚Ä≤¬†25‚Ä≥¬†E'\n\n\n4\nL1\nOlympique de Marseille\nStade V√©lodrome\n43¬∞¬†16‚Ä≤¬†11‚Ä≥¬†N'\n5¬∞¬†23‚Ä≤¬†45‚Ä≥¬†E'\n\n\n\n\n\n\n\nOn va transformer les coordonn√©es en degr√©s en coordonn√©es num√©riques\nafin d‚Äô√™tre en mesure de faire une carte.\n\nimport re\n\ndef dms2dd(degrees, minutes, seconds, direction):\n    dd = float(degrees) + float(minutes)/60 + float(seconds)/(60*60);\n    if direction in ('S', 'O'):\n        dd *= -1\n    return dd\n\ndef parse_dms(dms):\n    parts = re.split('[^\\d\\w]+', dms)\n    lat = dms2dd(parts[0], parts[1], parts[2], parts[3])\n    #lng = dms2dd(parts[4], parts[5], parts[6], parts[7])\n    return lat\n\n\ndata['latitude'] = data['latitude'].apply(parse_dms)\ndata['longitude'] = data['longitude'].apply(parse_dms)\n\nTous les √©l√©ments sont en place pour faire une belle carte √† ce stade. On\nva utiliser folium pour celle-ci, qui est pr√©sent√© dans la partie\nvisualisation.\n\n\nCarte des stades avec folium\n\nimport geopandas as gpd\nfrom pathlib import Path\nimport folium\n\ngdf = gpd.GeoDataFrame(\n    data, geometry=gpd.points_from_xy(data.longitude, data.latitude))\n\nPath(\"leaflet\").mkdir(parents=True, exist_ok=True)\n\ncenter = gdf[['latitude', 'longitude']].mean().values.tolist()\nsw = gdf[['latitude', 'longitude']].min().values.tolist()\nne = gdf[['latitude', 'longitude']].max().values.tolist()\n\nm = folium.Map(location = center, tiles='openstreetmap')\n\n# I can add marker one by one on the map\nfor i in range(0,len(gdf)):\n    folium.Marker([gdf.iloc[i]['latitude'], gdf.iloc[i]['longitude']], popup=gdf.iloc[i]['stade']).add_to(m) \n\nm.fit_bounds([sw, ne])\n\nLa carte obtenue doit ressembler √† la suivante :\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "content/manipulation/04a_webscraping_TP.html#r√©cup√©rer-des-informations-sur-les-pokemons",
    "href": "content/manipulation/04a_webscraping_TP.html#r√©cup√©rer-des-informations-sur-les-pokemons",
    "title": "Web scraping avec Python",
    "section": "R√©cup√©rer des informations sur les pokemons",
    "text": "R√©cup√©rer des informations sur les pokemons\nLe prochain exercice pour mettre en pratique le web scraping\nconsiste √† r√©cup√©rer des informations sur les\npokemons √† partir du\nsite internet pokemondb.net.\n\nVersion non guid√©e\n\n\n Exercice 2 : Les pokemon (version non guid√©e)\nPour cet exercice, nous vous demandons d‚Äôobtenir diff√©rentes informations sur les pok√©mons :\n\nles informations personnelles des 893 pokemons sur le site internet pokemondb.net.\nLes informations que nous aimerions obtenir au final dans un DataFrame sont celles contenues dans 4 tableaux :\n\n\nPok√©dex data\nTraining\nBreeding\nBase stats\n\n\nNous aimerions que vous r√©cup√©riez √©galement les images de chacun des pok√©mons et que vous les enregistriez dans un dossier\n\n\nPetit indice : utilisez les modules request et shutil\nPour cette question, il faut que vous cherchiez de vous m√™me certains √©l√©ments, tout n‚Äôest pas pr√©sent dans le TD.\n\n\n\nPour la question 1, l‚Äôobjectif est d‚Äôobtenir le code source d‚Äôun tableau comme\ncelui qui suit\n(Pokemon Nincada).\n\n\n\nPok√©dex data\n\n\n\n\n\n\nNational ‚Ññ\n\n\n290\n\n\n\n\nType\n\n\nBug Ground\n\n\n\n\nSpecies\n\n\nTrainee Pok√©mon\n\n\n\n\nHeight\n\n\n0.5¬†m (1‚Ä≤08‚Ä≥)\n\n\n\n\nWeight\n\n\n5.5¬†kg (12.1¬†lbs)\n\n\n\n\nAbilities\n\n\n1. Compound EyesRun Away (hidden ability)\n\n\n\n\nLocal ‚Ññ\n\n\n042 (Ruby/Sapphire/Emerald)111 (X/Y ‚Äî Central Kalos)043 (Omega Ruby/Alpha Sapphire)104 (Sword/Shield)\n\n\n\n\n\n\n\n\n\n\nTraining\n\n\n\n\n\n\nEV yield\n\n\n1 Defense\n\n\n\n\nCatch rate\n\n\n255 (33.3% with Pok√©Ball, full HP)\n\n\n\n\nBase Friendship\n\n\n70 (normal)\n\n\n\n\nBase Exp.\n\n\n53\n\n\n\n\nGrowth Rate\n\n\nErratic\n\n\n\n\n\n\n\n\nBreeding\n\n\n\n\n\n\nEgg Groups\n\n\nBug\n\n\n\n\nGender\n\n\n50% male, 50% female\n\n\n\n\nEgg cycles\n\n\n15 (3,599‚Äì3,855 steps)\n\n\n\n\n\n\n\n\n\n\n\n\n\nBase stats\n\n\n\n\n\n\n\nHP\n\n\n31\n\n\n\n\n\n\n\n172\n\n\n266\n\n\n\n\nAttack\n\n\n45\n\n\n\n\n\n\n\n85\n\n\n207\n\n\n\n\nDefense\n\n\n90\n\n\n\n\n\n\n\n166\n\n\n306\n\n\n\n\nSp. Atk\n\n\n30\n\n\n\n\n\n\n\n58\n\n\n174\n\n\n\n\nSp. Def\n\n\n30\n\n\n\n\n\n\n\n58\n\n\n174\n\n\n\n\nSpeed\n\n\n40\n\n\n\n\n\n\n\n76\n\n\n196\n\n\n\n\n\n\nTotal\n\n\n266\n\n\n\n\nMin\n\n\nMax\n\n\n\n\n\n\n\n\nPour la question 2, l‚Äôobjectif est d‚Äôobtenir\nles images des pokemon.\n\n\nVersion guid√©e\nLes prochaines parties permettront de faire l‚Äôexercice ci-dessus\n√©tape par √©tape,\nde mani√®re guid√©e.\nNous souhaitons tout d‚Äôabord obtenir les\ninformations personnelles de tous\nles pokemons sur pokemondb.net.\nLes informations que nous aimerions obtenir au final pour les pokemons sont celles contenues dans 4 tableaux :\n\nPok√©dex data\nTraining\nBreeding\nBase stats\n\nNous proposons ensuite de r√©cup√©rer et afficher les images.\n\nEtape 1: constituer un DataFrame de caract√©ristiques\n\n\n Exercice 2b : Les pok√©mons (version guid√©e)\nPour r√©cup√©rer les informations, le code devra √™tre divis√© en plusieurs √©tapes :\n\nTrouvez la page principale du site et la transformer en un objet intelligible pour votre code.\nLes fonctions suivantes vous seront utiles :\n\n\nurllib.request.Request\nurllib.request.urlopen\nbs4.BeautifulSoup\n\n\nCr√©ez une fonction qui permet de r√©cup√©rer la page d‚Äôun pok√©mon √† partir de son nom.\nA partir de la page de bulbasaur, obtenez les 4 tableaux qui nous int√©ressent :\n\n\non va chercher l‚Äô√©l√©ment suivant : ('table', { 'class' : \"vitals-table\"})\npuis stocker ses √©l√©ments dans un dictionnaire\n\n\nR√©cup√©rez par ailleurs la liste de noms des pok√©mons qui nous permettra de faire une boucle par la suite. Combien trouvez-vous de pok√©mons ?\nEcrire une fonction qui r√©cup√®re l‚Äôensemble des informations sur les dix premiers pok√©mons de la liste et les int√®gre dans un DataFrame\n\n\n\nA l‚Äôissue de la question 3,\nvous devriez obtenir une liste de caract√©ristiques proche de celle-ci:\n\n\n{'National ‚Ññ': '0001',\n 'name': 'bulbasaur',\n 'Type': ' Grass Poison ',\n 'Species': 'Seed Pok√©mon',\n 'Height': '0.7\\xa0m (2‚Ä≤04‚Ä≥)',\n 'Weight': '6.9\\xa0kg (15.2\\xa0lbs)',\n 'Abilities': '1. OvergrowChlorophyll (hidden ability)',\n 'Local ‚Ññ': \"0001 (Red/Blue/Yellow)0226 (Gold/Silver/Crystal)0001 (FireRed/LeafGreen)0231 (HeartGold/SoulSilver)0080 (X/Y ‚Äî Central Kalos)0001 (Let's Go Pikachu/Let's Go Eevee)0068 (The Isle of Armor)\",\n 'EV yield': ' 1 Sp. Atk ',\n 'Catch rate': ' 45 (5.9% with Pok√©Ball, full HP) ',\n 'Base Friendship': ' 50 (normal) ',\n 'Base Exp.': '64',\n 'Growth Rate': 'Medium Slow',\n 'Egg Groups': 'Grass, Monster',\n 'Gender': '87.5% male, 12.5% female',\n 'Egg cycles': '20 (4,884‚Äì5,140 steps) ',\n 'HP': '45',\n 'Attack': '49',\n 'Defense': '49',\n 'Sp. Atk': '65',\n 'Sp. Def': '65',\n 'Speed': '45'}\n\n\nLa structure est ici en dictionnaire, ce qui est pratique.\nEnfin, vous pouvez int√©grer les informations\ndes dix premiers pok√©mons √† un\nDataFrame, qui aura l‚Äôaspect suivant :\n\n\n\n\n\n\n\n\n\nNational ‚Ññ\nname\nType\nSpecies\nHeight\nWeight\nAbilities\nLocal ‚Ññ\nEV yield\nCatch rate\n...\nGrowth Rate\nEgg Groups\nGender\nEgg cycles\nHP\nAttack\nDefense\nSp. Atk\nSp. Def\nSpeed\n\n\n\n\n0\n0001\nbulbasaur\nGrass Poison\nSeed Pok√©mon\n0.7¬†m (2‚Ä≤04‚Ä≥)\n6.9¬†kg (15.2¬†lbs)\n1. OvergrowChlorophyll (hidden ability)\n0001 (Red/Blue/Yellow)0226 (Gold/Silver/Crysta...\n1 Sp. Atk\n45 (5.9% with Pok√©Ball, full HP)\n...\nMedium Slow\nGrass, Monster\n87.5% male, 12.5% female\n20 (4,884‚Äì5,140 steps)\n45\n49\n49\n65\n65\n45\n\n\n1\n0002\nivysaur\nGrass Poison\nSeed Pok√©mon\n1.0¬†m (3‚Ä≤03‚Ä≥)\n13.0¬†kg (28.7¬†lbs)\n1. OvergrowChlorophyll (hidden ability)\n0002 (Red/Blue/Yellow)0227 (Gold/Silver/Crysta...\n1 Sp. Atk, 1 Sp. Def\n45 (5.9% with Pok√©Ball, full HP)\n...\nMedium Slow\nGrass, Monster\n87.5% male, 12.5% female\n20 (4,884‚Äì5,140 steps)\n60\n62\n63\n80\n80\n60\n\n\n2\n0003\nvenusaur\nGrass Poison\nSeed Pok√©mon\n2.0¬†m (6‚Ä≤07‚Ä≥)\n100.0¬†kg (220.5¬†lbs)\n1. OvergrowChlorophyll (hidden ability)\n0003 (Red/Blue/Yellow)0228 (Gold/Silver/Crysta...\n2 Sp. Atk, 1 Sp. Def\n45 (5.9% with Pok√©Ball, full HP)\n...\nMedium Slow\nGrass, Monster\n87.5% male, 12.5% female\n20 (4,884‚Äì5,140 steps)\n80\n82\n83\n100\n100\n80\n\n\n3\n0004\ncharmander\nFire\nLizard Pok√©mon\n0.6¬†m (2‚Ä≤00‚Ä≥)\n8.5¬†kg (18.7¬†lbs)\n1. BlazeSolar Power (hidden ability)\n0004 (Red/Blue/Yellow)0229 (Gold/Silver/Crysta...\n1 Speed\n45 (5.9% with Pok√©Ball, full HP)\n...\nMedium Slow\nDragon, Monster\n87.5% male, 12.5% female\n20 (4,884‚Äì5,140 steps)\n39\n52\n43\n60\n50\n65\n\n\n4\n0005\ncharmeleon\nFire\nFlame Pok√©mon\n1.1¬†m (3‚Ä≤07‚Ä≥)\n19.0¬†kg (41.9¬†lbs)\n1. BlazeSolar Power (hidden ability)\n0005 (Red/Blue/Yellow)0230 (Gold/Silver/Crysta...\n1 Sp. Atk, 1 Speed\n45 (5.9% with Pok√©Ball, full HP)\n...\nMedium Slow\nDragon, Monster\n87.5% male, 12.5% female\n20 (4,884‚Äì5,140 steps)\n58\n64\n58\n80\n65\n80\n\n\n\n\n5 rows √ó 22 columns\n\n\n\n\n\nEtape 2: r√©cup√©rer et afficher des photos de Pokemon\nNous aimerions que vous r√©cup√©riez √©galement les images des 5 premiers pok√©mons\net que vous les enregistriez dans un dossier.\n\n\n Exercice 2b : Les pok√©mons (version guid√©e)\n\nLes URL des images des pokemon prennent la forme ‚Äúhttps://img.pokemondb.net/artwork/{pokemon}.jpg‚Äù.\nUtiliser les modules requests et shutil pour t√©l√©charger\net enregistrer en local les images.\nImporter ces images stock√©es au format JPEG dans Python gr√¢ce √† la fonction imread du package skimage.io\n\n\n\n\n!pip install scikit-image\n\nRequirement already satisfied: scikit-image in /opt/mamba/lib/python3.9/site-packages (0.21.0)\nRequirement already satisfied: numpy&gt;=1.21.1 in /opt/mamba/lib/python3.9/site-packages (from scikit-image) (1.26.0)\nRequirement already satisfied: scipy&gt;=1.8 in /opt/mamba/lib/python3.9/site-packages (from scikit-image) (1.11.2)\nRequirement already satisfied: networkx&gt;=2.8 in /opt/mamba/lib/python3.9/site-packages (from scikit-image) (3.1)\nRequirement already satisfied: pillow&gt;=9.0.1 in /opt/mamba/lib/python3.9/site-packages (from scikit-image) (10.0.0)\nRequirement already satisfied: imageio&gt;=2.27 in /opt/mamba/lib/python3.9/site-packages (from scikit-image) (2.31.4)\nRequirement already satisfied: tifffile&gt;=2022.8.12 in /opt/mamba/lib/python3.9/site-packages (from scikit-image) (2023.9.18)\nRequirement already satisfied: PyWavelets&gt;=1.1.1 in /opt/mamba/lib/python3.9/site-packages (from scikit-image) (1.4.1)\nRequirement already satisfied: packaging&gt;=21 in /opt/mamba/lib/python3.9/site-packages (from scikit-image) (21.3)\nRequirement already satisfied: lazy_loader&gt;=0.2 in /opt/mamba/lib/python3.9/site-packages (from scikit-image) (0.3)\nRequirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /opt/mamba/lib/python3.9/site-packages (from packaging&gt;=21-&gt;scikit-image) (3.0.9)\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv"
  },
  {
    "objectID": "content/manipulation/04a_webscraping_TP.html#selenium-mimer-le-comportement-dun-utilisateur-internet",
    "href": "content/manipulation/04a_webscraping_TP.html#selenium-mimer-le-comportement-dun-utilisateur-internet",
    "title": "Web scraping avec Python",
    "section": "Selenium : mimer le comportement d‚Äôun utilisateur internet",
    "text": "Selenium : mimer le comportement d‚Äôun utilisateur internet\nJusqu‚Äô√† pr√©sent,\nnous avons raisonn√© comme si nous connaissions toujours l‚Äôurl qui nous int√©resse.\nDe plus, les pages que nous visitons sont ‚Äústatiques‚Äù,\nelles ne d√©pendent pas d‚Äôune action ou d‚Äôune recherche de l‚Äôinternaute.\nNous allons voir √† pr√©sent comment nous en sortir pour remplir\ndes champs sur un site web et r√©cup√©rer ce qui nous int√©resse.\nLa r√©action d‚Äôun site web √† l‚Äôaction d‚Äôun utilisateur passe r√©guli√®rement par\nl‚Äôusage de JavaScript dans le monde du d√©veloppement web.\nLe package Selenium permet\nde reproduire, depuis un code automatis√©, le comportement\nmanuel d‚Äôun utilisateur. Il permet ainsi\nd‚Äôobtenir des informations du site qui ne sont pas dans le\ncode HTML mais qui apparaissent uniquement √† la suite de\nl‚Äôex√©cution de script JavaScript en arri√®re plan.\nSelenium se comporte comme un utilisateur lambda sur internet :\nil clique sur des liens, il remplit des formulaires, etc.\n\nPremier exemple en scrapant un moteur de recherche\nDans cet exemple, nous allons essayer d‚Äôaller sur le\nsite de Bing Actualit√©s\net entrer dans la barre de recherche un sujet donn√©.\nPour tester, nous allons faire une recherche avec le mot-cl√© ‚ÄúTrump‚Äù.\nL‚Äôinstallation de Selenium n√©cessite d‚Äôavoir Chromium qui est un\nnavigateur Google Chrome minimaliste.\nLa version de chromedriver\ndoit √™tre &gt;= 2.36 et d√©pend de la version de Chrome que vous avez sur votre environnement\nde travail. Pour installer cette version minimaliste de Chrome sur un environnement\nLinux, vous pouvez vous r√©f√©rer √† l‚Äôencadr√© d√©di√©.\n\n\n Installation de Selenium\nD‚Äôabord, il convient d‚Äôinstaller les d√©pendances.\nSur Colab, vous pouvez utiliser les commandes suivantes :\n\n!sudo apt-get update\n!sudo apt install -y unzip xvfb libxi6 libgconf-2-4 -y\n!sudo apt install chromium-chromedriver -y\n!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n\n\nSi vous √™tes sur le SSP Cloud, vous pouvez\nex√©cuter les commandes suivantes :\n\n!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb -O /tmp/chrome.deb\n!sudo apt-get update\n!sudo -E apt-get install -y /tmp/chrome.deb\n!pip install chromedriver-autoinstaller selenium\n\nimport chromedriver_autoinstaller\nchromedriver_autoinstaller.install()\n\n\nVous pouvez ensuite installer Selenium.\nPar exemple, depuis une\ncellule de Notebook :\n\n\nApr√®s avoir install√© Chromium,\nil est n√©cessaire d‚Äôindiquer √† Python o√π\nle trouver. Si vous √™tes sur Linux et que vous\navez suivi les consignes pr√©c√©dentes, vous pouvez faire :\n\nimport selenium\nfrom webdriver_manager.chrome import ChromeDriverManager\n\npath_to_web_driver = ChromeDriverManager().install()\n\nEn premier lieu, il convient d‚Äôinitialiser le comportement\nde Selenium en r√©pliquant les param√®tres\ndu navigateur. Pour cela, on va d‚Äôabord initialiser\nnotre navigateur avec quelques options :\n\nimport time\n\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\n\nchrome_options = webdriver.ChromeOptions()\nchrome_options.add_argument('--headless')\nchrome_options.add_argument('--no-sandbox')\nchrome_options.add_argument('--disable-dev-shm-usage')\n#chrome_options.add_argument('--verbose') \n\nPuis on lance le navigateur :\n\nfrom selenium.webdriver.chrome.service import Service\nservice = Service(executable_path=path_to_web_driver)\n\nbrowser = webdriver.Chrome(service=service,\n                           options=chrome_options)\n\nOn va sur le site de Bing Actualit√©s,\net on lui indique le mot cl√© que nous souhaitons chercher.\nEn l‚Äôoccurrence, on s‚Äôint√©resse aux actualit√©s de Donald Trump.\nApr√®s avoir inspect√© la page depuis les outils de d√©veloppement du navigateur,\non voit que la barre de recherche est un √©lement du code appel√© q (comme query).\nOn va ainsi demander √† selenium de chercher cet √©l√©ment:\n\nbrowser.get('https://www.bing.com/news')\n\nsearch = browser.find_element(\"name\", \"q\")\nprint(search)\nprint([search.text, search.tag_name, search.id])\n\n# on envoie √† cet endroit le mot qu'on aurait tap√© dans la barre de recherche\nsearch.send_keys(\"Trump\")\n\nsearch_button = browser.find_element(\"xpath\", \"//input[@id='sb_form_go']\") \nsearch_button.click()\n\nSelenium permet de capturer l‚Äôimage qu‚Äôon verrait dans le navigateur\navec get_screenshot_as_png. Cela peut √™tre utile pour v√©rifier qu‚Äôon\na fait la bonne action:\n\n\n\n\n\n\n\n\n\nEnfin, on peut extraire les r√©sultats. Plusieurs\nm√©thodes sont disponibles. La m√©thode la plus\npratique, lorsqu‚Äôelle est disponible,\nest d‚Äôutiliser le XPath qui est un chemin\nnon ambigu pour acc√©der √† un √©lement. En effet,\nplusieurs √©l√©ments peuvent partager la m√™me classe ou\nle m√™me attribut ce qui peut faire qu‚Äôune recherche\nde ce type peut renvoyer plusieurs √©chos.\nPour d√©terminer le XPath d‚Äôun objet, les outils\nde d√©veloppeurs de votre site web sont pratiques.\nPar exemple, sous Firefox, une fois que vous\navez trouv√© un √©l√©ment dans l‚Äôinspecteur, vous\npouvez faire click droit &gt; Copier &gt; XPath.\nEnfin, pour mettre fin √† notre session, on demande\n√† Python de quitter le navigateur:\n\nbrowser.quit()\n\nOn a obtenu les r√©sultats suivants :\n\n\n['https://www.msn.com/en-us/money/companies/republican-megadonor-mercer-family-weighs-backing-trump-as-they-maintain-massive-war-chest/ar-AA1kijjY', 'https://news.yahoo.com/not-law-legal-experts-torch-140917639.html', 'https://www.msn.com/en-us/news/other/trump-critics-appeal-colorado-ruling-that-said-insurrectionist-ban-doesn-t-apply-to-presidents/ar-AA1kit2w', 'https://www.msn.com/en-us/news/politics/louisiana-fraud-case-could-silence-donald-trump/ar-AA1khSUH', 'https://www.msn.com/en-us/news/politics/trump-fraud-trial-trump-continues-to-assail-judge-clerk/ar-AA1hyBsB', 'https://www.nytimes.com/2023/11/21/us/politics/trump-democrats-biden.html', 'https://www.msn.com/en-us/news/other/ex-trump-aide-says-democracy-won-t-survive-if-he-wins-2024-election/ar-AA1ki3uq', 'https://www.cbsnews.com/video/appeals-court-considers-reinstating-trumps-gag-order-in-2020-election-case/']\n\n\nLes autres m√©thodes utiles de Selenium:\n\n\n\n\n\n\n\nM√©thode\nR√©sultat\n\n\n\n\nfind_element(****).click()\nUne fois qu‚Äôon a trouv√© un √©l√©ment r√©actif, notamment un bouton, on peut cliquer dessus pour activer une nouvelle page\n\n\nfind_element(****).send_keys(\"toto\")\nUne fois qu‚Äôon a trouv√© un √©l√©ment, notamment un champ o√π s‚Äôauthentifier, on peut envoyer une valeur, ici ‚Äútoto‚Äù.\n\n\n\n\n\nUtiliser Selenium pour jouer √† 2048\nDans cet exemple, on utilise le module pour que Python\nappuie lui m√™me sur les touches du clavier afin de jouer √† 2048.\nNote : ce bout de code ne donne pas une solution √† 2048,\nil permet juste de voir ce qu‚Äôon peut faire avec Selenium.\n\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.common.keys import Keys\n\n# on ouvre la page internet du jeu 2048\nservice = Service(executable_path=path_to_web_driver)\n\nbrowser = webdriver.Chrome(service=service,\n                           options=chrome_options)\nbrowser.get('https://play2048.co//')\n\n# Ce qu'on va faire : une boucle qui r√©p√®te inlassablement la m√™me chose : haut / droite / bas / gauche\n\n# on commence par cliquer sur la page pour que les touches sachent \nbutton = browser.find_element(\"class name\", 'grid-container')\nbrowser.execute_script(\"arguments[0].click();\", button)\ntime.sleep(0.5)\n\ngrid = browser.find_element(\"tag name\", 'body')\n\n# pour savoir quels coups faire √† quel moment, on cr√©e un dictionnaire\ndirection = {0: Keys.UP, 1: Keys.RIGHT, 2: Keys.DOWN, 3: Keys.LEFT}\ncount = 0\n\nwhile True:\n    try: # on v√©rifie que le bouton \"Try again\" n'est pas l√† - sinon √ßa veut dire que le jeu est fini\n        retryButton = browser.find_element(\"link text\",'Try again')\n        scoreElem = browser.find_element(\"class name\", 'score-container')\n        break\n    except:\n        #Do nothing.  Game is not over yet\n        pass\n    # on continue le jeu - on appuie sur la touche suivante pour le coup d'apr√®s\n    count += 1\n    grid.send_keys(direction[count % 4]) \n    time.sleep(0.1)\n\nprint('Score final : {} en {} coups'.format(scoreElem.text, count))    \nbrowser.quit()"
  },
  {
    "objectID": "content/manipulation/04a_webscraping_TP.html#exercice-suppl√©mentaire",
    "href": "content/manipulation/04a_webscraping_TP.html#exercice-suppl√©mentaire",
    "title": "Web scraping avec Python",
    "section": "Exercice suppl√©mentaire",
    "text": "Exercice suppl√©mentaire\nPour d√©couvrir une autre application possible du web scraping, vous pouvez √©galement vous lancer dans le sujet 5 de l‚Äô√©dition 2023 d‚Äôun hackathon non comp√©titif organis√© par l‚ÄôInsee :\n\nSur Github\nSur le SSPCloud\n\nLe contenu de la section NLP du cours pourra vous √™tre utile pour la seconde partie du sujet !"
  },
  {
    "objectID": "content/manipulation/04b_regex_TP.html#introduction",
    "href": "content/manipulation/04b_regex_TP.html#introduction",
    "title": "Ma√Ætriser les expressions r√©guli√®res",
    "section": "Introduction",
    "text": "Introduction\nPython offre √©norm√©ment de fonctionalit√©s tr√®s pratiques pour la manipulation de donn√©es\ntextuelles. C‚Äôest l‚Äôune des raisons de son\nsucc√®s dans la communaut√© du traitement automatis√© du langage (NLP, voir partie d√©di√©e).\nDans les chapitres pr√©c√©dents, nous avons parfois √©t√© amen√©s √† chercher des √©l√©ments textuels basiques. Cela √©tait possible avec la m√©thode str.find du package Pandas qui constitue une version vectoris√©e de la m√©thode find\nde base. Nous avons d‚Äôailleurs\npu utiliser cette derni√®re directement, notamment lorsqu‚Äôon a fait du web scraping.\nCependant, cette fonction de recherche\ntrouve rapidement ses limites.\nPar exemple, si on d√©sire trouver √† la fois les occurrences d‚Äôun terme au singulier\net au pluriel, il sera n√©cessaire d‚Äôutiliser\nau moins deux fois la m√©thode find.\nPour des verbes conjugu√©s, cela devient encore plus complexe, en particulier si ceux-ci changent de forme selon le sujet.\nPour des expressions compliqu√©es, il est conseill√© d‚Äôutiliser les expressions r√©guli√®res,\nou ‚Äúregex‚Äù. C‚Äôest une fonctionnalit√© qu‚Äôon retrouve dans beaucoup de langages. C‚Äôest une forme de grammaire qui permet de rechercher des expressions.\nUne partie du contenu de cette partie\nest une adaptation de la\ndocumentation collaborative sur R nomm√©e utilitR √† laquelle j‚Äôai particip√©. Ce chapitre reprend aussi du contenu du\nlivre R for Data Science qui pr√©sente un chapitre\ntr√®s p√©dagogique sur les regex.\nNous allons utiliser le package re pour illustrer nos exemples d‚Äôexpressions\nr√©guli√®res. Il s‚Äôagit du package de r√©f√©rence, qui est utilis√©, en arri√®re-plan,\npar Pandas pour vectoriser les recherches textuelles.\n\nimport re\nimport pandas as pd\n\n\n\n Hint\nLes expressions r√©guli√®res (regex) sont notoirement difficiles √† ma√Ætriser. Il existe des outils qui facilitent le travail avec les expressions r√©guli√®res.\n\nL‚Äôoutil de r√©f√©rence pour ceci est [https://regex101.com/] qui permet de tester des regex en Python\ntout en ayant une explication qui accompagne ce test\nDe m√™me pour ce site qui comporte une cheat sheet en bas de la page.\nLes jeux de Regex Crossword permettent d‚Äôapprendre les expressions r√©guli√®res en s‚Äôamusant\n\nIl peut √™tre pratique de demander √† des IA assistantes, comme Github Copilot ou ChatGPT, une\npremi√®re version d‚Äôune regex en expliquant le contenu qu‚Äôon veut extraire.\nCela peut faire √©conomiser pas mal de temps, sauf quand l‚ÄôIA fait preuve d‚Äôune confiance excessive\net vous propose avec aplomb une regex totalement fausse‚Ä¶"
  },
  {
    "objectID": "content/manipulation/04b_regex_TP.html#principe",
    "href": "content/manipulation/04b_regex_TP.html#principe",
    "title": "Ma√Ætriser les expressions r√©guli√®res",
    "section": "Principe",
    "text": "Principe\nLes expressions r√©guli√®res sont un outil permettant de d√©crire un ensemble de cha√Ænes de caract√®res possibles selon une syntaxe pr√©cise, et donc de d√©finir un motif (ou pattern). Les expressions r√©guli√®res servent par exemple lorsqu‚Äôon veut extraire une partie d‚Äôune cha√Æne de caract√®res, ou remplacer une partie d‚Äôune cha√Æne de caract√®res. Une expression r√©guli√®re prend la forme d‚Äôune cha√Æne de caract√®res, qui peut contenir √† la fois des √©l√©ments litt√©raux et des caract√®res sp√©ciaux qui ont un sens logique.\nPar exemple, \"ch.+n\" est une expression r√©guli√®re qui d√©crit le motif suivant : la cha√Æne litt√©rale ch, suivi de n‚Äôimporte quelle cha√Æne d‚Äôau moins un caract√®re (.+), suivie de la lettre n. Dans la cha√Æne \"J'ai un chien.\", la sous-cha√Æne \"chien\" correspond √† ce motif. De m√™me pour \"chapeau ron\" dans \"J'ai un chapeau rond\". En revanche, dans la cha√Æne \"La soupe est chaude.\", aucune sous-cha√Æne ne correpsond √† ce motif (car aucun n n‚Äôappara√Æt apr√®s le ch).\nPour s‚Äôen convaincre, nous pouvons d√©j√† regarder\nles deux premiers cas:\n\npattern = \"ch.+n\"\nprint(re.search(pattern, \"J'ai un chien.\"))\nprint(re.search(pattern, \"J'ai un chapeau rond.\"))\n\n&lt;re.Match object; span=(8, 13), match='chien'&gt;\n&lt;re.Match object; span=(8, 19), match='chapeau ron'&gt;\n\n\nCependant, dans le dernier cas, nous ne trouvons pas\nle pattern recherch√©:\n\nprint(re.search(pattern, \"La soupe est chaude.\"))\n\nNone\n\n\nLa regex pr√©c√©dente comportait deux types de caract√®res:\n\nles caract√®res litt√©raux : lettres et nombres qui sont reconnus de mani√®re litt√©rale\nles m√©ta-caract√®res : symboles qui ont un sens particulier dans les regex.\n\nLes principaux m√©ta-caract√®res sont ., +, *, [, ], ^ et $ mais il\nen existe beaucoup d‚Äôautres.\nParmi cet ensemble, on utilise principalement les quantifieurs (., +, *‚Ä¶),\nles classes de caract√®res (ensemble qui sont d√©limit√©s par [ et ])\nou les ancres (^, $‚Ä¶)\nDans l‚Äôexemple pr√©c√©dent,\nnous retrouvions deux quantifieurs accol√©s .+. Le premier (.) signifie n‚Äôimporte quel caract√®re1. Le deuxi√®me (+) signifie ‚Äúr√©p√®te le pattern pr√©c√©dent‚Äù.\nDans notre cas, la combinaison .+ permet ainsi de r√©p√©ter n‚Äôimporte quel caract√®re avant de trouver un n.\nLe nombre de fois est indetermin√©: cela peut ne pas √™tre pas n√©cessaire d‚Äôintercaler des caract√®res avant le n\nou cela peut √™tre n√©cessaire d‚Äôen intercepter plusieurs :\n\nprint(re.search(pattern, \"J'ai un chino\"))\nprint(re.search(pattern, \"J'ai un chiot tr√®s mignon.\"))\n\n&lt;re.Match object; span=(8, 12), match='chin'&gt;\n&lt;re.Match object; span=(8, 25), match='chiot tr√®s mignon'&gt;\n\n\n\nClasses de caract√®res\nLors d‚Äôune recherche, on s‚Äôint√©resse aux caract√®res et souvent aux classes de caract√®res : on cherche un chiffre, une lettre, un caract√®re dans un ensemble pr√©cis ou un caract√®re qui n‚Äôappartient pas √† un ensemble pr√©cis. Certains ensembles sont pr√©d√©finis, d‚Äôautres doivent √™tre d√©finis √† l‚Äôaide de crochets.\nPour d√©finir un ensemble de caract√®res, il faut √©crire cet ensemble entre crochets. Par exemple, [0123456789] d√©signe un chiffre. Comme c‚Äôest une s√©quence de caract√®res cons√©cutifs, on peut r√©sumer cette √©criture en [0-9].\nPar\nexemple, si on d√©sire trouver tous les pattern qui commencent par un c suivi\nd‚Äôun h puis d‚Äôune voyelle (a, e, i, o, u), on peut essayer\ncette expression r√©guli√®re.\n\nre.findall(\"[c][h][aeiou]\", \"chat, chien, veau, vache, ch√®vre\")\n\n['cha', 'chi', 'che']\n\n\nIl serait plus pratique d‚Äôutiliser Pandas dans ce cas pour isoler les\nlignes qui r√©pondent √† la condition logique (en ajoutant les accents\nqui ne sont pas compris sinon):\n\nimport pandas as pd\ntxt = pd.Series(\"chat, chien, veau, vache, ch√®vre\".split(\", \"))\ntxt.str.match(\"ch[ae√©√®iou]\")\n\n0     True\n1     True\n2    False\n3    False\n4     True\ndtype: bool\n\n\nCependant, l‚Äôusage ci-dessus des classes de caract√®res\nn‚Äôest pas le plus fr√©quent.\nOn privil√©gie celles-ci pour identifier des\npattern complexe plut√¥t qu‚Äôune suite de caract√®res litt√©raux.\nLes tableaux d‚Äôaide m√©moire illustrent une partie des\nclasses de caract√®res les plus fr√©quentes\n([:digit:] ou \\d‚Ä¶)\n\n\nQuantifieurs\nNous avons rencontr√© les quantifieurs avec notre premi√®re expression\nr√©guli√®re. Ceux-ci contr√¥lent le nombre de fois\nqu‚Äôun pattern est rencontr√©.\nLes plus fr√©quents sont:\n\n? : 0 ou 1 match ;\n+ : 1 ou plus de matches ;\n* : 0 or more matches.\n\nPar exemple, colou?r permettra de matcher √† la fois l‚Äô√©criture am√©ricaine et anglaise\n\nre.findall(\"colou?r\", \"Did you write color or colour?\")\n\n['color', 'colour']\n\n\nCes quantifiers peuvent bien s√ªr √™tre associ√©s √†\nd‚Äôautres types de caract√®res, notamment les classes de caract√®res.\nCela peut √™tre extr√®mement pratique.\nPar exemple, \\d+ permettra de capturer un ou plusieurs chiffres, \\s?\npermettra d‚Äôajouter en option un espace,\n[\\w]{6,8} un mot entre six et huit lettres qu‚Äôon √©crira‚Ä¶\nIl est aussi possible de d√©finir le nombre de r√©p√©titions\navec {}:\n\n{n} matche exactement n fois ;\n{n,} matche au moins n fois ;\n{n,m} matche entre n et m fois.\n\nCependant, la r√©p√©tition des termes\nne s‚Äôapplique par d√©faut qu‚Äôau dernier\ncaract√®re pr√©c√©dent le quantifier.\nOn peut s‚Äôen convaincre avec l‚Äôexemple ci-dessus:\n\nprint(re.match(\"toc{4}\",\"toctoctoctoc\"))\n\nNone\n\n\nPour pallier ce probl√®me, il existe les parenth√®ses.\nLe principe est le m√™me qu‚Äôavec les r√®gles num√©riques:\nles parenth√®ses permettent d‚Äôintroduire une hi√©rarchie.\nPour reprendre l‚Äôexemple pr√©c√©dent, on obtient\nbien le r√©sultat attendu gr√¢ce aux parenth√®ses:\n\nprint(re.match(\"(toc){4}\",\"toctoctoctoc\"))\nprint(re.match(\"(toc){5}\",\"toctoctoctoc\"))\nprint(re.match(\"(toc){2,4}\",\"toctoctoctoc\"))\n\n&lt;re.Match object; span=(0, 12), match='toctoctoctoc'&gt;\nNone\n&lt;re.Match object; span=(0, 12), match='toctoctoctoc'&gt;\n\n\n\n\n Note\nL‚Äôalgorithme des expressions r√©guli√®res essaye toujours de faire correspondre le plus grand morceau √† l‚Äôexpression r√©guli√®re.\nPar exemple, soit une chaine de caract√®re HTML:\n\ns = \"&lt;h1&gt;Super titre HTML&lt;/h1&gt;\"\n\nL‚Äôexpression r√©guli√®re re.findall(\"&lt;.*&gt;\", s) correspond, potentiellement,\n√† trois morceaux :\n\n&lt;h1&gt;\n&lt;/h1&gt;\n&lt;h1&gt;Super titre HTML&lt;/h1&gt;\n\nC‚Äôest ce dernier qui sera choisi, car le plus grand. Pour\ns√©lectionner le plus petit,\nil faudra √©crire les multiplicateurs comme ceci : *?, +?.\nEn voici quelques exemples:\n\ns = \"&lt;h1&gt;Super titre HTML&lt;/h1&gt;\\n&lt;p&gt;&lt;code&gt;Python&lt;/code&gt; est un langage tr√®s flexible&lt;/p&gt;\"\nprint(re.findall(\"&lt;.*&gt;\", s))\nprint(re.findall(\"&lt;p&gt;.*&lt;/p&gt;\", s))\nprint(re.findall(\"&lt;p&gt;.*?&lt;/p&gt;\", s))\nprint(re.compile(\"&lt;.*?&gt;\").findall(s))\n\n['&lt;h1&gt;Super titre HTML&lt;/h1&gt;', '&lt;p&gt;&lt;code&gt;Python&lt;/code&gt; est un langage tr√®s flexible&lt;/p&gt;']\n['&lt;p&gt;&lt;code&gt;Python&lt;/code&gt; est un langage tr√®s flexible&lt;/p&gt;']\n['&lt;p&gt;&lt;code&gt;Python&lt;/code&gt; est un langage tr√®s flexible&lt;/p&gt;']\n['&lt;h1&gt;', '&lt;/h1&gt;', '&lt;p&gt;', '&lt;code&gt;', '&lt;/code&gt;', '&lt;/p&gt;']\n\n\n\n\n\n\nAide-m√©moire\nLe tableau ci-dessous peut servir d‚Äôaide-m√©moire\nsur les regex:\n\n\n\n\n\n\n\nExpression r√©guli√®re\nSignification\n\n\n\n\n\"^\"\nD√©but de la cha√Æne de caract√®res\n\n\n\"$\"\nFin de la cha√Æne de caract√®res\n\n\n\"\\\\.\"\nUn point\n\n\n\".\"\nN‚Äôimporte quel caract√®re\n\n\n\".+\"\nN‚Äôimporte quelle suite de caract√®res non vide\n\n\n\".*\"\nN‚Äôimporte quelle suite de caract√®res, √©ventuellement vi\n\n\n\"[:alnum:]\"\nUn caract√®re alphanum√©rique\n\n\n\"[:alpha:]\"\nUne lettre\n\n\n\"[:digit:]\"\nUn chiffre\n\n\n\"[:lower:]\"\nUne lettre minuscule\n\n\n\"[:punct:]\"\nUn signe de ponctuation\n\n\n\"[:space:]\"\nun espace\n\n\n\"[:upper:]\"\nUne lettre majuscule\n\n\n\"[[:alnum:]]+\"\nUne suite d‚Äôau moins un caract√®re alphanum√©rique\n\n\n\"[[:alpha:]]+\"\nUne suite d‚Äôau moins une lettre\n\n\n\"[[:digit:]]+\"\nUne suite d‚Äôau moins un chiffre\n\n\n\"[[:lower:]]+\"\nUne suite d‚Äôau moins une lettre minuscule\n\n\n\"[[:punct:]]+\"\nUne suite d‚Äôau moins un signe de ponctuation\n\n\n\"[[:space:]]+\"\nUne suite d‚Äôau moins un espace\n\n\n\"[[:upper:]]+\"\nUne suite d‚Äôau moins une lettre majuscule\n\n\n\"[[:alnum:]]*\"\nUne suite de caract√®res alphanum√©riques, √©ventuellement vide\n\n\n\"[[:alpha:]]*\"\nUne suite de lettres, √©ventuellement vide\n\n\n\"[[:digit:]]*\"\nUne suite de chiffres, √©ventuellement vide\n\n\n\"[[:lower:]]*\"\nUne suite de lettres minuscules, √©ventuellement vide\n\n\n\"[[:upper:]]*\"\nUne suite de lettres majuscules, √©ventuellement vide\n\n\n\"[[:punct:]]*\"\nUne suite de signes de ponctuation, √©ventuellement vide\n\n\n\"[^[:alpha:]]+\"\nUne suite d‚Äôau moins un caract√®re autre qu‚Äôune lettre\n\n\n\"[^[:digit:]]+\"\nUne suite d‚Äôau moins un caract√®re autre qu‚Äôun chiffre\n\n\n\"\\|\"\nL‚Äôune des expressions x ou y est pr√©sente\n\n\n[abyz]\nUn seul des caract√®res sp√©cifi√©s\n\n\n[abyz]+\nUn ou plusieurs des caract√®res sp√©cifi√©s (√©ventuellement r√©p√©t√©s)\n\n\n[^abyz]\nAucun des caract√®res sp√©cifi√©s n‚Äôest pr√©sent\n\n\n\nCertaines classes de caract√®res b√©n√©ficient d‚Äôune syntaxe plus l√©g√®re car\nelles sont tr√®s fr√©quentes. Parmi-celles:\n\n\n\n\n\n\n\nExpression r√©guli√®re\nSignification\n\n\n\n\n\\d\nN‚Äôimporte quel chiffre\n\n\n\\D\nN‚Äôimporte quel caract√®re qui n‚Äôest pas un caract√®re\n\n\n\\s\nN‚Äôimporte quel espace (espace, tabulation, retour √† la ligne)\n\n\n\\S\nN‚Äôimporte quel caract√®re qui n‚Äôest pas un espace\n\n\n\\w\nN‚Äôimporte quel type de mot (lettres et nombres)\n\n\n\\W\nN‚Äôimporte quel ensemble qui n‚Äôest pas un mot (lettres et nombres)\n\n\n\nDans l‚Äôexercice suivant, vous allez pouvoir mettre en pratique\nles exemples pr√©c√©dents sur une regex un peu plus compl√®te.\nCet exercice ne n√©cessite pas la connaissance des subtilit√©s\ndu package re, vous n‚Äôaurez besoin que de re.findall.\nCet exercice utilisera la chaine de caract√®re suivante :\n\ns = \"\"\"date 0 : 14/9/2000\ndate 1 : 20/04/1971     date 2 : 14/09/1913     date 3 : 2/3/1978\ndate 4 : 1/7/1986     date 5 : 7/3/47     date 6 : 15/10/1914\ndate 7 : 08/03/1941     date 8 : 8/1/1980     date 9 : 30/6/1976\"\"\"\ns\n\n'date 0 : 14/9/2000\\ndate 1 : 20/04/1971     date 2 : 14/09/1913     date 3 : 2/3/1978\\ndate 4 : 1/7/1986     date 5 : 7/3/47     date 6 : 15/10/1914\\ndate 7 : 08/03/1941     date 8 : 8/1/1980     date 9 : 30/6/1976'\n\n\n\n\n Exercice 1\n\nOn va d‚Äôabord s‚Äôoccuper d‚Äôextraire le jour de naissance.\n\nLe premier chiffre du jour est 0, 1, 2 ou 3. Traduire cela sous la forme d‚Äôune s√©quence [X-X]\nLe deuxi√®me chiffre du jour est lui entre 0 et 9. Traduire cela sous la s√©quence ad√©quate\nRemarquez que le premier jour est facultatif. Intercaler entre les deux classes de caract√®re ad√©quate\nle quantifieur qui convient\nAjouter le slash √† la suite du motif\nTester avec re.findall. Vous devriez obtenir beaucoup plus d‚Äô√©chos que n√©cessaire.\nC‚Äôest normal, √† ce stade la\nregex n‚Äôest pas encore finalis√©e\n\nSuivre la m√™me logique pour les mois en notant que les mois du calendrier gr√©gorien ne d√©passent\njamais la premi√®re dizaine. Tester avec re.findall\nDe m√™me pour les ann√©es de naissance en notant que jusqu‚Äô√† preuve du contraire, pour des personnes vivantes\naujourd‚Äôhui, les mill√©naires concern√©s sont restreints. Tester avec re.findall\nCette regex n‚Äôest pas naturelle, on pourrait tr√®s bien se satisfaire de classes de\ncaract√®res g√©n√©riques \\d m√™me si elles pourraient, en pratique, nous s√©lectionner des\ndates de naissance non possibles (43/78/4528 par exemple). Cela permettrait\nd‚Äôall√©ger la regex afin de la rendre plus intelligible. Ne pas oublier l‚Äôutilit√© des quantifieurs.\nComment adapter la regex pour qu‚Äôelle soit toujours valide pour nos cas mais permette aussi de\ncapturer les dates de type YYYY/MM/DD ? Tester sur 1998/07/12\n\n\n\nA l‚Äôissue de la question 1, vous devriez avoir ce r√©sultat :\n\n\n['14/',\n '9/',\n '20/',\n '04/',\n '14/',\n '09/',\n '2/',\n '3/',\n '1/',\n '7/',\n '7/',\n '3/',\n '15/',\n '10/',\n '08/',\n '03/',\n '8/',\n '1/',\n '30/',\n '6/']\n\n\nA l‚Äôissue de la question 2, vous devriez avoir ce r√©sultat, qui\ncommence √† prendre forme:\n\n\n['14/9',\n '20/04',\n '14/09',\n '2/3',\n '1/7',\n '7/3',\n '15/10',\n '08/03',\n '8/1',\n '30/6']\n\n\nA l‚Äôissue de la question 3, on parvient bien\n√† extraire les dates :\n\n\n['14/9/2000',\n '20/04/1971',\n '14/09/1913',\n '2/3/1978',\n '1/7/1986',\n '7/3/47',\n '15/10/1914',\n '08/03/1941',\n '8/1/1980',\n '30/6/1976']\n\n\nSi tout va bien, √† la question 5, votre regex devrait\nfonctionner:\n\n\n['14/9/2000',\n '20/04/1971',\n '14/09/1913',\n '2/3/1978',\n '1/7/1986',\n '7/3/47',\n '15/10/1914',\n '08/03/1941',\n '8/1/1980',\n '30/6/1976',\n '1998/07/12']"
  },
  {
    "objectID": "content/manipulation/04b_regex_TP.html#principales-fonctions-de-re",
    "href": "content/manipulation/04b_regex_TP.html#principales-fonctions-de-re",
    "title": "Ma√Ætriser les expressions r√©guli√®res",
    "section": "Principales fonctions de re",
    "text": "Principales fonctions de re\nVoici un tableau r√©capitulatif des principales\nfonctions du package re suivi d‚Äôexemples.\nNous avons principalement\nutilis√© jusqu‚Äô√† pr√©sent re.findall qui est\nl‚Äôune des fonctions les plus pratiques du package.\nre.sub et re.search sont √©galement bien pratiques.\nLes autres sont moins vitales mais peuvent dans des\ncas pr√©cis √™tre utiles.\n\n\n\n\n\n\n\nFonction\nObjectif\n\n\n\n\nre.match(&lt;regex&gt;, s)\nTrouver et renvoyer le premier match de l‚Äôexpression r√©guli√®re &lt;regex&gt; √† partir du d√©but du string s\n\n\nre.search(&lt;regex&gt;, s)\nTrouver et renvoyer le premier match de l‚Äôexpression r√©guli√®re &lt;regex&gt; quelle que soit sa position dans le string s\n\n\nre.finditer(&lt;regex&gt;, s)\nTrouver et renvoyer un it√©rateur stockant tous les matches de l‚Äôexpression r√©guli√®re &lt;regex&gt; quelle que soit leur(s) position(s) dans le string s. En g√©n√©ral, on effectue ensuite une boucle sur cet it√©rateur\n\n\nre.findall(&lt;regex&gt;, s)\nTrouver et renvoyer tous les matches de l‚Äôexpression r√©guli√®re &lt;regex&gt; quelle que soit leur(s) position(s) dans le string s sous forme de liste\n\n\nre.sub(&lt;regex&gt;, new_text, s)\nTrouver et remplacer tous les matches de l‚Äôexpression r√©guli√®re &lt;regex&gt; quelle que soit leur(s) position(s) dans le string s\n\n\n\nPour illustrer ces fonctions, voici quelques exemples:\n\nExemple de re.match üëá\nre.match ne peut servir qu‚Äô√† capturer un pattern en d√©but\nde string. Son utilit√© est donc limit√©e.\nCapturons n√©anmoins toto :\n\nre.match(\"(to){2}\", \"toto √† la plage\")\n\n&lt;re.Match object; span=(0, 4), match='toto'&gt;\n\n\n\n\n\nExemple de re.search üëá\nre.search est plus puissant que re.match, on peut\ncapturer des termes quelle que soit leur position\ndans un string. Par exemple, pour capturer age :\n\nre.search(\"age\", \"toto a l'age d'aller √† la plage\")\n\n&lt;re.Match object; span=(9, 12), match='age'&gt;\n\n\nEt pour capturer exclusivement ‚Äúage‚Äù en fin\nde string :\n\nre.search(\"age$\", \"toto a l'age d'aller √† la plage\")\n\n&lt;re.Match object; span=(28, 31), match='age'&gt;\n\n\n\n\n\nExemple de re.finditer üëá\nre.finditer est, √† mon avis,\nmoins pratique que re.findall. Son utilit√©\nprincipale par rapport √† re.findall\nest de capturer la position dans un champ textuel:\n\ns = \"toto a l'age d'aller √† la plage\"\nfor match in re.finditer(\"age\", s):\n    start = match.start()\n    end = match.end()\n    print(f'String match \"{s[start:end]}\" at {start}:{end}')\n\nString match \"age\" at 9:12\nString match \"age\" at 28:31\n\n\n\n\n\nExemple de re.sub üëá\nre.sub permet de capturer et remplacer des expressions.\nPar exemple, rempla√ßons ‚Äúage‚Äù par ‚Äú√¢ge‚Äù. Mais attention,\nil ne faut pas le faire lorsque le motif est pr√©sent dans ‚Äúplage‚Äù.\nOn va donc mettre une condition n√©gative: capturer ‚Äúage‚Äù seulement\ns‚Äôil n‚Äôest pas en fin de string (ce qui se traduit en regex par ?!$)\n\nre.sub(\"age(?!$)\", \"√¢ge\", \"toto a l'age d'aller √† la plage\")\n\n\"toto a l'√¢ge d'aller √† la plage\"\n\n\n\n\n\n\n Quand utiliser re.compile et les raw strings ?\nre.compile peut √™tre int√©ressant lorsque\nvous utilisez une expression r√©guli√®re plusieurs fois dans votre code.\nCela permet de compiler l‚Äôexpression r√©guli√®re en un objet reconnu par re,\nce qui peut √™tre plus efficace en termes de performance lorsque l‚Äôexpression r√©guli√®re\nest utilis√©e √† plusieurs reprises ou sur des donn√©es volumineuses.\nLes cha√Ænes brutes (raw string) sont des cha√Ænes de caract√®res sp√©ciales en Python,\nqui commencent par r. Par exemple r\"toto √† la plage\".\nElles peuvent √™tre int√©ressantes\npour √©viter que les caract√®res d‚Äô√©chappement ne soient interpr√©t√©s par Python\nPar exemple, si vous voulez chercher une cha√Æne qui contient une barre oblique inverse \\ dans une cha√Æne, vous devez utiliser une cha√Æne brute pour √©viter que la barre oblique inverse ne soit interpr√©t√©e comme un caract√®re d‚Äô√©chappement (\\t, \\n, etc.).\nLe testeur https://regex101.com/ suppose d‚Äôailleurs que\nvous utilisez des raw string, cela peut donc √™tre utile de s‚Äôhabituer √† les utiliser."
  },
  {
    "objectID": "content/manipulation/04b_regex_TP.html#g√©n√©ralisation-avec-pandas",
    "href": "content/manipulation/04b_regex_TP.html#g√©n√©ralisation-avec-pandas",
    "title": "Ma√Ætriser les expressions r√©guli√®res",
    "section": "G√©n√©ralisation avec Pandas",
    "text": "G√©n√©ralisation avec Pandas\nLes m√©thodes de Pandas sont des extensions de celles de re\nqui √©vitent de faire une boucle pour regarder,\nligne √† ligne, une regex. En pratique, lorsqu‚Äôon traite des\nDataFrames, on utilise plut√¥t l‚ÄôAPI Pandas que re. Les\ncodes de la forme df.apply(lambda x: re.&lt;fonction&gt;(&lt;regex&gt;,x), axis = 1)\nsont √† bannir car tr√®s peu efficaces.\nLes noms changent parfois l√©g√®rement par rapport √† leur\n√©quivalent re.\n\n\n\n\n\n\n\nM√©thode\nDescription\n\n\n\n\nstr.count()\nCompter le nombre d‚Äôoccurrences du pattern dans chaque ligne\n\n\nstr.replace()\nRemplacer le pattern par une autre valeur. Version vectoris√©e de re.sub()\n\n\nstr.contains()\nTester si le pattern appara√Æt, ligne √† ligne. Version vectoris√©e de re.search()\n\n\nstr.extract()\nExtraire les groupes qui r√©pondent √† un pattern et les renvoyer dans une colonne\n\n\nstr.findall()\nTrouver et renvoyer toutes les occurrences d‚Äôun pattern. Si une ligne comporte plusieurs √©chos, une liste est renvoy√©e. Version vectoris√©e de re.findall()\n\n\n\nA ces fonctions, s‚Äôajoutent les m√©thodes str.split() et str.rsplit() qui sont bien pratiques.\n\nExemple de str.count üëá\nOn peut compter le nombre de fois qu‚Äôun pattern appara√Æt avec\nstr.count\n\ndf = pd.DataFrame({\"a\": [\"toto\", \"titi\"]})\ndf['a'].str.count(\"to\")\n\n0    2\n1    0\nName: a, dtype: int64\n\n\n\n\n\nExemple de str.replace üëá\nRempla√ßons le motif ‚Äúti‚Äù en fin de phrase\n\ndf = pd.DataFrame({\"a\": [\"toto\", \"titi\"]})\ndf['a'].str.replace(\"ti$\", \" punch\")\n\n0    toto\n1    titi\nName: a, dtype: object\n\n\n\n\n\nExemple de str.contains üëá\nV√©rifions les cas o√π notre ligne termine par ‚Äúti‚Äù :\n\ndf = pd.DataFrame({\"a\": [\"toto\", \"titi\"]})\ndf['a'].str.contains(\"ti$\")\n\n0    False\n1     True\nName: a, dtype: bool\n\n\n\n\n\nExemple de str.findall üëá\n\ndf = pd.DataFrame({\"a\": [\"toto\", \"titi\"]})\ndf['a'].str.findall(\"to\")\n\n0    [to, to]\n1          []\nName: a, dtype: object\n\n\n\n\n\n\n Warning\nA l‚Äôheure actuelle, il n‚Äôest pas n√©cessaire d‚Äôajouter l‚Äôargument regex = True mais cela\ndevrait √™tre le cas dans une future version de Pandas.\nCela peut valoir le coup de s‚Äôhabituer √† l‚Äôajouter."
  },
  {
    "objectID": "content/manipulation/04b_regex_TP.html#pour-en-savoir-plus",
    "href": "content/manipulation/04b_regex_TP.html#pour-en-savoir-plus",
    "title": "Ma√Ætriser les expressions r√©guli√®res",
    "section": "Pour en savoir plus",
    "text": "Pour en savoir plus\n\ndocumentation collaborative sur R nomm√©e utilitR\nR for Data Science\nRegular Expression HOWTO dans la documentation officielle de Python\nL‚Äôoutil de r√©f√©rence [https://regex101.com/] pour tester des expressions r√©guli√®res\nCe site qui comporte une cheat sheet en bas de la page.\nLes jeux de Regex Crossword permettent d‚Äôapprendre les expressions r√©guli√®res en s‚Äôamusant"
  },
  {
    "objectID": "content/manipulation/04b_regex_TP.html#exercices-suppl√©mentaires",
    "href": "content/manipulation/04b_regex_TP.html#exercices-suppl√©mentaires",
    "title": "Ma√Ætriser les expressions r√©guli√®res",
    "section": "Exercices suppl√©mentaires",
    "text": "Exercices suppl√©mentaires\n\nExtraction d‚Äôadresses email\nIl s‚Äôagit d‚Äôun usage classique des regex\n\ntext_emails = 'Hello from toto@gmail.com to titi.grominet@yahoo.com about the meeting @2PM'\n\n\n\n Exercice : extraction d'adresses email\nUtiliser la structure d‚Äôune adresse mail [XXXX]@[XXXX] pour r√©cup√©rer\nce contenu\n\n\n\n\n['toto@gmail.com', 'titi.grominet@yahoo.com']\n\n\n\n\nExtraire des ann√©es depuis un DataFrame Pandas\nL‚Äôobjectif g√©n√©ral de l‚Äôexercice est de nettoyer des colonnes d‚Äôun DataFrame en utilisant des expressions r√©guli√®res.\n\n\n Exercice\nLa base en question contient des livres de la British Library et quelques informations les concernant. Le jeu de donn√©es est disponible ici : https://raw.githubusercontent.com/realpython/python-data-cleaning/master/Datasets/BL-Flickr-Images-Book.csv\nLa colonne ‚ÄúDate de Publication‚Äù n‚Äôest pas toujours une ann√©e, il y a parfois d‚Äôautres informations. Le but de l‚Äôexercice est d‚Äôavoir une date de publication du livre propre et de regarder la distribution des ann√©es de publications.\nPour ce faire, vous pouvez :\n\nSoit choisir de r√©aliser l‚Äôexercice sans aide. Votre lecture de l‚Äô√©nonc√© s‚Äôarr√™te donc ici. Vous devez alors faire attention √† bien regarder vous-m√™me la base de donn√©es et la transformer avec attention.\nSoit suivre les diff√©rentes √©tapes qui suivent pas √† pas.\n\nVersion guid√©e üëá\n\nLire les donn√©es depuis l‚Äôurl https://raw.githubusercontent.com/realpython/python-data-cleaning/master/Datasets/BL-Flickr-Images-Book.csv. Attention au s√©parateur\nNe garder que les colonnes ['Identifier', 'Place of Publication', 'Date of Publication', 'Publisher', 'Title', 'Author']\nObserver la colonne ‚ÄòDate of Publication‚Äô et remarquer le probl√®me sur certaines lignes (par exemple la ligne 13)\nCommencez par regarder le nombre d‚Äôinformations manquantes. On ne pourra pas avoir mieux apr√®s la regex, et normalement on ne devrait pas avoir moins‚Ä¶\nD√©terminer la forme de la regex pour une date de publication. A priori, il y a 4 chiffres qui forment une ann√©e.\nUtiliser la m√©thode str.extract() avec l‚Äôargument expand = False (pour ne conserver que la premi√®re date concordant avec notre pattern)?\nOn a 2 NaN qui n‚Äô√©taient pas pr√©sents au d√©but de l‚Äôexercice. Quels sont-ils et pourquoi ?\nQuelle est la r√©partition des dates de publications dans le jeu de donn√©es ? Vous pouvez par exemple afficher un histogramme gr√¢ce √† la m√©thode plot avec l‚Äôargument kind =\"hist\".\n\n\n\n\nVoici par exemple le probl√®me qu‚Äôon demande de d√©tecter √† la question 3 :\n\n\n\n\n\n\n\n\n\nDate of Publication\nTitle\n\n\n\n\n13\n1839, 38-54\nDe Aardbol. Magazijn van hedendaagsche land- e...\n\n\n14\n1897\nCronache Savonesi dal 1500 al 1570 ... Accresc...\n\n\n15\n1865\nSee-Saw; a novel ... Edited [or rather, writte...\n\n\n16\n1860-63\nGeÃÅodeÃÅsie d'une partie de la Haute EÃÅthiopie,...\n\n\n17\n1873\n[With eleven maps.]\n\n\n18\n1866\n[Historia geograÃÅfica, civil y politica de la ...\n\n\n19\n1899\nThe Crisis of the Revolution, being the story ...\n\n\n\n\n\n\n\n\n\n181\n\n\nGr√¢ce √† notre regex (question 5), on obtient ainsi un DataFrame plus conforme √† nos attentes\n\n\n\n\n\n\n\n\n\nDate of Publication\nyear\n\n\n\n\n0\n1879 [1878]\n1879\n\n\n7\nNaN\nNaN\n\n\n13\n1839, 38-54\n1839\n\n\n16\n1860-63\n1860\n\n\n23\n1847, 48 [1846-48]\n1847\n\n\n...\n...\n...\n\n\n8278\n1883, [1884]\n1883\n\n\n8279\n1898-1912\n1898\n\n\n8283\n1831, 32\n1831\n\n\n8284\n[1806]-22\n1806\n\n\n8286\n1834-43\n1834\n\n\n\n\n1759 rows √ó 2 columns\n\n\n\nQuant aux nouveaux NaN,\nil s‚Äôagit de lignes qui ne contenaient pas de cha√Ænes de caract√®res qui ressemblaient √† des ann√©es:\n\n\n\n\n\n\n\n\n\nDate of Publication\nyear\n\n\n\n\n1081\n112. G. & W. B. Whittaker\nNaN\n\n\n7391\n17 vols. University Press\nNaN\n\n\n\n\n\n\n\nEnfin, on obtient l‚Äôhistogramme suivant des dates de publications:\n\n\n&lt;Axes: ylabel='Frequency'&gt;"
  },
  {
    "objectID": "content/manipulation/04b_regex_TP.html#footnotes",
    "href": "content/manipulation/04b_regex_TP.html#footnotes",
    "title": "Ma√Ætriser les expressions r√©guli√®res",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nN‚Äôimporte quel caract√®re √† part le retour √† la ligne (\\n). Ceci est √† garder en t√™te, j‚Äôai d√©j√† perdu des heures √† chercher pourquoi mon . ne capturait pas ce que je voulais qui s‚Äô√©talait sur plusieurs lignes‚Ä¶‚Ü©Ô∏é"
  },
  {
    "objectID": "content/visualisation/index.html",
    "href": "content/visualisation/index.html",
    "title": "Partie 2: visualiser les donn√©es",
    "section": "",
    "text": "La visualisation de donn√©es est l‚Äôart et la science de repr√©senter visuellement des informations complexes et abstraites √† l‚Äôaide d‚Äô√©l√©ments visuels.\nSon objectif principal est de synth√©tiser l‚Äôinformation pr√©sente dans un ensemble de donn√©es afin de faciliter\nla compr√©hension des enjeux de celle-ci pour une analyse ult√©rieure.\nLa visualisation de donn√©es permet, entre autres, de mettre en √©vidence des tendances, des corr√©lations ou\ndes anomalies qui pourraient √™tre difficiles voire impossibles √† saisir simplement en examinant des donn√©es brutes, ces derni√®res n√©cessitant\nune certaine mise en contexte pour porter du sens.\nLa visualisation de donn√©es joue un r√¥le crucial dans le\nprocessus d‚Äôanalyse de donn√©es en fournissant des moyens visuels pour explorer, interpr√©ter et communiquer des informations.\nElle facilite la communication entre experts de la donn√©es, d√©cideurs et grand public,\nen permettant de raconter des histoires bas√©es sur les donn√©es de mani√®re plus convaincante et engageante.\nLa visualisation de donn√©es a une place √† part dans\nl‚Äôensemble des techniques de la data science.\nElle intervient √† tous les stades du processus de\nproduction de la donn√©e, de\nl‚Äôamont (analyse exploratoire) √†\nl‚Äôaval (restitution √† des publics multiples) et\npeut, si elle est bien construite, permettre de\nsaisir de mani√®re intuitive la structure des donn√©es\nou les enjeux de son analyse.\nArt de la synth√®se, la visualisation de donn√©es\nest √©galement l‚Äôart de raconter une histoire et\npeut m√™me, lorsqu‚Äôelle est bien construite, pr√©tendre\nau rang de production artistique.\nLa dataviz est un m√©tier en soi dont on trouve de\nplus en plus de praticiens dans les titres de presse\nou dans des entreprises\nsp√©cialis√©es (Datawrapper par exemple).\nSans pr√©tendre construire\ndes visualisations aussi riches que celles des sp√©cialistes,\ntout data scientist se doit d‚Äô√™tre en mesure de pouvoir\nproduire rapidement quelques visualisations permettant\nde synth√©tiser les jeux de donn√©es √† sa disposition.\nUne visualisation claire et lisible tout en restant simple\npeut √™tre meilleure qu‚Äôun discours pour faire passer un message.\nJe recommande notamment\nce post de blog\nd‚ÄôEric Mauvi√®re qui revient sur deux graphiques dans une publication\nr√©cente\ndu Service statistique du Minist√®re de la Sant√© (DREES)\net montre la mani√®re dont on peut am√©liorer le message transmis\npar des figures :\nDe m√™me qu‚Äôun discours, une visualisation est une communication\npour laquelle un locuteur - la personne construisant la visualisation -\ncherche √† transmettre une information √† un r√©cepteur - √©ventuellement\nla m√™me personne que le locuteur puisqu‚Äôune visualisation peut\n√™tre construite pour soi-m√™me dans une analyse exploratoire. Il n‚Äôest\ndonc pas surprenant qu‚Äô√† l‚Äô√©poque o√π la s√©miologie occupait une\npart importante dans les d√©bats intellectuels, notamment autour\nde la figure de Roland Barthes, le concept de s√©miologie\ngraphique ait √©merg√©\nautour de la personne de Jacques Bertin (Bertin 1967; Palsky 2017).\nCette approche permet de r√©fl√©chir sur la pertinence des\ntechniques mises en oeuvre pour transmettre un message\ngraphique et de nombreuses visualisations, si elles\nsuivaient quelques-unes de ces r√®gles, pourraient\n√™tre am√©lior√©es √† peu de frais.\nL‚ÄôInsee a publi√©, il y a quelques ann√©es, un guide de\ns√©miologie graphique tr√®s utile qu‚Äôil\nest int√©ressant de consulter de temps en temps (Insee 2018).\nPour revenir √† notre cours,\nnous pr√©senterons dans cette partie quelques librairies\net visualisations basiques en Python permettant de\npartir sur de bonnes bases. Les ressources pour\napprofondir et progresser dans l‚Äôart de la visualisation\nne manquent pas, comme cet ouvrage (Wilke 2019)."
  },
  {
    "objectID": "content/visualisation/index.html#l√©cosyst√®me-python",
    "href": "content/visualisation/index.html#l√©cosyst√®me-python",
    "title": "Partie 2: visualiser les donn√©es",
    "section": "L‚Äô√©cosyst√®me Python",
    "text": "L‚Äô√©cosyst√®me Python\nL‚Äô√©cosyst√®me Python pour la valorisation de donn√©es est tr√®s riche et\ntr√®s √©clat√©.\nIl est\npossible de consacrer des livres entiers √† celui-ci (Dale 2022).\nPython propose\nde nombreuses librairies pour produire de mani√®re rapide et relativement\nsimple des visualisations de donn√©es1.\nLes librairies graphiques se distinguent principalement en deux familles:\n\nLes librairies de repr√©sentations fig√©es. Celles-ci ont plut√¥t vocation √† √™tre int√©gr√©es\ndans des publications fig√©es type PDF ou documents texte. Nous pr√©senterons\nprincipalement Matplotlib et Seaborn mais il en existe d‚Äôautres,\ncomme Plotnine.\nLes librairies de repr√©sentations dynamiques. Celles-ci sont adapt√©es √† des repr√©sentations\nweb et offrent la possibilit√© aux lecteurs d‚Äôagir sur la repr√©sentation graphique affich√©e.\nLes librairies qui proposent ces fonctionnalit√©s reposent g√©n√©ralement sur JavaScript, l‚Äô√©cosyst√®me\ndu d√©veloppement web, pour lequel elles offrent un point d‚Äôentr√©e via Python.\nNous √©voquerons principalement Plotly et Folium dans cette famille mais il existe de nombreux\nautres frameworks dans ce domaine2.\n\nDans le domaine de la visualisation, ce cours adopte le parti pris\nd‚Äôexplorer quelques\nlibrairies centrales √† partir d‚Äôun nombre restreint d‚Äôexemples en\nr√©pliquant des graphiques qu‚Äôon peut trouver sur le site d‚Äôopen data de la\nmairie de Paris.\nLa meilleure √©cole pour la visualisation est la pratique sur des jeux de donn√©es.\n\nLes applications de visualisation\nCette partie du cours se focalise sur des repr√©sentations synth√©tiques simples.\nElle n‚Äô√©voque pas (encore ?) la construction d‚Äôapplications de visualisation\nde donn√©es o√π un ensemble de graphiques se mettent √† jour de mani√®re synchrone\nen fonction d‚Äôactions d‚Äôutilisateurs.\nCeci d√©passe en effet le cadre d‚Äôun cours d‚Äôintroduction car cela implique\nde ma√Ætriser des concepts plus complexes comme l‚Äôinteraction entre une page\nweb et un serveur (local). N√©anmoins, j‚Äôai d√©j√† construit\navec Romain Avouac\nun tutoriel 101 tr√®s d√©taill√© sur Streamlit\n(permettant de cr√©er une application type Yuka)\npour une formation √† l‚ÄôInsee."
  },
  {
    "objectID": "content/visualisation/index.html#r√©sum√©-de-cette-partie",
    "href": "content/visualisation/index.html#r√©sum√©-de-cette-partie",
    "title": "Partie 2: visualiser les donn√©es",
    "section": "R√©sum√© de cette partie",
    "text": "R√©sum√© de cette partie\nCette partie est divis√©e en deux et chaque chapitre est lui-m√™me\ndual, selon qu‚Äôon s‚Äôint√©resse aux repr√©sentations fig√©es\nou dynamiques :\n\nDans un premier temps, nous √©voquerons des\nrepr√©sentations graphiques standards (histogrammes, diagrammes\nen barre‚Ä¶) pour synth√©tiser certaines informations quantitatives ;\n\nLes repr√©sentations fixes reposeront sur Pandas, Matplotlib et Seaborn\nLes graphiques r√©actifs s‚Äôappuieront sur Plotly\n\nDans un deuxi√®me temps, nous pr√©senterons les repr√©sentations\ncartographiques:\n\nLes cartes fixes avec Geopandas ou Geoplot\nLes cartes r√©actives avec Folium (adaptation Python de la librairie Leaflet.js)"
  },
  {
    "objectID": "content/visualisation/index.html#r√©f√©rences-utiles",
    "href": "content/visualisation/index.html#r√©f√©rences-utiles",
    "title": "Partie 2: visualiser les donn√©es",
    "section": "R√©f√©rences utiles",
    "text": "R√©f√©rences utiles\nLa visualisation de donn√©es est un art qui s‚Äôapprend, au d√©but, principalement\npar la pratique. N√©anmoins, il n‚Äôest pas √©vident de produire\ndes visualisations lisibles et ergonomiques\net il est utile de s‚Äôinspirer d‚Äôexemples de\nsp√©cialistes (les grands titres de presse disposent d‚Äôexcellentes visualisations).\nVoici quelques ressources utiles sur ces sujets :\n\nDatawrapper propose un excellent blog sur les\nbonnes pratiques de visualisation, notamment\navec les articles de Lisa Charlotte Muth. Je recommande notamment cet article sur\nles couleurs ou\ncelui-ci sur les textes ;\nLe blog d‚ÄôEric Mauvi√®re ;\n‚ÄúLa S√©miologie graphique de Jacques Bertin a cinquante ans‚Äù ;\nLes visualisations trending sur Observable ;\nLe New York Times (les rois de la dataviz) revient tous les ans sur les meilleures visualisations\nde l‚Äôann√©e dans la veine du data scrollytelling. Voir par exemple la r√©trospective de l‚Äôann√©e 2022.\n\nEt quelques r√©f√©rences suppl√©mentaires, cit√©es dans cette introduction :\n\n\nBertin, Jacques. 1967. S√©miologie Graphique. Paris: Mouton/Gauthier-Villars.\n\n\nDale, Kyran. 2022. Data Visualization with Python and JavaScript. \" O‚ÄôReilly Media, Inc.\".\n\n\nInsee. 2018. ‚ÄúGuide de S√©miologie Cartographique.‚Äù\n\n\nPalsky, Gilles. 2017. ‚ÄúLa s√©miologie Graphique de Jacques Bertin a Cinquante Ans.‚Äù Visions Carto (En Ligne).\n\n\nWilke, Claus O. 2019. Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures. O‚ÄôReilly Media."
  },
  {
    "objectID": "content/visualisation/index.html#footnotes",
    "href": "content/visualisation/index.html#footnotes",
    "title": "Partie 2: visualiser les donn√©es",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPour √™tre honn√™te, Python est sur ce point un peu moins agr√©able\nque R qui b√©n√©ficie de\nl‚Äôincontournable librairie ggplot2.\nN‚Äô√©tant pas\nconstruite sur la grammaire des graphiques,\nla principe librairie de graphiques en Python qu‚Äôest Matplotlib est plus fastidieuse\n√† utiliser que ggplot2.\nseaborn, que nous pr√©senterons,\nfacilite un peu le travail de repr√©sentation graphique mais, l√† encore, il est difficile de faire\nplus mall√©able et universel que ggplot2.\nLa librairie plotnine vise √† proposer une impl√©mentation similaire\n√† ggplot pour les utilisateurs de Python. Son d√©veloppement est √† suivre.‚Ü©Ô∏é\nA cet √©gard, je recommande vivement de suivre l‚Äôactualit√© de la dataviz\nsur la plateforme Observable qui tend √†\nrapprocher les communaut√©s des sp√©cialistes de la dataviz et des analystes\nde donn√©es. La librairie Plot pourrait devenir\nun nouveau standard dans les prochaines ann√©es, sorte d‚Äôinterm√©diaire\nentre ggplot et d3.‚Ü©Ô∏é"
  },
  {
    "objectID": "content/visualisation/maps.html",
    "href": "content/visualisation/maps.html",
    "title": "De belles cartes avec python : mise en pratique",
    "section": "",
    "text": "La pratique de la cartographie se fera, dans ce cours, en r√©pliquant des cartes qu‚Äôon peut trouver sur\nla page de l‚Äôopen-data de la ville de Paris\nici.\nNote\nProduire de belles cartes demande du temps mais aussi du bon sens. En fonction de la structure des donn√©es, certaines repr√©sentations sont √† √©viter voire √† exclure. L‚Äôexcellent guide disponible ici propose quelques r√®gles et √©voque les erreurs √† √©viter lorsqu‚Äôon d√©sire effectuer des\nrepr√©sentations spatiales.\nCelui-ci reprend un guide de s√©miologie cartographique\nproduit par l‚ÄôInsee qui propose de nombreux conseils pratiques pour produire des repr√©sentations\ncartographiques sens√©es.\nCe TP vise √† initier :\nLes donn√©es utilis√©es sont :\nAttention\nCertaines librairies g√©ographiques d√©pendent de rtree qui est parfois difficile √† installer car\nce package d√©pend de librairies compil√©es qui sont compliqu√©es √† installer sur Windows.\nPour installer rtree sur Windows, le mieux est d‚Äôutiliser Anaconda.\nAvant de pouvoir commencer, il est n√©cessaire d‚Äôinstaller quelques\npackages au pr√©alable:\n# Sur colab\n!pip install pandas fiona shapely pyproj rtree # √† faire obligatoirement en premier pour utiliser rtree ou pygeos pour les jointures spatiales\n!pip install contextily\n!pip install geopandas\n!pip install geoplot\nDans la premi√®re partie, nous allons utiliser les packages suivants :\nimport pandas as pd\nimport geopandas as gpd\nimport contextily as ctx\nimport geoplot\nimport matplotlib.pyplot as plt\nimport folium\n\n/opt/mamba/lib/python3.9/site-packages/geopandas/_compat.py:123: UserWarning:\n\nThe Shapely GEOS version (3.12.0-CAPI-1.18.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n\n/tmp/ipykernel_3997/3556787358.py:2: UserWarning:\n\nShapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n\nimport os\nos.environ['USE_PYGEOS'] = '0'\nimport geopandas\n\nIn a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html)."
  },
  {
    "objectID": "content/visualisation/maps.html#premi√®re-carte-avec-lapi-matplotlib-de-geopandas",
    "href": "content/visualisation/maps.html#premi√®re-carte-avec-lapi-matplotlib-de-geopandas",
    "title": "De belles cartes avec python : mise en pratique",
    "section": "Premi√®re carte avec l‚ÄôAPI matplotlib de geopandas",
    "text": "Premi√®re carte avec l‚ÄôAPI matplotlib de geopandas\n\n\n Exercice 1: Importer les donn√©es\nImporter les donn√©es de compteurs de v√©los en deux temps.\n\nD‚Äôabord, les comptages peuvent √™tre trouv√©s √† l‚Äôadresse https://minio.lab.sspcloud.fr/projet-formation/diffusion/python-datascientist/bike.csv. ‚ö†Ô∏è Il s‚Äôagit de donn√©es\ncompress√©es au format gzip, il faut donc utiliser l‚Äôoption compression. Nommer cet objet comptages.\nImporter les donn√©es de localisation des compteurs √† partir de l‚Äôurl https://parisdata.opendatasoft.com/api/explore/v2.1/catalog/datasets/comptage-velo-compteurs/exports/geojson?lang=fr&timezone=Europe%2FBerlin. Nommer cet objet compteurs.\nFaire attention √† deux valeurs aberrantes. Utiliser\nla fonctionalit√© str.contains pour exclure les\nobservations contenant ‚ÄúBike IN‚Äù ou ‚ÄúBike OUT‚Äù\ndans la variable\nnom_compteur\nOn va √©galement utiliser les donn√©es d‚Äôarrondissements de la ville de Paris. Importer ces donn√©es depuis https://opendata.paris.fr/explore/dataset/arrondissements/download/?format=geojson&timezone=Europe/Berlin&lang=fr. Nommer cet objet arrondissements.\nUtiliser la m√©thode plot pour repr√©senter les localisations des compteurs dans l‚Äôespace. C‚Äôest, on peut l‚Äôavouer, peu informatif sans apport ext√©rieur. Il va donc falloir travailler un peu l‚Äôesth√©tique\n\n\n\n\ncompteurs = compteurs.loc[~compteurs[\"nom_compteur\"].str.contains(r\"(Bike IN|Bike OUT)\")]\n\n/tmp/ipykernel_3997/3602001210.py:1: UserWarning:\n\nThis pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n\n\n\n\n\n Warning\nOn serait tent√© de faire un merge de la base compteurs et comptages.\nEn l‚Äôoccurrence, il s‚Äôagirait d‚Äôun produit cart√©sien puisqu‚Äôil s‚Äôagit de faire exploser la base spatiale.\nAvec des donn√©es spatiales, c‚Äôest souvent une tr√®s mauvaise id√©e. Cela duplique les points, cr√©ant des difficult√©s √† repr√©senter les donn√©es mais aussi ralentit les calculs.\nSauf √† utiliser la m√©thode dissolve (qui va agr√©ger k fois la m√™me g√©om√©trie‚Ä¶), les g√©om√©tries sont perdues lorsqu‚Äôon effectue des groupby.\n\n\nMaintenant, tout est pr√™t pour une premi√®re carte. matplotlib fonctionne selon\nle principe des couches. On va de la couche la plus lointaine √† celle le plus\nen surface. L‚Äôexception est lorsqu‚Äôon ajoute un fond de carte contextily via\nctx.add_basemap: on met cet appel en dernier.\n\n\n Exercice 2: Premi√®re carte\nRepr√©senter une carte des compteurs avec le fonds de carte des arrondissements\n\nFaire attention √† avoir des arrondissements dont l‚Äôint√©rieur est transparent (argument √† utiliser: facecolor).\nFaire des bordures d‚Äôarrondissements noires et affichez les compteurs en rouge.\nPour obtenir un graphique plus grand, vous pouvez utiliser l‚Äôargument figsize = (10,10).\nPour les localisations, les points doivent √™tre rouges en √©tant plus transparent au centre (argument √† utiliser: alpha)\n\n\n\nVous devriez obtenir cette carte:\n\n\n\n\n\n\n\n\n\n\n\n Exercice 3 : Ajouter un fonds de carte avec contextily\nRepartir de la carte pr√©c√©dente.\n\nUtiliser ctx.add_basemap pour ajouter un fonds de carte. Pour ne pas afficher les axes, vous pouvez utiliser ax.set_axis_off().\n\n:warning: Par d√©faut, contextily d√©sire un syst√®me de projection (crs) qui est le Web Mercator (epsg: 3857). Il faut changer la valeur de l‚Äôargument crs.\n:warning: Avec les versions anciennes des packages, il faut utiliser .to_string sur un objet CRS pour qu‚Äôil soit reconnu par contextily. Sur des versions r√©centes, la valeur num√©rique du code EPSG est suffisante.\n\nTrouver un fonds de carte plus esth√©tique, qui permette de visualiser les grands axes, parmi ceux possibles. Pour tester l‚Äôesth√©tique, vous pouvez utiliser cet url. La documentation de r√©f√©rence sur les tuiles disponibles est ici\n\n\n\n\nax.get_figure()\n\n\n\n\n\n\n\n\n\nax.get_figure()\n\n\n\n\n\n\n\n\n\nax.get_figure().savefig(\"featured_maps.png\")\n\nLe principe de la heatmap est de construire, √† partir d‚Äôun nuage de point bidimensionnel, une distribution 2D liss√©e. La m√©thode repose sur les estimateurs √† noyaux qui sont des m√©thodes de lissage local.\n\n\n Exercice 3 : Ajouter un fonds de carte avec contextily\nPour le moment, la fonction geoplot.kdeplot n‚Äôincorpore pas toutes les fonctionalit√©s de seaborn.kdeplot. Pour √™tre en mesure de construire une heatmap avec des donn√©es pond√©r√©es (cf.¬†cette issue dans le d√©p√¥t seaborn), il y a une astuce. Il faut simuler k points de valeur 1 autour de la localisation observ√©e. La fonction ci-dessous, qui m‚Äôa √©t√© bien utile, est pratique\n\nimport numpy as np\ndef expand_points(shapefile,\n                  index_var = \"grid_id\",\n                  weight_var = 'prop',\n                  radius_sd = 100,\n                  crs = 2154):\n    \"\"\"\n    Multiply number of points to be able to have a weighted heatmap\n    :param shapefile: Shapefile to consider\n    :param index_var: Variable name to set index\n    :param weight_var: Variable that should be used\n    :param radius_sd: Standard deviation for the radius of the jitter\n    :param crs: Projection system that should be used. Recommended option\n      is Lambert 93 because points will be jitterized using meters\n    :return:\n      A geopandas point object with as many points by index as weight\n    \"\"\"\n\n    shpcopy = shapefile\n    shpcopy = shpcopy.set_index(index_var)\n    shpcopy['npoints'] = np.ceil(shpcopy[weight_var])\n    shpcopy['geometry'] = shpcopy['geometry'].centroid\n    shpcopy['x'] = shpcopy.geometry.x\n    shpcopy['y'] = shpcopy.geometry.y\n    shpcopy = shpcopy.to_crs(crs)\n    shpcopy = shpcopy.loc[np.repeat(shpcopy.index.values, shpcopy.npoints)]\n    shpcopy['x'] = shpcopy['x'] + np.random.normal(0, radius_sd, shpcopy.shape[0])\n    shpcopy['y'] = shpcopy['y'] + np.random.normal(0, radius_sd, shpcopy.shape[0])\n\n    gdf = gpd.GeoDataFrame(\n        shpcopy,\n        geometry = gpd.points_from_xy(shpcopy.x, shpcopy.y),\n        crs = crs)\n\n    return gdf\n\n\n\n\n\n Exercice 4 : Data cleaning avant de pouvoir faire une heatmap\n\nCalculer le trafic moyen, pour chaque station, entre 7 heures et 10 heures (bornes incluses) et nommer cet objet df1. Faire la m√™me chose, en nommant df2, pour le trafic entre 17 et 20 heures (bornes incluses)\nNous allons d√©sormais pr√©parer les donn√©es de mani√®re √† faire une heatmap. Apr√®s avoir compris ce que permet de faire la fonction expand_points ci-dessus, cr√©er une fonction explode_data qui suive les √©tapes suivantes.\n\n\nConvertir un DataFrame dans le syst√®me de projection Lambert 93 (epsg: 2154)\nAppliquer expand_points aux noms de variable ad√©quats. Vous pouvez fixer la valeur de radius_sd √† 100.\nReconvertir l‚Äôoutput au format WGS84 (epsg: 4326)\n\n\nAppliquer cette fonction √† df1 et df2\n\n\n\n\n\n Exercice 5 : Heatmap, enfin !\nRepr√©senter, pour ces deux moments de la journ√©e, la heatmap du trafic de v√©lo avec geoplot.kdeplot. Pour cela :\n\nAppliquer la fonction geoplot.kdeplot avec comme consignes :\n\nd‚Äôutiliser les arguments shade=True et shade_lowest=True pour colorer l‚Äôint√©rieur des courbes de niveaux obtenues ;\nd‚Äôutiliser une palette de couleur rouge avec une transparence mod√©r√©e (alpha = 0.6)\nd‚Äôutiliser l‚Äôargument clip pour ne pas d√©border hors de Paris (en cas de doute, se r√©f√©rer √† l‚Äôaide de geoplot.kdeplot)\nL‚Äôargument bw (pour bandwidth) d√©termine le plus ou moins fort lissage spatial. Vous pouvez partir d‚Äôun bandwidth √©gal √† 0.01 et le faire varier pour voir l‚Äôeffet sur le r√©sultat\n\nNe pas oublier d‚Äôajouter les arrondissements. Avec geoplot, il faut utiliser geoplot.polyplot.\n\n\n\n\nax.get_figure()"
  },
  {
    "objectID": "content/visualisation/maps.html#des-cartes-r√©actives-gr√¢ce-√†-folium",
    "href": "content/visualisation/maps.html#des-cartes-r√©actives-gr√¢ce-√†-folium",
    "title": "De belles cartes avec python : mise en pratique",
    "section": "Des cartes r√©actives gr√¢ce √† folium",
    "text": "Des cartes r√©actives gr√¢ce √† folium\nDe plus en plus de donn√©es de visualisation reposent sur la cartographie r√©active. Que ce soit dans l‚Äôexploration des donn√©es ou dans la repr√©sentation finale de r√©sultats, la cartographie r√©active est tr√®s appr√©ciable.\nfolium offre une interface tr√®s flexible et tr√®s facile √† prendre √† main. Les cartes sont construites gr√¢ce √† la librairie JavaScript Leaflet.js mais, sauf si on d√©sire aller loin dans la customisation du r√©sultat, il n‚Äôest pas n√©cessaire d‚Äôavoir des notions dans le domaine.\nUn objet folium se construit par couche. La premi√®re est l‚Äôinitialisation de la carte. Les couches suivantes sont les √©l√©ments √† mettre en valeur. L‚Äôinitialisation de la carte n√©cessite la d√©finition d‚Äôun point central (param√®tre location) et d‚Äôun zoom de d√©part (zoom_start). Plut√¥t que de fournir manuellement le point central et le zoom on peut :\n\nD√©terminer le point central en construisant des colonnes longitudes et latitudes et en prenant la moyenne de celles-ci ;\nUtiliser la m√©thode fit_bounds qui cale la carte sur les coins sud-ouest et nord-est. En supposant que la carte s‚Äôappelle m, on fera m.fit_bounds([sw, ne])\n\nLe bout de code suivant permet de calculer le centre de la carte\n\ncompteurs['lon'] = compteurs.geometry.x\ncompteurs['lat'] = compteurs.geometry.y\ncenter = compteurs[['lat', 'lon']].mean().values.tolist()\nprint(center)\n\n[48.8546374609375, 2.349252640625]\n\n\nAlors que le code suivant permet de calculer les coins:\n\nsw = compteurs[['lat', 'lon']].min().values.tolist()\nne = compteurs[['lat', 'lon']].max().values.tolist()\nprint(sw, ne)\n\n[48.81964, 2.26526] [48.898946, 2.41143]\n\n\n\n\n Hint\nSi un fond gris s‚Äôaffiche, c‚Äôest qu‚Äôil y a un probl√®me de localisation ou d‚Äôacc√®s √† internet. Pour le premier cas, cela provient g√©n√©ralement d‚Äôun probl√®me de projection ou d‚Äôune inversion des longitudes et latitudes.\nLes longitudes repr√©sentent les x (axe ouest-est) et les latitudes y (axe sud-nord). De mani√®re contrintuitive, folium attend qu‚Äôon lui fournisse les donn√©es sous la forme [latitude, longitude] donc [y,x]\n\n\n\n\n Exercice 6 : Visualiser la localisation des stations\n\nCalculer le centre centerde la carte des donn√©es compteurs. Il s‚Äôobtient en agr√®geant l‚Äôensemble des g√©om√©tries, calculant le centroid et r√©cup√®rant la valeur sous forme de liste. Avec une logique similaire, calculez les bornes du sud-ouest sw et du nord-est ne de la carte.\nRepr√©senter la localisation des stations en utilisant un zoom optimal.\n\n\n\n\n# Afficher la carte\nm\n\n\n\n Exercice 7: Repr√©senter les stations\nFaire la m√™me carte, avec des ronds proportionnels au nombre de comptages :\n\nPour le rayon de chaque cercle, vous pouvez appliquer la r√®gle 500*x/max(x) (r√®gle au doigt mouill√©)\nVous pouvez r√©duire la taille des bordures de cercle avec l‚Äôoption weight = 1 et fixer la couleur avec color = 'grey'\n(Optionnel) Colorer en rouge les 10 plus grosses stations. L‚Äôopacit√© √©tant, par d√©faut, un peu faible, le param√®tre fill_opacity = 0.4 am√©liore le rendu.\n(Optionnel) Afficher, en suppl√©ment du nom du compteur lorsqu‚Äôon clique, la valeur du comptage en revenant √† la ligne\n\n\n\nLa carte obtenue doit ressembler √† la suivante :\n\n# Afficher la carte\nm"
  },
  {
    "objectID": "content/visualisation/maps.html#exercices-suppl√©mentaires",
    "href": "content/visualisation/maps.html#exercices-suppl√©mentaires",
    "title": "De belles cartes avec python : mise en pratique",
    "section": "Exercices suppl√©mentaires",
    "text": "Exercices suppl√©mentaires\n\nDensit√© de population dans la petite couronne parisienne\nPour cet exercice, le package cartiflette\nva √™tre pratique pour r√©cup√©rer un fonds de carte m√©langeant arrondissements\nparisiens et communes dans les autres villes.\nNous allons privil√©gier une carte √† ronds proportionnels (bubble map)\naux cartes chorol√®pthes qui trompent\nl‚Äôoeil. Les instructions d‚Äôinstallation du package topojson\nsont disponibles dans la partie manipulation\n\n\n Exercice: bubble map de densit√© des populations\n\nR√©cup√©rer le fond de carte des d√©partements 75, 92, 93 et 94\navec cartiflette. Pour cela, utiliser download_vectorfile_url_all\ndepuis cartiflette.s3 en fixant l‚Äôoption level √† COMMUNE_ARRONDISSEMENT.\nNommer cet objet df.\nAfin que les calculs ult√©rieurs de surface ne soient pas fauss√©s,\nassurez-vous que les donn√©es sont en Lambert 93 en reprojetant\nnos contours (code EPSG: 2154).\nCr√©er un objet departements avec dissolve pour √©galement disposer\nd‚Äôun fond de carte des d√©partements\nCr√©er une variable surface et utilisant la m√©thode area. L‚Äôunit√©\ndoit √™tre le km¬≤, il faut donc diviser par \\(10^6\\)\nCr√©er une variable densite\nUtiliser pd.cut avec les seuils 5000, 15000 et 30000 personnes\npar km¬≤. Vous pouvez utiliser l‚Äôoption label pour d√©nommer les tranches\nCr√©er un GeoDataFrame de points en utilisant la m√©thode centroid. Celui-ci\nnous servira √† localiser le centre de nos ronds.\nRepr√©senter la densit√© communale sous forme de carte avec ronds proportionnels.\nVous pouvez utiliser la variable cr√©√©e √† la question 5 pour les couleurs.\n\n\n\nLa carte obtenue devrait ressembler √† celle-ci:\n\n\n/opt/mamba/lib/python3.9/site-packages/geopandas/plotting.py:730: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\nText(0.3, 0.15, 'Source: IGN - AdminExpress')"
  },
  {
    "objectID": "content/visualisation/maps.html#r√©f√©rences-suppl√©mentaires",
    "href": "content/visualisation/maps.html#r√©f√©rences-suppl√©mentaires",
    "title": "De belles cartes avec python : mise en pratique",
    "section": "R√©f√©rences suppl√©mentaires",
    "text": "R√©f√©rences suppl√©mentaires\n\nGeocomputation with Python"
  },
  {
    "objectID": "content/modelisation/0_preprocessing.html",
    "href": "content/modelisation/0_preprocessing.html",
    "title": "Pr√©paration des donn√©es pour construire un mod√®le",
    "section": "",
    "text": "Ce chapitre utilise le jeu de donn√©es pr√©sent√© dans l‚Äôintroduction\nde cette partie :\nles donn√©es de vote aux √©lections pr√©sidentielles am√©ricaines de 2020 au niveau des comt√©s\ncrois√©es √† des variables socio-d√©mographiques.\nLe code de consitution de la base de donn√©es\nest disponible sur Github.\nL‚Äôexercice 1 permet, √† ceux qui le d√©sirent, d‚Äôessayer de le reconstituer pas √† pas.\nLe guide utilisateur de Scikit est une r√©f√©rence pr√©cieuse,\n√† consulter r√©guli√®rement. La partie sur le preprocessing est\ndisponible ici.\nL‚Äôobjectif de ce chapitre est de pr√©senter quelques √©l√©ments de\npr√©paration des donn√©es. Il s‚Äôagit d‚Äôune √©tape fondamentale, √† ne\npas n√©gliger. Les mod√®les reposent sur certaines hypoth√®ses, g√©n√©ralement\nrelatives √† la distribution th√©orique des variables qui y sont int√©gr√©es.\nIl est n√©cessaire de faire correspondre la distribution empirique\n√† ces hypoth√®ses ce qui implique un travail de restructuration des donn√©es.\nCelui-ci permettra d‚Äôavoir des r√©sultats de mod√©lisation plus pertinents.\nNous verrons dans le chapitre sur les pipelines comment industrialiser\nces √©tapes de preprocessing afin de se simplifier la vie pour appliquer\nun mod√®le sur un jeu de donn√©es diff√©rent de celui sur lequel il a √©t√© estim√©.\nScikit-Learn \nscikit-learn est aujourd‚Äôhui la librairie de r√©f√©rence dans l‚Äô√©cosyst√®me du\nMachine Learning. Il s‚Äôagit d‚Äôune librairie qui, malgr√© les tr√®s nombreuses\nm√©thodes impl√©ment√©es, pr√©sente l‚Äôavantage d‚Äô√™tre un point d‚Äôentr√©e unifi√©.\nCet aspect unifi√© est l‚Äôune des raisons du succ√®s pr√©coce de celle-ci. R n‚Äôa\nb√©n√©fici√© que plus r√©cemment d‚Äôune librairie unifi√©e,\n√† savoir tidymodels.\nUne autre raison du succ√®s de scikit est son approche op√©rationnelle: la mise\nen production de mod√®les d√©velopp√©s via les pipelines scikit est peu co√ªteuse.\nUn chapitre sp√©cial de ce cours est d√©di√© aux pipelines.\nAvec Romain Avouac, nous proposons un cours plus avanc√©\nen derni√®re ann√©e d‚ÄôENSAE o√π nous pr√©sentons certains enjeux relatifs\n√† la mise en production de mod√®les d√©velopp√©s avec scikit.\nLe coeur de l‚Äô√©quipe de d√©veloppement de scikit-learn est situ√©\n√† l‚ÄôInria üá´üá∑.\nPour d√©couvrir la richesse de l‚Äô√©cosyst√®me scikit, il\nest recommand√© de suivre le\nMOOC scikit,\nd√©velopp√© dans le cadre de l‚Äôinitiative Inria Academy.\nLes packages suivants sont n√©cessaires pour importer et visualiser\nles donn√©es d‚Äô√©lection:\n!pip install --upgrade xlrd #colab bug verson xlrd\n!pip install geopandas\nDans ce chapitre, nous allons nous focaliser sur la pr√©paration\ndes donn√©es √† faire en amont du travail de mod√©lisation.\nCette √©tape est indispensable pour s‚Äôassurer de la coh√©rence\nentre les donn√©es et les hypoth√®ses de mod√©lisation mais aussi\npour produire des analyses valides scientifiquement.\nLa d√©marche g√©n√©rale que nous adopterons dans ce chapitre, et\nqui sera ensuite raffin√©e dans les prochains chapitres,\nest la suivante:\nC‚Äôest l‚Äôapproche classique du machine learning. On d√©coupe\nl‚Äôensemble des donn√©es disponibles en deux parties, √©chantillons\nd‚Äôapprentissage et de validation. Le premier sert √† entra√Æner\nun mod√®le et la qualit√© des pr√©dictions de celui-ci est\n√©valu√©e sur le deuxi√®me pour limiter\nle biais de surapprentissage. Le chapitre suivant approfondira\ncette question de l‚Äô√©valuation des mod√®les. A ce stade de notre\nprogression, on se concentrera dans ce chapitre\nsur la question des donn√©es. La librairie Scikit est non seulement\nparticuli√®rement\npratique parce qu‚Äôelle propose √©norm√©ment d‚Äôalgorithmes de machine learning\nmais aussi parce qu‚Äôelle facilite la pr√©paration des donn√©es en amont,\nce qui est l‚Äôobjet de ce chapitre.\nN√©anmoins, avant de se concentrer sur la pr√©paration des donn√©es, nous\nallons passer un peu de temps √† explorer la structure des donn√©es\n√† partir de laquelle nous d√©sirons construire une mod√©lisation. Ceci\nest indispensable afin de comprendre la nature de celles-ci et choisir\nune mod√©lisation ad√©quate."
  },
  {
    "objectID": "content/modelisation/0_preprocessing.html#construction-de-la-base-de-donn√©es",
    "href": "content/modelisation/0_preprocessing.html#construction-de-la-base-de-donn√©es",
    "title": "Pr√©paration des donn√©es pour construire un mod√®le",
    "section": "Construction de la base de donn√©es",
    "text": "Construction de la base de donn√©es\nLes sources de donn√©es √©tant diverses, le code qui construit la base finale est directement fourni.\nLe travail de construction d‚Äôune base unique\nest un peu fastidieux mais il s‚Äôagit d‚Äôun bon exercice, que vous pouvez tenter,\npour r√©viser Pandas :\n\n\n Exercice 1 : Importer les donn√©es des √©lections US\nCet exercice est OPTIONNEL\n\nT√©l√©charger et importer le shapefile depuis ce lien\nExclure les Etats suivants : ‚Äú02‚Äù, ‚Äú69‚Äù, ‚Äú66‚Äù, ‚Äú78‚Äù, ‚Äú60‚Äù, ‚Äú72‚Äù, ‚Äú15‚Äù\nImporter les r√©sultats des √©lections depuis ce lien\nImporter les bases disponibles sur le site de l‚ÄôUSDA en faisant attention √† renommer les variables de code FIPS de mani√®re identique\ndans les 4 bases\nMerger ces 4 bases dans une base unique de caract√©ristiques socio√©conomiques\nMerger aux donn√©es √©lectorales √† partir du code FIPS\nMerger au shapefile √† partir du code FIPS. Faire attention aux 0 √† gauche dans certains codes. Il est\nrecommand√© d‚Äôutiliser la m√©thode str.lstrip pour les retirer\nImporter les donn√©es des √©lections 2000 √† 2016 √† partir du MIT Election Lab?\nLes donn√©es peuvent √™tre directement requ√™t√©es depuis l‚Äôurl\nhttps://dataverse.harvard.edu/api/access/datafile/3641280?gbrecs=false\nCr√©er une variable share comptabilisant la part des votes pour chaque candidat.\nNe garder que les colonnes \"year\", \"FIPS\", \"party\", \"candidatevotes\", \"share\"\nFaire une conversion long to wide avec la m√©thode pivot_table pour garder une ligne\npar comt√© x ann√©e avec en colonnes les r√©sultats de chaque candidat dans cet √©tat.\nMerger √† partir du code FIPS au reste de la base.\n\n\n\nSi vous ne faites pas l‚Äôexercice 1, pensez √† charger les donn√©es en executant la fonction get_data.py :\n\nimport requests\n\nurl = 'https://raw.githubusercontent.com/linogaliana/python-datascientist/master/content/modelisation/get_data.py'\nr = requests.get(url, allow_redirects=True)\nopen('getdata.py', 'wb').write(r.content)\n\nimport getdata\nvotes = getdata.create_votes_dataframes()\n\nCe code introduit une base nomm√©e votes dans l‚Äôenvironnement. Il s‚Äôagit d‚Äôune\nbase rassemblant les diff√©rentes sources. Elle a l‚Äôaspect\nsuivant:\n\nvotes.head(3)\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nCOUNTYNS\nAFFGEOID\nGEOID\nNAME\nLSAD\nALAND\nAWATER\ngeometry\n...\nshare_2008_democrat\nshare_2008_other\nshare_2008_republican\nshare_2012_democrat\nshare_2012_other\nshare_2012_republican\nshare_2016_democrat\nshare_2016_other\nshare_2016_republican\nwinner\n\n\n\n\n0\n29\n227\n00758566\n0500000US29227\n29227\nWorth\n06\n690564983\n493903\nPOLYGON ((-94.63203 40.57176, -94.53388 40.570...\n...\n0.363714\n0.034072\n0.602215\n0.325382\n0.041031\n0.633588\n0.186424\n0.041109\n0.772467\nrepublican\n\n\n1\n31\n061\n00835852\n0500000US31061\n31061\nFranklin\n06\n1491355860\n487899\nPOLYGON ((-99.17940 40.35068, -98.72683 40.350...\n...\n0.284794\n0.019974\n0.695232\n0.250000\n0.026042\n0.723958\n0.149432\n0.045427\n0.805140\nrepublican\n\n\n2\n36\n013\n00974105\n0500000US36013\n36013\nChautauqua\n06\n2746047476\n1139407865\nPOLYGON ((-79.76195 42.26986, -79.62748 42.324...\n...\n0.495627\n0.018104\n0.486269\n0.425017\n0.115852\n0.459131\n0.352012\n0.065439\n0.582550\nrepublican\n\n\n\n\n3 rows √ó 383 columns\n\n\n\nLa carte choropl√®the suivante permet de visualiser rapidement les r√©sultats\n(l‚ÄôAlaska et Hawa√Ø ont √©t√© exclus).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# republican : red, democrat : blue\ncolor_dict = {'republican': '#FF0000', 'democrats': '#0000FF'}\n\nfig, ax = plt.subplots(figsize = (12,12))\ngrouped = votes.groupby('winner')\nfor key, group in grouped:\n    group.plot(ax=ax, column='winner', label=key, color=color_dict[key])\nplt.axis('off')\n\n(-127.6146362, -64.0610978, 23.253819649999997, 50.628669349999996)\n\n\n\n\n\n\n\n\n\nLes cartes choropl√®thes peuvent donner une impression fallacieuse\nce qui explique que\nce type de carte a servi\nde justification pour contester les r√©sultats du vote.\nEn effet, un biais\nconnu des repr√©sentations choropl√®thes est qu‚Äôelles donnent une importance\nvisuelle excessive aux grands espaces. Or, ceux-ci sont souvent des espaces\npeu denses et influencent donc moins la variable d‚Äôint√©r√™t (en l‚Äôoccurrence\nle taux de vote en faveur des r√©publicains/d√©mocrates). Une repr√©sentation √†\nprivil√©gier pour ce type de ph√©nom√®nes est les\nronds proportionnels (voir Insee (2018), ‚ÄúLe pi√®ge territorial en cartographie‚Äù).\nLe GIF ‚ÄúLand does not vote, people do‚Äù\nqui avait eu un certain succ√®s en 2020 propose un autre mode de visualisation.\nLa carte originale a √©t√© construite avec JavaScript. Cependant,\non dispose avec Python de plusieurs outils\npour r√©pliquer, √† faible co√ªt, cette carte\ngr√¢ce √†\nl‚Äôune des surcouches √† JavaScript vues dans la partie visualisation.\nEn l‚Äôoccurrence, on peut utiliser Plotly pour tenir compte de la population\net faire une carte en ronds proportionnels.\nLe code suivant permet de construire une carte adapt√©e:\n\nimport plotly\nimport plotly.graph_objects as go\nimport pandas as pd\nimport geopandas as gpd\n\n\ncentroids = votes.copy()\ncentroids.geometry = centroids.centroid\ncentroids['size'] = centroids['CENSUS_2010_POP'] / 10000  # to get reasonable plotable number\n\ncolor_dict = {\"republican\": '#FF0000', 'democrats': '#0000FF'}\ncentroids[\"winner\"] =  np.where(centroids['votes_gop'] &gt; centroids['votes_dem'], 'republican', 'democrats') \n\n\ncentroids['lon'] = centroids['geometry'].x\ncentroids['lat'] = centroids['geometry'].y\ncentroids = pd.DataFrame(centroids[[\"county_name\",'lon','lat','winner', 'CENSUS_2010_POP',\"state_name\"]])\ngroups = centroids.groupby('winner')\n\ndf = centroids.copy()\n\ndf['color'] = df['winner'].replace(color_dict)\ndf['size'] = df['CENSUS_2010_POP']/6000\ndf['text'] = df['CENSUS_2010_POP'].astype(int).apply(lambda x: '&lt;br&gt;Population: {:,} people'.format(x))\ndf['hover'] = df['county_name'].astype(str) +  df['state_name'].apply(lambda x: ' ({}) '.format(x)) + df['text']\n\nfig_plotly = go.Figure(\n  data=go.Scattergeo(\n  locationmode = 'USA-states',\n  lon=df[\"lon\"], lat=df[\"lat\"],\n  text = df[\"hover\"],\n  mode = 'markers',\n  marker_color = df[\"color\"],\n  marker_size = df['size'],\n  hoverinfo=\"text\"\n  )\n)\n\nfig_plotly.update_traces(\n  marker = {'opacity': 0.5, 'line_color': 'rgb(40,40,40)', 'line_width': 0.5, 'sizemode': 'area'}\n)\n\nfig_plotly.update_layout(\n  title_text = \"Reproduction of the \\\"Acres don't vote, people do\\\" map &lt;br&gt;(Click legend to toggle traces)\",\n  showlegend = True,\n  geo = {\"scope\": 'usa', \"landcolor\": 'rgb(217, 217, 217)'}\n)\n\n\nfig_plotly.show()\n\n\n                                                \n\n\nLes cercles proportionnels permettent ainsi √† l‚Äôoeil de se concentrer sur les\nzones les plus denses et non sur les grands espaces. Cette fois, on voit bien\nque le vote d√©mocrate est majoritaire, ce que cachait l‚Äôaplat de couleur."
  },
  {
    "objectID": "content/modelisation/0_preprocessing.html#explorer-la-structure-des-donn√©es",
    "href": "content/modelisation/0_preprocessing.html#explorer-la-structure-des-donn√©es",
    "title": "Pr√©paration des donn√©es pour construire un mod√®le",
    "section": "Explorer la structure des donn√©es",
    "text": "Explorer la structure des donn√©es\nLa premi√®re √©tape n√©cessaire √† suivre avant de se lancer dans la mod√©lisation\nest de d√©terminer les variables √† inclure dans le mod√®le.\nLes fonctionnalit√©s de Pandas sont, √† ce niveau, suffisantes pour explorer des structures simples.\nN√©anmoins, lorsqu‚Äôon est face √† un jeu de donn√©es pr√©sentant de\nnombreuses variables explicatives (features en machine learning, covariates en √©conom√©trie),\nil est souvent judicieux d‚Äôavoir une premi√®re √©tape de s√©lection de variables,\nce que nous verrons par la suite dans la partie d√©di√©e.\nAvant d‚Äô√™tre en mesure de s√©lectionner le meilleur ensemble de variables explicatives,\nnous allons en prendre un nombre restreint et arbitraire.\nLa premi√®re t√¢che est de repr√©senter les relations entre les donn√©es,\nnotamment la relation des variables explicatives\n√† la variable d√©pendante (le score du parti r√©publicain)\nainsi que les relations entre les variables explicatives.\n\n\n Exercice 2 : Regarder les corr√©lations entre les variables\n\nCr√©er un DataFrame df2 plus petit avec les variables winner, votes_gop, Unemployment_rate_2019,\nMedian_Household_Income_2019,\nPercent of adults with less than a high school diploma, 2015-19,\nPercent of adults with a bachelor's degree or higher, 2015-19\nRepr√©senter gr√¢ce √† un graphique la matrice de corr√©lation. Vous pouvez utiliser le package seaborn et sa fonction seaborn.\nRepr√©senter une matrice de nuages de points des variables de la base df2 avec pd.plotting.scatter_matrix\n(optionnel) Refaire ces figures avec Plotly qui offre √©galement la possibilit√© de faire une matrice de corr√©lation.\n\n\n\nLa matrice construite avec seaborn (question 2) aura l‚Äôaspect suivant :\n\n\n&lt;Axes: &gt;\n\n\nAlors que celle construite directement avec corr de Pandas\nressemblera plut√¥t √† ce tableau :\n\n\n\n\n\n\n\n¬†\nvotes_gop\nUnemployment_rate_2019\nMedian_Household_Income_2019\nPercent of adults with less than a high school diploma, 2015-19\nPercent of adults with a bachelor's degree or higher, 2015-19\n\n\n\n\nvotes_gop\n1.00\n-0.08\n0.35\n-0.11\n0.37\n\n\nUnemployment_rate_2019\n-0.08\n1.00\n-0.43\n0.36\n-0.36\n\n\nMedian_Household_Income_2019\n0.35\n-0.43\n1.00\n-0.51\n0.71\n\n\nPercent of adults with less than a high school diploma, 2015-19\n-0.11\n0.36\n-0.51\n1.00\n-0.59\n\n\nPercent of adults with a bachelor's degree or higher, 2015-19\n0.37\n-0.36\n0.71\n-0.59\n1.00\n\n\n\n\n\nLe nuage de point obtenu √† l‚Äôissue de la question 3 ressemblera √† :\n\n\n\n\n\n\n\n\n\n\n\narray([[&lt;Axes: xlabel='votes_gop', ylabel='votes_gop'&gt;,\n        &lt;Axes: xlabel='Unemployment_rate_2019', ylabel='votes_gop'&gt;,\n        &lt;Axes: xlabel='Median_Household_Income_2019', ylabel='votes_gop'&gt;,\n        &lt;Axes: xlabel='Percent of adults with less than a high school diploma, 2015-19', ylabel='votes_gop'&gt;,\n        &lt;Axes: xlabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\", ylabel='votes_gop'&gt;],\n       [&lt;Axes: xlabel='votes_gop', ylabel='Unemployment_rate_2019'&gt;,\n        &lt;Axes: xlabel='Unemployment_rate_2019', ylabel='Unemployment_rate_2019'&gt;,\n        &lt;Axes: xlabel='Median_Household_Income_2019', ylabel='Unemployment_rate_2019'&gt;,\n        &lt;Axes: xlabel='Percent of adults with less than a high school diploma, 2015-19', ylabel='Unemployment_rate_2019'&gt;,\n        &lt;Axes: xlabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\", ylabel='Unemployment_rate_2019'&gt;],\n       [&lt;Axes: xlabel='votes_gop', ylabel='Median_Household_Income_2019'&gt;,\n        &lt;Axes: xlabel='Unemployment_rate_2019', ylabel='Median_Household_Income_2019'&gt;,\n        &lt;Axes: xlabel='Median_Household_Income_2019', ylabel='Median_Household_Income_2019'&gt;,\n        &lt;Axes: xlabel='Percent of adults with less than a high school diploma, 2015-19', ylabel='Median_Household_Income_2019'&gt;,\n        &lt;Axes: xlabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\", ylabel='Median_Household_Income_2019'&gt;],\n       [&lt;Axes: xlabel='votes_gop', ylabel='Percent of adults with less than a high school diploma, 2015-19'&gt;,\n        &lt;Axes: xlabel='Unemployment_rate_2019', ylabel='Percent of adults with less than a high school diploma, 2015-19'&gt;,\n        &lt;Axes: xlabel='Median_Household_Income_2019', ylabel='Percent of adults with less than a high school diploma, 2015-19'&gt;,\n        &lt;Axes: xlabel='Percent of adults with less than a high school diploma, 2015-19', ylabel='Percent of adults with less than a high school diploma, 2015-19'&gt;,\n        &lt;Axes: xlabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\", ylabel='Percent of adults with less than a high school diploma, 2015-19'&gt;],\n       [&lt;Axes: xlabel='votes_gop', ylabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\"&gt;,\n        &lt;Axes: xlabel='Unemployment_rate_2019', ylabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\"&gt;,\n        &lt;Axes: xlabel='Median_Household_Income_2019', ylabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\"&gt;,\n        &lt;Axes: xlabel='Percent of adults with less than a high school diploma, 2015-19', ylabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\"&gt;,\n        &lt;Axes: xlabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\", ylabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\"&gt;]],\n      dtype=object)\n\n\nLe r√©sultat de la question 4 devrait, quant √† lui,\nressembler au graphique suivant :"
  },
  {
    "objectID": "content/modelisation/0_preprocessing.html#transformer-les-donn√©es",
    "href": "content/modelisation/0_preprocessing.html#transformer-les-donn√©es",
    "title": "Pr√©paration des donn√©es pour construire un mod√®le",
    "section": "Transformer les donn√©es",
    "text": "Transformer les donn√©es\nLes diff√©rences d‚Äô√©chelle ou de distribution entre les variables peuvent\ndiverger des hypoth√®ses sous-jacentes dans les mod√®les.\nPar exemple, dans le cadre\nde la r√©gression lin√©aire, les variables cat√©gorielles ne sont pas trait√©es √† la m√™me\nenseigne que les variables ayant valeur dans \\(\\mathbb{R}\\). Une variable\ndiscr√®te (prenant un nombre fini de valeurs) devra √™tre transform√©es en suite de\nvariables 0/1 par rapport √† une modalit√© de r√©f√©rence pour √™tre en ad√©quation\navec les hypoth√®ses de la r√©gression lin√©aire.\nOn appelle ce type de transformation\none-hot encoding, sur lequel nous reviendrons. Il s‚Äôagit d‚Äôune transformation,\nparmi d‚Äôautres, disponibles dans scikit pour mettre en ad√©quation un jeu de\ndonn√©es et des hypoth√®ses math√©matiques.\nL‚Äôensemble de ces t√¢ches s‚Äôappelle le preprocessing. L‚Äôun des int√©r√™ts\nd‚Äôutiliser Scikit est qu‚Äôon peut consid√©rer qu‚Äôune t√¢che de preprocessing\nest, en fait, une t√¢che d‚Äôapprentissage. En effet, le preprocessing\nconsiste √† apprendre des param√®tres d‚Äôune structure\nde donn√©es (par exemple estimer moyennes et variances pour les retrancher √† chaque\nobservation) et on peut tr√®s bien appliquer ces param√®tres\n√† des observations qui n‚Äôont pas servi √† construire\nceux-ci. Ainsi, en gardant en t√™te l‚Äôapproche g√©n√©rale avec Scikit\n\n\n\n\n\nnous allons voir deux processus tr√®s classiques de preprocessing :\n\nLa standardisation transforme des donn√©es pour que la distribution empirique suive une loi \\(\\mathcal{N}(0,1)\\).\nLa normalisation transforme les donn√©es de mani√®re √† obtenir une norme (\\(\\mathcal{l}_1\\) ou \\(\\mathcal{l}_2\\)) unitaire. Autrement dit, avec la norme ad√©quate, la somme des √©l√©ments est √©gale √† 1.\n\n\n\n Warning\nPour un statisticien,\nle terme normalization dans le vocable Scikit peut avoir un sens contre-intuitif.\nOn s‚Äôattendrait √† ce que la normalisation consiste √† transformer une variable de mani√®re √† ce que \\(X \\sim \\mathcal{N}(0,1)\\).\nC‚Äôest, en fait, la standardisation en Scikit qui fait cela.\n\n\n\nStandardisation\nLa standardisation consiste √† transformer des donn√©es pour que la distribution empirique suive une loi \\(\\mathcal{N}(0,1)\\). Pour √™tre performants, la plupart des mod√®les de machine learning n√©cessitent souvent d‚Äôavoir des donn√©es dans cette distribution.\n\n\n Exercice 3: Standardisation\n\nStandardiser la variable Median_Household_Income_2019 (ne pas √©craser les valeurs !) et regarder l‚Äôhistogramme avant/apr√®s normalisation.\n\nNote : On obtient bien une distribution centr√©e √† z√©ro et on pourrait v√©rifier que la variance empirique soit bien √©gale √† 1. On pourrait aussi v√©rifier que ceci est vrai √©galement quand on transforme plusieurs colonnes √† la fois.\n\nCr√©er scaler, un Transformer que vous construisez sur les 1000 premi√®res lignes de votre DataFrame df2 √† l‚Äôexception de la variable √† expliquer winner. V√©rifier la moyenne et l‚Äô√©cart-type de chaque colonne sur ces m√™mes observations.\n\nNote : Les param√®tres qui seront utilis√©s pour une standardisation ult√©rieure sont stock√©s dans les attributs .mean_ et .scale_\nOn peut voir ces attributs comme des param√®tres entra√Æn√©s sur un certain jeu de\ndonn√©es et qu‚Äôon peut r√©utiliser sur un autre, √† condition que les\ndimensions co√Øncident.\n\nAppliquer scaler sur les autres lignes du DataFrame et comparer les distributions obtenues de la variable Median_Household_Income_2019.\n\nNote : Une fois appliqu√©s √† un autre DataFrame, on peut remarquer que la distribution n‚Äôest pas exactement centr√©e-r√©duite dans le DataFrame sur lequel les param√®tres n‚Äôont pas √©t√© estim√©s. C‚Äôest normal, l‚Äô√©chantillon initial n‚Äô√©tait pas al√©atoire, les moyennes et variances de cet √©chantillon n‚Äôont pas de raison de co√Øncider avec les moments de l‚Äô√©chantillon complet.\n\n\n\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n\n\n&lt;Axes: xlabel='y_standard', ylabel='Count'&gt;\n\n\n\n\n\n\n\n\n\n\n\nMoyenne de chaque variable sur 1000 premi√®res observations avant :  [ 1.73616500e+04  3.84530000e+00  5.51891150e+04  1.29669150e+01\n  2.15813433e+01 -2.73468885e-02]\nEcart-type de chaque variable sur 1000 premi√®res observations avant :  [3.28113703e+04 1.28903822e+00 1.33256197e+04 6.45536365e+00\n 9.41139584e+00 9.23129044e-01]\nMoyenne de chaque variable sur 1000 premi√®res observations apr√®s :  [-3.37507799e-17  2.66453526e-17  1.58095759e-16  1.42108547e-17\n  1.24344979e-17 -1.59872116e-17]\nEcart-type de chaque variable sur 1000 premi√®res observations apr√®s :  [1. 1. 1. 1. 1. 1.]\n\n\n\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n\n\n&lt;Axes: ylabel='Count'&gt;\n\n\n\n\n\n\n\n\n\n\n\nNormalisation\nLa normalisation est l‚Äôaction de transformer les donn√©es de mani√®re\n√† obtenir une norme (\\(\\mathcal{l}_1\\) ou \\(\\mathcal{l}_2\\)) unitaire.\nAutrement dit, avec la norme ad√©quate, la somme des √©l√©ments est √©gale √† 1.\nPar d√©faut, la norme est dans \\(\\mathcal{l}_2\\).\nCette transformation est particuli√®rement utilis√©e en classification de texte ou pour effectuer du clustering.\n\n\n Exercice 4 : Normalisation\n\nNormaliser la variable Median_Household_Income_2019 (ne pas √©craser les valeurs !) et regarder l‚Äôhistogramme avant/apr√®s normalisation.\nV√©rifier que la norme \\(\\mathcal{l}_2\\) est bien √©gale √† 1.\n\n\n\n\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n\n\n&lt;Axes: ylabel='Count'&gt;\n\n\n\n\n\n\n\n\n\n\n\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n\n\n\n\nEncodage des valeurs cat√©gorielles\nLes donn√©es cat√©gorielles doivent √™tre recod√©es\nsous forme de valeurs num√©riques pour √™tre int√©gr√©s aux mod√®les de machine learning.\nCela peut √™tre fait de plusieurs mani√®res :\n\nLabelEncoder: transforme un vecteur [\"a\",\"b\",\"c\"] en vecteur num√©rique [0,1,2].\nCette approche a l‚Äôinconv√©nient d‚Äôintroduire un ordre dans les modalit√©s, ce qui n‚Äôest pas toujours souhaitable\nOrdinalEncoder: une version g√©n√©ralis√©e du LabelEncoder qui a vocation √† s‚Äôappliquer sur des matrices (\\(X\\)),\nalors que LabelEncoder s‚Äôapplique plut√¥t √† un vecteur (\\(y\\))\npandas.get_dummies effectue une op√©ration de dummy expansion.\nUn vecteur de taille n avec K cat√©gories sera transform√© en matrice de taille \\(n \\times K\\)\npour lequel chaque colonne sera une variable dummy pour la modalit√© k.\nIl y a ici \\(K\\) modalit√©s et il y a donc multicolin√©arit√©.\nAvec une r√©gression lin√©aire avec constante,\nil convient de retirer une modalit√© avant l‚Äôestimation.\nOneHotEncoder est une version g√©n√©ralis√©e (et optimis√©e) de la dummy expansion.\nIl a plut√¥t vocation √† s‚Äôappliquer sur les features (\\(X\\)) du mod√®le\n\n\n\n Exercice 5 : Encoder des variables cat√©gorielles\n\nCr√©er df qui conserve uniquement les variables state_name et county_name dans votes.\nAppliquer √† state_name un LabelEncoder\nNote : Le r√©sultat du label encoding est relativement intuitif, notamment quand on le met en relation avec le vecteur initial.\nRegarder la dummy expansion de state_name\nAppliquer un OrdinalEncoder √† df[['state_name', 'county_name']]\nNote : Le r√©sultat du ordinal encoding est coh√©rent avec celui du label encoding\nAppliquer un OneHotEncoder √† df[['state_name', 'county_name']]\n\nNote : scikit optimise l‚Äôobjet n√©cessaire pour stocker le r√©sultat d‚Äôun mod√®le de transformation. Par exemple, le r√©sultat de l‚Äôencoding One Hot est un objet tr√®s volumineux. Dans ce cas, scikit utilise une matrice Sparse.\n\n\n\n\narray([[23, 'Missouri'],\n       [25, 'Nebraska'],\n       [30, 'New York'],\n       ...,\n       [41, 'Texas'],\n       [41, 'Texas'],\n       [41, 'Texas']], dtype=object)\n\n\n\n\n\n\n\n\n\n\n\nAlabama\nArizona\nArkansas\nCalifornia\nColorado\nConnecticut\nDelaware\nDistrict of Columbia\nFlorida\nGeorgia\n...\nSouth Dakota\nTennessee\nTexas\nUtah\nVermont\nVirginia\nWashington\nWest Virginia\nWisconsin\nWyoming\n\n\n\n\n0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3102\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3103\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3104\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3105\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3106\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n3107 rows √ó 49 columns\n\n\n\n\n\narray([23., 25., 30., ..., 41., 41., 41.])\n\n\n\n\n&lt;3107x1891 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 6214 stored elements in Compressed Sparse Row format&gt;"
  },
  {
    "objectID": "content/modelisation/0_preprocessing.html#r√©f√©rences",
    "href": "content/modelisation/0_preprocessing.html#r√©f√©rences",
    "title": "Pr√©paration des donn√©es pour construire un mod√®le",
    "section": "R√©f√©rences",
    "text": "R√©f√©rences\n\n\nInsee. 2018. ‚ÄúGuide de S√©miologie Cartographique.‚Äù"
  },
  {
    "objectID": "content/modelisation/2_SVM.html",
    "href": "content/modelisation/2_SVM.html",
    "title": "Classification: premier mod√®le avec les SVM",
    "section": "",
    "text": "Pour pouvoir importer les donn√©es,\nnous allons importer les donn√©es suivantes:\nNous allons partir du m√™me jeu de donn√©es que pr√©c√©demment,\nc‚Äôest-√†-dire les r√©sultats des √©lections US 2020 pr√©sent√©s dans l‚Äôintroduction\nde cette partie: les donn√©es de vote aux √©lections pr√©sidentielles am√©ricaines\ncrois√©es √† des variables sociod√©mographiques.\nLe code\nest disponible sur Github.\nimport requests\n\nurl = 'https://raw.githubusercontent.com/linogaliana/python-datascientist/master/content/modelisation/get_data.py'\nr = requests.get(url, allow_redirects=True)\nopen('getdata.py', 'wb').write(r.content)\n\nimport getdata\nvotes = getdata.create_votes_dataframes()\nPour ce TP, nous aurons besoin des packages suivants :"
  },
  {
    "objectID": "content/modelisation/2_SVM.html#la-m√©thode-des-svm-support-vector-machines",
    "href": "content/modelisation/2_SVM.html#la-m√©thode-des-svm-support-vector-machines",
    "title": "Classification: premier mod√®le avec les SVM",
    "section": "La m√©thode des SVM (Support Vector Machines)",
    "text": "La m√©thode des SVM (Support Vector Machines)\nL‚Äôune des m√©thodes de machine learning les\nplus utilis√©es en classification sont les SVM (Support Vector Machines).\nIl s‚Äôagit de trouver, dans un syst√®me de projection ad√©quat (noyau ou kernel),\nles param√®tres de l‚Äôhyperplan (en fait d‚Äôun hyperplan √† marges maximales)\ns√©parant les classes de donn√©es :\n\n\n\n\n\n\n\n Formalisation math√©matique\nLes SVM sont l‚Äôune des m√©thodes de machine learning les plus intuitives\ndu fait de l‚Äôinterpr√©tation g√©om√©trique simple de la m√©thode. Il s‚Äôagit\naussi d‚Äôun des algorithmes de machine learning √† la formalisation\nla moins complexe pour les praticiens ayant des notions en statistique\ntraditionnelle. Cette bo√Æte revient dessus. N√©anmoins,\ncelle-ci n‚Äôest pas n√©cessaire √† la compr√©hension du chapitre.\nEn machine learning, plus que les d√©tails math√©matiques, l‚Äôimportant\nest d‚Äôavoir des intuitions.\nL‚Äôobjectif des SVM est, rappelons-le, de trouver un hyperplan qui permette\nde s√©parer les diff√©rentes classes au mieux. Par exemple, dans un espace\n√† deux dimensions, il s‚Äôagit de trouver une droite avec des marges\nqui permette de s√©parer au mieux l‚Äôespace en partie avec\ndes labels homog√®nes.\nOn peut, sans perdre de g√©n√©ralit√©,\nsupposer que le probl√®me consiste √† supposer l‚Äôexistence d‚Äôune loi de probabilit√© \\(\\mathbb{P}(x,y)\\) (\\(\\mathbb{P} \\to \\{-1,1\\}\\)) qui est inconnue. Le probl√®me de discrimination\nvise √† construire un estimateur de la fonction de d√©cision id√©ale qui minimise la probabilit√© d‚Äôerreur, autrement dit\n\\[\n\\theta = \\arg\\min_\\Theta \\mathbb{P}(h_\\theta(X) \\neq y |x)\n\\]\nLes SVM les plus simples sont les SVM lin√©aires. Dans ce cas, on suppose qu‚Äôil existe un s√©parateur lin√©aire qui permet d‚Äôassocier chaque classe √† son signe:\n\\[\nh_\\theta(x) = \\text{signe}(f_\\theta(x)) ; \\text{ avec } f_\\theta(x) = \\theta^T x + b\n\\]\navec \\(\\theta \\in \\mathbb{R}^p\\) et \\(w \\in \\mathbb{R}\\).\n\n\n\n\n\nLorsque des observations sont lin√©airement s√©parables,\nil existe une infinit√© de fronti√®res de d√©cision lin√©aire s√©parant les deux classes. Le ‚Äúmeilleur‚Äù choix est de prendre la marge maximale permettant de s√©parer les donn√©es. La distance entre les deux marges est \\(\\frac{2}{||\\theta||}\\). Donc maximiser cette distance entre deux hyperplans revient √† minimiser \\(||\\theta||^2\\) sous la contrainte \\(y_i(\\theta^Tx_i + b) \\geq 1\\).\nDans le cas non lin√©airement s√©parable, la hinge loss \\(\\max\\big(0,y_i(\\theta^Tx_i + b)\\big)\\) permet de lin√©ariser la fonction de perte:\n\n\n\n\n\nce qui donne le programme d‚Äôoptimisation suivant :\n\\[\n\\frac{1}{n} \\sum_{i=1}^n \\max\\big(0,y_i(\\theta^Tx_i + b)\\big) + \\lambda ||\\theta||^2\n\\]\nLa g√©n√©ralisation au cas non lin√©aire implique d‚Äôintroduire des noyaux transformant l‚Äôespace de coordonn√©es des observations."
  },
  {
    "objectID": "content/modelisation/2_SVM.html#application",
    "href": "content/modelisation/2_SVM.html#application",
    "title": "Classification: premier mod√®le avec les SVM",
    "section": "Application",
    "text": "Application\nPour appliquer un mod√®le de classification, il nous faut\ntrouver une variable dichotomique. Le choix naturel est\nde prendre la variable dichotomique qu‚Äôest la victoire ou\nd√©faite d‚Äôun des partis.\nM√™me si les R√©publicains ont perdu en 2020, ils l‚Äôont emport√©\ndans plus de comt√©s (moins peupl√©s). Nous allons consid√©rer\nque la victoire des R√©publicains est notre label 1 et la d√©faite 0.\n\n\n Exercice 1 : Premier algorithme de classification\n\nCr√©er une variable dummy appel√©e y dont la valeur vaut 1 quand les r√©publicains l‚Äôemportent.\nEn utilisant la fonction pr√™te √† l‚Äôemploi nomm√©e train_test_split de la librairie sklearn.model_selection,\ncr√©er des √©chantillons de test (20 % des observations) et d‚Äôestimation (80 %) avec comme features: 'Unemployment_rate_2019', 'Median_Household_Income_2019', 'Percent of adults with less than a high school diploma, 2015-19', \"Percent of adults with a bachelor's degree or higher, 2015-19\" et comme label la variable y.\n\nNote: Il se peut que vous ayez le warning suivant:\n\nA column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel()\n\nNote : Pour √©viter ce warning √† chaque fois que vous estimez votre mod√®le, vous pouvez utiliser DataFrame[['y']].values.ravel() plut√¥t que DataFrame[['y']] lorsque vous constituez vos √©chantillons.\n\nEntra√Æner un classifieur SVM avec comme param√®tre de r√©gularisation C = 1. Regarder les mesures de performance suivante : accuracy, f1, recall et precision.\nV√©rifier la matrice de confusion : vous devriez voir que malgr√© des scores en apparence pas si mauvais, il y a un probl√®me notable.\nRefaire les questions pr√©c√©dentes avec des variables normalis√©es. Le r√©sultat est-il diff√©rent ?\nChanger de variables x. Utiliser uniquement le r√©sultat pass√© du vote d√©mocrate et le revenu (votes_gop et Median_Household_Income_2019). Regarder les r√©sultats, notamment la matrice de confusion.\n[OPTIONNEL] Faire une 5-fold validation crois√©e pour d√©terminer le param√®tre C id√©al.\n\n\n\nA l‚Äôissue de la question 3,\nle classifieur avec C = 1\ndevrait avoir les performances suivantes :\n\n\n\n\nScore\n\n\n\n\nAccuracy\n0.882637\n\n\nRecall\n0.897297\n\n\nPrecision\n0.968872\n\n\nF1\n0.931712\n\n\n\nLa matrice de confusion associ√©e\nprend cette forme:\n\n\n\n\n\n\n\n\n\n\nA l‚Äôissue de la question 6,\nle nouveau classifieur avec devrait avoir les performances suivantes :\n\n\n\n\nScore\n\n\n\n\nAccuracy\n0.882637\n\n\nRecall\n0.897297\n\n\nPrecision\n0.968872\n\n\nF1\n0.931712\n\n\n\nEt la matrice de confusion associ√©e:"
  },
  {
    "objectID": "content/modelisation/4_featureselection.html",
    "href": "content/modelisation/4_featureselection.html",
    "title": "S√©lection de variables : une introduction",
    "section": "",
    "text": "Pour illustrer le travail de donn√©es n√©cessaire pour faire de la s√©lection de variables,\nnous allons partir du m√™me jeu de donn√©es que pr√©c√©demment,\nc‚Äôest-√†-dire les r√©sultats des √©lections US 2020 pr√©sent√©s dans l‚Äôintroduction\nde cette partie: les donn√©es de vote aux √©lections pr√©sidentielles am√©ricaines\ncrois√©es √† des variables sociod√©mographiques.\nLe code\nest disponible sur Github.\n#!pip install --upgrade xlrd #colab bug verson xlrd\n#!pip install geopandas\n\nimport requests\n\nurl = 'https://raw.githubusercontent.com/linogaliana/python-datascientist/master/content/modelisation/get_data.py'\nr = requests.get(url, allow_redirects=True)\nopen('getdata.py', 'wb').write(r.content)\n\nimport getdata\nvotes = getdata.create_votes_dataframes()\nJusqu‚Äô√† pr√©sent, nous avons suppos√© que les variables utiles √† la pr√©vision du\nvote R√©publicain √©taient connues du mod√©lisateur. Nous n‚Äôavons ainsi exploit√© qu‚Äôune partie\nlimit√©e des variables disponibles dans nos donn√©es. N√©anmoins, outre le fl√©au\ncomputationnel que repr√©senterait la construction d‚Äôun mod√®le avec un grand\nnombre de variables, le choix d‚Äôun nombre restreint de variables\n(mod√®le parcimonieux) limite le risque de sur-apprentissage.\nComment, d√®s-lors, choisir le bon nombre de variables et la meilleure\ncombinaison de ces variables ? Il existe de multiples m√©thodes, parmi lesquelles :"
  },
  {
    "objectID": "content/modelisation/4_featureselection.html#principe-du-lasso",
    "href": "content/modelisation/4_featureselection.html#principe-du-lasso",
    "title": "S√©lection de variables : une introduction",
    "section": "Principe du LASSO",
    "text": "Principe du LASSO\n\nPrincipe g√©n√©ral\nLa classe des mod√®les de feature selection est ainsi tr√®s vaste et regroupe\nun ensemble tr√®s diverse de mod√®les. Nous allons nous focaliser sur le LASSO\n(Least Absolute Shrinkage and Selection Operator)\nqui est une extension de la r√©gression lin√©aire qui vise √† s√©lectionner des\nmod√®les sparses. Ce type de mod√®le est central dans le champ du\nCompressed sensing (o√π on emploie plut√¥t le terme\nde L1-regularization que de LASSO). Le LASSO est un cas particulier des\nr√©gressions elastic-net dont un autre cas fameux est la r√©gression ridge.\nContrairement √† la r√©gression lin√©aire classique, elles fonctionnent √©galement\ndans un cadre o√π \\(p&gt;N\\), c‚Äôest √† dire o√π le nombre de r√©gresseurs est tr√®s grand puisque sup√©rieur\nau nombre d‚Äôobservations.\n\n\nP√©nalisation\nEn adoptant le principe d‚Äôune fonction objectif p√©nalis√©e,\nle LASSO permet de fixer un certain nombre de coefficients √† 0.\nLes variables dont la norme est non nulle passent ainsi le test de s√©lection.\n\n\n Hint\nLe LASSO est un programme d‚Äôoptimisation sous contrainte. On cherche √† trouver l‚Äôestimateur \\(\\beta\\) qui minimise l‚Äôerreur quadratique (r√©gression lin√©aire) sous une contrainte additionnelle r√©gularisant les param√®tres:\n\\[\n\\min_{\\beta} \\frac{1}{2}\\mathbb{E}\\bigg( \\big( X\\beta - y  \\big)^2 \\bigg) \\\\\n\\text{s.t. } \\sum_{j=1}^p |\\beta_j| \\leq t\n\\]\nCe programme se reformule gr√¢ce au Lagrangien est permet ainsi d‚Äôobtenir un programme de minimisation plus maniable :\n\\[\n\\beta^{\\text{LASSO}} = \\arg \\min_{\\beta} \\frac{1}{2}\\mathbb{E}\\bigg( \\big( X\\beta - y  \\big)^2 \\bigg) + \\alpha \\sum_{j=1}^p |\\beta_j| = \\arg \\min_{\\beta} ||y-X\\beta||_{2}^{2} + \\alpha ||\\beta||_1\n\\]\no√π \\(\\lambda\\) est une r√©√©criture de la r√©gularisation pr√©c√©dente.\n\n\n\n\nPremi√®re r√©gression LASSO\nAvant de se lancer dans les exercices, on va √©liminer quelques colonnes redondantes,\ncelles qui concernent les votes des partis concurrents (forc√©ment tr√®s\ncorr√©l√©s au vote R√©publicain‚Ä¶) :\n\ndf2 = votes.loc[:,~votes.columns.str.endswith(\n  ('_democrat','_green','_other', 'per_point_diff', 'per_dem')\n  )]\n\nNous allons utiliser par la suite les fonctions ou\npackages suivants :\n\n# packages utiles\nimport numpy as np\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Lasso\nimport sklearn.metrics\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import lasso_path\nimport seaborn as sns\n\n\n\n Exercice 1 : Premier LASSO\nOn cherche toujours √† pr√©dire la variable per_gop.\n\nPr√©parez les variables √† utiliser.\n\n\nNe garder que les colonnes num√©riques (id√©alement on transformerait\nles variables non num√©riques en num√©riques)\nRemplacer les valeurs infinies par des NaN et les valeurs manquantes par 0.\nStandardiser les features (c‚Äôest-√†-dire les variables autres que la variable per_gop) avec StandardScaler\n\n\nOn cherche toujours √† pr√©dire la variable per_gop. Cr√©ez un √©chantillon d‚Äôentra√Ænement et un √©chantillon test.\nEstimer un mod√®le LASSO p√©nalis√© avec \\(alpha = 0.1\\). Afficher les valeurs des coefficients. Quelles variables ont une valeur non nulle ?\nMontrer que les variables s√©lectionn√©es sont parfois tr√®s corr√©l√©es.\nComparer la performance de ce mod√®le parcimonieux avec celle d‚Äôun mod√®le avec plus de variables\nUtiliser la fonction lasso_path pour √©valuer le nombre de param√®tres s√©lectionn√©s par LASSO lorsque \\(\\alpha\\)\nvarie (parcourir \\(\\alpha \\in [0.001,0.01,0.02,0.025,0.05,0.1,0.25,0.5,0.8,1.0]\\) ).\n\n\n\nA l‚Äôissue de la question 3,\nles variables s√©lectionn√©es sont :\n\n\n['ALAND',\n 'FIPS_y',\n 'INTERNATIONAL_MIG_2017',\n 'DOMESTIC_MIG_2014',\n 'DOMESTIC_MIG_2017',\n 'RESIDUAL_2010',\n 'RESIDUAL_2019',\n 'R_death_2012',\n 'R_death_2019',\n 'R_NATURAL_INC_2019',\n 'R_INTERNATIONAL_MIG_2011',\n 'R_DOMESTIC_MIG_2012',\n \"Percent of adults with a bachelor's degree or higher, 1990\",\n 'Percent of adults with a high school diploma only, 2000',\n \"Percent of adults with a bachelor's degree or higher, 2000\",\n \"Percent of adults with a bachelor's degree or higher, 2015-19\",\n 'Rural_urban_continuum_code_2013',\n 'Metro_2013',\n 'Unemployment_rate_2002',\n 'Unemployment_rate_2003',\n 'Unemployment_rate_2012',\n 'Rural-urban_Continuum_Code_2003',\n 'Rural-urban_Continuum_Code_2013',\n 'CI90LB517P_2019',\n 'candidatevotes_2016_republican',\n 'share_2012_republican',\n 'share_2016_republican']\n\n\nCertaines variables font sens, comme les variables d‚Äô√©ducation par exemple. Notamment, un des meilleurs pr√©dicteurs pour le score des R√©publicains en 2020 est‚Ä¶ le score des R√©publicains (et m√©caniquement des d√©mocrates) en 2016.\nPar ailleurs, on s√©lectionne des variables redondantes. Une phase plus approfondie de nettoyage des donn√©es serait en r√©alit√© n√©cessaire.\n\n\n\n\n\n\n\n\n\nOn voit que plus \\(\\alpha\\) est √©lev√©, moins le mod√®le s√©lectionne de variables."
  },
  {
    "objectID": "content/modelisation/4_featureselection.html#validation-crois√©e-pour-s√©lectionner-le-mod√®le",
    "href": "content/modelisation/4_featureselection.html#validation-crois√©e-pour-s√©lectionner-le-mod√®le",
    "title": "S√©lection de variables : une introduction",
    "section": "Validation crois√©e pour s√©lectionner le mod√®le",
    "text": "Validation crois√©e pour s√©lectionner le mod√®le\nQuel \\(\\alpha\\) faut-il privil√©gier ? Pour cela,\nil convient d‚Äôeffectuer une validation crois√©e afin de choisir le mod√®le pour\nlequel les variables qui passent la phase de s√©lection permettent de mieux\npr√©dire le r√©sultat R√©publicain :\n\nfrom sklearn.linear_model import LassoCV\n\ndf3 = df2.select_dtypes(include=np.number)\ndf3.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf3 = df3.fillna(0)\nscaler = StandardScaler()\nyindex = df3.columns.get_loc(\"per_gop\")\ndf3_scale = scaler.fit(df3).transform(df3)\n# X_train, X_test , y_train, y_test = train_test_split(np.delete(data, yindex, axis = 1),data[:,yindex], test_size=0.2, random_state=0)\n\nlcv = LassoCV(alphas=my_alphas, fit_intercept=False,random_state=0,cv=5).fit(np.delete(df3_scale, yindex, axis = 1), df3_scale[:,yindex])\n\n\nprint(\"alpha optimal :\", lcv.alpha_)\n\nalpha optimal : 0.001\n\n\n\nlasso2 = Lasso(fit_intercept=True, alpha = lcv.alpha_).fit(X_train,y_train)\nfeatures_selec2 = df2.select_dtypes(include=np.number).drop(\"per_gop\", axis = 1).columns[np.abs(lasso2.coef_)&gt;0].tolist()\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning:\n\nObjective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.486e+03, tolerance: 6.352e+01\n\n\n\nLes variables s√©lectionn√©es sont :\n\nprint(features_selec2)\n\n['ALAND', 'AWATER', 'votes_gop', 'diff', 'Rural-urban_Continuum Code_2003', 'Rural-urban_Continuum Code_2013', 'Urban_Influence_Code_2013', 'Economic_typology_2015', 'CENSUS_2010_POP', 'N_POP_CHG_2013', 'N_POP_CHG_2016', 'N_POP_CHG_2017', 'N_POP_CHG_2018', 'N_POP_CHG_2019', 'Births_2011', 'Births_2015', 'Deaths_2015', 'Deaths_2017', 'Deaths_2018', 'NATURAL_INC_2012', 'NATURAL_INC_2013', 'NATURAL_INC_2014', 'NATURAL_INC_2016', 'NATURAL_INC_2018', 'INTERNATIONAL_MIG_2010', 'INTERNATIONAL_MIG_2011', 'INTERNATIONAL_MIG_2012', 'INTERNATIONAL_MIG_2013', 'INTERNATIONAL_MIG_2014', 'INTERNATIONAL_MIG_2015', 'INTERNATIONAL_MIG_2016', 'INTERNATIONAL_MIG_2017', 'INTERNATIONAL_MIG_2018', 'INTERNATIONAL_MIG_2019', 'DOMESTIC_MIG_2010', 'DOMESTIC_MIG_2012', 'DOMESTIC_MIG_2013', 'DOMESTIC_MIG_2015', 'DOMESTIC_MIG_2016', 'DOMESTIC_MIG_2018', 'NET_MIG_2011', 'NET_MIG_2014', 'NET_MIG_2018', 'NET_MIG_2019', 'RESIDUAL_2010', 'RESIDUAL_2011', 'RESIDUAL_2012', 'RESIDUAL_2013', 'RESIDUAL_2014', 'RESIDUAL_2015', 'RESIDUAL_2016', 'RESIDUAL_2017', 'RESIDUAL_2018', 'RESIDUAL_2019', 'GQ_ESTIMATES_BASE_2010', 'GQ_ESTIMATES_2013', 'GQ_ESTIMATES_2015', 'GQ_ESTIMATES_2017', 'R_birth_2011', 'R_birth_2013', 'R_birth_2014', 'R_birth_2016', 'R_birth_2017', 'R_birth_2019', 'R_death_2011', 'R_death_2012', 'R_death_2013', 'R_death_2014', 'R_death_2015', 'R_death_2016', 'R_death_2017', 'R_death_2018', 'R_death_2019', 'R_NATURAL_INC_2012', 'R_NATURAL_INC_2015', 'R_NATURAL_INC_2017', 'R_NATURAL_INC_2018', 'R_NATURAL_INC_2019', 'R_INTERNATIONAL_MIG_2011', 'R_INTERNATIONAL_MIG_2012', 'R_INTERNATIONAL_MIG_2013', 'R_INTERNATIONAL_MIG_2014', 'R_INTERNATIONAL_MIG_2015', 'R_INTERNATIONAL_MIG_2016', 'R_INTERNATIONAL_MIG_2017', 'R_INTERNATIONAL_MIG_2018', 'R_INTERNATIONAL_MIG_2019', 'R_DOMESTIC_MIG_2011', 'R_DOMESTIC_MIG_2012', 'R_DOMESTIC_MIG_2013', 'R_DOMESTIC_MIG_2015', 'R_DOMESTIC_MIG_2016', 'R_DOMESTIC_MIG_2018', 'R_NET_MIG_2014', 'R_NET_MIG_2017', 'R_NET_MIG_2019', '2003 Rural-urban Continuum Code', 'Less than a high school diploma, 1970', 'High school diploma only, 1970', 'Some college (1-3 years), 1970', 'Four years of college or higher, 1970', 'Percent of adults with less than a high school diploma, 1970', 'Percent of adults with a high school diploma only, 1970', 'Percent of adults completing some college (1-3 years), 1970', 'Percent of adults completing four years of college or higher, 1970', 'Less than a high school diploma, 1980', 'High school diploma only, 1980', 'Some college (1-3 years), 1980', 'Four years of college or higher, 1980', 'Percent of adults with less than a high school diploma, 1980', 'Percent of adults with a high school diploma only, 1980', 'Percent of adults completing some college (1-3 years), 1980', 'Percent of adults completing four years of college or higher, 1980', 'Less than a high school diploma, 1990', 'Percent of adults with less than a high school diploma, 1990', 'Percent of adults with a high school diploma only, 1990', 'Less than a high school diploma, 2000', 'High school diploma only, 2000', \"Some college or associate's degree, 2000\", \"Bachelor's degree or higher, 2000\", 'Percent of adults with less than a high school diploma, 2000', 'Percent of adults with a high school diploma only, 2000', \"Percent of adults completing some college or associate's degree, 2000\", \"Percent of adults with a bachelor's degree or higher, 2000\", 'Less than a high school diploma, 2015-19', 'High school diploma only, 2015-19', \"Some college or associate's degree, 2015-19\", \"Bachelor's degree or higher, 2015-19\", 'Percent of adults with less than a high school diploma, 2015-19', 'Percent of adults with a high school diploma only, 2015-19', \"Percent of adults completing some college or associate's degree, 2015-19\", \"Percent of adults with a bachelor's degree or higher, 2015-19\", 'Metro_2013', 'Unemployed_2000', 'Unemployment_rate_2000', 'Unemployment_rate_2001', 'Unemployed_2002', 'Unemployment_rate_2002', 'Unemployed_2003', 'Unemployment_rate_2003', 'Civilian_labor_force_2004', 'Employed_2004', 'Unemployment_rate_2004', 'Civilian_labor_force_2005', 'Unemployed_2005', 'Unemployment_rate_2005', 'Civilian_labor_force_2006', 'Unemployed_2006', 'Unemployment_rate_2006', 'Unemployed_2007', 'Unemployment_rate_2007', 'Unemployed_2008', 'Unemployment_rate_2008', 'Employed_2009', 'Unemployment_rate_2009', 'Employed_2010', 'Unemployment_rate_2010', 'Civilian_labor_force_2011', 'Employed_2011', 'Unemployed_2011', 'Civilian_labor_force_2012', 'Employed_2012', 'Unemployed_2012', 'Unemployment_rate_2012', 'Unemployed_2013', 'Unemployment_rate_2013', 'Unemployed_2014', 'Unemployment_rate_2014', 'Civilian_labor_force_2015', 'Employed_2015', 'Unemployment_rate_2015', 'Unemployed_2016', 'Unemployment_rate_2016', 'Unemployed_2017', 'Unemployment_rate_2017', 'Unemployed_2018', 'Unemployment_rate_2018', 'Unemployment_rate_2019', 'Med_HH_Income_Percent_of_State_Total_2019', 'Rural-urban_Continuum_Code_2003', 'Urban_Influence_Code_2003', 'Rural-urban_Continuum_Code_2013', 'POVALL_2019', 'CI90LBALL_2019', 'CI90UBALL_2019', 'CI90LBALLP_2019', 'CI90UBALLP_2019', 'POV017_2019', 'CI90LB017_2019', 'CI90UB017_2019', 'CI90LB017P_2019', 'CI90LB517_2019', 'CI90UB517_2019', 'PCTPOV517_2019', 'CI90LB517P_2019', 'CI90LBINC_2019', 'CI90UBINC_2019', 'candidatevotes_2000_republican', 'candidatevotes_2004_republican', 'candidatevotes_2008_republican', 'candidatevotes_2012_republican', 'candidatevotes_2016_republican', 'share_2000_republican', 'share_2008_republican', 'share_2012_republican', 'share_2016_republican']\n\n\n\ndf2.select_dtypes(include=np.number).drop(\"per_gop\", axis = 1).columns[np.abs(lasso2.coef_)&gt;0]\nnlasso = sum(np.abs(lasso2.coef_)&gt;0)\n\nCela correspond √† un mod√®le avec 206 variables s√©lectionn√©es.\n\n\n Hint\nDans le cas o√π le mod√®le para√Ætrait trop peu parcimonieux, il faudrait revoir la phase de d√©finition des variables pertinentes pour comprendre si des √©chelles diff√©rentes de certaines variables ne seraient pas plus appropri√©es (par exemple du log)."
  },
  {
    "objectID": "content/modelisation/6_pipeline.html#pourquoi-utiliser-les-pipelines",
    "href": "content/modelisation/6_pipeline.html#pourquoi-utiliser-les-pipelines",
    "title": "Premier pas vers l‚Äôindustrialisation avec les pipelines scikit",
    "section": "Pourquoi utiliser les pipelines ?",
    "text": "Pourquoi utiliser les pipelines ?\nLes chapitres pr√©c√©dents ont permis de montrer des bouts de code\n√©pars pour entra√Æner des mod√®les ou faire du preprocessing.\nCette d√©marche est int√©ressante pour t√¢tonner mais risque d‚Äô√™tre co√ªteuse\nult√©rieurement s‚Äôil est n√©cessaire d‚Äôajouter une √©tape de preprocessing\nou de changer d‚Äôalgorithme.\nHeureusement, scikit propose un excellent outil pour proposer un cadre\ng√©n√©ral pour cr√©er une cha√Æne de production machine learning. Il\ns‚Äôagit des\npipelines.\nIls pr√©sentent de nombreux int√©r√™ts, parmi lesquels:\n\nIls sont tr√®s pratiques et lisibles. On rentre des donn√©es en entr√©e, on n‚Äôappelle qu‚Äôune seule fois les m√©thodes fit et predict ce qui permet de s‚Äôassurer une gestion coh√©rente des transformations de variables, par exemple apr√®s l‚Äôappel d‚Äôun StandardScaler\nLa modularit√© rend ais√©e la mise √† jour d‚Äôun pipeline et renforce la capacit√© √† le r√©utiliser\nIls permettent de facilement chercher les hyperparam√®tres d‚Äôun mod√®le. Sans pipeline, √©crire un code qui fait du tuning d‚Äôhyperparam√®tres peut √™tre p√©nible. Avec les pipelines, c‚Äôest une ligne de code.\nLa s√©curit√© d‚Äô√™tre certain que les √©tapes de preprocessing sont bien appliqu√©es aux jeux de donn√©es d√©sir√©s avant l‚Äôestimation.\n\n\n\n Hint\nUn des int√©r√™ts des pipelines scikit est qu‚Äôils fonctionnent aussi avec\ndes m√©thodes qui ne sont pas issues de scikit.\nIl est possible d‚Äôintroduire un mod√®le de r√©seau de neurones Keras dans\nun pipeline scikit.\nPour introduire un mod√®le √©conom√©trique statsmodels\nc‚Äôest un peu plus co√ªteux mais nous allons proposer des exemples\nqui peuvent servir de mod√®le et qui montrent que c‚Äôest faisable\nsans trop de difficult√©."
  },
  {
    "objectID": "content/modelisation/6_pipeline.html#comment-cr√©er-un-pipeline",
    "href": "content/modelisation/6_pipeline.html#comment-cr√©er-un-pipeline",
    "title": "Premier pas vers l‚Äôindustrialisation avec les pipelines scikit",
    "section": "Comment cr√©er un pipeline",
    "text": "Comment cr√©er un pipeline\nUn pipeline est un encha√Ænement d‚Äôop√©rations qu‚Äôon code en enchainant\ndes pairs (cl√©, valeur):\n\nla cl√© est le nom du pipeline, cela peut √™tre utile lorsqu‚Äôon va\nrepr√©senter le pipeline sous forme de diagramme acyclique (visualisation DAG)\nou qu‚Äôon veut afficher des informations sur une √©tape\nla valeur repr√©sente la transformation √† mettre en oeuvre dans le pipeline\n(c‚Äôest-√†-dire, √† l‚Äôexception de la derni√®re √©tape,\nmettre en oeuvre une m√©thode transform et √©ventuellement une\ntransformation inverse).\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\n\nestimators = [('reduce_dim', PCA()), ('clf', SVC())]\npipe = Pipeline(estimators)\n\n\n\n Hint\nIl est pratique de visualiser un pipeline sous forme de DAG.\nPour cela, dans un notebook, on utilise la configuration\nsuivante:\n#| eval: false\nfrom sklearn import set_config\nset_config(display='diagram') \n\n\npipe\nAu sein d‚Äôune √©tape de pipeline, les param√®tres d‚Äôun estimateur\nsont accessibles avec la notation &lt;estimator&gt;__&lt;parameter&gt;.\nCela permet de fixer des valeurs pour les arguments des fonctions scikit\nqui sont appel√©es au sein d‚Äôun pipeline.\nC‚Äôest cela qui rendra l‚Äôapproche des pipelines particuli√®rement utile\npour la grid search:\n\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {\"reduce_dim__n_components\":[2, 5, 10], \"clf__C\":[0.1, 10, 100]}\ngrid_search = GridSearchCV(pipe, param_grid=param_grid)\n\n\nDonn√©es utilis√©es\nNous allons utiliser les donn√©es de transactions immobili√®res DVF pour chercher\nla meilleure mani√®re de pr√©dire, sachant les caract√©ristiques d‚Äôun bien, son\nprix.\nCes donn√©es peuvent √™tre import√©es directement depuis data.gouv:\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nmutations = pd.read_csv('https://www.data.gouv.fr/fr/datasets/r/3004168d-bec4-44d9-a781-ef16f41856a2', sep = \"|\", decimal=\",\")\n\nOn propose d‚Äôenrichir la base de quelques variables qui pourraient servir\nult√©rieurement:\n\nmutations['Date mutation'] = pd.to_datetime(mutations['Date mutation'], format = \"%d/%m/%Y\")\nmutations['year'] = mutations['Date mutation'].dt.year\nmutations['month'] = mutations['Date mutation'].dt.month\nmutations['dep'] = mutations['Code postal'].astype(str).str[:2]\nmutations['lprix'] = np.log(mutations[\"Valeur fonciere\"])\n\nSi vous travaillez avec les donn√©es de 2020, n‚Äôoubliez pas\nd‚Äôint√©grer l‚Äôeffet\nconfinement strict dans vos mod√®les. Pour cela, vous pouvez cr√©er une variable\nindicatrice entre les dates en question:\n\nmutations['confinement'] = mutations['Date mutation'].between(pd.to_datetime(\"2020-03-17\"), pd.to_datetime(\"2020-05-03\")).astype(int)\n\nLes donn√©es DVF proposent une observation par transaction. Ces transactions\npeuvent concerner plusieurs lots.\nPour simplifier,\non va cr√©er une variable de surface qui agr√®ge les diff√©rentes informations\nde surface disponibles dans le jeu de donn√©es. En effet, les variables\nen question sont tr√®s corr√©l√©es les unes entre elles :\n\ng.figure.get_figure()\n\nLes agr√©ger revient √† supposer que le mod√®le de fixation des prix est le m√™me\nentre chaque lot. C‚Äôest une hypoth√®se simplificatrice qu‚Äôune personne plus\nexperte du march√© immobilier, ou qu‚Äôune approche propre de s√©lection\nde variable pourrait amener √† nier\n\nmutations['surface'] = mutations.loc[:, mutations.columns[mutations.columns.str.startswith('Surface Carrez')].tolist()].sum(axis = 1)"
  },
  {
    "objectID": "content/modelisation/6_pipeline.html#un-premier-pipeline-random-forest-sur-des-variables-standardis√©es",
    "href": "content/modelisation/6_pipeline.html#un-premier-pipeline-random-forest-sur-des-variables-standardis√©es",
    "title": "Premier pas vers l‚Äôindustrialisation avec les pipelines scikit",
    "section": "Un premier pipeline: random forest sur des variables standardis√©es",
    "text": "Un premier pipeline: random forest sur des variables standardis√©es\nNotre premier pipeline va nous permettre d‚Äôint√©grer ensemble:\n\nUne √©tape de preprocessing avec la standardisation de variables\nUne √©tape d‚Äôestimation du prix en utilisant un mod√®le de random forest\n\nPour le moment, on va prendre comme acquis un certain nombre de variables\nexplicatives (les features) et les hyperparam√®tres du mod√®le\n\nD√©finition des ensembles train/test\nNous allons donc nous restreindre √† un sous-ensemble de colonnes dans un\npremier temps :\n\nxvars = ['dep', 'Nombre de lots', 'Code type local', 'surface', 'Nombre pieces principales']\nxvars2 = pd.Series(xvars).str.replace(\" \",\"_\").tolist()\n\nmutations2 = mutations.loc[:, xvars + [\"Valeur fonciere\"]]\n\nNous allons √©galement ne conserver que les transactions inf√©rieures √† 5 millions\nd‚Äôeuros (on anticipe que celles ayant un montant sup√©rieur sont des transactions\nexceptionnelles dont le m√©canisme de fixation du prix diff√®re)\n\nmutations2  = mutations2.dropna()\nmutations2 = mutations2.loc[mutations2['Valeur fonciere'] &lt; 5e6] #keep only values below 10 millions\n\nmutations2.columns = mutations2.columns.str.replace(\" \", \"_\")\nnumeric_features = mutations2.columns[~mutations2.columns.isin(['dep','Code_type_local', 'confinement'])].tolist()\ncategorical_features = ['dep','Code_type_local']\n\nAu passage, nous avons abandonn√© la variable de code postal pour privil√©gier\nla commune afin de r√©duire la dimension de notre jeu de donn√©es. Si on voulait\nvraiment avoir un bon mod√®le, il faudrait faire autrement car le code postal\nest probablement un tr√®s bon pr√©dicteur du prix d‚Äôun bien, une fois que\nles caract√©ristiques du bien sont contr√¥l√©es.\nNous allons stratifier notre √©chantillonage de train/test par d√©partement\nafin de tenir compte, de mani√®re minimale, de la g√©ographie.\nPour acc√©l√©rer les calculs pour ce tutoriel, nous n‚Äôallons consid√©rer que\n20% des transactions observ√©es sur chaque d√©partement.\n\nfrom sklearn.model_selection import train_test_split\n\nmutations2 = mutations2.groupby('dep').sample(frac = 0.2, random_state = 123)\n\nX_train, X_test, y_train, y_test = train_test_split(mutations2[xvars2], mutations2[[\"Valeur_fonciere\"]].values.ravel(), test_size = 0.2, random_state = 0, stratify=mutations2[['dep']])\n\n\n\nD√©finition du premier pipeline\nNous allons donc partir d‚Äôun random forest avec des valeurs d‚Äôhyperparam√®tres\ndonn√©es.\nLes random forest sont une m√©thode d‚Äôaggr√©gation1 d‚Äôarbres de d√©cision.\nOn calcule \\(K\\) arbres de d√©cision et en tire, par une m√©thode d‚Äôagr√©gation,\nune r√®gle de d√©cision moyenne qu‚Äôon va appliquer pour tirer une\npr√©diction de nos donn√©es.\n\n\n\n\n\nC‚Äôest un article de L√©o Breiman (2001)2, statisticien √† Berkeley, qui\nest √† l‚Äôorigine du succ√®s des random forests. L‚Äôun des int√©r√™ts\ndes random forest est qu‚Äôil existe des m√©thodes pour d√©terminer\nl‚Äôimportance relative de chaque variable dans la pr√©diction.\nPour commencer, nous allons fixer la taille des arbres de d√©cision avec\nl‚Äôhyperparam√®tre max_depth = 2.\n\nfrom sklearn.ensemble import RandomForestRegressor\nregr = RandomForestRegressor(max_depth=2, random_state=123)\n\nNotre pipeline va int√©grer les √©tapes suivantes :\n\nPreprocessing :\n\nLes variables num√©riques vont √™tre standardis√©es avec un StandardScaler.\nPour cela, nous allons utiliser la liste numeric_features d√©finie pr√©c√©demment.\nLes variables cat√©gorielles vont √™tre explos√©es avec un one hot encoding\n(m√©thode OneHotEncoder de scikit)\nPour cela, nous allons utiliser la liste categorical_features\n\nRandom forest : nous allons appliquer l‚Äôestimateur regr d√©fini plus haut\n\nJ‚Äôajoute en commentaire un exemple de comment s‚Äôintroduirait une imputation\nde valeurs manquantes. La version 1.0 de scikit facilite l‚Äôint√©gration\nd‚Äô√©tapes complexes dans les pipelines3. Si vous utilisez une\nversion ant√©rieure √† la 1.0 de scikit, vous pouvez vous rendre dans\nla section Annexe pour avoir des exemples de d√©finition alternative\n(attention cependant, vous ne pourrez r√©cup√©rer le nom des features\ntransform√©es comme ici, ce qui peut p√©naliser l‚Äôanalyse d‚Äôimportance\nde variables)\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import make_column_transformer\n\nnumeric_pipeline = make_pipeline(\n  #SimpleImputer(),\n  StandardScaler()\n)\ntransformer = make_column_transformer(\n    (numeric_pipeline, numeric_features[:-1]),\n    (OneHotEncoder(sparse = False, handle_unknown = \"ignore\"), categorical_features))\npipe = Pipeline(steps=[('preprocessor', transformer),\n                      ('randomforest', regr)])\n\nNous avons construit ce pipeline sous forme de couches successives. La couche\nrandomforest prendra automatiquement le r√©sultat de la couche preprocessor\nen input. La couche features permet d‚Äôintroduire de mani√®re relativement\nsimple (quand on a les bonnes m√©thodes) la complexit√© du preprocessing\nsur donn√©es r√©elles dont les types divergent.\nOn peut visualiser le graphe et ainsi se repr√©senter la mani√®re dont\nce pipeline op√®re:\npipe\nMaintenant, il ne reste plus qu‚Äô√† estimer notre mod√®le sur l‚Äôensemble\nd‚Äôentra√Ænement. C‚Äôest tr√®s simple avec un pipeline : il suffit d‚Äôutiliser\nde mettre √† jour le pipeline avec la m√©thode fit\nOn peut utiliser le nom du pipeline en conjonction de certaines m√©thodes\npour appliquer cette √©tape sur un jeu de donn√©es pour visualiser\nl‚Äôeffet de la transformation.\nPar exemple, pour visualiser le jeu de donn√©es transform√© avant l‚Äô√©tape\nd‚Äôestimation, on peut\nfaire\n\npipe[:-1].fit_transform(X_train)\n\nDe m√™me, si on veut r√©cup√©rer le nom des features en sortie du preprocessing,\non utilisera la m√©thode .get_feature_names_out qui est bien pratique\n(c‚Äôest cette m√©thode qui est plus complexe √† appeler dans les versions scikit\nancienne qui nous a fait privil√©gier le pipeline ci-dessous)\n\nfeatures_names=pipe['preprocessor'].get_feature_names_out()\nfeatures_names\n\n\n\nVariable importance\nOn ne va repr√©senter, parmi notre ensemble important de colonnes, que celles\nqui ont une importance non nulle. Gr√¢ce √† notre vecteur features_names,\non va pouvoir facilement afficher le nom des colonnes en question (et donc\ncomprendre les features d√©terminantes)\nL‚Äôimportance va √™tre d√©finie √† partir\nde la mesure d‚Äôimpuret√©4\nOn voit donc que deux variables d√©terminantes sont des effets fixes\ng√©ographiques (qui servent √† ajuster de la diff√©rence de prix entre\nParis et les Hauts de Seine et le reste de la France), une autre variable\nest un effet fixe type de bien. Les deux variables qui pourraient introduire\nde la variabilit√©, √† savoir la surface et, dans une moindre mesure, le\nnombre de lots, ont une importance moindre.\n\n\n Note\nId√©alement, on utiliserait yellowbrick pour repr√©senter l‚Äôimportance des variables\nMais en l‚Äô√©tat actuel du pipeline on a beaucoup de variables dont le poids\nest nul qui viennent polluer la visualisation. Vous pouvez\nconsulter la\ndocumentation de yellowbrick sur ce sujet\n\n\n\n\nPr√©diction\nL‚Äôanalyse de l‚Äôimportance de variables permet de mieux comprendre\nle fonctionnement interne des random forests.\nOn obtient un mod√®le dont les performances sont les suivantes :\n\nfrom sklearn.metrics import mean_squared_error\n\n\ncompar = pd.DataFrame([y_test, pipe.predict(X_test)]).T\ncompar.columns = ['obs','pred']\ncompar['diff'] = compar.obs - compar.pred\n\nprint(\"Le RMSE sur le jeu de test est {:,}\".format(\n   int(np.sqrt(mean_squared_error(y_test, pipe.predict(X_test))))\n))\n\nLe RMSE n‚Äôest pas tr√®s bon. Pour comprendre pourquoi, repr√©sentons\nnotre nuage de point des valeurs observ√©es et pr√©dites:\nC‚Äôest tr√®s d√©cevant. La pr√©diction a trop peu de variabilit√© pour capturer\nla variance des prix observ√©e. Cela vient du fait que les variables\nayant de l‚Äôimportance dans la pr√©diction sont principalement des effets fixes,\nqui ne permettent donc qu‚Äôune variabilit√© limit√©e.\n\ng.figure.get_figure()"
  },
  {
    "objectID": "content/modelisation/6_pipeline.html#recherche-des-hyperparam√®tres-optimaux-avec-une-validation-crois√©e",
    "href": "content/modelisation/6_pipeline.html#recherche-des-hyperparam√®tres-optimaux-avec-une-validation-crois√©e",
    "title": "Premier pas vers l‚Äôindustrialisation avec les pipelines scikit",
    "section": "Recherche des hyperparam√®tres optimaux avec une validation crois√©e",
    "text": "Recherche des hyperparam√®tres optimaux avec une validation crois√©e\nOn d√©tecte que le premier mod√®le n‚Äôest pas tr√®s bon et ne nous aidera\npas vraiment √† √©valuer de mani√®re fiable la maison de nos r√™ves.\nOn va essayer de voir si notre mod√®le ne serait pas meilleur avec des\nhyperparam√®tres plus adapt√©s. Apr√®s tout, nous avons choisi par d√©faut\nla profondeur de l‚Äôarbre mais c‚Äô√©tait un choix au doigt mouill√©.\nQuels sont les hyperparam√®tres qu‚Äôon peut essayer d‚Äôoptimiser ?\n\npipe['randomforest'].get_params()\n\nUn d√©tour par la documentation\nnous aide √† comprendre ceux sur lesquels on va jouer. Par exemple, il serait\nabsurde de jouer sur le param√®tre random_state qui est la racine du g√©n√©rateur\npseudo-al√©atoire.\nComme l‚Äôobjectif est de se concentrer sur la d√©marche plus qu‚Äôessayer de\ntrouver un bon mod√®le,\nnous allons √©galement r√©duire la taille des donn√©es pour acc√©l√©rer\nles calculs\n\nmutations2 = mutations2.groupby('dep').sample(frac = 0.5, random_state = 123)\n\nX_train, X_test, y_train, y_test = train_test_split(mutations2[xvars2], mutations2[[\"Valeur_fonciere\"]].values.ravel(), test_size = 0.2, random_state = 0, stratify=mutations2[['dep']])\n\nX = pd.concat((X_train, X_test), axis=0)\nY = np.concatenate([y_train,y_test])\n\nNous allons nous contenter de jouer sur les param√®tres:\n\nn_estimators: Le nombre d‚Äôarbres de d√©cision que notre for√™t contient\nmax_depth: La profondeur de chaque arbre\n\nIl existe plusieurs mani√®res de faire de la validation crois√©e. Nous allons ici\nutiliser la grid search qui consiste √† estimer et tester le mod√®le sur chaque\ncombinaison d‚Äôune grille de param√®tres et s√©lectionner le couple de valeurs\ndes hyperparam√®tres amenant √† la meilleure pr√©diction. Par d√©faut, scikit\neffectue une 5-fold cross validation. Nous n‚Äôallons pas changer\nce comportement.\nComme expliqu√© pr√©c√©demment, les param√®tres s‚Äôappelent sous la forme\n&lt;step&gt;__&lt;parameter_name&gt;\nLa validation crois√©e pouvant √™tre tr√®s consommatrice de temps, nous\nn‚Äôallons l‚Äôeffectuer que sur un nombre r√©duit de valeurs de notre grille.\nIl est possible de passer la liste des valeurs √† passer au crible sous\nforme de liste (comme pour l‚Äôargument max_depth ci-dessous) ou\nsous forme d‚Äôarray (comme pour l‚Äôargument n_estimators) ce qui est\nsouvent pratique pour g√©n√©rer un criblage d‚Äôun intervalle avec np.linspace.\nOn peut r√©cup√©rer les param√®tres optimaux avec la m√©thode best_params_:\n\ngrid_search.best_params_\n\nOn pourra aussi r√©-utiliser le mod√®le optimal de la mani√®re suivante :\ngrid_search.best_estimator_\nToutes les performances sur les ensembles d‚Äô√©chantillons et de test sur la grille\nd‚Äôhyperparam√®tres sont disponibles dans l‚Äôattribut:\n\nperf_random_forest = pd.DataFrame(grid_search.cv_results_)\n\nRegardons les r√©sultats moyens pour chaque valeur des hyperparam√®tres:\nGlobalement, √† profondeur d‚Äôarbre donn√©e, le nombre d‚Äôarbres change\nmarginalement la performance (cela d√©t√©riore\nla performance quand la profondeur est de 4, cela am√©liore quand\non fixe la profondeur de 2).\nEn revanche, changer la profondeur de l‚Äôarbre am√©liore la\nperformance de mani√®re plus marqu√©e.\nMaintenant, il nous reste √† re-entra√Æner le mod√®le avec ces nouveaux\nparam√®tres sur l‚Äôensemble du jeu de train et l‚Äô√©valuer sur l‚Äôensemble\ndu jeu de test:\nOn obtient le RMSE suivant :\n\nprint(\"Le RMSE sur le jeu de test est {:,}\".format(\n   int(np.sqrt(mean_squared_error(y_test, pipe_optimal.predict(X_test))))\n))\n\nEt si on regarde la qualit√© en pr√©diction:\n\ng.figure.get_figure()\n\nOn obtient plus de variance dans la pr√©diction, c‚Äôest d√©j√† un peu mieux.\nCependant, cela reste d√©cevant pour plusieurs raisons:\n\nnous n‚Äôavons pas fait d‚Äô√©tape de s√©lection de variable\nnous n‚Äôavons pas chercher √† d√©terminer si la variable √† pr√©dire la plus\npertinente √©tait le prix ou une transformation de celle-ci\n(par exemple le prix au \\(m^2\\))\n\n\nfeatures_names=pipe_optimal['preprocessor'].get_feature_names_out()\nimportances = pipe_optimal['randomforest'].feature_importances_\nstd = np.std([tree.feature_importances_ for tree in pipe_optimal['randomforest'].estimators_], axis=0)\n\nforest_importances = pd.Series(importances[importances&gt;0], index=features_names[importances&gt;0])\n\nfig, ax = plt.subplots()\nforest_importances.plot.bar(yerr=std[importances&gt;0], ax=ax)\nax.set_title(\"Feature importances using MDI\")\nax.set_ylabel(\"Mean decrease in impurity\")\nfig.tight_layout()\n\n\nRemarque sur la performance\nLes estimations sont, par d√©faut, men√©es de mani√®re s√©quentielle (l‚Äôune apr√®s\nl‚Äôautre). Nous sommes cependant face √† un probl√®me\nembarassingly parallel.\nPour gagner en performance, il est recommand√© d‚Äôutiliser l‚Äôargument\nn_jobs=-1.\n\n\nRemarque sur l‚Äô√©chantillonnage\nEn l‚Äô√©tat actuel de l‚Äô√©chantillonnage entre train et test au sein de la\ngrid search,\non est face √† un probl√®me de data leaking car l‚Äô√©chantillon\nn‚Äôest pas balanc√© entre nos classes (les d√©partements).\nCertaines classes se\nretrouvent hors de l‚Äô√©chantillon d‚Äôestimation mais dans l‚Äô√©chantillon de pr√©diction.\nAutrement dit, notre pipeline de preprocessing se retrouve √† devoir\nnettoyer des valeurs qu‚Äôil ne conna√Æt pas.\nNous avons choisi une option, dans notre pipeline pour se faciliter la vie\n√† ce propos. Nous ne rencontrons pas d‚Äôerreur car nous avons utilis√© l‚Äôoption\nhandle_unknown = \"ignore\" plut√¥t que\nhandle_unknown = \"error\" (d√©faut) dans le one hot encoding.\nCette option est dangereuse et n‚Äôest pas recommand√©e pour un vrai pipeline.\nDe mani√®re g√©n√©rale, il vaut mieux adopter une approche de\nprogrammation d√©fensive en n‚Äôh√©sitant pas √† renvoyer une erreur si la\nstructure du DataFrame de pr√©diction diff√®re vraiment de celle du DataFrame\nd‚Äôentra√Ænement.\nPour √©viter cette erreur, il serait mieux de d√©finir explicitement le sch√©ma de\nvalidation crois√©e √† mettre en oeuvre.\nPr√©c√©demment, nous avions utilis√© un √©chantillonnage stratifi√©.\nCela pourrait √™tre fait ici avec\nla m√©thode StratifiedGroupKFold (plus d‚Äô√©l√©ments √† venir)\nfrom sklearn.model_selection import StratifiedGroupKFold\ncv = StratifiedGroupKFold(n_splits=5)\n#grid_search.fit(pd.concat((X_train, X_test), axis=0), np.concatenate([y_train,y_test]), cv = cv, groups = pd.concat((X_train, X_test), axis=0)['dep'])"
  },
  {
    "objectID": "content/modelisation/6_pipeline.html#el√©ments-suppl√©mentaires-√†-venir",
    "href": "content/modelisation/6_pipeline.html#el√©ments-suppl√©mentaires-√†-venir",
    "title": "Premier pas vers l‚Äôindustrialisation avec les pipelines scikit",
    "section": "El√©ments suppl√©mentaires √† venir",
    "text": "El√©ments suppl√©mentaires √† venir\nCe chapitre est amen√© √† √™tre enrichi des √©l√©ments suivants\n(cf.¬†#207)\n\nComparaison performance entre mod√®les gr√¢ce aux pipelines\nInt√©gration d‚Äôune √©tape de s√©lection de variable dans un pipeline\nstatsmodels dans un pipeline\nKeras dans un pipeline"
  },
  {
    "objectID": "content/modelisation/6_pipeline.html#annexes-pipelines-alternatifs",
    "href": "content/modelisation/6_pipeline.html#annexes-pipelines-alternatifs",
    "title": "Premier pas vers l‚Äôindustrialisation avec les pipelines scikit",
    "section": "Annexes : pipelines alternatifs",
    "text": "Annexes : pipelines alternatifs\n\nPr√©alable : quelques m√©thodes pour gagner en flexibilit√© dans le preprocessing\nNotre DataFrame comporte des types h√©t√©rog√®nes de variables:\n\nDes variables num√©riques dont les variances sont tr√®s h√©t√©rog√®nes\nDes variables textuelles qui m√©riteraient un recodage sous forme num√©rique\nDes variables discr√®tes dont les modalit√©s devraient √™tre √©clat√©es (one hot encoding)\n\nPour gagner en flexibilit√©, nous allons proposer certaines m√©thodes qui permettent\nd‚Äôappliquer les √©tapes de preprocessing ad√©quates √† un sous-ensemble de\nvariables5. Ces m√©thodes ne sont plus n√©cessaires dans les versions\nr√©centes de scikit.\nPour cela, il convient d‚Äôadopter l‚Äôapproche de la programmation orient√©e objet.\nOn va cr√©er des classes avec des m√©thodes transform et fit_transform\nqui pourront ainsi √™tre int√©gr√©es directement dans les pipelines, comme s‚Äôil\ns‚Äôagissait de m√©thodes issues de scikit.\nLa premi√®re g√©n√©ralise LabelEncoder √† un sous-ensemble de colonnes:\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nclass MultiColumnLabelEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)\n\nLa seconde g√©n√©ralise cette fois le one hot encoding √† un sous ensemble de\nfonctions\n\nclass MultiColumnOneHotEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                output[col] = OneHotEncoder(sparse=False).fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = OneHotEncoder(sparse=False).fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)\n\nLes m√©thodes suivantes vont nous permettre de passer en arguments les noms\nde colonnes pour int√©grer la r√©cup√©ration des bonnes colonnes de nos\ndataframes dans le pipeline:\n\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n\nclass Columns(BaseEstimator, TransformerMixin):\n    def __init__(self, names=None):\n        self.names = names\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X):\n        return X[self.names]\n\nclass Normalize(BaseEstimator, TransformerMixin):\n    def __init__(self, func=None, func_param={}):\n        self.func = func\n        self.func_param = func_param\n\n    def transform(self, X):\n        if self.func != None:\n            return self.func(X, **self.func_param)\n        else:\n            return X\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\nEnfin, on va cr√©er une m√©thode interm√©diaire sous forme de hack\n(elle prend une matrice en entr√©e et renvoie la m√™me matrice)\npour\npouvoir facilement r√©cup√©rer notre matrice de feature afin de v√©rifier\nses caract√©ristiques (notamment le nombre de colonnes disponibles):\n\nclass Collect(BaseEstimator, TransformerMixin):\n\n    def transform(self, X):\n        #print(X.shape)\n        #self.shape = shape\n        # what other output you want\n        return X\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n\nfrom sklearn.pipeline import make_pipeline, FeatureUnion\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\n\npipe2 = Pipeline([\n    (\"features\", FeatureUnion([\n        ('numeric', make_pipeline(Columns(names=numeric_features[:-1]),StandardScaler())),\n        ('categorical', make_pipeline(Columns(names=categorical_features),OneHotEncoder(sparse=False)))\n    ])),\n    ('identity', Collect()),\n    ('randomforest', regr)\n])\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('numeric', StandardScaler(), numeric_features[:-1]),\n        ('categorical', OneHotEncoder(sparse=False, handle_unknown = \"ignore\"), categorical_features)])\n\npipe3 = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('randomforest', regr)])"
  },
  {
    "objectID": "content/modelisation/6_pipeline.html#r√©f√©rences",
    "href": "content/modelisation/6_pipeline.html#r√©f√©rences",
    "title": "Premier pas vers l‚Äôindustrialisation avec les pipelines scikit",
    "section": "R√©f√©rences",
    "text": "R√©f√©rences\n\nBreiman L (2001). ‚ÄúRandom Forests‚Äù. Machine Learning. 45 (1): 5‚Äì32."
  },
  {
    "objectID": "content/modelisation/6_pipeline.html#footnotes",
    "href": "content/modelisation/6_pipeline.html#footnotes",
    "title": "Premier pas vers l‚Äôindustrialisation avec les pipelines scikit",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEn machine learning on retrouve un principe inspir√© du\nbootstrap\nqui permet d‚Äôagr√©ger un ensemble d‚Äôestimateurs en un estimateur ‚Äúmoyennis√©‚Äù.\nIl s‚Äôagit du bagging.\nEn √©conom√©trie, le bootstrap consiste √† r√©-estimer sur K sous-√©chantillons\nal√©atoires des donn√©es un estimateur afin d‚Äôen tirer, par exemple, un intervalle\nde confiance empirique √† 95%. Le principe du bagging est le m√™me. On r√©-estime\nK fois notre estimateur (par exemple un arbre de d√©cision) et propose une\nr√®gle d‚Äôagr√©gation pour en tirer une r√®gle moyennis√©e et donc une pr√©diction.‚Ü©Ô∏é\nExtrait de ce blog:\nGini importance (or mean decrease impurity), which is computed from the Random Forest structure. Let‚Äôs look how the Random Forest is constructed. It is a set of Decision Trees. Each Decision Tree is a set of internal nodes and leaves. In the internal node, the selected feature is used to make decision how to divide the data set into two separate sets with similars responses within. The features for internal nodes are selected with some criterion, which for classification tasks can be gini impurity or infomation gain, and for regression is variance reduction. We can measure how each feature decrease the impurity of the split (the feature with highest decrease is selected for internal node). For each feature we can collect how on average it decreases the impurity. The average over all trees in the forest is the measure of the feature importance.‚Ü©Ô∏é\nVoir ce thread stackoverflow‚Ü©Ô∏é\nExtrait de ce blog:\nGini importance (or mean decrease impurity), which is computed from the Random Forest structure. Let‚Äôs look how the Random Forest is constructed. It is a set of Decision Trees. Each Decision Tree is a set of internal nodes and leaves. In the internal node, the selected feature is used to make decision how to divide the data set into two separate sets with similars responses within. The features for internal nodes are selected with some criterion, which for classification tasks can be gini impurity or infomation gain, and for regression is variance reduction. We can measure how each feature decrease the impurity of the split (the feature with highest decrease is selected for internal node). For each feature we can collect how on average it decreases the impurity. The average over all trees in the forest is the measure of the feature importance.‚Ü©Ô∏é\nUn certain nombre des √©l√©ments suivants ont √©t√© glann√©s, par ci par l√†,\ndepuis stackoverflow.‚Ü©Ô∏é"
  },
  {
    "objectID": "content/NLP/01_intro.html",
    "href": "content/NLP/01_intro.html",
    "title": "Quelques √©l√©ments pour comprendre les enjeux du NLP",
    "section": "",
    "text": "Le NLP est un domaine immense de recherche. Cette page est une introduction\nfort incompl√®te √† la question. Il s‚Äôagit de montrer la logique, quelques exemples\navec Python \net s‚Äôamuser avec comme base d‚Äôexemple un livre formidable :books: :\nLe Comte de Monte Cristo.\nDans le cadre de l‚Äôintroduction au NLP que vous pouvez retrouver dans\nles diff√©rents chapitres, nous √©voquons principalement les champs suivants du NLP:\nCela laisse de c√¥t√© des champs tr√®s actifs de recherche\ndu NLP, notamment l‚Äôanalyse de sentiment ou les mod√®les de\nlangage (mod√®les GPT par exemple). Les outils d√©couverts\ndans cette partie du cours permettront, si vous le d√©sirez,\nde b√©n√©ficier d‚Äôune base solide pour approfondir tel ou tel\nsujet."
  },
  {
    "objectID": "content/NLP/01_intro.html#base-dexemple",
    "href": "content/NLP/01_intro.html#base-dexemple",
    "title": "Quelques √©l√©ments pour comprendre les enjeux du NLP",
    "section": "Base d‚Äôexemple",
    "text": "Base d‚Äôexemple\nLa base d‚Äôexemple est le Comte de Monte Cristo d‚ÄôAlexandre Dumas.\nIl est disponible\ngratuitement sur le site\nProject Gutemberg comme des milliers\nd‚Äôautres livres du domaine public. La mani√®re la plus simple de le r√©cup√©rer\nest de t√©l√©charger avec le package request le fichier texte et le retravailler\nl√©g√®rement pour ne conserver que le corpus du livre :\n\nfrom urllib import request\n\nurl = \"https://www.gutenberg.org/files/17989/17989-0.txt\"\nresponse = request.urlopen(url)\nraw = response.read().decode('utf8')\n\ndumas = raw.split(\"*** START OF THE PROJECT GUTENBERG EBOOK LE COMTE DE MONTE-CRISTO, TOME I ***\")[1].split(\"*** END OF THE PROJECT GUTENBERG EBOOK LE COMTE DE MONTE-CRISTO, TOME I ***\")[0]\n\nimport re\n\ndef clean_text(text):\n    text = text.lower() # mettre les mots en minuscule\n    text = \" \".join(text.split())\n    return text\n\ndumas = clean_text(dumas)\n\ndumas[10000:10500]\n\n\" mes yeux. --vous avez donc vu l'empereur aussi? --il est entr√© chez le mar√©chal pendant que j'y √©tais. --et vous lui avez parl√©? --c'est-√†-dire que c'est lui qui m'a parl√©, monsieur, dit dant√®s en souriant. --et que vous a-t-il dit? --il m'a fait des questions sur le b√¢timent, sur l'√©poque de son d√©part pour marseille, sur la route qu'il avait suivie et sur la cargaison qu'il portait. je crois que s'il e√ªt √©t√© vide, et que j'en eusse √©t√© le ma√Ætre, son intention e√ªt √©t√© de l'acheter; mais je lu\""
  },
  {
    "objectID": "content/NLP/01_intro.html#la-particularit√©-des-donn√©es-textuelles",
    "href": "content/NLP/01_intro.html#la-particularit√©-des-donn√©es-textuelles",
    "title": "Quelques √©l√©ments pour comprendre les enjeux du NLP",
    "section": "La particularit√© des donn√©es textuelles",
    "text": "La particularit√© des donn√©es textuelles\n\nObjectif\nLe natural language processing (NLP) ou\ntraitement automatis√© de la langue (TAL) en Fran√ßais,\nvise √† extraire de l‚Äôinformation de textes √† partir d‚Äôune analyse statistique du contenu.\nCette d√©finition permet d‚Äôinclure de nombreux champs d‚Äôapplications au sein\ndu NLP (traduction, analyse de sentiment, recommandation, surveillance, etc. ) ainsi que de m√©thodes.\nCette approche implique de transformer un texte, qui est une information compr√©hensible par un humain, en un nombre, information appropri√©e pour un ordinateur et une approche statistique ou algorithmique.\nTransformer une information textuelle en valeurs num√©riques propres √† une analyse statistique n‚Äôest pas une t√¢che √©vidente. Les donn√©es textuelles sont non structur√©es puisque l‚Äôinformation cherch√©e, qui est propre √† chaque analyse, est perdue au milieu d‚Äôune grande masse d‚Äôinformations qui doit, de plus, √™tre interpr√©t√©e dans un certain contexte (un m√™me mot ou une phrase n‚Äôayant pas la m√™me signification selon le contexte).\nSi cette t√¢che n‚Äô√©tait pas assez difficile comme √ßa, on peut ajouter d‚Äôautres difficult√©s propres √† l‚Äôanalyse textuelle car ces donn√©es sont :\n\nbruit√©es : ortographe, fautes de frappe‚Ä¶\nchangeantes : la langue √©volue avec de nouveaux mots, sens‚Ä¶\ncomplexes : structures variables, accords‚Ä¶\nambigues : synonymie, polys√©mie, sens cach√©‚Ä¶\npropres √† chaque langue : il n‚Äôexiste pas de r√®gle de passage unique entre deux langues\ngrande dimension : des combinaisons infinies de s√©quences de mots\n\n\n\nM√©thode\nL‚Äôunit√© textuelle peut √™tre le mot ou encore une s√©quence de n\nmots (un n-gramme) ou encore une cha√Æne de caract√®res (e.g.¬†la\nponctuation peut √™tre signifiante). On parle de token. L‚Äôanalyse textuelle vise √† transformer le texte en donn√©es\nnum√©riques manipulables.\nOn peut ensuite utiliser diverses techniques (clustering,\nclassification supervis√©e) suivant l‚Äôobjectif poursuivi pour exploiter\nl‚Äôinformation transform√©e. Mais les √©tapes de nettoyage de texte sont indispensables car sinon un algorithme sera incapable de d√©tecter une information pertinente dans l‚Äôinfini des possibles."
  },
  {
    "objectID": "content/NLP/01_intro.html#nettoyer-un-texte",
    "href": "content/NLP/01_intro.html#nettoyer-un-texte",
    "title": "Quelques √©l√©ments pour comprendre les enjeux du NLP",
    "section": "Nettoyer un texte",
    "text": "Nettoyer un texte\nLes wordclouds sont des repr√©sentations graphiques assez pratiques pour visualiser\nles mots les plus fr√©quents. Elles sont tr√®s simples √† impl√©menter en Python\navec le module wordcloud qui permet m√™me d‚Äôajuster la forme du nuage √†\nune image :\n\n\nimport wordcloud\nimport numpy as np\nimport io\nimport requests\nimport PIL\nimport matplotlib.pyplot as plt\n\nimg = \"https://raw.githubusercontent.com/linogaliana/python-datascientist/master/content/NLP/book.png\"\nbook_mask = np.array(PIL.Image.open(io.BytesIO(requests.get(img).content)))\n\nfig = plt.figure()\n\ndef make_wordcloud(corpus):\n    wc = wordcloud.WordCloud(background_color=\"white\", max_words=2000, mask=book_mask, contour_width=3, contour_color='steelblue')\n    wc.generate(corpus)\n    return wc\n\nplt.imshow(make_wordcloud(dumas), interpolation='bilinear')\nplt.axis(\"off\")\n#plt.show()\n#plt.savefig('word.png', bbox_inches='tight')\n\n\n(-0.5, 1429.5, 783.5, -0.5)\n(a) Nuage de mot produit √† partir du Comte de Monte Cristo\n\n\n\n\n\n\n(b)\n\n\n\nFigure¬†1: ?(caption)\n\n\nCela montre clairement qu‚Äôil est n√©cessaire de nettoyer notre texte. Le nom\ndu personnage principal, Dant√®s, est ainsi masqu√© par un certain nombre\nd‚Äôarticles ou mots de liaison qui perturbent l‚Äôanalyse. Ces mots sont des\nstop-words. La librairie NLTK (Natural Language ToolKit), librairie\nde r√©f√©rence dans le domaine du NLP, permet de facilement retirer ces\nstopwords (cela pourrait √©galement √™tre fait avec\nla librairie plus r√©cente, spaCy). Avant cela, il est n√©cessaire\nde transformer notre texte en le d√©coupant par unit√©s fondamentales (les tokens).\nLes exemples suivants, extraits de Galiana and Castillo (2022), montrent l‚Äôint√©r√™t du\nnettoyage de textes lorsqu‚Äôon d√©sire comparer des corpus\nentre eux. En l‚Äôoccurrence, il s‚Äôagit de comparer un corpus de\nnoms de produits dans des collectes automatis√©es de produits\nde supermarch√© (scanner-data) avec des noms de produits\ndans les donn√©es de l‚ÄôOpenFoodFacts, une base de donn√©es\ncontributive. Sans nettoyage, le bruit l‚Äôemporte sur le signal\net il est impossible de d√©celer des similarit√©s entre les jeux\nde donn√©es. Le nettoyage permet d‚Äôharmoniser\nun peu ces jeux de donn√©es pour avoir une chance d‚Äô√™tre en\nmesure de les comparer.\n\n\n\n\n\nOpenFoodFacts avant nettoyage\n\n\n\n\n\nScanner-data avant nettoyage\n\n\n\n\n\n\n\n\n\nOpenFoodFacts apr√®s nettoyage\n\n\n\n\n\nScanner-data apr√®s nettoyage\n\n\n\n\n\nTokenisation\n\n\n Hint\nLors de la premi√®re utilisation de NLTK, il est n√©cessaire de t√©l√©charger\nquelques √©l√©ments n√©cessaires √† la tokenisation, notamment la ponctuation.\nPour cela, il est recommand√© d‚Äôutiliser la commande suivante :\nimport nltk\nnltk.download('punkt')\n\n\n\nimport nltk\nnltk.download('punkt')\n\n[nltk_data] Downloading package punkt to /github/home/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n\n\nTrue\n\n\nLa tokenisation consiste √† d√©couper un texte en morceaux. Ces morceaux\npourraient √™tre des phrases, des chapitres, des n-grammes ou des mots. C‚Äôest\ncette derni√®re option que l‚Äôon va choisir, plus simple pour retirer les\nstopwords :\n\nimport nltk\n\nwords = nltk.word_tokenize(dumas, language='french')\nwords[1030:1050]\n\n['que',\n 'voulez-vous',\n ',',\n 'monsieur',\n 'edmond',\n ',',\n 'reprit',\n \"l'armateur\",\n 'qui',\n 'paraissait',\n 'se',\n 'consoler',\n 'de',\n 'plus',\n 'en',\n 'plus',\n ',',\n 'nous',\n 'sommes',\n 'tous']\n\n\nOn remarque que les mots avec apostrophes sont li√©s en un seul, ce qui est\npeut-√™tre faux sur le plan de la grammaire mais peu avoir un sens pour une\nanalyse statistique. Il reste des signes de ponctuation qu‚Äôon peut √©liminer\navec la m√©thode isalpha:\n\nwords = [word for word in words if word.isalpha()]\nwords[1030:1050]\n\n['assez',\n 'sombre',\n 'obs√©quieux',\n 'envers',\n 'ses',\n 'sup√©rieurs',\n 'insolent',\n 'envers',\n 'ses',\n 'subordonn√©s',\n 'aussi',\n 'outre',\n 'son',\n 'titre',\n 'comptable',\n 'qui',\n 'est',\n 'toujours',\n 'un',\n 'motif']\n\n\nComme indiqu√© ci-dessus, pour t√©l√©charger\nle corpus de ponctuation, il est\nn√©cessaire d‚Äôex√©cuter la ligne de\ncommande suivante :\n\n\nRetirer les stop-words\nLe jeu de donn√©es est maintenant propre. On peut d√©sormais retirer les\nmots qui n‚Äôapportent pas de sens et servent seulement √† faire le\nlien entre deux pr√©positions. On appelle ces mots des\nstop words dans le domaine du NLP.\n\n\n Hint\nLors de la premi√®re utilisation de NLTK, il est n√©cessaire de t√©l√©charger\nles stopwords.\nimport nltk\nnltk.download('stopwords')\n\n\nComme indiqu√© ci-dessus, pour t√©l√©charger\nle corpus de stopwords1, il est\nn√©cessaire d‚Äôex√©cuter la ligne de\ncommande suivante :\n\nimport nltk\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to /github/home/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n\n\nTrue\n\n\n\nfrom nltk.corpus import stopwords\nprint(stopwords.words(\"french\"))\n\nstop_words = set(stopwords.words('french'))\n\n\nwords = [w for w in words if not w in stop_words]\nprint(words[1030:1050])\n\n['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'm√™me', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', '√†', 'm', 'n', 's', 't', 'y', '√©t√©', '√©t√©e', '√©t√©es', '√©t√©s', '√©tant', '√©tante', '√©tants', '√©tantes', 'suis', 'es', 'est', 'sommes', '√™tes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', '√©tais', '√©tait', '√©tions', '√©tiez', '√©taient', 'fus', 'fut', 'f√ªmes', 'f√ªtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'f√ªt', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'e√ªmes', 'e√ªtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'e√ªt', 'eussions', 'eussiez', 'eussent']\n['celui', 'dant√®s', 'a', 'd√©pos√©', 'passant', 'comment', 'paquet', 'd√©poser', 'danglars', 'rougit', 'passais', 'devant', 'porte', 'capitaine', 'entrouverte', 'vu', 'remettre', 'paquet', 'cette', 'lettre']\n\n\nCes retraitements commencent √† porter leurs fruits puisque des mots ayant plus\nde sens commencent √† se d√©gager, notamment les noms des personnages\n(Fernand, Merc√©d√®s, Villefort, etc.)\n\nwc = make_wordcloud(' '.join(words))\n\nfig = plt.figure()\n\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\n\n(-0.5, 1429.5, 783.5, -0.5)\n\n\n\n\n\n\n\n\n\n\n\nStemming\nPour r√©duire la complexit√© d‚Äôun texte, on peut tirer partie de\n‚Äúclasses d‚Äô√©quivalence‚Äù : on peut\nconsid√©rer que diff√©rentes formes d‚Äôun m√™me mot (pluriel,\nsingulier, conjugaison) sont √©quivalentes et les remplacer par une\nm√™me forme dite canonique. Il existe deux approches dans le domaine :\n\nla lemmatisation qui requiert la connaissance des statuts\ngrammaticaux (exemple : chevaux devient cheval)\nla racinisation (stemming) plus fruste mais plus rapide, notamment\nen pr√©sence de fautes d‚Äôorthographes. Dans ce cas, chevaux peut devenir chev\nmais √™tre ainsi confondu avec chevet ou cheveux\n\nLa racinisation est plus simple √† mettre en oeuvre car elle peut s‚Äôappuyer sur\ndes r√®gles simples pour extraire la racine d‚Äôun mot.\nPour r√©duire un mot dans sa forme ‚Äúracine‚Äù, c‚Äôest-√†-dire en s‚Äôabstrayant des\nconjugaisons ou variations comme les pluriels, on applique une m√©thode de\nstemming. Le but du stemming est de regrouper de\nnombreuses variantes d‚Äôun mot comme un seul et m√™me mot.\nPar exemple, une fois que l‚Äôon applique un stemming, ‚Äúchats‚Äù et ‚Äúchat‚Äù\ndeviennent un m√™me mot.\nCette approche a l‚Äôavantage de r√©duire la taille du vocabulaire √† ma√Ætriser\npour l‚Äôordinateur et le mod√©lisateur. Il existe plusieurs algorithmes de\nstemming, notamment le Porter Stemming Algorithm ou le\nSnowball Stemming Algorithm. Nous pouvons utiliser ce dernier en Fran√ßais :\n\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer(language='french')\n\nstemmed = [stemmer.stem(word) for word in words]\nprint(stemmed[1030:1050])\n\n['celui', 'dantes', 'a', 'd√©pos', 'pass', 'comment', 'paquet', 'd√©pos', 'danglar', 'roug', 'pass', 'dev', 'port', 'capitain', 'entrouvert', 'vu', 'remettr', 'paquet', 'cet', 'lettr']\n\n\nA ce niveau, les mots commencent √† √™tre moins intelligibles par un humain.\nLa machine prendra le relais, on lui a pr√©par√© le travail\n\n\n Note\nIl existe aussi le stemmer suivant :\nfrom nltk.stem.snowball import FrenchStemmer\nstemmer = FrenchStemmer()"
  },
  {
    "objectID": "content/NLP/01_intro.html#reconnaissance-des-entit√©s-nomm√©es",
    "href": "content/NLP/01_intro.html#reconnaissance-des-entit√©s-nomm√©es",
    "title": "Quelques √©l√©ments pour comprendre les enjeux du NLP",
    "section": "Reconnaissance des entit√©s nomm√©es",
    "text": "Reconnaissance des entit√©s nomm√©es\nCette √©tape n‚Äôest pas une √©tape de pr√©paration mais illustre la capacit√©\ndes librairies Python a extraire du sens d‚Äôun texte. La librairie\nspaCy permet de faire de la reconnaissance d‚Äôentit√©s nomm√©es, ce qui peut\n√™tre pratique pour extraire rapidement certains personnages de notre oeuvre.\n\n\nLa librairie spaCy\nNTLK est la librairie historique d‚Äôanalyse textuelle en Python. Elle existe\ndepuis les ann√©es 1990. L‚Äôutilisation industrielle du NLP dans le monde\nde la data science est n√©anmoins plus r√©cente et doit beaucoup √† la collecte\naccrue de donn√©es non structur√©es par les r√©seaux sociaux. Cela a amen√© √†\nun renouvelement du champ du NLP, tant dans le monde de la recherche que dans\nsa mise en application dans l‚Äôindustrie de la donn√©e.\nLe package spaCy est l‚Äôun des packages qui a permis\ncette industrialisation des m√©thodes de NLP. Con√ßu autour du concept\nde pipelines de donn√©es, il est beaucoup plus pratique √† mettre en oeuvre\npour une cha√Æne de traitement de donn√©es textuelles mettant en oeuvre\nplusieurs √©tapes de transformation des donn√©es.\n\n\n#!pip install deplacy\n#!python -m spacy download fr_core_news_sm\nimport spacy\n\nnlp=spacy.load(\"fr_core_news_sm\")\ndoc = nlp(dumas)\nimport spacy\nfrom spacy import displacy\ndisplacy.render(doc, style=\"ent\", jupyter=True)"
  },
  {
    "objectID": "content/NLP/01_intro.html#repr√©sentation-dun-texte-sous-forme-vectorielle",
    "href": "content/NLP/01_intro.html#repr√©sentation-dun-texte-sous-forme-vectorielle",
    "title": "Quelques √©l√©ments pour comprendre les enjeux du NLP",
    "section": "Repr√©sentation d‚Äôun texte sous forme vectorielle",
    "text": "Repr√©sentation d‚Äôun texte sous forme vectorielle\nUne fois nettoy√©, le texte est plus propice √† une repr√©sentation vectorielle.\nEn fait, implicitement, on a depuis le d√©but adopt√© une d√©marche bag of words.\nIl s‚Äôagit d‚Äôune repr√©sentation, sans souci de contexte (ordre des mots, contexte d‚Äôutilisation),\no√π chaque token repr√©sente un √©l√©ment dans un vocabulaire de taille \\(|V|\\).\nOn peut ainsi avoir une repr√©sentation matricielle les occurrences de\nchaque token dans plusieurs documents (par exemple plusieurs livres,\nchapitres, etc.) pour, par exemple, en d√©duire une forme de similarit√©.\nAfin de r√©duire la dimension de la matrice bag of words,\non peut s‚Äôappuyer sur des pond√©rations.\nOn √©limine ainsi certains mots tr√®s fr√©quents ou au contraire tr√®s rares.\nLa pond√©ration la plus simple est bas√©e sur la fr√©quence des mots dans le document.\nC‚Äôest l‚Äôobjet de la m√©trique tf-idf (term frequency - inverse document frequency)\nabord√©e dans un prochain chapitre."
  },
  {
    "objectID": "content/NLP/01_intro.html#footnotes",
    "href": "content/NLP/01_intro.html#footnotes",
    "title": "Quelques √©l√©ments pour comprendre les enjeux du NLP",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLe corpus de stop-words de NLTK\nest relativement limit√©. Il est recommand√©\nde privil√©gier celui de spaCy, plus\ncomplet, pour √©liminer plus de mots\nvalises.‚Ü©Ô∏é"
  },
  {
    "objectID": "content/NLP/03_lda.html",
    "href": "content/NLP/03_lda.html",
    "title": "Latent Dirichlet Allocation (LDA)",
    "section": "",
    "text": "Cette page approfondit les exercices pr√©sent√©s dans la\nsection pr√©c√©dente.\nOn va ainsi continuer notre exploration de la litt√©rature anglophones :\nLes donn√©es sont disponibles dans la base de\ndonn√©es spooky.csv et peuvent √™tre\nimport√©es par Python en utilisant directement l‚Äôurl\nhttps://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv.\nLe but va √™tre dans un premier temps de regarder dans le d√©tail les termes les plus fr√©quents utilis√©s par les auteurs, et les repr√©senter graphiquement.\nCe notebook est librement inspir√© de :\nLa LDA est une technique d‚Äôestimation bay√©sienne.\nLe cours d‚ÄôAlberto Brietti\nsur le sujet constitue une tr√®s bonne ressource pour comprendre\nles fondements de cette technique."
  },
  {
    "objectID": "content/NLP/03_lda.html#librairies-n√©cessaires",
    "href": "content/NLP/03_lda.html#librairies-n√©cessaires",
    "title": "Latent Dirichlet Allocation (LDA)",
    "section": "Librairies n√©cessaires",
    "text": "Librairies n√©cessaires\nCette page √©voquera les principales librairies pour faire du NLP, notamment :\n\nWordCloud\nNLTK\nSpaCy\nKeras\nTensorFlow\n\n\n\n Hint\nComme dans la partie pr√©c√©dente, il faut t√©l√©charger quelques\n√©l√©ments pour que NTLK puisse fonctionner correctement. Pour cela, faire:\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('genesis')\nnltk.download('omw-1.4')\n\n\nLa liste des modules √† importer est assez longue, la voici :\n\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('genesis')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n#from IPython.display import display\nimport base64\nimport string\nimport re\nimport nltk\n\nfrom collections import Counter\nfrom time import time\n# from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords\nfrom sklearn.metrics import log_loss\nimport matplotlib.pyplot as plt\n\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\n\n[nltk_data] Downloading package stopwords to /github/home/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /github/home/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package genesis to /github/home/nltk_data...\n[nltk_data]   Package genesis is already up-to-date!\n[nltk_data] Downloading package wordnet to /github/home/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /github/home/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!"
  },
  {
    "objectID": "content/NLP/03_lda.html#donn√©es-utilis√©es",
    "href": "content/NLP/03_lda.html#donn√©es-utilis√©es",
    "title": "Latent Dirichlet Allocation (LDA)",
    "section": "Donn√©es utilis√©es",
    "text": "Donn√©es utilis√©es\nSi vous avez d√©j√† lu la section pr√©c√©dente et import√© les donn√©es, vous\npouvez passer √† la section suivante\nLe code suivant permet d‚Äôimporter le jeu de donn√©es spooky:\n\nimport pandas as pd\n\nurl='https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv'\nimport pandas as pd\ntrain = pd.read_csv(url,\n                    encoding='latin-1')\ntrain.columns = train.columns.str.capitalize()\n                    \ntrain['ID'] = train['Id'].str.replace(\"id\",\"\")\ntrain = train.set_index('Id')\n\nLe jeu de donn√©es met ainsi en regard un auteur avec une phrase qu‚Äôil a √©crite:\n\ntrain.head()\n\n\n\n\n\n\n\n\nText\nAuthor\nID\n\n\nId\n\n\n\n\n\n\n\nid26305\nThis process, however, afforded me no means of...\nEAP\n26305\n\n\nid17569\nIt never once occurred to me that the fumbling...\nHPL\n17569\n\n\nid11008\nIn his left hand was a gold snuff box, from wh...\nEAP\n11008\n\n\nid27763\nHow lovely is spring As we looked from Windsor...\nMWS\n27763\n\n\nid12958\nFinding nothing else, not even gold, the Super...\nHPL\n12958\n\n\n\n\n\n\n\nLes √©tapes de preprocessing sont expliqu√©es dans le chapitre pr√©c√©dent. On applique les √©tapes suivantes :\n\nTokeniser\nRetirer la ponctuation et les stopwords\nLemmatiser le texte\n\n\nlemma = WordNetLemmatizer()\n\ntrain_clean = (train\n    .groupby([\"ID\",\"Author\"])\n    .apply(lambda s: nltk.word_tokenize(' '.join(s['Text'])))\n    .apply(lambda words: [word for word in words if word.isalpha()])\n)\n\nfrom nltk.corpus import stopwords  \nstop_words = set(stopwords.words('english'))\n\ntrain_clean = (train_clean\n    .apply(lambda words: [lemma.lemmatize(w) for w in words if not w in stop_words])\n    .reset_index(name='tokenized')\n)\n\ntrain_clean.head(2)\n\n\n\n\n\n\n\n\nID\nAuthor\ntokenized\n\n\n\n\n0\n00001\nMWS\n[Idris, well, content, resolve, mine]\n\n\n1\n00002\nHPL\n[I, faint, even, fainter, hateful, modernity, ..."
  },
  {
    "objectID": "content/NLP/03_lda.html#principe-de-la-lda-latent-dirichlet-allocation",
    "href": "content/NLP/03_lda.html#principe-de-la-lda-latent-dirichlet-allocation",
    "title": "Latent Dirichlet Allocation (LDA)",
    "section": "Principe de la LDA (Latent Dirichlet Allocation)",
    "text": "Principe de la LDA (Latent Dirichlet Allocation)\nLe mod√®le Latent Dirichlet Allocation (LDA) est un mod√®le probabiliste g√©n√©ratif qui permet\nde d√©crire des collections de documents de texte ou d‚Äôautres types de donn√©es discr√®tes. LDA fait\npartie d‚Äôune cat√©gorie de mod√®les appel√©s ‚Äútopic models‚Äù, qui cherchent √† d√©couvrir des structures\nth√©matiques cach√©es dans des vastes archives de documents.\nCeci permet d‚Äôobtenir des m√©thodes\nefficaces pour le traitement et l‚Äôorganisation des documents de ces archives : organisation automatique\ndes documents par sujet, recherche, compr√©hension et analyse du texte, ou m√™me r√©sumer des\ntextes.\nAujourd‚Äôhui, ce genre de m√©thodes s‚Äôutilisent fr√©quemment dans le web, par exemple pour\nanalyser des ensemble d‚Äôarticles d‚Äôactualit√©, les regrouper par sujet, faire de la recommandation\nd‚Äôarticles, etc.\nLa LDA est une m√©thode qui consid√®re les corpus comme des m√©langes de sujets et\nde mots. Chaque document peut √™tre repr√©sent√© comme le r√©sultat d‚Äôun m√©lange :\n\nde sujets\net, au sein de ces sujets, d‚Äôun choix de mots.\n\nL‚Äôestimation des\nparam√®tres de la LDA passe par l‚Äôestimation des distributions des variables\nlatentes √† partir des donn√©es observ√©es (posterior inference).\nMath√©matiquement, on peut se repr√©senter la LDA comme une\ntechnique de maximisation de log vraisemblance avec un algorithme EM (expectation maximisation)\ndans un mod√®le de m√©lange.\nLa matrice termes-documents qui sert de point de d√©part est la suivante :\n\n\n\n\nword_1\nword_2\nword_3\n‚Ä¶\nword_J\n\n\n\n\ndoc_1\n3\n0\n1\n‚Ä¶\n0\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\ndoc_N\n1\n0\n0\n‚Ä¶\n5\n\n\n\nOn dit que cette matrice est sparse (creuse en Fran√ßais) car elle contient principalement des 0. En effet, un document n‚Äôutilise qu‚Äôune partie mineure du vocabulaire complet.\nLa LDA consiste √† transformer cette matrice sparsedocument-terme en deux matrices de moindre dimension:\n\nUne matrice document-sujet\nUne matrice sujet-mots\n\nEn notant \\(K_i\\) le sujet \\(i\\). On obtient donc\n\nUne matrice document-sujet ayant la structure suivante :\n\n\n\n\n\nK_1\nK_2\nK_3\n‚Ä¶\nK_M\n\n\n\n\ndoc_1\n1\n0\n1\n‚Ä¶\n0\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\ndoc_N\n1\n1\n1\n‚Ä¶\n0\n\n\n\n\nUne matrice sujets-mots ayant la structure suivante :\n\n\n\n\n\nword_1\nword_2\nword_3\n‚Ä¶\nword_J\n\n\n\n\nK_1\n1\n0\n0\n‚Ä¶\n0\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\nK_M\n1\n1\n1\n‚Ä¶\n0\n\n\n\nCes deux matrices ont l‚Äôinterpr√©tation suivante :\n\nLa premi√®re nous renseigne sur la pr√©sence d‚Äôun sujet dans un document\nLa seconde nous renseigne sur la pr√©sence d‚Äôun mot dans un sujet\n\nEn fait, le principe de la LDA est de construire ces deux matrices √† partir des fr√©quences d‚Äôapparition des mots dans le texte.\nOn va se concentrer sur Edgar Allan Poe.\n\ncorpus = train_clean[train_clean[\"Author\"] == \"EAP\"]"
  },
  {
    "objectID": "content/NLP/03_lda.html#entra√Æner-une-lda",
    "href": "content/NLP/03_lda.html#entra√Æner-une-lda",
    "title": "Latent Dirichlet Allocation (LDA)",
    "section": "Entra√Æner une LDA",
    "text": "Entra√Æner une LDA\nIl existe plusieurs mani√®res d‚Äôentra√Æner une LDA.\nNous allons utiliser Scikit ici avec la m√©thode LatentDirichletAllocation.\nComme expliqu√© dans la partie mod√©lisation :\n\nOn initialise le mod√®le ;\nOn le met √† jour avec la m√©thode fit.\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\n# Initialise the count vectorizer with the English stop words\ncount_vectorizer = CountVectorizer(stop_words='english')# Fit and transform the processed titles\ncount_data = count_vectorizer.fit_transform(corpus['tokenized'].apply(lambda s: ' '.join(s)))\n\n# Tweak the two parameters below\nnumber_topics = 5\nnumber_words = 10# Create and fit the LDA model\nlda = LatentDirichletAllocation(n_components=11, max_iter=5,\n                                learning_method = 'online',\n                                learning_offset = 50.,\n                                random_state = 0,\n                                n_jobs = 1)\nlda.fit(count_data)"
  },
  {
    "objectID": "content/NLP/03_lda.html#visualiser-les-r√©sultats",
    "href": "content/NLP/03_lda.html#visualiser-les-r√©sultats",
    "title": "Latent Dirichlet Allocation (LDA)",
    "section": "Visualiser les r√©sultats",
    "text": "Visualiser les r√©sultats\nOn peut d√©j√† commencer par utiliser une fonction pour afficher les\nr√©sultats :\n\n# Helper function\ndef print_topics(model, count_vectorizer, n_top_words):\n    words = count_vectorizer.get_feature_names_out()\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"\\nTopic #%d:\" % topic_idx)\n        print(\" \".join([words[i]\n                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n\nprint_topics(lda, count_vectorizer, number_words)\n\n\nTopic #0:\narm looking thousand respect hour table woman rest ah seen\n\nTopic #1:\nsaid dupin ha end write smith chair phenomenon quite john\n\nTopic #2:\ntime thing say body matter course day place object immediately\n\nTopic #3:\nmere memory felt sat movement case sole green principle bone\n\nTopic #4:\ndoor room open small friend lady replied night window hand\n\nTopic #5:\nword man day idea good point house shall mind say\n\nTopic #6:\neye figure form left sea hour ordinary life deep world\n\nTopic #7:\nfoot great little earth let le year nature come nearly\n\nTopic #8:\nhand strange head color hair spoken read ear ghastly neck\n\nTopic #9:\ncame looked shadow low dream like death light spirit tree\n\nTopic #10:\neye know heart saw character far tell oh voice wall\n\n\nLa repr√©sentation sous forme de liste de mots n‚Äôest pas la plus pratique‚Ä¶\nOn peut essayer de se repr√©senter un wordcloud de chaque sujet pour mieux voir si cette piste est pertinente :\n\ntf_feature_names = count_vectorizer.get_feature_names_out()\n\ndef wordcloud_lda(lda, tf_feature_names):\n\n  fig, axs = plt.subplots(len(lda.components_) // 3 + 1, 3)\n  \n  for i in range(len(lda.components_)):\n      corpus_lda = lda.components_[i]\n      first_topic_words = [tf_feature_names[l] for l in corpus_lda.argsort()[:-50-1:-1]]\n      k = i // 3\n      j = (i - k*3)\n      wordcloud = WordCloud(stopwords=stop_words, background_color=\"black\",width = 2500, height = 1800)\n      wordcloud = wordcloud.generate(\" \".join(first_topic_words))\n      axs[k][j].set_title(\"Wordcloud pour le \\nsujet {}\".format(i))\n      axs[k][j].axis('off')\n      axs[k][j].imshow(wordcloud)\n  \n  r = len(lda.components_) % 3\n  [fig.delaxes(axs[len(lda.components_) // 3,k-1]) for k in range(r+1, 3+1) if r != 0]\n\nwc = wordcloud_lda(lda, tf_feature_names)\nwc\n\n\n\n\n\n\n\n\n\nwc\n\nLe module pyLDAvis offre quelques visualisations bien pratiques lorsqu‚Äôon\nd√©sire repr√©senter de mani√®re synth√©tique les r√©sultats d‚Äôune LDA et observer la distribution sujet x mots.\n\n\n Hint\nDans un notebook faire :\nimport pyLDAvis.sklearn\n\npyLDAvis.enable_notebook()\nPour les utilisateurs de Windows, il est n√©cessaire d‚Äôajouter l‚Äôargument\nn_jobs = 1. Sinon, Python tente d‚Äôentra√Æner le mod√®le avec de la\nparall√©lisation. Le probl√®me est que les processus sont des FORKs, ce que\nWindows ne supporte pas. Sur un syst√®me Unix (Linux, Mac OS), on peut se passer de cet\nargument.\n\n\n\n#!pip install pyLDAvis #√† faire en haut du notebook sur colab\nimport pyLDAvis\nimport pyLDAvis.sklearn\n\n# pyLDAvis.enable_notebook()\nvis_data = pyLDAvis.sklearn.prepare(lda, count_data, count_vectorizer, n_jobs = 1)\npyLDAvis.display(vis_data)\n\n\nChaque bulle repr√©sente un sujet. Plus la bulle est grande, plus il y a de documents qui traitent de ce sujet.\n\nPlus les barres sont loin les unes des autres, plus elles sont diff√©rentes. Un bon mod√®le aura donc tendance √† avoir de grandes bulles qui ne se recoupent pas. Ce n‚Äôest pas vraiment le cas ici‚Ä¶\n\nLes barres bleues repr√©sentent la fr√©quence de chaque mot dans le corpus.\nLes barres rouges repr√©sentent une estimation du nombre de termes g√©n√©r√©s dans un sujet pr√©cis. La barre rouge la plus longue correspond au mot le plus utilis√© dans ce sujet."
  },
  {
    "objectID": "content/NLP/03_lda.html#r√©f√©rences",
    "href": "content/NLP/03_lda.html#r√©f√©rences",
    "title": "Latent Dirichlet Allocation (LDA)",
    "section": "R√©f√©rences",
    "text": "R√©f√©rences\n\nLe poly d‚ÄôAlberto Brietti"
  },
  {
    "objectID": "content/NLP/05_exo_supp.html",
    "href": "content/NLP/05_exo_supp.html",
    "title": "Exercices suppl√©mentaires",
    "section": "",
    "text": "Cette page approfondit certains aspects pr√©sent√©s dans les autres tutoriels. Il s‚Äôagit d‚Äôune suite d‚Äôexercice, avec corrections, pour pr√©senter d‚Äôautres aspects du NLP ou pratiquer sur des donn√©es diff√©rentes\n\nExploration des libell√©s de l‚Äôopenfood database\n{{% box status=‚Äúexercise‚Äù title=‚ÄúExercise: les noms de produits dans l‚Äôopenfood database‚Äù icon=‚Äúfas fa-pencil-alt‚Äù %}}\nL‚Äôobjectif de cet exercice est d‚Äôanalyser les termes les plus fr√©quents\ndans les noms de produits de l‚Äôopenfood database. Au passage, cela permet de r√©viser les √©tapes de preprocessing (LIEN XXXXX) et d‚Äôexplorer les enjeux de reconnaissance d‚Äôentit√©s nomm√©es.\n{{% /box %}}\nDans cet exercice:\n\ntokenisation (nltk)\nretrait des stop words (nltk)\nnuage de mots (wordcloud)\nreconnaissance du langage (fasttext)\nreconnaissance d‚Äôentit√©s nomm√©es (spacy)\n\nle tout sur l‚ÄôOpenFood Database, une base de donn√©es alimentaire qui est enrichie de mani√®re collaborative.\n{{% box status=‚Äúhint‚Äù title=‚ÄúHint‚Äù icon=‚Äúfa fa-lightbulb‚Äù %}}\nPour pouvoir utiliser les mod√®les pr√©-entra√Æn√©s de spaCy, il faut les t√©l√©charger. La m√©thode pr√©conis√©e est d‚Äôutiliser, depuis un terminal, la commande suivante\npython -m spacy download fr_core_news_sm\nDans un notebook jupyter, il se peut qu‚Äôil soit n√©cessaire de relancer le kernel.\nSi l‚Äôacc√®s √† la ligne de commande n‚Äôest pas possible, ou si la commande √©choue, il est possible de t√©l√©charger le mod√®le pr√©-entra√Æn√© directement depuis une session Python\nimport spacy\nspacy.cli.download('fr_core_news_sm')\n{{% /box %}}\n\nImporter le mod√®le de reconnaissance de langage qui sera utilis√© par la suite\nainsi que le corpus Fran√ßais utilis√© par spacy\n\n\nimport tempfile\nimport os\nimport spacy\n\ntemp_dir = tempfile.NamedTemporaryFile()\ntemp_dir = temp_dir.name\n\nos.system(\"wget -O {} https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\".format( \"%s.model.bin\" % temp_dir))\nspacy.cli.download('fr_core_news_sm')\n\n\nImporter les donn√©es de l‚Äôopenfood database √† partir du code suivant\n\n\nimport pandas as pd\nimport urllib.request\n\n\nurllib.request.urlretrieve('https://static.openfoodfacts.org/data/en.openfoodfacts.org.products.csv', \"%s.openfood.csv\" % temp_dir)\ndf_openfood = pd.read_csv(\"%s.openfood.csv\" % temp_dir, delimiter=\"\\t\",\n                          usecols=['product_name'], encoding = 'utf-8', dtype = \"str\")\n\nCes donn√©es devraient avoir l‚Äôaspect suivant :\n\ndf_openfood.iloc[:2, :5]\n\n\nCr√©er une fonction de nettoyage des noms de produits effectuant les\n√©tapes suivantes :\n\n\ntokeniser le texte en question\nretirer la ponctuation et les stopwords\n\nAppliquer cette fonction √† l‚Äôensemble des noms de produits (variable\nproduct_name)\n\nEffectuer un nuage de mot sur les libell√©s avant et apr√®s nettoyage\npour comprendre la structure du corpus en question.\nLe r√©sultat devrait avoir l‚Äôapparence suivante\n\n\nimport wordcloud as wc\nimport matplotlib.pyplot as plt\n\n\ndef graph_wordcloud(data, by = None, valueby = None, yvar = \"Text\"):\n    if (by is not None) & (valueby is not None):        \n        txt = data[data[by]==valueby][yvar].astype(str)\n    else:\n        txt = data[yvar].astype(str)\n    all_text = ' '.join([text for text in txt])\n    wordcloud = wc.WordCloud(width=800, height=500,\n                          random_state=21,\n                      max_words=2000).generate(all_text)\n    return wordcloud\n\ndef graph_wordcloud_by(data, by, yvar = \"Text\"):\n    n_topics = data[by].unique().tolist()\n    width=20\n    height=80\n    rows = len(n_topics)//2\n    cols = 2\n    fig=plt.figure(figsize=(width, height))\n    axes = []\n    for i in range(cols*rows):\n        b = graph_wordcloud(data, by = by, valueby = n_topics[i], yvar = yvar)\n        axes.append( fig.add_subplot(rows, cols, i+1) )\n        axes[-1].set_title(\"{}\".format(n_topics[i]))  \n        plt.imshow(b)\n        plt.axis('off')\n        plt.savefig('{}.png'.format(yvar), bbox_inches='tight')\n\n\ndef wordcount_words(data, yvar, by = None):\n    plt.figure( figsize=(15,15) )\n    if by is None:\n        wordcloud = graph_wordcloud(data, yvar = yvar, by = by)\n        plt.imshow(wordcloud)\n        plt.axis('off')\n        plt.savefig('{}.png'.format(yvar), bbox_inches='tight')\n    else:\n        graph_wordcloud_by(data, by = by, yvar = yvar)\n\nwordcount_words(df_openfood, yvar = \"product_name\")\nwordcount_words(df_openfood, \"tokenized\")\n\n\nUtiliser la librairie Fasttext pour extraire les noms de produits\nfran√ßais\n\n\nAppliquer le mod√®le t√©l√©charg√© pr√©c√©demment pour d√©terminer le langage\nNe r√©cup√©rer que les libell√©s fran√ßais\n\n\nimport fasttext\n\nPRETRAINED_MODEL_PATH = \"%s.model.bin\" % temp_dir\nmodel = fasttext.load_model(PRETRAINED_MODEL_PATH)\nnewcols = ['language','score_language']\ndf_openfood[newcols] = pd.DataFrame(df_openfood['product_name'].astype(str).apply(lambda s: list(model.predict(s))).apply(lambda l: [l[0][0],l[1][0]]).tolist(), columns = newcols)\ndf_openfood['language'] = df_openfood['language'].str.replace(\"__label__\",\"\")\ndf_openfood_french = df_openfood[df_openfood['language'] == \"fr\"]\ndf_openfood_french.head(2)\n\n\nVisualiser avec spacy.displacy le r√©sultat d‚Äôune reconnaissance\nd‚Äôentit√©s nomm√©es sur 50 donn√©es al√©atoires. Cela vous semble-t-il satisfaisant ?\n\n\nimport spacy\nimport fr_core_news_sm\n\nnlp = fr_core_news_sm.load()\n\nexample = \" \\n \".join(df_openfood_french['product_name'].astype(\"str\").sample(50))\n\nfrom spacy import displacy\nhtml = displacy.render(nlp(example), style='ent', page=True)\n\n\nprint(html)\n\n\nR√©cup√©rer dans un vecteur les entit√©s nomm√©es reconnues par spaCy.\nRegarder les entit√©s reconnues dans les 20 premiers libell√©s de produits\n\n\nx = []\nfor doc in nlp.pipe(df_openfood_french.head(20)['product_name'].astype(\"unicode\"), disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n    # Do something with the doc here\n    x.append([(ent.text, ent.label_) for ent in doc.ents])\n    \nx\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@book{galiana2023,\n  author = {Galiana, Lino},\n  title = {Python Pour La Data Science},\n  date = {2023},\n  url = {https://pythonds.linogaliana.fr/},\n  doi = {10.5281/zenodo.8229676},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGaliana, Lino. 2023. Python Pour La Data Science. https://doi.org/10.5281/zenodo.8229676."
  },
  {
    "objectID": "content/modern-ds/continuous_integration.html",
    "href": "content/modern-ds/continuous_integration.html",
    "title": "Int√©gration continue avec Python",
    "section": "",
    "text": "Cette page sera actualis√©e prochainement, une version plus √† jour et plus compl√®te peut √™tre trouv√©e sur https://ensae-reproductibilite.github.io/website/\nL‚Äôun des apports principaux des innovations\nr√©centes de la data science est la\nmani√®re dont des projets, malgr√©\nleur complexit√©, peuvent facilement\n√™tre converti en projets p√©rennes\n√† partir\nd‚Äôun prototype bien construit.\nEn s‚Äôinspirant de l‚Äôapproche devops ,\nm√©thode de travail qui consiste √† adopter un certain\nnombre de gestes pour\nautomatiser la production de livrables ou de tests\nd√®s la\nconception du produit, les data scientists\nont adopt√© une m√©thode de travail tr√®s efficace\npour favoriser la r√©utilisation de leur travail\npar d‚Äôautres √©quipes que celles √† l‚Äôorigine de\nla conception du protype initial.\nCette approche devops a √©t√© reprise et √©tendue\npour donner un autre buzz-word, le MLops.\nIl s‚Äôagit d‚Äôune approche qui vise √† cr√©er\net mettre √† disposition des mod√®les de machine\nlearning de mani√®re fiable et automatis√©e\n√† chaque nouvelle √©tape du projet, en parall√®le\nde la mise √† jour du code ayant produit ces\noutput.\nCes nouvelles m√©thodes de travail permettent\ndes gains substantiels de productivit√©\npour les √©quipes d√©veloppant des mod√®les\net r√©duit fortement le co√ªt de reprise d‚Äôun\ncode par une √©quipe en charge de sa\np√©renisation. Ce co√ªt est en effet le principal\nfrein √† la mise en production de nouveaux\nprojets ce qui peut repr√©senter un g√¢chis\nnon n√©gligeable de temps et de ressources.\nComme nous l‚Äôexpliquons avec Romain Avouac\ndans un cours de derni√®re ann√©e de l‚ÄôENSAE\n(https://ensae-reproductibilite.github.io/website/),\nl‚Äôadoption de certaines bonnes pratiques\nde d√©veloppement de code et d‚Äôune d√©marche\nexploitant les derni√®res innovations de\nla data science peut substantiellement\naugmenter les chances d‚Äôun succ√®s\nd‚Äôun projet. Le nouveau paradigme, qui\nconsiste √† int√©grer en amont du projet\ncertaines contraintes de la production\net tester continuellement la mani√®re dont les\nlivrables √©voluent, √©vite que la mise\nen production d‚Äôun projet, qui est co√ªteuse\nen temps et en ressources, n‚Äôaboutisse qu‚Äôau\nmoment o√π le projet est d√©j√† caduc\n(car les donn√©es ou les besoins ont √©volu√©s‚Ä¶)."
  },
  {
    "objectID": "content/modern-ds/continuous_integration.html#fonctionnement-des-actions-github",
    "href": "content/modern-ds/continuous_integration.html#fonctionnement-des-actions-github",
    "title": "Int√©gration continue avec Python",
    "section": "Fonctionnement des actions Github",
    "text": "Fonctionnement des actions Github\nLes actions Github fonctionnent par couches successives au sein desquelles\non effectue un certain nombre d‚Äôinstructions.\nLa meilleure mani√®re d‚Äôapprendre les actions Github est, certes, de lire la\ndocumentation officielle mais surtout,\n√† mon avis, de regarder quelques pipelines pour comprendre la d√©marche.\nL‚Äôun des int√©r√™ts des Github Actions est la possibilit√© d‚Äôavoir un pipeline\nproposant une intrication de langages diff√©rents pour avoir une chaine de\nproduction qui propose les outils les plus efficaces pour r√©pondre √† un\nobjectif en limitant les verrous techniques.\nPar exemple, le pipeline de ce cours, disponible\nsur Github propose une intrication des langages\nPython et R avec des technologies Anaconda (pour contr√¥ler\nl‚Äôenvironnement Python comme expliqu√© dans les chapitres pr√©c√©dents)\net Javascript (pour le d√©ploiement d‚Äôun site web avec le service tiers\nNetlify)2. Cette cha√Æne de production multi-langage permet que\nles m√™mes fichiers sources g√©n√®rent un site web et des notebooks disponibles\nsur plusieurs environnements.\n\n\nname: Production deployment\n\non:\n  push:\n    branches:\n      - main\n      - master\n      - light\n\njobs:\n  pages:\n    name: Render-Blog\n    runs-on: ubuntu-latest\n    container: linogaliana/python-datascientist:latest\n    if: ${{ !github.event.pull_request.head.repo.fork }}\n    steps:\n      - uses: actions/checkout@v2\n        with:\n          #fetch-depth: 0\n          ref: ${{ github.event.pull_request.head.ref }}\n          repository: ${{github.event.pull_request.head.repo.full_name}}\n      - name: Configure safe.directory  # Workaround for actions/checkout#760\n        run: git config --global --add safe.directory /__w/python-datascientist/python-datascientist\n      - name: Render website\n        run: |\n          quarto render --to html\n      - name: Publish to Pages\n        if: github.ref == 'refs/heads/master'\n        run: |\n          git config --global user.email quarto-github-actions-publish@example.com\n          git config --global user.name \"Quarto GHA Workflow Runner\"\n          quarto publish gh-pages . --no-render --no-browser\n\n\n\nLes couches qui constituent les √©tapes du pipeline\nportent ainsi le nom de steps. Un step peut comporter un certain\nnombre d‚Äôinstructions ou ex√©cuter des instructions pr√©-d√©finies.\nL‚Äôune de ces instructions pr√©d√©finies est, par exemple,\nl‚Äôinstallation de Python\nou l‚Äôinitialisation d‚Äôun environnement conda.\nLa documentation officielle de Github propose un\nfichier qui peut servir de mod√®le\npour tester un script Python voire l‚Äôuploader de mani√®re automatique\nsur Pypi."
  },
  {
    "objectID": "content/modern-ds/continuous_integration.html#int√©gration-continue-avec-python-tester-un-notebook",
    "href": "content/modern-ds/continuous_integration.html#int√©gration-continue-avec-python-tester-un-notebook",
    "title": "Int√©gration continue avec Python",
    "section": "Int√©gration continue avec Python: tester un notebook",
    "text": "Int√©gration continue avec Python: tester un notebook\nCette section n‚Äôest absolument pas exhaustive. Au contraire, elle ne fournit\nqu‚Äôun exemple minimal pour expliquer la logique de l‚Äôint√©gration continue. Il\nne s‚Äôagit ainsi pas d‚Äôune garantie absolue de reproductibilit√© d‚Äôun notebook.\nGithub propose une action officielle pour utiliser Python dans un\npipeline d‚Äôint√©gration continue. Elle est disponible sur le\nMarketPlace Github.\nIl s‚Äôagit d‚Äôun bon point de d√©part, √† enrichir.\nLe fichier qui contr√¥le les instructions ex√©cut√©es dans l‚Äôenvironnement Actions\ndoit se trouver dans le dossier .github/workflows/\n(:warning: ne pas oublier le point au d√©but du\nnom du dossier). Il doit √™tre au format YAML avec une extension .yml\nou .yaml.\nIl peut avoir n‚Äôimporte quel nom n√©anmoins il\nvaut mieux lui donner un nom signifiant,\npar exemple prod.yml pour un fichier contr√¥lant une cha√Æne de production.\n\nLister les d√©pendances\nAvant d‚Äô√©crire les instructions √† ex√©cuter par Github, il faut d√©finir un\nenvironnement d‚Äôex√©cution car Github ne conna√Æt pas la configuration Python\ndont vous avez besoin.\nIl convient ainsi de lister les d√©pendances n√©cessaires dans un fichier\nrequirements.txt (si on utilise un environnement virtuel)\nou un fichier environment.yml (si on pr√©f√®re\nutiliser un environnement conda).\nBien que le principe sous-jacent soit l√©g√®rement diff√©rent,\nces fichiers ont la m√™me fonction:\npermettre la cr√©ation d‚Äôun environnement ex-nihilo\navec un certain nombre de d√©pendances pr√©-install√©es3.\nSi on fait le choix de l‚Äôoption environment.yml,\nle fichier prendra ainsi la forme\nsuivante, √† enrichir en fonction de la\nrichesse de l‚Äôenvironnement souhait√©. :\nchannels:\n  - conda-forge\n\ndependencies:\n  - python&gt;=3.10\n  - jupyter\n  - jupytext\n  - matplotlib\n  - nbconvert\n  - numpy\n  - pandas\n  - scipy\n  - seaborn\nLe m√™me fichier sous le format requirements.txt aura\nla forme suivante :\njupyter\njupytext\nmatplotlib\nnbconvert\nnumpy\npandas\nscipy\nseaborn\nSous leur apparente √©quivalence, au-del√† de\nla question du formatage, ces fichiers ont\ndeux diff√©rences principales :\n\nla version minimale de Python est d√©finie dans\nle fichier environment.yml alors qu‚Äôelle ne l‚Äôest\npas dans un fichier requirements.txt. C‚Äôest\nparce que le second installe les d√©pendances dans\nun environnement d√©j√† existant par ailleurs alors\nque le premier peut servir √† cr√©er l‚Äôenvironnement\navec une certaine configuration de Python ;\nle mode d‚Äôinstallation des packages n‚Äôest pas le\nm√™me. Avec un environment.yml on installera des\npackages via conda alors qu‚Äôavec un requirements.txt\non privil√©giera plut√¥t pip4.\n\nDans le cas de l‚Äôenvironnement conda,\nle choix du channel conda-forge vise √† contr√¥ler le d√©p√¥t utilis√© par\nAnaconda.\n\n\n Hint\nLa conda forge est un d√©p√¥t de package alternatif\nau canal par d√©faut d‚ÄôAnaconda qui est maintenu par\nl‚Äô√©quipe de d√©veloppeurs officiels d‚ÄôAnaconda.\nComme cette derni√®re cherche en priorit√© √†\nassurer la stabilit√© de l‚Äô√©cosyst√®me Anaconda,\nles versions de package √©voluent moins vite\nque le rythme voulu par les d√©veloppeurs de\npackages. Pour cette raison, un d√©p√¥t\nalternatif, o√π les mont√©es de version sont\nplus simples parce qu‚Äôelles d√©pendent des\nd√©veloppeurs de chaque package, a √©merg√©.\nIl s‚Äôagit de la conda forge. Lorsqu‚Äôon\nd√©sire utiliser des fonctionalit√©s r√©centes\nde l‚Äô√©cosyst√®me de la data science,\nil est conseill√© de l‚Äôutiliser.\n\n\nNe pas oublier de mettre ce fichier sous contr√¥le de version et de l‚Äôenvoyer\nsur le d√©p√¥t par un push.\n\n\nCr√©er un environnement reproductible dans Github Actions\nDeux approches sont possibles √† ce niveau, selon le degr√©\nde reproductibilit√© d√©sir√©5:\n\nCr√©er l‚Äôenvironnement via une action existante. L‚Äôaction\nconda-incubator/setup-miniconda@v2\nest un bon point de d√©part.\nCr√©er l‚Äôenvironnement dans une image Docker.\n\nLa deuxi√®me solution permet de contr√¥ler de mani√®re\nbeaucoup plus fine l‚Äôenvironnement dans lequel\nPython s‚Äô√©x√©cutera ainsi que la mani√®re dont\nl‚Äôenvironnement sera cr√©√©6. N√©anmoins, elle n√©cessite\ndes connaissances plus pouss√©es dans la principe\nde la conteneurisation qui peuvent √™tre co√ªteuses\n√† acqu√©rir. Selon l‚Äôambition du projet, notamment\nles r√©utilisation qu‚Äôil d√©sire,\nun data scientist pourra privil√©gier\ntelle ou telle option. Les deux solutions sont pr√©sent√©es\ndans l‚Äôexemple fil-rouge du cours que nous\ndonnons avec Romain Avouac\n(https://ensae-reproductibilite.github.io/website/application/).\n\n\nTester un notebook myfile.ipynb\nDans cette partie, on va supposer que le notebook √† tester s‚Äôappelle myfile.ipynb\net se trouve √† la racine du d√©p√¥t. Les\nd√©pendances pour l‚Äôex√©cuter sont\nlist√©es dans un fichier requirements.txt.\nLe mod√®le suivant, expliqu√© en dessous, fournit un mod√®le de recette pour\ntester un notebook. Supposons que ce fichier soit pr√©sent\ndans un chemin .github/workflows/test-notebook.yml\n\n\nEnvironnement virtuel\n\nname: Test notebook execution using Github Actions\n\non: [push]\n\njobs:\n  build-linux:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up Python 3.10\n      uses: actions/setup-python@v3\n      with:\n        python-version: '3.10'\n      - shell: bash\n      run: |\n        python --version\n    - name: Install dependencies\n      run:\n        pip install -r requirements.txt\n        pip install jupyter nbconvert\n    - name: Test jupyter from command line\n      run:\n        jupyter nbconvert --execute --to notebook --inplace myfile.ipynb\n    - uses: actions/upload-artifact@v3\n      with:\n        name: Notebook\n        path: myfile.ipynb\n        retention-days: 5\n\n\n\nEnvironnement conda\n\nname: Test notebook execution using Github Actions\n\non: [push]\n\njobs:\n  build-linux:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up Python 3.10\n      uses: actions/setup-python@v3\n      with:\n        python-version: '3.10'\n    - name: Add conda to system path\n      run: |\n        # $CONDA is an environment variable pointing to the root of the miniconda directory\n        echo $CONDA/bin &gt;&gt; $GITHUB_PATH\n    - name: Install dependencies\n      run: |\n        conda env update --file environment.yml --name base\n        conda install jupyter nbconvert\n    - name: Test jupyter from command line\n      run:\n        jupyter nbconvert --execute --to notebook --inplace myfile.ipynb\n    - uses: actions/upload-artifact@v3\n      with:\n        name: Notebook\n        path: myfile.ipynb\n        retention-days: 5\n\nDans les deux cas, la d√©marche est la m√™me:\n\non r√©cup√®re les fichiers pr√©sents dans le d√©p√¥t\n(action checkout) ;\non installe Python ;\non installe les d√©pendances pour ex√©cuter le code.\nDans l‚Äôapproche conda, il est √©galement n√©cessaire\nde faire quelques configurations suppl√©mentaires (notamment\najouter conda aux logiciels reconnus par la ligne\nde commande) ;\non teste le notebook en ligne de commande et remplace\ncelui existant, sur la machine temporaire, par la version\nproduite sur cet environnement neutre.\non rend possible le t√©l√©chargement du\nnotebook produit automatiquement pendant 5 jours7. Ceci\nrepose sur les artefacts qui sont un √©l√©ment r√©cup√©r√©\ndes machines temporaires qui n‚Äôexistent plus d√®s que le\ncode a fini d‚Äô√™tre ex√©cut√©.\n\nCes actions sont ex√©cut√©es √† chaque interaction avec\nle d√©p√¥t distant (push), quelle que soit la\nbranche. A partir de ce mod√®le, il est possible de\nraffiner pour, par exemple, automatiquement\nfaire un commit du notebook valid√© et le pusher\nvia le robot Github8"
  },
  {
    "objectID": "content/modern-ds/continuous_integration.html#les-services-de-mise-√†-disposition-de-github-et-gitlab",
    "href": "content/modern-ds/continuous_integration.html#les-services-de-mise-√†-disposition-de-github-et-gitlab",
    "title": "Int√©gration continue avec Python",
    "section": "Les services de mise √† disposition de Github et Gitlab",
    "text": "Les services de mise √† disposition de Github et Gitlab\nGithub et Gitlab, les deux plateformes de partage\nde code, proposent non seulement des services\ngratuits d‚Äôint√©gration continue mais aussi des services\nde mise √† disposition de sites web pleinement int√©gr√©s\naux services de stockage de code.\nCes services, Gitlab Pages et Github Pages, auxquels\non peut associer le service externe Netlify qui r√©pond\nau m√™me principe9 permettent, √† chaque modification\ndu code source d‚Äôun projet, de reconstruire le site web (le livrable)\nqui peut √™tre directement produit √† partir de certains fichiers\n(des slides revealJS par exemple) ou qui\nsert d‚Äôoutput √† l‚Äôint√©gration continue apr√®s compilation\nde fichiers plus complexes (des fichiers quarto par exemple).\nChaque d√©p√¥t sur Github ou Gitlab peut ainsi √™tre associ√©\n√† un URL de d√©ploiement disponible sur internet. A chaque\ncommit sur le d√©p√¥t, le site web qui sert de livrable\nest ainsi mis √† jour. La version d√©ploy√©e √† partir de la\nbranche principale peut ainsi √™tre consid√©r√©e\ncomme la version de production alors que les branches\nsecondaires peuvent servir d‚Äôespace bac √† sable pour\nv√©rifier que des changements dans le code source\nne mettent pas en p√©ril le livrable. Cette m√©thode,\nqui s√©curise la production d‚Äôun livrable sous forme\nde site web, est ainsi particuli√®rement appr√©ciable."
  },
  {
    "objectID": "content/modern-ds/continuous_integration.html#les-services-externes-disponibles-sans-infrastructure-sp√©ciale",
    "href": "content/modern-ds/continuous_integration.html#les-services-externes-disponibles-sans-infrastructure-sp√©ciale",
    "title": "Int√©gration continue avec Python",
    "section": "Les services externes disponibles sans infrastructure sp√©ciale",
    "text": "Les services externes disponibles sans infrastructure sp√©ciale\nPour fonctionner, l‚Äôint√©gration continue\nn√©cessite de mettre en oeuvre des environnements normalis√©s.\nComme √©voqu√© pr√©c√©demment,\nla technologie sous-jacente est celle de la conteneurisation.\nLes images qui servent de point de d√©part au lancement\nd‚Äôun conteneur sont elles-m√™mes mises √† disposition\ndans des espaces communautaires (des registres d‚Äôimages).\nIl en existe plusieurs, les plus connus √©tant\nle dockerhub ou le registry de Gitlab.\nCes registres servent d‚Äôespaces de stockage pour des images,\nqui sont des objets volumineux (potentiellement plusieurs\nGigas) mais aussi d‚Äôespace de mutualisation en permettant\n√† d‚Äôautres de r√©utiliser une image pr√™te √† l‚Äôemploi ou,\nau contraire, √† partir de\nlaquelle on peut ajouter un certain nombre de couches\npour obtenir l‚Äôenvironnement minimal\nde reproductibilit√©.\nIl est possible d‚Äôutiliser certaines actions Github\npr√™te √† l‚Äôemploi pour constuire une image Docker\n√† partir d‚Äôun fichier Dockerfile. Apr√®s avoir\ncr√©e une connexion entre un compte sur la\nplateforme Github et l‚Äôautre sur DockerHub,\nune mise √† disposition automatis√©e d‚Äôun livrable\nsous forme d‚Äôimage Docker est ainsi possible.\nUne image Docker peut offrir une grande vari√©t√©\nd‚Äôoutput. Elle peut servir uniquement √†\nmettre √† disposition un environnement de\nreproductibilit√© mais elle peut servir √† mettre\n√† disposition, pour les personnes ma√Ætrisant\nDocker, des output plus raffin√©s. Par exemple,\ndans le cours que nous donnons √† l‚ÄôENSAE, nous\nmontrons comment docker peut servir √†\nmettre √† disposition √† un utilisateur tiers\nune application minimaliste (construite avec flask)\nqu‚Äôil fera tourner\nsur son ordinateur.\nSi une image Docker peut √™tre tr√®s utile pour la mise\n√† disposition, elle n√©cessite pour sa r√©utilisation\nun niveau avanc√© d‚Äôexpertise en programmation.\nCela ne conviendra pas √† tous les publics. Certains\nne d√©sireront que b√©n√©ficier d‚Äôune application interactive\no√π ils pourrons visualiser certains r√©sultats en fonction\nd‚Äôactions comme des filtres sur des sous-champs ou le choix\nde certaines plages de donn√©es. D‚Äôautres publics seront\nplut√¥t int√©ress√© par la r√©utilisation d‚Äôun programme\nou des r√©sultats d‚Äôun mod√®le sous forme d‚ÄôAPI mais n‚Äôauront\npas l‚Äôinfrastructure interne pour faire tourner le code\nd‚Äôorigine ou une image Docker. C‚Äôest pour r√©pondre √† ces\nlimites qu‚Äôil peut devenir int√©ressant, pour une √©quipe\nde data science de d√©velopper une architecture\nkubernetes interne, si l‚Äôorganisation en a les moyens, ou\nde payer un fournisseur de service, comme AWS, qui permet\ncela."
  },
  {
    "objectID": "content/modern-ds/continuous_integration.html#kubernetes-le-sommet-de-la-pente-du-d√©ploiement",
    "href": "content/modern-ds/continuous_integration.html#kubernetes-le-sommet-de-la-pente-du-d√©ploiement",
    "title": "Int√©gration continue avec Python",
    "section": "Kubernetes: le sommet de la pente du d√©ploiement",
    "text": "Kubernetes: le sommet de la pente du d√©ploiement\nKubernetes est une technologie qui pousse la logique\nde la conteneurisation √† son paroxysme.\nIl s‚Äôagit d‚Äôun syst√®me open-source, d√©velopp√©\npar Google, permettant\nd‚Äôautomatiser le d√©ploiement, la mise √† l‚Äô√©chelle\net la gestion d‚Äôapplications conteneuris√©es.\nGr√¢ce √† Kubernetes, une application, par exemple\nun site web proposant de la r√©activit√©,\npeut √™tre mise √† disposition et reporter les calculs,\nlorsqu‚Äôils sont n√©cessaires, sur\nun serveur. L‚Äôutilisation de Kubernetes dans\nun projet de data science permet ainsi\nd‚Äôanticiper √† la fois l‚Äôinterface d‚Äôune application\nvalorisant un projet mais aussi le fonctionnement\ndu back-office, par exemple en testant la capacit√©\nde charge de cette application. Une introduction\n√† Kubernetes orient√© donn√©e peut √™tre trouv√©e dans\nle cours d√©di√© √† la mise en production\nque nous donnons avec Romain Avouac et dans ce\npost de blog tr√®s bien fait.\nDans les grandes organisations, o√π les r√¥les sont\nplus sp√©cialis√©s que dans les petites structures,\nce ne sont pas n√©cessairement les data scientists\nqui devront ma√Ætriser Kubernetes mais plut√¥t\nles data-architect ou les data-engineer. N√©anmoins,\nles data scientists devront √™tre capable de\ndialoguer avec eux et mettre en oeuvre une m√©thode\nde travail adapt√©e (celle-ci reposera en principe sur\nl‚Äôapproche CI/CD). Dans les petites structures, les\ndata scientist peuvent √™tre en mesure\nde mettre en oeuvre le d√©ploiement en continu. En\nrevanche, il est plus rare, dans ces structures,\no√π les moyens humains de maintenance sont limit√©s,\nque les serveurs sur lesquels fonctionnent Kubernetes\nsoient d√©tenus en propres. En g√©n√©ral, ils sont lou√©s\ndans des services de paiement √† la demande de type\nAWS."
  },
  {
    "objectID": "content/modern-ds/continuous_integration.html#footnotes",
    "href": "content/modern-ds/continuous_integration.html#footnotes",
    "title": "Int√©gration continue avec Python",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCes services d‚Äôint√©gration continue √©taient utilis√©s lorsque Github\nne proposait pas encore de service int√©gr√©, comme le faisait Gitlab.\nIls sont de moins en moins fr√©quemment utilis√©s.‚Ü©Ô∏é\nPour r√©duire le temps n√©cessaire pour construire le site web, ce\npipeline s‚Äôappuie sur un environnement Docker construit sur un autre d√©p√¥t\ndisponible √©galement sur Github\n.\nCelui-ci part d‚Äôune configuration syst√®me Linux et construit un environnement\nAnaconda √† partir d‚Äôun fichier environment.yml qui liste toutes les d√©pendances\nn√©cessaires pour ex√©cuter les morceaux de code du site web.\nCet environnement Anaconda est construit gr√¢ce √† l‚Äôoutil mamba qui permet\nd‚Äôaller beaucoup plus vite dans la constitution d‚Äôenvironnements que ne le\npermet conda.‚Ü©Ô∏é\nSur la diff√©rence entre les environnements virtuels\net les environnements conda, voir\ncette partie de cours\nplus avanc√© que nous donnons\navec Romain Avouac sur la mise en production\nde projets data science.‚Ü©Ô∏é\nIl est possible d‚Äôinstaller une partie des packages\navec pip en d√©finissant un champ pip dans le\nfichier environment.yml. N√©anmoins, les concepteurs\nd‚ÄôAnaconda recommandent d‚Äô√™tre prudent avec cette m√©thode\nqui pr√©sente certes l‚Äôavantage d‚Äôacc√©l√©rer le temps de\ncr√©ation de l‚Äôenvironnement mais peut cr√©er des\ndifficult√©s avec des librairies n√©cessitant d‚Äôautres\nlangages syst√®me comme le C.‚Ü©Ô∏é\nLe point de vue que nous d√©fendons avec\nRomain Avouac dans notre cours sur la reproductibilit√©\nest qu‚Äôil s‚Äôagit d‚Äôun continuum dans lequel on investit\nplus ou moins en fonction de ses contraintes, de ses\nbesoins, de ses comp√©tences, du temps humain qu‚Äôon\npeut d√©dier √† d√©velopper des output reproductibles\net le temps gagn√© en d√©veloppant une telle approche.\nSelon o√π on se trouve sur ce cursus, en fonction\ndes solutions d√©j√† existantes qu‚Äôon peut trouver\nsur internet, on va plus ou moins raffiner\nnotre int√©gration et nos d√©ploiements\ncontinus.‚Ü©Ô∏é\nIl est recommand√© de ne pas garder la p√©riode de r√©tention\ndes artefacts par d√©faut car celle-ci est assez longue (90 jours).\nLes output pouvant √™tre assez volumineux et expirant rapidement\n(en g√©n√©ral ce qui nous int√©resse est la derni√®re ou l‚Äôavant\nderni√®re version de l‚Äô_output), pour des raisons √©cologiques,\nil est recommand√© de fixer des p√©riodes courtes. Cela peut √™tre\nfait directement dans le fichier configurant l‚Äôint√©gration\ncontinue comme ici ou dans les param√®tres par d√©faut\ndu d√©p√¥t pour que cette r√®gle s‚Äôapplique √† toutes les\nproductions faites par int√©gration continue.‚Ü©Ô∏é\nIl est recommand√© de ne pas garder la p√©riode de r√©tention\ndes artefacts par d√©faut car celle-ci est assez longue (90 jours).\nLes output pouvant √™tre assez volumineux et expirant rapidement\n(en g√©n√©ral ce qui nous int√©resse est la derni√®re ou l‚Äôavant\nderni√®re version de l‚Äô_output), pour des raisons √©cologiques,\nil est recommand√© de fixer des p√©riodes courtes. Cela peut √™tre\nfait directement dans le fichier configurant l‚Äôint√©gration\ncontinue comme ici ou dans les param√®tres par d√©faut\ndu d√©p√¥t pour que cette r√®gle s‚Äôapplique √† toutes les\nproductions faites par int√©gration continue.‚Ü©Ô∏é\nIl s‚Äôagit du service utilis√©, par exemple,\npour ce cours. Netlify est un service de mise √† disposition\nqui offre des fonctionalit√©s plus compl√®tes que celles\npermises par Gitlab Pages et Github Pages. Outre cet\navantage, il est plus facile √† configurer que Github Pages\nqui n√©cessite l‚Äôusage d‚Äôune branche d√©di√©e nomm√©e gh-pages,\nce qui peut\nrebutant.‚Ü©Ô∏é\nIl s‚Äôagit du service utilis√©, par exemple,\npour ce cours. Netlify est un service de mise √† disposition\nqui offre des fonctionalit√©s plus compl√®tes que celles\npermises par Gitlab Pages et Github Pages. Outre cet\navantage, il est plus facile √† configurer que Github Pages\nqui n√©cessite l‚Äôusage d‚Äôune branche d√©di√©e nomm√©e gh-pages,\nce qui peut\nrebutant.‚Ü©Ô∏é"
  },
  {
    "objectID": "content/modern-ds/s3.html",
    "href": "content/modern-ds/s3.html",
    "title": "Les nouveaux modes d‚Äôacc√®s aux donn√©es : le format parquet et les donn√©es sur le cloud",
    "section": "",
    "text": "Ce chapitre est une introduction √† la question\ndu stockage des donn√©es et aux innovations\nr√©centes dans ce domaine. L‚Äôobjectif\nest d‚Äôabord de pr√©senter les avantages\ndu format Parquet et la mani√®re dont\non peut utiliser les\nlibrairies pyarrow\nou duckdb pour traiter\nde mani√®re efficace des donn√©es volumineuses\nau format Parquet. Ensuite, on pr√©sentera\nla mani√®re dont ce format parquet s‚Äôint√®gre\nbien avec des syst√®mes de stockage cloud,\nqui tendent √† devenir la norme dans le monde\nde la data science."
  },
  {
    "objectID": "content/modern-ds/s3.html#principe-du-stockage-de-la-donn√©e",
    "href": "content/modern-ds/s3.html#principe-du-stockage-de-la-donn√©e",
    "title": "Les nouveaux modes d‚Äôacc√®s aux donn√©es : le format parquet et les donn√©es sur le cloud",
    "section": "Principe du stockage de la donn√©e",
    "text": "Principe du stockage de la donn√©e\nPour comprendre les apports du format Parquet, il est n√©cessaire\nde faire un d√©tour pour comprendre la mani√®re dont une information\nest stock√©e et accessible √† un langage de traitement de la donn√©e.\nIl existe deux approches dans le monde du stockage de la donn√©e.\nLa premi√®re est celle de la base de donn√©es relationnelle. La seconde est le\nprincipe du fichier.\nLa diff√©rence entre les deux est dans la mani√®re dont l‚Äôacc√®s aux\ndonn√©es est organis√©."
  },
  {
    "objectID": "content/modern-ds/s3.html#les-fichiers",
    "href": "content/modern-ds/s3.html#les-fichiers",
    "title": "Les nouveaux modes d‚Äôacc√®s aux donn√©es : le format parquet et les donn√©es sur le cloud",
    "section": "Les fichiers",
    "text": "Les fichiers\nDans un fichier, les donn√©es sont organis√©es selon un certain format et\nle logiciel de traitement de la donn√©e va aller chercher et structurer\nl‚Äôinformation en fonction de ce format. Par exemple, dans un fichier\n.csv, les diff√©rentes informations seront stock√©es au m√™me niveau\navec un caract√®re pour les s√©parer (la virgule , dans les .csv anglosaxons, le point virgule dans les .csv fran√ßais, la tabulation dans les .tsv). Le fichier suivant\nnom ; profession \nAst√©rix ; \nOb√©lix ; Tailleur de menhir ;\nAssurancetourix ; Barde\nsera ainsi organis√© naturellement sous forme tabul√©e par Python\n\n\n\n\n\n\n\n\n\nnom\nprofession\n\n\n\n\n0\nAst√©rix\n\n\n\n1\nOb√©lix\nTailleur de menhir\n\n\n2\nAssurancetourix\nBarde\n\n\n\n\n\n\n\nA propos des fichiers de ce type, on parle de fichiers plats car\nles enregistrements relatifs √† une observation sont stock√©s ensemble,\nsans hi√©rarchie.\nCertains formats de donn√©es vont permettre d‚Äôorganiser les informations\nde mani√®re diff√©rente. Par exemple, le format JSON va\nhi√©rarchiser diff√©remment la m√™me information [^1]:\n[\n  {\n    \"nom\": \"Ast√©rix\"\n  },\n  {\n    \"nom\": \"Ob√©lix\",\n    \"profession\": \"Tailleur de menhir\"\n  },\n  {\n    \"nom\": \"Assurancetourix\",\n    \"profession\": \"Barde\"\n  }\n]\n\n\n Hint \nLa diff√©rence entre le CSV et le format JSON va au-del√† d‚Äôun simple ‚Äúformattage‚Äù des donn√©es.\nPar sa nature non tabulaire, le format JSON permet des mises √† jour beaucoup plus facile de la donn√©e dans les entrep√¥ts de donn√©es.\nPar exemple, un site web qui collecte de nouvelles donn√©es n‚Äôaura pas √† mettre √† jour l‚Äôensemble de ses enregistrements ant√©rieurs\npour stocker la nouvelle donn√©e (par exemple pour indiquer que pour tel ou tel client cette donn√©e n‚Äôa pas √©t√© collect√©e)\nmais pourra la stocker dans\nun nouvel item. Ce sera √† l‚Äôoutil de requ√™te (Python ou un autre outil)\nde cr√©er une relation entre les enregistrements stock√©s √† des endroits\ndiff√©rents.\nCe type d‚Äôapproche flexible est l‚Äôun des fondements de l‚Äôapproche NoSQL,\nsur laquelle nous allons revenir, qui a permis l‚Äô√©mergence de technologies au coeur de l‚Äô√©cosyst√®me actuel du big-data comme Hadoop ou ElasticSearch.\n\n\nCette fois, quand on n‚Äôa pas d‚Äôinformation, on ne se retrouve pas avec nos deux s√©parateurs accol√©s (cf.¬†la ligne ‚ÄúAst√©rix‚Äù) mais l‚Äôinformation\nn‚Äôest tout simplement pas collect√©e.\n\n\n Note\nIl se peut tr√®s bien que l‚Äôinformation sur une observation soit diss√©min√©e\ndans plusieurs fichiers dont les formats diff√®rent.\nPar exemple, dans le domaine des donn√©es g√©ographiques,\nlorsqu‚Äôune donn√©e est disponible sous format de fichier(s), elle peut l‚Äô√™tre de deux mani√®res!\n\nSoit la donn√©e est stock√©e dans un seul fichier qui m√©lange contours g√©ographiques et valeurs attributaires\n(la valeur associ√©e √† cette observation g√©ographique, par exemple le taux d‚Äôabstention). Ce principe est celui du geojson.\nSoit la donn√©e est stock√©e dans plusieurs fichiers qui sont sp√©cialis√©s: un fichier va stocker les contours g√©ographiques,\nl‚Äôautre les donn√©es attributaires et d‚Äôautres fichiers des informations annexes (comme le syst√®me de projection). Ce principe est celui du shapefile.\nC‚Äôest alors le logiciel qui requ√™te\nles donn√©es (Python par exemple) qui saura o√π aller chercher l‚Äôinformation\ndans les diff√©rents fichiers et associer celle-ci de mani√®re coh√©rente.\n\n\n\nUn concept suppl√©mentaire dans le monde du fichier est celui du file system. Le file system est\nle syst√®me de localisation et de nommage des fichiers.\nPour simplifier, le file system est la mani√®re dont votre ordinateur saura\nretrouver, dans son syst√®me de stockage, les bits pr√©sents dans tel ou tel fichier\nappartenant √† tel ou tel dossier."
  },
  {
    "objectID": "content/modern-ds/s3.html#les-bases-de-donn√©es",
    "href": "content/modern-ds/s3.html#les-bases-de-donn√©es",
    "title": "Les nouveaux modes d‚Äôacc√®s aux donn√©es : le format parquet et les donn√©es sur le cloud",
    "section": "Les bases de donn√©es",
    "text": "Les bases de donn√©es\nLa logique des bases de donn√©es est diff√©rente. Elle est plus syst√©mique.\nUn syst√®me de gestion de base de donn√©es (Database Management System)\nest un logiciel qui g√®re √† la fois le stockage d‚Äôun ensemble de donn√©es reli√©e,\npermet de mettre √† jour celle-ci (ajout ou suppression d‚Äôinformations, modification\ndes caract√©ristiques d‚Äôune table‚Ä¶)\net qui g√®re √©galement\nles modalit√©s d‚Äôacc√®s √† la donn√©e (type de requ√™te, utilisateurs\nayant les droits en lecture ou en √©criture‚Ä¶).\nLa relation entre les entit√©s pr√©sentes dans une base de donn√©es\nprend g√©n√©ralement la forme d‚Äôun sch√©ma en √©toile. Une base va centraliser\nles informations disponibles qui seront ensuite d√©taill√©es dans des tables\nd√©di√©es.\n\nSource: La documentation Databricks sur le sch√©ma en √©toile\nLe logiciel associ√© √† la base de donn√©es fera ensuite le lien\nentre ces tables √† partir de requ√™tes SQL. L‚Äôun des logiciels les plus efficaces dans ce domaine\nest PostgreSQL. Python est tout √† fait\nutilisable pour passer une requ√™te SQL √† un gestionnaire de base de donn√©es.\nLes packages sqlalchemy et psycopg2\npeuvent servir √† utiliser PostgreSQL pour requ√™ter une\nbase de donn√©e ou la mettre √† jour.\nLa logique de la base de donn√©es est donc tr√®s diff√©rente de celle du fichier.\nCes derniers sont beaucoup plus l√©gers pour plusieurs raisons.\nD‚Äôabord, parce qu‚Äôils sont moins adh√©rents √†\nun logiciel gestionnaire. L√† o√π le fichier ne n√©cessite, pour la gestion,\nqu‚Äôun file system, install√© par d√©faut sur\ntout syst√®me d‚Äôexploitation, une base de donn√©es va n√©cessiter un\nlogiciel sp√©cialis√©. L‚Äôinconv√©nient de l‚Äôapproche fichier, sous sa forme\nstandard, est qu‚Äôelle\nne permet pas une gestion fine des droits d‚Äôacc√®s et am√®ne g√©n√©ralement √† une\nduplication de la donn√©e pour √©viter que la source initiale soit\nr√©-√©crite (involontairement ou de mani√®re intentionnelle par un utilisateur malveillant).\nR√©soudre ce probl√®me est l‚Äôune des\ninnovations des syst√®mes cloud, sur lesquelles nous reviendrons en √©voquant le\nsyst√®me S3.\nUn deuxi√®me inconv√©nient de l‚Äôapproche base de donn√©es par\nrapport √† l‚Äôapproche fichier, pour un utilisateur de Python,\nest que les premiers n√©cessitent l‚Äôinterm√©diation du logiciel de gestion\nde base de donn√©es l√† o√π, dans le second cas, on va se contenter d‚Äôune\nlibrairie, donc un syst√®me beaucoup plus l√©ger,\nqui sait comment transformer la donn√©e brute en DataFrame.\nPour ces raisons, entre autres, les bases de donn√©es sont donc moins √† la\nmode dans l‚Äô√©cosyst√®me r√©cent de la data science que les fichiers."
  },
  {
    "objectID": "content/modern-ds/s3.html#lire-un-parquet-en-python-la-librairie-pyarrow",
    "href": "content/modern-ds/s3.html#lire-un-parquet-en-python-la-librairie-pyarrow",
    "title": "Les nouveaux modes d‚Äôacc√®s aux donn√©es : le format parquet et les donn√©es sur le cloud",
    "section": "Lire un parquet en Python: la librairie pyarrow",
    "text": "Lire un parquet en Python: la librairie pyarrow\nLa librairie pyarrow permet la lecture et l‚Äô√©criture\nde fichiers parquet avec Python1. Elle repose\nsur un type particulier de dataframe, le pyarrow.Table\nqui peut √™tre utilis√© en substitut ou en compl√©ment\ndu DataFrame\nde pandas. Il est recommand√© de r√©guli√®rement\nconsulter la documentation officielle de pyarrow\nconcernant la lecture et √©criture de fichiers et celle relative\naux manipulations de donn√©es.\nPour illustrer les fonctionalit√©s de pyarrow,\nrepartons de notre CSV initial que nous allons\nenrichir d‚Äôune nouvelle variable num√©rique\net que nous\nallons\nconvertir en objet pyarrow avant de l‚Äô√©crire au format parquet:\n\nimport pandas as pd\nfrom io import StringIO \nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\ns = \"\"\"\nnom;cheveux;profession\nAst√©rix;blond;\nOb√©lix;roux;Tailleur de menhir\nAssurancetourix;blond;Barde\n\"\"\"\n\nsource = StringIO(s)\n\ndf = pd.read_csv(source, sep = \";\", index_col=False)\ndf[\"taille\"] = [155, 190, 175]\ntable = pa.Table.from_pandas(df)\n\ntable\n\npq.write_table(table, 'example.parquet')\n\n\n\n Hint \nL‚Äôutilisation des noms pa pour pyarrow et pq pour\npyarrow.parquet est une convention communautaire\nqu‚Äôil est recommand√© de suivre.\n\n\nPour importer et traiter ces donn√©es, on peut conserver\nles donn√©es sous le format pyarrow.Table\nou transformer en pandas.DataFrame. La deuxi√®me\noption est plus lente mais pr√©sente l‚Äôavantage\nde permettre ensuite d‚Äôappliquer toutes les\nmanipulations offertes par l‚Äô√©cosyst√®me\npandas qui est g√©n√©ralement mieux connu que\ncelui d‚ÄôArrow.\nSupposons qu‚Äôon ne s‚Äôint√©resse qu‚Äô√† la taille et √† la couleur\nde cheveux de nos gaulois.\nIl n‚Äôest pas n√©cessaire d‚Äôimporter l‚Äôensemble de la base, cela\nferait perdre du temps pour rien. On appelle\ncette approche le column pruning qui consiste √†\nne parcourir, dans le fichier, que les colonnes qui nous\nint√©ressent. Du fait du stockage orient√© colonne du parquet,\nil suffit de ne consid√©rer que les blocs qui nous\nint√©ressent (alors qu‚Äôavec un CSV il faudrait scanner tout\nle fichier avant de pouvoir √©liminer certaines colonnes).\nCe principe du column pruning se mat√©rialise avec\nl‚Äôargument columns dans parquet.\nEnsuite, avec pyarrow, on pourra utiliser pyarrow.compute pour\neffectuer des op√©rations directement sur une table\nArrow :\n\nimport pyarrow.compute as pc\n\ntable = pq.read_table('example.parquet', columns=['taille', 'cheveux'])\n\ntable.group_by(\"cheveux\").aggregate([(\"taille\", \"mean\")])\n\nLa mani√®re √©quivalente de proc√©der en passant\npar l‚Äôinterm√©diaire de pandas est\n\ntable = pq.read_table('example.parquet', columns=['taille', 'cheveux'])\n\ntable.to_pandas().groupby(\"cheveux\")[\"taille\"].mean()\n\ncheveux\nblond    165.0\nroux     190.0\nName: taille, dtype: float64\n\n\nIci, comme les donn√©es sont peu volumineuses, deux des\navantages du parquet par rapport\nau CSV (donn√©es moins\nvolumineuses et vitesse de l‚Äôimport)\nne s‚Äôappliquent pas vraiment.\n\n\n Note\nUn autre principe d‚Äôoptimisation de la performance qui est\nau coeur de la librairie Arrow est le filter pushdown\n(ou predicate pushdown).\nQuand on ex√©cute un filtre de s√©lection de ligne\njuste apr√®s avoir charg√© un jeu de donn√©es,\nArrow va essayer de le mettre en oeuvre lors de l‚Äô√©tape de lecture\net non apr√®s. Autrement dit, Arrow va modifier le plan\nd‚Äôex√©cution pour pousser le filtre en amont de la s√©quence d‚Äôex√©cution\nafin de ne pas essayer de lire les lignes inutiles."
  },
  {
    "objectID": "content/modern-ds/s3.html#quest-ce-que-le-syst√®me-de-stockage-s3",
    "href": "content/modern-ds/s3.html#quest-ce-que-le-syst√®me-de-stockage-s3",
    "title": "Les nouveaux modes d‚Äôacc√®s aux donn√©es : le format parquet et les donn√©es sur le cloud",
    "section": "Qu‚Äôest-ce que le syst√®me de stockage S3 ?",
    "text": "Qu‚Äôest-ce que le syst√®me de stockage S3 ?\nDans les entreprises et administrations,\nun nombre croissant de donn√©es sont\ndisponibles depuis un syst√®me de stockage\nnomm√© S3.\nLe syst√®me S3 (Simple Storage System) est un syst√®me de stockage d√©velopp√©\npar Amazon et qui est maintenant devenu une r√©f√©rence pour le stockage en ligne.\nIl s‚Äôagit d‚Äôune architecture √† la fois\ns√©curis√©e (donn√©es crypt√©es, acc√®s restreints) et performante.\nLe concept central du syst√®me S3 est le bucket.\nUn bucket est un espace (priv√© ou partag√©) o√π on peut stocker une\narborescence de fichiers. Pour acc√©der aux fichiers figurant\ndans un bucket priv√©, il faut des jetons d‚Äôacc√®s (l‚Äô√©quivalent d‚Äôun mot de passe)\nreconnus par le serveur de stockage. On peut alors lire et √©crire dans le bucket.\n\n\n Note\nLes exemples suivants seront r√©plicables pour les utilisateurs de la plateforme\nSSP Cloud\n\nIls peuvent √©galement l‚Äô√™tre pour des utilisateurs ayant un\nacc√®s √† AWS, il suffit de changer l‚ÄôURL du endpoint\npr√©sent√© ci-dessous."
  },
  {
    "objectID": "content/modern-ds/s3.html#comment-faire-avec-python",
    "href": "content/modern-ds/s3.html#comment-faire-avec-python",
    "title": "Les nouveaux modes d‚Äôacc√®s aux donn√©es : le format parquet et les donn√©es sur le cloud",
    "section": "Comment faire avec Python ?",
    "text": "Comment faire avec Python ?\n\nLes librairies principales\nL‚Äôinteraction entre ce syst√®me distant de fichiers et une session locale de Python\nest possible gr√¢ce √† des API. Les deux principales librairies sont les suivantes :\n\nboto3, une librairie cr√©√©e et maintenue par Amazon ;\ns3fs, une librairie qui permet d‚Äôinteragir avec les fichiers stock√©s √† l‚Äôinstar d‚Äôun filesystem classique.\n\nLa librairie pyarrow que nous avons d√©j√† pr√©sent√©e permet √©galement\nde traiter des donn√©es stock√©es sur le cloud comme si elles\n√©taient sur le serveur local. C‚Äôest extr√™mement pratique\net permet de fiabiliser la lecture ou l‚Äô√©criture de fichiers\ndans une architecture cloud.\nUn exemple, assez court, est disponible\ndans la documentation officielle\nIl existe √©galement d‚Äôautres librairies permettant de g√©rer\ndes pipelines de donn√©es (chapitre √† venir) de mani√®re\nquasi indiff√©rente entre une architecture locale et une architecture\ncloud. Parmi celles-ci, nous pr√©senterons quelques exemples\navec snakemake.\nEn arri√®re-plan, snakemake\nva utiliser boto3 pour communiquer avec le syst√®me\nde stockage.\nEnfin, selon le m√™me principe du comme si les donn√©es\n√©taient en local, il existe l‚Äôoutil en ligne de commande\nmc (Minio Client) qui permet de g√©rer par des lignes\nde commande Linux les d√©p√¥ts distants comme s‚Äôils √©taient\nlocaux.\nToutes ces librairies offrent la possibilit√© de se connecter depuis Python,\n√† un d√©p√¥t de fichiers distant, de lister les fichiers disponibles dans un\nbucket, d‚Äôen t√©l√©charger un ou plusieurs ou de faire de l‚Äôupload\nNous allons pr√©senter quelques-unes des op√©rations les plus fr√©quentes,\nen mode cheatsheet."
  },
  {
    "objectID": "content/modern-ds/s3.html#connexion-√†-un-bucket",
    "href": "content/modern-ds/s3.html#connexion-√†-un-bucket",
    "title": "Les nouveaux modes d‚Äôacc√®s aux donn√©es : le format parquet et les donn√©es sur le cloud",
    "section": "Connexion √† un bucket",
    "text": "Connexion √† un bucket\nPar la suite, on va utiliser des alias pour les trois valeurs suivantes, qui servent\n√† s‚Äôauthentifier.\nkey_id = 'MY_KEY_ID'\naccess_key = 'MY_ACCESS_KEY'\ntoken = \"MY_TOKEN\"\nCes valeurs peuvent √™tre √©galement disponibles dans\nles variables d‚Äôenvironnement de Python. Comme il s‚Äôagit d‚Äôune information\nd‚Äôauthentification personnelle, il ne faut pas stocker les vraies valeurs de ces\nvariables dans un projet, sous peine de partager des traits d‚Äôidentit√© sans le\nvouloir lors d‚Äôun partage de code.\n\nboto3 üëá\nAvec boto3, on cr√©√© d‚Äôabord un client puis on ex√©cute des requ√™tes dessus.\nPour initialiser un client, il suffit, en supposant que l‚Äôurl du d√©p√¥t S3 est\n\"https://minio.lab.sspcloud.fr\", de faire:\nimport boto3\ns3 = boto3.client(\"s3\",endpoint_url = \"https://minio.lab.sspcloud.fr\")\n\n\n\nS3FS üëá\nLa logique est identique avec s3fs.\nSi on a des jetons d‚Äôacc√®s √† jour et dans les variables d‚Äôenvironnement\nad√©quates:\nimport s3fs\nfs = s3fs.S3FileSystem(\n  client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})\n\n\n\nArrow üëá\nLa logique d‚ÄôArrow est proche de celle de s3fs. Seuls les noms\nd‚Äôarguments changent\nSi on a des jetons d‚Äôacc√®s √† jour et dans les variables d‚Äôenvironnement\nad√©quates:\nfrom pyarrow import fs\ns3 = fs.S3FileSystem(endpoint_override=\"http://\"+\"minio.lab.sspcloud.fr\")\n\n\n\nSnakemake üëá\nLa logique de Snakemake est, quant √† elle,\nplus proche de celle de boto3. Seuls les noms\nd‚Äôarguments changent\nSi on a des jetons d‚Äôacc√®s √† jour et dans les variables d‚Äôenvironnement\nad√©quates:\nfrom snakemake.remote.S3 import RemoteProvider as S3RemoteProvider\nS3 = S3RemoteProvider(host = \"https://\" + os.getenv('AWS_S3_ENDPOINT'))\n\n\nIl se peut que la connexion √† ce stade soit refus√©e (HTTP error 403).\nCela peut provenir\nd‚Äôune erreur dans l‚ÄôURL utilis√©. Cependant, cela refl√®te plus g√©n√©ralement\ndes param√®tres d‚Äôauthentification erron√©s.\n\nboto3 üëá\nLes param√®tres d‚Äôauthentification sont des arguments suppl√©mentaires:\nimport boto3\ns3 = boto3.client(\"s3\",endpoint_url = \"https://minio.lab.sspcloud.fr\",\n                  aws_access_key_id=key_id, \n                  aws_secret_access_key=access_key, \n                  aws_session_token = token)\n\n\n\nS3FS üëá\nLa logique est la m√™me, seuls les noms d‚Äôarguments diff√®rent\nimport s3fs\nfs = s3fs.S3FileSystem(\n  client_kwargs={'endpoint_url': 'https://'+'minio.lab.sspcloud.fr'},\n  key = key_id, secret = access_key,\n  token = token)\n\n\n\nArrow üëá\nTout est en argument cette fois:\nfrom pyarrow import fs\n\ns3 = fs.S3FileSystem(\n    access_key = key_id,\n    secret_key = access_key,\n    session_token = token,\n    endpoint_override = 'https://'+'minio.lab.sspcloud.fr',\n    scheme = \"https\"\n    )\n\n\n\nSnakemake üëá\nLa logique est la m√™me, seuls les noms d‚Äôarguments diff√®rent\nfrom snakemake.remote.S3 import RemoteProvider as S3RemoteProvider\nS3 = S3RemoteProvider(host = \"https://\" + os.getenv('AWS_S3_ENDPOINT'), access_key_id=key_id, secret_access_key=access_key)\n\n\n\n\n Note\nDans le SSP Cloud,\nlorsque l‚Äôinitialisation du service Jupyter du SSP Cloud est r√©cente\n(moins de 12 heures), il est possible d‚Äôutiliser\nautomatiquement les jetons stock√©s automatiquement √† la cr√©ation du d√©p√¥t.\nSi on d√©sire acc√©der aux donn√©es du SSP Cloud depuis une session Python du\ndatalab (service VSCode, Jupyter‚Ä¶),\nil faut remplacer l‚Äôurl par http://minio.lab.sspcloud.fr"
  },
  {
    "objectID": "content/modern-ds/s3.html#lister-les-fichiers",
    "href": "content/modern-ds/s3.html#lister-les-fichiers",
    "title": "Les nouveaux modes d‚Äôacc√®s aux donn√©es : le format parquet et les donn√©es sur le cloud",
    "section": "Lister les fichiers",
    "text": "Lister les fichiers\nS‚Äôil n‚Äôy a pas d‚Äôerreur √† ce stade, c‚Äôest que la connexion est bien effective.\nPour le v√©rifier, on peut essayer de faire la liste des fichiers disponibles\ndans un bucket auquel on d√©sire acc√©der.\nPar exemple, on peut vouloir\ntester l‚Äôacc√®s aux bases FILOSOFI (donn√©es de revenu localis√©es disponibles\nsur https://www.insee.fr) au sein du bucket donnees-insee.\n\nboto3 üëá\nPour cela,\nla m√©thode list_objects offre toutes les options n√©cessaires:\nimport boto3\ns3 = boto3.client(\"s3\",endpoint_url = \"https://minio.lab.sspcloud.fr\")\nfor key in s3.list_objects(Bucket='donnees-insee', Prefix='diffusion/FILOSOFI')['Contents']:\n    print(key['Key'])\n\n\n\nS3FS üëá\nPour lister les fichiers, c‚Äôest la m√©thode ls (celle-ci ne liste pas par\nd√©faut les fichiers de mani√®re r√©cursive comme boto3):\nimport s3fs\nfs = s3fs.S3FileSystem(\n  client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})\nfs.ls(\"donnees-insee/diffusion\")\n\n\n\nArrow üëá\nfrom pyarrow import fs\ns3 = fs.S3FileSystem(endpoint_override='https://'+'minio.lab.sspcloud.fr')\ns3.get_file_info(fs.FileSelector('donnees-insee/diffusion', recursive=True))\n\n\n\nmc üëá\nmc ls -r"
  },
  {
    "objectID": "content/modern-ds/s3.html#t√©l√©charger-un-fichier-depuis-s3-pour-lenregistrer-en-local",
    "href": "content/modern-ds/s3.html#t√©l√©charger-un-fichier-depuis-s3-pour-lenregistrer-en-local",
    "title": "Les nouveaux modes d‚Äôacc√®s aux donn√©es : le format parquet et les donn√©es sur le cloud",
    "section": "T√©l√©charger un fichier depuis S3 pour l‚Äôenregistrer en local",
    "text": "T√©l√©charger un fichier depuis S3 pour l‚Äôenregistrer en local\nCette m√©thode n‚Äôest en g√©n√©ral pas recommand√©e car, comme on va le voir\npar la suite, il est possible de lire √† la vol√©e des fichiers. Cependant,\nt√©l√©charger un fichier depuis le cloud pour l‚Äô√©crire sur le disque\nlocal peut parfois √™tre utile (par exemple, lorsqu‚Äôil est n√©cessaire\nde d√©zipper un fichier).\n\nboto3 üëá\nOn utilise cette fois la m√©thode download_file\nimport boto3\ns3 = boto3.client(\"s3\",endpoint_url = \"https://minio.lab.sspcloud.fr\")\ns3.download_file('donnees-insee', \"diffusion/FILOSOFI/2014/FILOSOFI_COM.csv\", 'data.csv')\n\n\n\nS3FS üëá\nimport s3fs\nfs = s3fs.S3FileSystem(\n  client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})\nfs.download('donnees-insee/diffusion/FILOSOFI/2014/FILOSOFI_COM.csv','test.csv')\n\n\n\nSnakemake üëá\nfrom snakemake.remote.S3 import RemoteProvider as S3RemoteProvider\nS3 = S3RemoteProvider(host = \"https://\" + os.getenv('AWS_S3_ENDPOINT'))\nbucket = \"mon-bucket\"\n\nrule ma_super_regle_s3:\n    input:\n        fichier = S3.remote(f'{bucket}/moninput.csv')\n    output:\n        fichier='mon_dossier_local/monoutput.csv'\n    run:\n        shell(\"cp {input[0]} {output[0]}\")\n\n\n\nmc üëá\nmc cp \"donnees-insee/FILOSOFI/2014/FILOSOFI_COM.csv\" 'data.csv'"
  },
  {
    "objectID": "content/modern-ds/s3.html#lire-un-fichier-directement",
    "href": "content/modern-ds/s3.html#lire-un-fichier-directement",
    "title": "Les nouveaux modes d‚Äôacc√®s aux donn√©es : le format parquet et les donn√©es sur le cloud",
    "section": "Lire un fichier directement",
    "text": "Lire un fichier directement\nLa m√©thode pr√©c√©dente n‚Äôest pas optimale. En effet, l‚Äôun des int√©r√™ts des API\nest qu‚Äôon peut traiter un fichier sur S3 comme s‚Äôil s‚Äôagissait d‚Äôun fichier\nsur son PC. Cela est d‚Äôailleurs une mani√®re plus s√©curis√©e de proc√©der puisqu‚Äôon\nlit les donn√©es √† la vol√©e, sans les √©crire dans un filesystem local.\n\nboto3 üëá\nimport boto3\ns3 = boto3.client(\"s3\",endpoint_url = \"https://minio.lab.sspcloud.fr\")\nobj = s3.get_object(Bucket='donnees-insee', Key=\"diffusion/FILOSOFI/2014/FILOSOFI_COM.csv\")\ndf = pd.read_csv(obj['Body'], sep = \";\")\ndf.head(2)\n\n\n\nS3FS üëá\nLe code suivant devrait permettre d‚Äôeffectuer la m√™me op√©ration avec s3fs\nimport pandas as pd\nimport s3fs\nfs = s3fs.S3FileSystem(\n  client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})\ndf = pd.read_csv(fs.open('{}/{}'.format('donnees-insee', \"diffusion/FILOSOFI/2014/FILOSOFI_COM.csv\"),\n                         mode='rb'), sep = \";\"\n                 )\n\ndf.head(2)\n\n\n\nSnakemake üëá\nfrom snakemake.remote.S3 import RemoteProvider as S3RemoteProvider\nS3 = S3RemoteProvider(host = \"https://\" + os.getenv('AWS_S3_ENDPOINT'))\nbucket = \"mon-bucket\"\n\nrule ma_super_regle_s3:\n    input:\n        fichier = S3.remote(f'{bucket}/moninput.csv')\n    run:\n        import pandas as pd\n        df = pd.read_csv(input.fichier)\n        # PLUS D'OPERATIONS\n\n\n\nArrow üëá\nArrow est une librairie qui permet de lire des CSV.\nIl est n√©anmoins\nbeaucoup plus pratique d‚Äôutiliser le format parquet avec arrow.\nDans un premier temps, on configure le filesystem avec les\nfonctionalit√©s d‚ÄôArrow (cf.¬†pr√©c√©demment).\n\nfrom pyarrow import fs\n\ns3 = fs.S3FileSystem(endpoint_override='http://'+'minio.lab.sspcloud.fr')\n\nPour lire un csv, on fera:\nfrom pyarrow import fs\nfrom pyarrow import csv\n\ns3 = fs.S3FileSystem(endpoint_override='https://'+'minio.lab.sspcloud.fr')\n\nwith s3.open_input_file(\"donnees-insee/diffusion/FILOSOFI/2014/FILOSOFI_COM.csv\") as file:\n    df = csv.read_csv(file, parse_options=csv.ParseOptions(delimiter=\";\")).to_pandas()\nPour un fichier au format parquet, la d√©marche est plus simple gr√¢ce √† l‚Äôargument\nfilesystem dans pyarrow.parquet.ParquetDataset :\nimport pyarrow.parquet as pq\n\n#bucket = \"\"\n#parquet_file=\"\"\ndf = pq.ParquetDataset(f'{bucket}/{parquet_file}', filesystem=s3).read_pandas().to_pandas()"
  },
  {
    "objectID": "content/modern-ds/s3.html#uploader-un-fichier",
    "href": "content/modern-ds/s3.html#uploader-un-fichier",
    "title": "Les nouveaux modes d‚Äôacc√®s aux donn√©es : le format parquet et les donn√©es sur le cloud",
    "section": "Uploader un fichier",
    "text": "Uploader un fichier\n\nboto3 üëá\ns3.upload_file(file_name, bucket, object_name)\n\n\n\nS3FS üëá\nfs.put(filepath, f\"{bucket}/{object_name}\", recursive=True)\n\n\n\nArrow üëá\nSupposons que df soit un pd.DataFrame\nDans un syst√®me local, on convertirait\nen table Arrow puis on √©crirait en parquet\n(voir la documentation officielle).\nQuand on est sur un syst√®me S3, il s‚Äôagit seulement d‚Äôajouter\nnotre connexion √† S3 dans l‚Äôargument filesystem\n(voir la page sur ce sujet dans la documentation Arrow)\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\ntable = pa.Table.from_pandas(df)\npq.write_table(table, f\"{bucket}/{path}\", filesystem=s3)\n\n\n\nSnakemake üëá\nfrom snakemake.remote.S3 import RemoteProvider as S3RemoteProvider\nS3 = S3RemoteProvider(host = \"https://\" + os.getenv('AWS_S3_ENDPOINT'))\nbucket = \"mon-bucket\"\n\nrule ma_super_regle_s3:\n    input:\n        fichier='mon_dossier_local/moninput.csv'\n    output:\n        fichier=S3.remote(f'{bucket}/monoutput.csv')\n    run:\n        shell(\"cp output.fichier input.fichier\")\n\n\n\nmc üëá\nmc cp 'data.csv' \"MONBUCKET/monoutput.csv\""
  },
  {
    "objectID": "content/modern-ds/s3.html#pour-aller-plus-loin",
    "href": "content/modern-ds/s3.html#pour-aller-plus-loin",
    "title": "Les nouveaux modes d‚Äôacc√®s aux donn√©es : le format parquet et les donn√©es sur le cloud",
    "section": "Pour aller plus loin",
    "text": "Pour aller plus loin\n\nLa documentation sur MinIO du SSPCloud"
  },
  {
    "objectID": "content/modern-ds/s3.html#footnotes",
    "href": "content/modern-ds/s3.html#footnotes",
    "title": "Les nouveaux modes d‚Äôacc√®s aux donn√©es : le format parquet et les donn√©es sur le cloud",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nElle permet aussi la lecture et l‚Äô√©criture\nde .csv.‚Ü©Ô∏é\nD‚Äôailleurs, les g√©n√©rations n‚Äôayant connu nativement\nque ce type de stockage ne sont pas familiaris√©es\nau concept de file system et pr√©f√®rent\npayer le temps de recherche. Voir\ncet article\nsur le sujet.‚Ü©Ô∏é\nD‚Äôailleurs, les g√©n√©rations n‚Äôayant connu nativement\nque ce type de stockage ne sont pas familiaris√©es\nau concept de file system et pr√©f√®rent\npayer le temps de recherche. Voir\ncet article\nsur le sujet.‚Ü©Ô∏é\nD‚Äôailleurs, les g√©n√©rations n‚Äôayant connu nativement\nque ce type de stockage ne sont pas familiaris√©es\nau concept de file system et pr√©f√®rent\npayer le temps de recherche. Voir\ncet article\nsur le sujet.‚Ü©Ô∏é"
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html",
    "href": "content/modern-ds/elastic_intro.html",
    "title": "Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "",
    "text": "Pour essayer les exemples pr√©sents dans ce tutoriel :\nCe chapitre a √©t√© √©crit avec Milena Suarez-Castillo\net pr√©sente quelques √©l√©ments qui servent de base √† un travail en cours\nsur les in√©galit√©s socio√©conomiques dans les\nchoix de consommation alimentaire."
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#r√©plication-de-ce-chapitre",
    "href": "content/modern-ds/elastic_intro.html#r√©plication-de-ce-chapitre",
    "title": "Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "R√©plication de ce chapitre",
    "text": "R√©plication de ce chapitre\nCe chapitre est plus exigeant en termes d‚Äôinfrastructures que les pr√©c√©dents.\nSi la premi√®re partie de ce chapitre peut √™tre men√©e avec une\ninstallation standard de Python, ce n‚Äôest pas le cas de la\ndeuxi√®me qui n√©cessite un serveur ElasticSearch. Les utilisateurs du\nSSP Cloud pourront r√©pliquer les exemples de ce cours\ncar cette technologie est disponible (que ce soit pour indexer une base ou\npour requ√™ter une base existante).\n‚ö†Ô∏è Ce\nchapitre n√©cessite une version particuli√®re du\npackage ElasticSearch pour tenir compte de l‚Äôh√©ritage de la version 7 du moteur Elastic.\nPour cela, faire\n\n!pip install elasticsearch==8.2.0\n!pip install unidecode\n!pip install rapidfuzz\n!pip install xlrd\n\nLa premi√®re partie de ce tutoriel ne n√©cessite pas d‚Äôarchitecture particuli√®re et\npeut ainsi √™tre ex√©cut√©e en utilisant les packages suivants :\n\nimport time\nimport pandas as pd\n\nLe script functions.py, disponible sur Github,\nregroupe un certain nombre de fonctions utiles permettant\nd‚Äôautomatiser certaines t√¢ches de nettoyage classiques\nen NLP.\n\n\n Hint\nPlusieurs m√©thodes peuvent √™tre mises en oeuvre pour r√©cup√©rer\nle script d‚Äôutilitaires. Vous pouvez trouver en dessous\nde cet encadr√© une m√©thode qui va chercher la derni√®re\nversion sur le d√©p√¥t Github du cours\n\n\n\nimport requests\nurl = \"https://github.com/linogaliana/python-datascientist/raw/master/content/modern-ds/functions.py\"\nr = requests.get(url, allow_redirects=True)\n\nopen('functions.py', 'wb').write(r.content)\n\nApr√®s l‚Äôavoir r√©cup√©r√© (cf.¬†encadr√© d√©di√©),\nil convient d‚Äôimporter les fonctions sous forme de module:\n\nimport functions as fc"
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#cas-dusage",
    "href": "content/modern-ds/elastic_intro.html#cas-dusage",
    "title": "Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "Cas d‚Äôusage",
    "text": "Cas d‚Äôusage\nCe notebook recense et propose d‚Äôappr√©hender quelques outils utilis√©s\npour le papier pr√©sent√© aux\nJourn√©es de M√©thodologie Statistiques 2022: Galiana and Suarez-Castillo, ‚ÄúFuzzy matching on big-data: an illustration with scanner data and crowd-sourced nutritional data‚Äù\n(travail en cours!)\nOn va partir du cas d‚Äôusage suivant :\n\nCombien de calories dans ma recette de cuisine de ce soir? Combien de calories dans mes courses de la semaine?\n\nL‚Äôobjectif est de reconstituer, √† partir de libell√©s de produits, les caract√©ristiques nutritionnelles d‚Äôune recette.\nLe probl√®me est que les libell√©s des tickets de caisse ne sont pas des champs textuels tr√®s propres, ils contiennent,\npar exemple, beaucoup d‚Äôabbr√©viations, toutes n‚Äô√©tant pas √©videntes.\nVoici par exemple une s√©rie de noms de produits qu‚Äôon va utiliser par la suite:\n\nticket = ['CROISSANTS X6 400G',\n          'MAQUEREAUX MOUTAR.',\n          'IGP OC SAUVIGNON B',\n          'LAIT 1/2 ECRM UHT',\n          '6 OEUFS FRAIS LOCA',\n          'ANANAS C2',\n          'L POMME FUDJI X6 CAL 75/80 1KG ENV',\n          'PLT MIEL',\n          'STELLA ARTOIS X6',\n          'COTES DU LUBERON AIGUEBRUN 75C']\n\nA ces produits, s‚Äôajoutent les ingr√©dients suivants, issus de la\nrecette du velout√© de potiron et carottes de Marmiton\nqui sera notre plat principal :\n\ningredients = ['500 g de carottes',\n '2 pommes de terre',\n \"1 gousse d'ail\",\n '1/2 l de lait',\n '1/2 l de bouillon de volaille',\n \"1 cuill√®re √† soupe de huile d'olive\",\n '1 kg de potiron',\n '1 oignon',\n '10 cl de cr√®me liquide (facultatif)']\n\nEssayer de r√©cup√©rer par web scraping cette liste est un bon exercice pour r√©viser\nles concepts vus pr√©c√©demment\nOn va donc cr√©er une liste de course compilant\nces deux\nlistes h√©t√©rog√®nes de noms de produits:\n\nlibelles = ticket + ingredients\n\nOn part avec cette liste dans notre supermarch√© virtuel. L‚Äôobjectif sera de trouver\nune m√©thode permettant de passer √† l‚Äô√©chelle:\nautomatiser les traitements, effectuer des recherches efficaces, garder une certaine g√©n√©ralit√© et flexibilit√©.\nCe chapitre montrera par l‚Äôexemple l‚Äôint√©r√™t d‚ÄôElastic par rapport √† une solution\nqui n‚Äôutiliserait que du Python."
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#les-bases-offrant-des-informations-nutritionnelles",
    "href": "content/modern-ds/elastic_intro.html#les-bases-offrant-des-informations-nutritionnelles",
    "title": "Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "Les bases offrant des informations nutritionnelles",
    "text": "Les bases offrant des informations nutritionnelles\nPour un nombre restreint de produits, on pourrait bien s√ªr chercher √†\nla main les caract√©ristiques des produits en utilisant les\nfonctionalit√©s d‚Äôun moteur de recherche:\n\n\n\n\n\nCependant, cette approche serait tr√®s fastidieuse et\nn√©cessiterait de r√©cuperer, √† la main, chaque caract√©ristique\npour chaque produit. Ce n‚Äôest donc pas envisageable.\nLes donn√©es disponibles sur Google viennent de l‚ÄôUSDA,\nl‚Äô√©quivalent am√©ricain de notre Minist√®re de l‚ÄôAgriculture.\nCependant, pour des recettes comportant des noms de produits fran√ßais, ainsi que\ndes produits potentiellement transform√©s, ce n‚Äôest pas tr√®s pratique d‚Äôutiliser\nune base de donn√©es de produits agricoles en Fran√ßais. Pour cette raison,\nnous proposons d‚Äôutiliser les deux bases suivantes,\nqui servent de base au travail de\nGaliana and Suarez Castillo (2022)\n\nL‚ÄôOpenFoodFacts database qui est une base\ncollaborative fran√ßaise de produits alimentaires. Issue d‚Äôun projet Data4Good, il s‚Äôagit d‚Äôune\nalternative opensource et opendata √† la base de donn√©es de l‚Äôapplication Yuka.\nLa table de composition nutritionnelle Ciqual produite par l‚ÄôAnses. Celle-ci\npropose la composition nutritionnelle moyenne des aliments les plus consomm√©s en France. Il s‚Äôagit d‚Äôune base de donn√©es\nenrichie par rapport √† celle de l‚ÄôUSDA puisqu‚Äôelle ne se cantonne pas aux produits agricoles non transform√©s.\nAvec cette base, il ne s‚Äôagit pas de trouver un produit exact mais essayer de trouver un produit type proche du produit\ndont on d√©sire conna√Ætre les caract√©ristiques."
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#import",
    "href": "content/modern-ds/elastic_intro.html#import",
    "title": "Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "Import",
    "text": "Import\nQuelques fonctions utiles sont regroup√©es dans le script functions.py et import√©es dans le notebook.\nLa base OpenFood peut √™tre r√©cup√©r√©e en ligne\nvia la fonction fc.import_openfood. N√©anmoins, cette op√©ration n√©cessitant\nun certain temps (les donn√©es brutes faisant autour de 2Go), nous proposons une m√©thode\npour les utilisateurs du SSP Cloud o√π une version est disponible sur\nl‚Äôespace de stockage.\nLa base Ciqual, qui plus l√©g√®re, est r√©cup√©r√©e elle directement en ligne\nvia la fonction fc.import_ciqual.\n\n# Pour les utilisateurs du SSP Cloud\nopenfood = fc.import_openfood_s3()\n# Pour les utilisateurs hors du SSP Cloud\n# openfood = fc.import_openfood()\nciqual = fc.import_ciqual()\n\n\nopenfood.head()\n\n\nciqual.head()"
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#quest-ce-quelastic",
    "href": "content/modern-ds/elastic_intro.html#quest-ce-quelastic",
    "title": "Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "Qu‚Äôest-ce qu‚ÄôElastic ?",
    "text": "Qu‚Äôest-ce qu‚ÄôElastic ?\nElasticSearch c‚Äôest un logiciel qui fournit un moteur de recherche install√© sur\nun serveur (ou une machine personnelle) qu‚Äôil est possible de requ√™ter depuis un client\n(une session Python par exemple).\nC‚Äôest un moteur de recherche\ntr√®s performant, puissant et flexible, extr√™mement utilis√© dans le domaine de la datascience\nsur donn√©es textuelles.\nUn cas d‚Äôusage est par exemple de trouver,\ndans un corpus de grande dimension\n(plusieurs sites web, livres‚Ä¶), un certain texte en s‚Äôautorisant des termes voisins\n(verbes conjugu√©s, fautes de frappes‚Ä¶).\nUn index est une collection de documents dans lesquels on souhaite chercher, pr√©alablement ing√©r√©s dans un moteur de recherche les documents sont les √©tablissements.\nL‚Äôindexation consiste √† pr√©-r√©aliser les traitements des termes des documents pour gagner en efficacit√© lors de la phase de recherche.\nL‚Äôindexation est faite une fois pour de nombreuses recherches potentielles, pour lesquelles la rapidit√© de r√©ponse peut √™tre cruciale.\nApr√®s avoir index√© une base, on effectuera des requ√™tes qui sont des recherches\nd‚Äôun document dans la base index√© (√©quivalent de notre web) √† partir de\ntermes de recherche normalis√©s.\nLe principe est le m√™me que celui d‚Äôun moteur de recherche du web comme Google.\nD‚Äôun c√¥t√©, l‚Äôensemble √† parcourir est index√© pour √™tre en\nmesure de parcourir de mani√®re efficace l‚Äôensemble du corpus.\nDe l‚Äôautre c√¥t√©, la phase de recherche permet de retrouver l‚Äô√©l√©ment du corpus le\nplus coh√©rent avec la requ√™te de recherche.\nL‚Äôindexation consiste, par exemple,\n√† pr√©-d√©finir des traitements des termes du corpus pour gagner en efficacit√©\nlors de la phase de recherche. En effet, l‚Äôindexation est une op√©ration peu fr√©quente\npar rapport √† la recherche. Pour cette derni√®re, l‚Äôefficacit√© est cruciale (un site web\nqui prend plusieurs secondes √† interpr√©ter une requ√™te simple ne sera pas utilis√©). Mais, pour\nl‚Äôindexation, ceci est moins crucial.\nLes documents sont constitu√©s de variables, les champs (‚Äòfields‚Äô),\ndont le type est sp√©cifi√© (‚Äútext‚Äù, ‚Äúkeywoard‚Äù, ‚Äúgeo_point‚Äù, ‚Äúnumeric‚Äù‚Ä¶) √† l‚Äôindexation.\nElasticSearch propose une interface graphique nomm√©e Kibana.\nCelle-ci est pratique\npour tester des requ√™tes et pour superviser le serveur Elastic. Cependant,\npour le passage √† l‚Äô√©chelle, notamment pour mettre en lien une base index√©e dans\nElastic avec une autre source de donn√©es, les API propos√©es par ElasticSearch\nsont beaucoup plus pratiques. Ces API permettent de connecter une session Python (idem pour R)\n√† un serveur Elastic afin de communiquer avec lui\n(√©changer des flux via une API REST)."
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#elasticsearch-et-python",
    "href": "content/modern-ds/elastic_intro.html#elasticsearch-et-python",
    "title": "Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "ElasticSearch et Python",
    "text": "ElasticSearch et Python\nEn Python, le package officiel est elasticsearch.\nCe dernier permet de configurer les param√®tres pour interagir avec un serveur, indexer\nune ou plusieurs bases, envoyer de mani√®re automatis√©e un ensemble de requ√™tes\nau serveur, r√©cup√©rer les r√©sultats directement dans une session Python‚Ä¶"
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#premier-essai-les-produits-ciqual-les-plus-similaires-aux-produits-de-la-recette",
    "href": "content/modern-ds/elastic_intro.html#premier-essai-les-produits-ciqual-les-plus-similaires-aux-produits-de-la-recette",
    "title": "Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "Premier essai: les produits Ciqual les plus similaires aux produits de la recette",
    "text": "Premier essai: les produits Ciqual les plus similaires aux produits de la recette\nOn pourrait √©crire une fonction qui prend en argument\nune liste de libell√©s d‚Äôint√©r√™t et une liste de candidat au match et\nrenvoie le libell√© le plus proche.\nCependant, le risque est que cet algorithme soit relativement lent s‚Äôil n‚Äôest pas cod√©\nparfaitement.\nIl est, √† mon avis, plus simple, quand\non est habitu√© √† la logique Pandas,\nde faire un produit cart√©sien pour obtenir un vecteur mettant en miroir\nchaque produit de notre recette avec l‚Äôensembles des produits Ciqual et ensuite comparer les deux vecteurs pour prendre,\npour chaque produit, le meilleur match.\nLes bases √©tant de taille limit√©e, le produit cart√©sien n‚Äôest pas probl√©matique.\nAvec des bases plus cons√©quentes, une strat√©gie plus parcimonieuse en m√©moire devrait √™tre envisag√©e.\nPour faire cette op√©ration, on va utiliser la fonction match_product de\nnote script d‚Äôutilitaires.\n\ndist_leven = fc.match_product(libelles, ciqual)\ndist_leven\n\nCette premi√®re √©tape na√Øve est d√©cevante √† plusieurs √©gards:\n\nCertes, on a des matches coh√©rent (par exemple ‚ÄúOignon rouge, cru‚Äù et ‚Äú1 oignon‚Äù)\nmais on a plus de couples incoh√©rents ;\nLe temps de calcul peut appara√Ætre faible mais le passage √† l‚Äô√©chelle risque d‚Äô√™tre compliqu√© ;\nLes besoins m√©moires sont potentiellement importants lors de l‚Äôappel √†\nrapidfuzz.process.extract ce qui peut bloquer le passage √† l‚Äô√©chelle ;\nLa distance textuelle n‚Äôest pas n√©cessairement la plus pertinente.\n\nOn a, en fait, n√©glig√© une √©tape importante: la normalisation (ou nettoyage des textes) pr√©sent√©e dans la\npartie NLP, notamment:\n\nharmonisation de la casse, suppression des accents‚Ä¶\nsuppressions des mots outils (e.g.¬†ici on va d‚Äôabord n√©gliger les quantit√©s pour trouver la nature de l‚Äôaliment, en particulier pour Ciqual)\n\n\n\n\n\n\nScanner-data avant nettoyage\n\n\n\n\n\nOpenFood data avant nettoyage\n\n\n\n\n\n\n\nScanner-data apr√®s nettoyage\n\n\n\n\n\nOpenFood data apr√®s nettoyage\n\n\n\n\nFaisons donc en apparence un retour en arri√®re qui sera\nn√©anmoins salvateur pour am√©liorer\nla pertinence des liens faits entre nos\nbases de donn√©es."
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#objectif",
    "href": "content/modern-ds/elastic_intro.html#objectif",
    "title": "Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "Objectif",
    "text": "Objectif\nLe preprocessing correspond √† l‚Äôensemble des op√©rations\nayant lieu avant l‚Äôanalyse √† proprement parler.\nIci, ce preprocessing est int√©ressant √† plusieurs\n√©gards:\n\nIl r√©duit le bruit dans nos jeux de donn√©es (par exemple des mots de liaisons) ;\nIl permet de normaliser et harmoniser les syntaxes dans nos diff√©rentes sources.\n\nL‚Äôobjectif est ainsi de r√©duire nos noms de produits √† la substantifique moelle\npour am√©liorer la pertinence de la recherche.\nPour √™tre pertinent, le preprocessing comporte g√©n√©ralement deux types de\ntraitements. En premier lieu, ceux qui sont g√©n√©raux et applicables\n√† tous types de corpus textuels: retrait des stopwords, de la ponctuation, etc.\nles m√©thodes disponibles dans la partie NLP.\nEnsuite, il est n√©cessaire de mettre en oeuvre des nettoyages plus sp√©cifiques √† chaque corpus.\nPar exemple dans la source Ciqual,\nla cuisson est souvent renseign√©e et bruite les appariemments."
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#d√©marche",
    "href": "content/modern-ds/elastic_intro.html#d√©marche",
    "title": "Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "D√©marche",
    "text": "D√©marche\n\n\n Exercice 1: preprocessing\n\nPour transformer les lettres avec accents en leur √©quivalent\nsans accent, la fonction unidecode\n(du package du m√™me nom) est pratique.\nLa tester sur le jeu de donn√©es ciqual en cr√©ant une nouvelle\ncolonne nomm√©e libel_clean\nLa casse diff√©rente selon les jeux de donn√©es peut √™tre p√©nalisante\npour trouver des produits similaires. Pour √©viter ces probl√®mes,\nmettre tout en majuscule.\nLes informations sur les quantit√©s ou le packaging peuvent apporter\ndu bruit dans notre comparaison. Nous allons retirer ces mots,\n√† travers la liste ['KG','CL','G','L','CRUE?S?', 'PREEMBALLEE?S?'],\nqu‚Äôon peut consid√©rer comme un dictionnaire de stop-words m√©tier.\nPour cela, il convient d‚Äôutiliser une expression r√©guli√®re dans la m√©thode\nstr.replace de Pandas.\nAvec ceux-ci, on va utiliser la liste des stop-words de\nla librairie nltk pour retirer les stop-words classiques (_‚Äúle‚Äù,‚Äúla‚Äù, etc.).\nLa librairie SpaCy, plus riche, pourrait √™tre utilis√©e ; nous laissons\ncela sous la forme d‚Äôexercice suppl√©mentaire.\nOn a encore des signes de ponctuation ou des chiffres qui peuvent\npoluer la comparaison. Les retirer gr√¢ce √† la m√©thode replace et\nune regex [^a-zA-Z]\nEnfin, par s√©curit√©, on peut supprimer les espaces multiples.\nUtiliser la regex '([ ]{2,})' pour cela. Observer le r√©sultat\nfinal.\n(Optionnel). Comme exercice suppl√©mentaire, faire la m√™me chose avec les\npipelines SpaCy.\n\n\n\nA l‚Äôissue de la question 1, le jeu de donn√©es ciqual devrait\nressembler √† celui-ci:\nApr√®s avoir mis en majuscule, on se retrouve avec le jeu de donn√©es\nsuivant:\nApr√®s retrait des stop-words, nos libell√©s prennent\nla forme suivante :\nLa regex pour √©liminer les caract√®res de ponctuation permet ainsi d‚Äôobtenir:\nEnfin, √† l‚Äôissue de la question 5, le DataFrame obtenu est le suivant :\nCes √©tapes de nettoyage ont ainsi permis de concentrer l‚Äôinformation\ndans les noms de produits sur ce qui l‚Äôidentifie vraiment."
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#approche-syst√©matique",
    "href": "content/modern-ds/elastic_intro.html#approche-syst√©matique",
    "title": "Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "Approche syst√©matique",
    "text": "Approche syst√©matique\nPour syst√©matiser cette approche √† nos diff√©rents DataFrame, rien de mieux\nqu‚Äôune fonction. Celle-ci est pr√©sente dans le module functions\nsous le nom clean_libelle.\n\nfrom functions import clean_libelle\n\nPour r√©sumer l‚Äôexercice pr√©c√©dent, cette fonction va :\n\nHarmoniser la casse et retirer les accents (voir functions.py) ;\nRetirer tout les caract√®res qui ne sont pas des lettres (chiffres, ponctuations) ;\nRetirer les caract√®res isol√©s.\n\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\n\nstop_words = ['KG','CL','G','L','CRUE?S?', 'PREEMBALLEE?S?']\nstop_words += [l.upper() for l in stopwords.words('french')]\n\nreplace_regex = {r'[^A-Z]': ' ', r'\\b[A-Z0-9]{1,2}?\\b':' '} # \n\nCela permet d‚Äôobtenir les bases nettoy√©es suivantes :\n\nciqual = clean_libelle(ciqual, yvar = 'alim_nom_fr', replace_regex = replace_regex, stopWords = stop_words)\nciqual.sample(10)\n\n\nopenfood = clean_libelle(openfood, yvar = 'product_name', replace_regex = replace_regex, stopWords = stop_words)\nopenfood.sample(10)\n\n\ncourses = pd.DataFrame(libelles, columns = ['libel'])\ncourses = clean_libelle(courses, yvar = 'libel', replace_regex = replace_regex, stopWords = stop_words)\ncourses.sample(10)\n\nLes noms de produits sont d√©j√† plus harmonis√©s.\nVoyons voir si cela permet de trouver un\nmatch dans l‚ÄôOpenfood database:\n\ndist_leven_openfood = fc.match_product(courses[\"libel_clean\"], openfood, \"libel_clean\")\ndist_leven_openfood.sample(10)\n\nPas encore parfait, mais on progresse sur les produits appari√©s!\nConcernant le temps de calcul, les quelques secondes n√©cessaires √†\nce calcul peuvent appara√Ætre un faible prix √† payer. Cependant,\nil convient de rappeler que le nombre de produits dans l‚Äôensemble\nde recherche est faible. Cette solution n‚Äôest donc pas g√©n√©ralisable."
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#r√©duire-les-temps-de-recherche",
    "href": "content/modern-ds/elastic_intro.html#r√©duire-les-temps-de-recherche",
    "title": "Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "R√©duire les temps de recherche",
    "text": "R√©duire les temps de recherche\nFinalement, l‚Äôid√©al serait de disposer d‚Äôun moteur de recherche adapt√© √† notre besoin,\ncontenant les produits candidats, que l‚Äôon pourrait interroger, rapide en lecture, capable de classer les √©chos renvoy√©s par pertinence, que l‚Äôon pourrait requ√™ter de mani√®re flexible.\nPar exemple, on pourrait vouloir signaler qu‚Äôun\n√©cho nous int√©resse seulement si la donn√©e calorique n‚Äôest pas manquante.\nOn pourrait m√™me vouloir qu‚Äôil effectue pour nous des pr√©traitements sur les donn√©es.\nCela para√Æt beaucoup demander. Mais c‚Äôest exactement ce que fait ElasticSearch."
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#cr√©er-un-cluster-elastic-sur-le-datalab",
    "href": "content/modern-ds/elastic_intro.html#cr√©er-un-cluster-elastic-sur-le-datalab",
    "title": "Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "Cr√©er un cluster Elastic sur le DataLab",
    "text": "Cr√©er un cluster Elastic sur le DataLab\nPour lancer un service Elastic, il faut cliquer sur ce lien.\nUne fois cr√©√©, vous pouvez explorer l‚Äôinterface graphique Kibana.\nCependant, gr√¢ce √† l‚ÄôAPI Elastic\nde Python, on se passera de celle-ci. Donc, en pratique,\nune fois lanc√©, pas besoin d‚Äôouvrir ce service Elastic pour continuer √† suivre1.\nDans un terminal, vous pouvez aussi v√©rifier que vous √™tes en mesure de dialoguer avec votre cluster Elastic,\nqui est pr√™t √† vous √©couter:\nkubectl get statefulset\nPasser par la ligne de commande serait peu commode pour industrialiser notre\nrecherche.\nNous allons utiliser la librairie elasticsearch pour dialoguer avec notre moteur de recherche Elastic.\nLes instructions ci-dessous indiquent comment √©tablir la connection.\n\nfrom elasticsearch import Elasticsearch\nHOST = 'elasticsearch-master'\n\ndef elastic():\n    \"\"\"Connection avec Elastic sur le data lab\"\"\"\n    es = Elasticsearch([{'host': HOST, 'port': 9200, 'scheme': 'http'}], http_compress=True, request_timeout=200)\n    return es\n\nes = elastic()\n\n&lt;Elasticsearch([{'host': 'elasticsearch-master', 'port': 9200}])&gt;\nMaintenant que la connection est √©tablie, deux √©tapes nous attendent:\n\nIndexation Envoyer les documents parmi lesquels on veut chercher des echos pertinents dans notre elastic. Un index est une collection de document. Nous pourrions en cr√©er deux: un pour les produits ciqual, un pour les produits openfood\nRequ√™te Chercher les documents les plus pertinents suivant une recherche textuelle flexible. Nous allons rechercher les libell√©s de notre recette et de notre liste de course."
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#premi√®re-indexation",
    "href": "content/modern-ds/elastic_intro.html#premi√®re-indexation",
    "title": "Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "Premi√®re indexation",
    "text": "Premi√®re indexation\nOn cr√©e donc nos deux index:\n\nif not es.indices.exists(index = 'openfood'):\n    es.indices.create(index = 'openfood')\nif not es.indices.exists(index = 'ciqual'):\n    es.indices.create(index = 'ciqual')\n\nPour l‚Äôinstant, nos index sont vides! Ils contiennent 0 documents.\n\nes.count(index = 'openfood')\n\n{'count': 0, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}}\nNous allons en rajouter quelques-uns !\n\nes.create(index = 'openfood',  id = 1, body = {'product_name': 'Tarte noix de coco', 'product_name_clean': 'TARTE NOIX COCO'})\nes.create(index = 'openfood',  id = 2, body = {'product_name': 'Noix de coco', 'product_name_clean': 'NOIX COCO'})\nes.create(index = 'openfood',  id = 3, body = {'product_name': 'Beurre doux', 'product_name_clean': 'BEURRE DOUX'})\n\n\nes.count(index = 'openfood')\n\n{'count': 3, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}}\nDans l‚Äôinterface graphique Kibana,\non peut v√©rifier que l‚Äôindexation\na bien eue lieu en allant dans Management &gt; Stack Management"
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#premi√®re-recherche",
    "href": "content/modern-ds/elastic_intro.html#premi√®re-recherche",
    "title": "Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "Premi√®re recherche",
    "text": "Premi√®re recherche\nFaisons notre premi√®re recherche: cherchons des noix de p√©can!\n\nes.search(index = 'openfood', q = 'noix de p√©can')\n\nObjectApiResponse({'took': 116, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 2, 'relation': 'eq'}, 'max_score': 0.9400072, 'hits': [{'_index': 'openfood', '_type': '_doc', '_id': '2', '_score': 0.9400072, '_source': {'product_name': 'Noix de coco', 'product_name_clean': 'NOIX COCO'}}, {'_index': 'openfood', '_type': '_doc', '_id': '1', '_score': 0.8272065, '_source': {'product_name': 'Tarte noix de coco', 'product_name_clean': 'TARTE NOIX COCO'}}]}})\nInt√©ressons nous aux hits (r√©sultats pertinents, ou echos) : nous en avons 2.\nLe score maximal parmi les hits est mentionn√© dans max_score et correspond √† celui du deuxi√®me document index√©.\nElastic nous fournit ici un score de pertinence dans notre recherche d‚Äôinformation, et classe ainsi les documents renvoy√©s.\nIci nous utilisons la configuration par d√©faut. Mais comment est calcul√© ce score? Demandons √† Elastic de nous expliquer le score du document 2 dans la requ√™te \"noix de p√©can\".\n\nes.explain(index = 'openfood', id = 2, q = 'noix de p√©can')\n\nObjectApiResponse({'_index': 'openfood', '_type': '_doc', '_id': '2', 'matched': True, 'explanation': {'value': 0.9400072, 'description': 'max of:', 'details': [{'value': 0.49917626, 'description': 'sum of:', 'details': [{'value': 0.49917626, 'description': 'weight(product_name_clean:noix in 1) [PerFieldSimilarity], result of:', 'details': [{'value': 0.49917626, 'description': 'score(freq=1.0), computed as boost * idf * tf from:', 'details': [{'value': 2.2, 'description': 'boost', 'details': []}, {'value': 0.47000363, 'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:', 'details': [{'value': 2, 'description': 'n, number of documents containing term', 'details': []}, {'value': 3, 'description': 'N, total number of documents with field', 'details': []}]}, {'value': 0.48275858, 'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:', 'details': [{'value': 1.0, 'description': 'freq, occurrences of term within document', 'details': []}, {'value': 1.2, 'description': 'k1, term saturation parameter', 'details': []}, {'value': 0.75, 'description': 'b, length normalization parameter', 'details': []}, {'value': 2.0, 'description': 'dl, length of field', 'details': []}, {'value': 2.3333333, 'description': 'avgdl, average length of field', 'details': []}]}]}]}]}, {'value': 0.9400072, 'description': 'sum of:', 'details': [{'value': 0.4700036, 'description': 'weight(product_name:noix in 1) [PerFieldSimilarity], result of:', 'details': [{'value': 0.4700036, 'description': 'score(freq=1.0), computed as boost * idf * tf from:', 'details': [{'value': 2.2, 'description': 'boost', 'details': []}, {'value': 0.47000363, 'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:', 'details': [{'value': 2, 'description': 'n, number of documents containing term', 'details': []}, {'value': 3, 'description': 'N, total number of documents with field', 'details': []}]}, {'value': 0.45454544, 'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:', 'details': [{'value': 1.0, 'description': 'freq, occurrences of term within document', 'details': []}, {'value': 1.2, 'description': 'k1, term saturation parameter', 'details': []}, {'value': 0.75, 'description': 'b, length normalization parameter', 'details': []}, {'value': 3.0, 'description': 'dl, length of field', 'details': []}, {'value': 3.0, 'description': 'avgdl, average length of field', 'details': []}]}]}]}, {'value': 0.4700036, 'description': 'weight(product_name:de in 1) [PerFieldSimilarity], result of:', 'details': [{'value': 0.4700036, 'description': 'score(freq=1.0), computed as boost * idf * tf from:', 'details': [{'value': 2.2, 'description': 'boost', 'details': []}, {'value': 0.47000363, 'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:', 'details': [{'value': 2, 'description': 'n, number of documents containing term', 'details': []}, {'value': 3, 'description': 'N, total number of documents with field', 'details': []}]}, {'value': 0.45454544, 'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:', 'details': [{'value': 1.0, 'description': 'freq, occurrences of term within document', 'details': []}, {'value': 1.2, 'description': 'k1, term saturation parameter', 'details': []}, {'value': 0.75, 'description': 'b, length normalization parameter', 'details': []}, {'value': 3.0, 'description': 'dl, length of field', 'details': []}, {'value': 3.0, 'description': 'avgdl, average length of field', 'details': []}]}]}]}]}]}})\nElastic nous explique donc que le score 0.9400072 est le maximum entre deux sous-scores, 0.4991 et 0.9400072.\nPour chacun de ces sous-scores, le d√©tail de son calcul est donn√©.\nLe premier sous-score n‚Äôa accord√© un score que par rapport au premier mot (noix), tandis que le second a accord√© un score sur la base des deux mots d√©j√† connu dans les documents (‚Äúnoix‚Äù et ‚Äúde‚Äù). Il a ignor√© p√©can! Jusqu‚Äô√† pr√©sent, ce terme n‚Äôest pas connu dans l‚Äôindex.\nLa pertinence d‚Äôun mot pour notre recherche est construite sur une variante de la TF-IDF,\nconsid√©rant qu‚Äôun terme est pertinent s‚Äôil est souvent pr√©sent dans le document (Term Frequency)\nalors qu‚Äôil est peu fr√©quent dans les autres document (inverse document frequency).\nIci les notations des documents 1 et 2 sont tr√®s proches, la diff√©rence est d√ªe √† des IDF plus faibles dans le document 1,\nqui est p√©nalis√© pour √™tre l√©g√©rement plus long.\nBref, tout √ßa est un peu lourd, mais assez efficace,\nen tout cas moins rudimentaire que les distances caract√®res √† caract√®res pour ramener des echos pertinents."
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#limite-de-cette-premi√®re-indexation",
    "href": "content/modern-ds/elastic_intro.html#limite-de-cette-premi√®re-indexation",
    "title": "Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "Limite de cette premi√®re indexation",
    "text": "Limite de cette premi√®re indexation\nPour l‚Äôinstant, Elastic n‚Äôa pas l‚Äôair de g√©rer les fautes de frappes!\nPas le droit √† l‚Äôerreur dans la requ√™te:\n\nes.search(index = 'openfood',q = 'TART NOI')\n\nObjectApiResponse({'took': 38, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 0, 'relation': 'eq'}, 'max_score': None, 'hits': []}})\nCela s‚Äôexplique par la repr√©sentation des champs (‚Äòproduct_name‚Äô par exemple) qu‚ÄôElastic a inf√©r√©,\npuisque nous n‚Äôavons rien sp√©cifi√©.\nLa repr√©sentation d‚Äôune variable conditionne la fa√ßon dont les champs sont analys√©s pour calculer la pertinence.\nPar exemple, regardons la repr√©sentation du champ product_name\n\nes.indices.get_field_mapping(index = 'openfood', fields = 'product_name')\n\nObjectApiResponse({'openfood': {'mappings': {'product_name': {'full_name': 'product_name', 'mapping': {'product_name': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}}}}}})\nElastic a compris qu‚Äôil s‚Äôagissait d‚Äôun champ textuel.\nEn revanche, le type est keyword n‚Äôautorise pas des analyses approximatives donc\nne permet pas de tenir compte de fautes de frappes.\nPour qu‚Äôun echo remonte, un des termes doit matcher exactement. Dommage !\nMais c‚Äôest parce qu‚Äôon a utilis√© le mapping par d√©faut.\nEn r√©alit√©, il est assez simple de pr√©ciser un mapping plus riche,\nautorisant une analyse ‚Äúfuzzy‚Äù ou ‚Äúflou‚Äù."
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#nos-premi√®res-requ√™tes",
    "href": "content/modern-ds/elastic_intro.html#nos-premi√®res-requ√™tes",
    "title": "Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "Nos premi√®res requ√™tes",
    "text": "Nos premi√®res requ√™tes\nV√©rifions qu‚Äôon recup√®re quelques tartes aux noix m√™me si l‚Äôon fait plein de fautes:\n\nes.search(index = 'openfood', q = 'TART NOI', size = 3)\n\nObjectApiResponse({'took': 60, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 10000, 'relation': 'gte'}, 'max_score': 22.837925, 'hits': [{'_index': 'openfood', '_type': '_doc', '_id': '405332', '_score': 22.837925, '_source': {'product_name': 'Tarte noix', 'libel_clean': 'TARTE NOIX', 'energy_100g': 1833.0, 'nutriscore_score': 23.0}}, {'_index': 'openfood', '_type': '_doc', '_id': '1103594', '_score': 22.82367, '_source': {'product_name': 'Tarte aux noix', 'libel_clean': 'TARTE NOIX', 'energy_100g': 4.0, 'nutriscore_score': 4.0}}, {'_index': 'openfood', '_type': '_doc', '_id': '1150755', '_score': 22.82367, '_source': {'product_name': 'Tarte aux noix', 'libel_clean': 'TARTE NOIX', 'energy_100g': 1929.0, 'nutriscore_score': 21.0}}]}})\nSi on pr√©f√®re sous une forme de DataFrame:\n\ndf = pd.json_normalize(\n    es.search(index = 'openfood', q = 'TART NOI', size = 3)['hits']['hits']\n)\ndf.columns = df.columns.str.replace(\"_source.\", \"\", regex = False)\ndf.head(2)\n\n\n\\n  \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n  \\n  \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n  \\n_index_type_id_scoreproduct_namelibel_cleanenergy_100gnutriscore_score0openfood_doc40533222.837925Tarte noixTARTE NOIX1833.023.01openfood_doc110359422.823670Tarte aux noixTARTE NOIX4.04.02openfood_doc115075522.823670Tarte aux noixTARTE NOIX1929.021.0\nPour automatiser l‚Äôenvoi de requ√™tes et la r√©cup√©ration du meilleur\n√©cho, on peut d√©finir la fonction suivante\n\ndef matchElastic(libelles):\n    start_time = time.time()\n    matches = {}\n    for l in libelles:\n        response = es.search(index = 'openfood', q = l, size = 1)\n        if len(response['hits']['hits'])&gt;0:\n            matches[l] = pd.json_normalize(\n              response['hits']['hits']\n            )\n    print(80*'-')\n    print(f\"Temps d'ex√©cution total : {(time.time() - start_time):.2f} secondes ---\")\n    \n    return matches\n\n\nmatches = matchElastic(courses['libel_clean'])\nmatches = pd.concat(matches)\nmatches.sample(3)\n\n\n\\n  \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n  \\n  \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n  \\n_index_type_id_score_source.product_name_source.libel_clean_source.energy_100g_source.nutriscore_scoreGOUSSE AIL0openfood_doc198206257.93140Gousse d\\'ailGOUSSE AIL498.05.0IGP SAUVIGNON0openfood_doc180140696.55756vin blanc SauvignonVIN BLANC SAUVIGNON66.31.0POTIRON0openfood_doc104396175.96385PotironPOTIRON172.00.0\nEt voil√†, on a un outil tr√®s rapide de requ√™te !\nLa pertinence des r√©sultats est encore douteuse.\nPour cela, il conviendrait de pr√©ciser des requ√™tes plus sophistiqu√©es!2\n\nreq = {\n    \"bool\": {\n      \"should\": [\n        { \"match\": { \"libel_clean\":  { \"query\":  \"HUILE OLIVE\" , \"boost\" : 10}}},\n        { \"match\": { \"libel_clean.ngr\":   \"HUILE OLIVE\" }}\n        ],\n      \"minimum_should_match\": 1,\n      \"filter\": [\n      { \n            \"range\" : {\n                \"nutriscore_score\" : {\n                    \"gte\" : 10,\n                    \"lte\" : 20\n                    }\n                    }\n                    }\n      ]\n    }\n}\n\n\nout = es.search(index = 'openfood', query = req, size = 1)\npd.json_normalize(out['hits']['hits'])\n\n\n\\n  \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n  \\n  \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n  \\n_index_type_id_score_source.product_name_source.libel_clean_source.energy_100g_source.nutriscore_score0openfood_doc960041174.27896Huile d oliveHUILE OLIVE3761.011.0\nQu‚Äôa-t-on demand√© ici?\n- De renvoyer 1 et 1 seul echo (\"size\":\"1\") et seulement si celui ci a:\n+ \"should\": Au moins un (\"minimum_should_match\":\"1\") des termes des deux champs libel_clean et libel_clean.ngr qui matche sur un terme de HUILE OLIVE, l‚Äôanalyse (la d√©finition du ‚Äúterme‚Äù) √©tant r√©alis√© soit en tant que text (‚Äúlibel_clean‚Äù) soit en tant que n-gramme ngr (‚Äúlibel_clean.ngr‚Äù, une analyse que nous avons sp√©cifi√© dans le mapping)\n+ \"filter\": Le champ float nutriscore_score doit √™tre compris entre 10 et 20 (‚Äúfilter‚Äù).\nA noter :\n\nLes clauses (\"should\"+\"minimum_should_match\":\"1\") peuvent √™tre remplac√© par un \"must\". Auquel cas, l‚Äô√©cho doit obligatoirement matcher sur chaque clause.\nPr√©ciser dans \"filter\" (plut√¥t que dans \"should\") une condition signifie que celle-ci ne participe pas au score de pertinence.\n\nOn n‚Äôa pas encore un appariemment tr√®s satisfaisant, en particulier sur les boissons. Comment faire ? La r√©ponse est dans Galiana and Suarez Castillo (2022)\n\nA vous, de calculer le nombre de calories de notre recette de course !"
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#footnotes",
    "href": "content/modern-ds/elastic_intro.html#footnotes",
    "title": "Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLe lancement du service a cr√©√© dans votre NAMESPACE Kubernetes (l‚Äôensemble de tout vos services) un cluster Elastic.\nVous n‚Äôavez droit qu‚Äô√† un cluster par namespace (ou compte d‚Äôutilisateur).\nVotre service Jupyter, VSCode, RStudio, etc. est associ√© au m√™me namespace.\nDe m√™me qu‚Äôil n‚Äôest pas n√©cessaire de comprendre comment fonctionne le moteur d‚Äôune voiture pour conduire,\nil n‚Äôest pas n√©cessaire de comprendre la mani√®re dont tout ce beau monde dialogue pour pouvoir utiliser le SSP Cloud.‚Ü©Ô∏é\nVous pouvez aussi explorer les possibilit√©s de requ√™tes via la doc Elastic et vous entrainer √† un √©crire avec votre index tout neuf.‚Ü©Ô∏é"
  },
  {
    "objectID": "content/git/introgit.html",
    "href": "content/git/introgit.html",
    "title": "Git : un √©l√©ment essentiel au quotidien",
    "section": "",
    "text": "Cette page reprend des √©l√©ments pr√©sents dans\nun cours d√©di√© fait avec Romain Avouac."
  },
  {
    "objectID": "content/git/introgit.html#les-probl√®mes-classiques",
    "href": "content/git/introgit.html#les-probl√®mes-classiques",
    "title": "Git : un √©l√©ment essentiel au quotidien",
    "section": "Les probl√®mes classiques",
    "text": "Les probl√®mes classiques\nDans un projet, il est commun de se demander (ou de demander √† quelqu‚Äôun) :\n\nquelle √©tait la bonne version d‚Äôun programme ?\nqui √©tait l‚Äôauteur d‚Äôun bout de code en particulier ?\nsi un changement √©tait important ou juste un essai ?\no√π retrouver des traces d‚Äôun vieil essai abandonn√© mais potentiellement finalement prometteur ?\ncomment fusionner des programmes √©crits par plusieurs personnes ?\netc."
  },
  {
    "objectID": "content/git/introgit.html#la-solution-le-contr√¥le-de-version",
    "href": "content/git/introgit.html#la-solution-le-contr√¥le-de-version",
    "title": "Git : un √©l√©ment essentiel au quotidien",
    "section": "La solution: le contr√¥le de version",
    "text": "La solution: le contr√¥le de version\nIl existe un outil informatique puissant qui r√©pond √† tous ces besoins :\nla gestion de version (version control system (VCS) en anglais). Ses avantages sont incontestables et permettent de facilement :\n\nenregistrer l‚Äôhistorique des modifications d‚Äôun ensemble de fichiers ;\nrevenir √† des versions pr√©c√©dentes d‚Äôun ou plusieurs fichiers ;\nrechercher les modifications qui ont pu cr√©er des erreurs ;\npartager ses modifications et r√©cup√©rer celles des autres ;\nproposer des modifications, les discuter, sans pour autant modifier d‚Äôembl√©e la derni√®re version existante ;\nidentifier les auteurs et la date des modifications.\n\nEn outre, ces outils fonctionnent avec tous les langages\ninformatiques (texte, R, Python, Markdown, LaTeX, Java, etc.)\ncar reposent sur la comparaison des lignes et des caract√®res des programmes."
  },
  {
    "objectID": "content/git/introgit.html#avantages-du-contr√¥le-de-version",
    "href": "content/git/introgit.html#avantages-du-contr√¥le-de-version",
    "title": "Git : un √©l√©ment essentiel au quotidien",
    "section": "Avantages du contr√¥le de version",
    "text": "Avantages du contr√¥le de version\nOn peut ainsi r√©sumer les principaux avantages du contr√¥le de version\nde la mani√®re suivante :\n\nConserver et archiver l‚Äôensemble des versions d‚Äôun code ou d‚Äôune documentation\nTravailler efficacement en √©quipe\nAm√©liorer la qualit√© des codes\nSimplifier la communication autour d‚Äôun projet\n\n\nConserver et archiver du code\nUne des principales fonctionnalit√©s de la gestion de version est de conserver\nl‚Äôensemble des fichiers de fa√ßon s√©curis√©e et de proposer un archivage\nstructur√© des codes. Les fichiers sont stock√©s dans un d√©p√¥t, qui constitue le projet.\nTout repose dans la gestion et la pr√©sentation de l‚Äôhistorique des modifications.\nChaque modification (ajout, suppression ou changement) sur un ou plusieurs fichiers est identifi√©e par son auteur,\nsa date et un bref descriptif1.\nChaque changement est donc unique et ais√©ment identifiable quand les modifications sont class√©es par ordre chronologique. Les groupes de modifications transmis au d√©p√¥t sont appel√©es commit.\nAvec des outils graphiques, on peut v√©rifier l‚Äô\nensemble des √©volutions d‚Äôun fichier (history),\nou l‚Äôhistoire d‚Äôun d√©p√¥t.\nOn peut aussi\nse concentrer sur une modification particuli√®re d‚Äôun fichier ou v√©rifier, pour un fichier, la\nmodification qui a entra√Æn√© l‚Äôapparition de telle ou telle ligne (blame)\nSur son poste de travail, les dizaines (voire centaines) de programmes organis√©s √† la main n‚Äôexistent plus. Tout est regroup√© dans un seul dossier, rassemblant les √©l√©ments du d√©p√¥t. Au sein du d√©p√¥t, tout l‚Äôhistorique est stock√© et accessible rapidement. Si on souhaite travailler sur la derni√®re version des programmes (ou sur une ancienne version sp√©cifique), il n‚Äôy a plus besoin de conserver les autres fichiers car ils sont dans l‚Äôhistorique du projet. Il est alors possible de choisir sur quelle version on veut travailler (la derni√®re commune √† tout le monde, la sienne en train d‚Äô√™tre d√©velopp√©e, celle de l‚Äôann√©e derni√®re, etc.).\n\n\nTravailler efficacement en √©quipe\nLe deuxi√®me avantage de la gestion de version repr√©sente une am√©lioration notable du travail en √©quipe sur des codes en commun.\nLa gestion de version permet de collaborer simplement et avec m√©thode. De fa√ßon organis√©e, elle permet de :\n\ntravailler en parall√®le et fusionner facilement du code\npartager une documentation des programmes gr√¢ce :\n\naux commentaires des modifications\n√† la possibilit√© d‚Äôune documentation commune et collaborative\n\ntrouver rapidement des erreurs et en diffuser rapidement la\ncorrection\n\nA ces avantages s‚Äôajoutent les fonctionalit√©s collaboratives des forges\nqui sont des plateformes o√π peuvent √™tre stock√©s des d√©p√¥ts.\nN√©anmoins, ces forges proposent aujourd‚Äôhui beaucoup de fonctionalit√©s\nqui vont au-del√† de l‚Äôarchivage de code :\ninteragir via\ndes issues,\nfaire des suggestions de modifications, ex√©cuter du code dans des\nenvironnements normalis√©s, etc.\nIl faut vraiment les voir comme des r√©seaux sociaux du code.\nLes principales plateformes dans ce domaine √©tant Github et Gitlab.\nL‚Äôusage individuel, c‚Äôest-√†-dire seul sur son projet,\npermet aussi de ‚Äútravailler en √©quipe avec soi-m√™me‚Äù car il permet de retrouver des mois plus tard le contenu et le contexte des modifications. Cela est notamment pr√©cieux lors des changements de poste ou des travaux r√©guliers mais espac√©s dans le temps (par exemple, un mois par an chaque ann√©e). M√™me lorsqu‚Äôon travaille tout seul, on collabore avec un moi futur qui peut ne plus se souvenir de la modification des fichiers.\n\n\nAm√©liorer la qualit√© des codes\nLe fonctionnement de la gestion de version, reposant sur l‚Äôarchivage structur√© des modifications et les commentaires les accompagnant, renforce la qualit√© des programmes informatiques. Ils sont plus document√©s, plus riches et mieux structur√©s. C‚Äôest pour cette raison que le contr√¥le de version ne doit pas √™tre consid√©r√© comme un outil r√©serv√© √† des d√©veloppeurs : toute personne travaillant sur des programmes informatiques gagne √† utiliser du contr√¥le de version.\nLes services d‚Äôint√©gration continue permettent de faire des tests automatiques\nde programmes informatiques, notamment de packages, qui renforcent la\nr√©plicabilit√© des programmes. Mettre en place des m√©thodes de travail fond√©es\nsur l‚Äôint√©gration continue rend les programmes plus robustes en for√ßant\nceux-ci √† tourner sur des machines autres que celles du d√©veloppeur du code.\n\n\nSimplifier la communication autour d‚Äôun projet\nLes sites de d√©p√¥ts Github et Gitlab permettent de faire beaucoup plus\nque seulement archiver des codes. Les fonctionalit√©s de d√©ploiement\nen continu permettent ainsi de :\n\ncr√©er des sites web pour valoriser des projets (par exemple les sites\nreadthedocs en python)\nd√©ployer de la documentation en continu\nrendre visible la qualit√© d‚Äôun projet avec des services de code coverage,\nde tests automatiques ou d‚Äôenvironnements int√©gr√©s de travail (binder, etc.)\nqu‚Äôon rend g√©n√©ralement visible au moyen de badges\n(exemple ici)"
  },
  {
    "objectID": "content/git/introgit.html#copies-de-travail-et-d√©p√¥t-collectif",
    "href": "content/git/introgit.html#copies-de-travail-et-d√©p√¥t-collectif",
    "title": "Git : un √©l√©ment essentiel au quotidien",
    "section": "Copies de travail et d√©p√¥t collectif",
    "text": "Copies de travail et d√©p√¥t collectif\nGit est un syst√®me d√©centralis√© et asynchrone de gestion de version.\nCela signifie que:\n\nChaque membre d‚Äôun projet travaille sur une copie locale du d√©p√¥t\n(syst√®me decentralis√©). Cette copie de travail s‚Äôappelle un clone.\nCela signifie qu‚Äôon n‚Äôa pas une coh√©rence en continu de notre version\nde travail avec le d√©p√¥t ; on peut tr√®s bien ne jamais vouloir les\nmettre en coh√©rence (par exemple, si on teste une piste qui s‚Äôav√®re\ninfructueuse) ;\nC‚Äôest lorsqu‚Äôon propose la publication de modifications sur le d√©p√¥t\ncollectif qu‚Äôon doit s‚Äôassurer de la coh√©rence avec la version disponible\nen ligne (syst√®me asynchrone).\n\nLe d√©p√¥t distant est g√©n√©ralement stock√© sur\nune forge logicielle (Github ou Gitlab) et sert √† centraliser la version\ncollective d‚Äôun projet. Les copies locales sont des copies de travail\nqu‚Äôon utilise pour faire √©voluer un projet :\nIl est tout √† fait possible de faire du contr√¥le de version sans\nmettre en place de d√©p√¥t distant. Cependant,\n\nc‚Äôest dangereux puisque le d√©p√¥t distant fait office de sauvegarde\nd‚Äôun projet. Sans d√©p√¥t distant, on peut tout perdre en cas de probl√®me\nsur la copie locale de travail ;\nc‚Äôest d√©sirer √™tre moins efficace car, comme nous allons le montrer, les\nfonctionalit√©s des plateformes Github et Gitlab sont √©galement tr√®s\nb√©n√©fiques lorsqu‚Äôon travaille tout seul."
  },
  {
    "objectID": "content/git/introgit.html#principe",
    "href": "content/git/introgit.html#principe",
    "title": "Git : un √©l√©ment essentiel au quotidien",
    "section": "Principe",
    "text": "Principe\nLes trois manipulations les plus courantes sont les suivantes et repr√©sent√©es sur le diagramme ci-apr√®s :\n\ncommit : je valide les modifications que j‚Äôai faites en local avec un message qui les explique\npull : je r√©cup√®re la derni√®re version des codes du d√©p√¥t distant\npush : je transmets mes modifications valid√©es au d√©p√¥t distant\n\n\n\n\n\n\nLes deux derni√®res manipulations correspondent aux interactions (notamment\nla mise en coh√©rence) avec\nle d√©p√¥t commun alors que la premi√®re manipulation commit correspond √†\nla modification des fichiers faite pour faire √©voluer un projet.\nDe mani√®re plus pr√©cise, il y a trois √©tapes avant d‚Äôenvoyer les modifications valid√©es (commit) au d√©p√¥t. Elles se d√©finissent en fonction des commandes qui permettent de les appliquer quand Git est utilis√© en lignes de commandes :\n\ndiff : inspection des modifications. Cela permet de comparer les fichiers modifi√©s et de distinguer les fichiers ajout√©s ou supprim√©s.\nstaging area : s√©lection des modifications.\ncommit : validation des modifications s√©lectionn√©es (avec commentaire).\n\n\n\n\n\n\nLors des √©tapes de push et pull, des conflits peuvent appara√Ætre, par exemple lorsque deux personnes ont modifi√© le m√™me programme simultan√©ment. Le terme conflit peut faire peur mais en fait c‚Äôest\nl‚Äôun des apports principaux de Git que de faciliter √©norm√©ment la gestion\nde versions diff√©rentes. Les exercices du chapitre suivant l‚Äôillustreront."
  },
  {
    "objectID": "content/git/introgit.html#les-branches",
    "href": "content/git/introgit.html#les-branches",
    "title": "Git : un √©l√©ment essentiel au quotidien",
    "section": "Les branches",
    "text": "Les branches\nC‚Äôest une des fonctionnalit√©s les plus pratiques de la gestion de version.\nLa cr√©ation de branches dans un projet (qui devient ainsi un arbre)\npermet de d√©velopper en parall√®le des correctifs ou une nouvelle fonctionnalit√©\nsans modifier le d√©p√¥t commun.\nCela permet de s√©parer le nouveau d√©veloppement et de faire cohabiter plusieurs versions, pouvant √©voluer s√©par√©ment ou pouvant √™tre facilement rassembl√©es. Git est optimis√© pour le travail sur les branches.\nDans un projet collaboratif, une branche dite master joue le r√¥le du tronc.\nC‚Äôest autour d‚Äôelle que vont pousser ou se greffer les branches.\nL‚Äôun des avantages de Git est qu‚Äôon peut toujours revenir en arri√®re. Ce\nfilet de s√©curit√© permet d‚Äôoser des exp√©rimentations, y compris au sein\nd‚Äôune branche. Il faut √™tre pr√™t √† aller dans la ligne de commande pour cela\nmais c‚Äôest extr√™mement confortable.\n\n\n Note\nComment nommer les branches ? L√† encore, il y a √©norm√©ment de conventions diff√©rentes. Une fr√©quemment observ√©e est :\n\npour les nouvelles fonctionnalit√©s : feature/nouvelle-fonctionnalite o√π nouvelle-fontionnalite est un nom court r√©sumant la fonctionnalit√©\npour les corrections de bug : issue-num o√π num est le num√©ro de l‚Äôissue\n\nN‚Äôh√©sitez pas √† aller encore plus loin dans la normalisation !"
  },
  {
    "objectID": "content/git/introgit.html#footnotes",
    "href": "content/git/introgit.html#footnotes",
    "title": "Git : un √©l√©ment essentiel au quotidien",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPlus pr√©cis√©ment, chaque modification est identifi√©e de mani√®re unique par un code SHA auquel est associ√© l‚Äôauteur, l‚Äôhorodatage et des m√©tadonn√©es (par exemple le message descriptif associ√©).‚Ü©Ô∏é"
  },
  {
    "objectID": "content/annexes/evaluation.html",
    "href": "content/annexes/evaluation.html",
    "title": "Evaluation",
    "section": "",
    "text": "R√©sum√© :"
  },
  {
    "objectID": "content/annexes/evaluation.html#attentes-du-projet",
    "href": "content/annexes/evaluation.html#attentes-du-projet",
    "title": "Evaluation",
    "section": "Attentes du projet",
    "text": "Attentes du projet\nLe projet est une probl√©matique √† laquelle vous souhaitez r√©pondre √†\nl‚Äôaide d‚Äôun ou de plusieurs jeu(s) de donn√©es.\nIl faut donc dans un premier temps se pencher sur la recherche de probl√©matisation et de contextualisation. Nous vous recommandons de prendre un sujet qui vous int√©resse pour int√©resser √©galement le lecteur.\nTrois dimensions doivent √™tre pr√©sentes dans le projet.\nPour chacune de ces parties, il est possible d‚Äôaller plus ou moins loin. Il est recommand√© d‚Äôaller loin sur au moins une des 3 dimensions.\n\nLa r√©cup√©ration et le traitement des donn√©es\nCes donn√©es peuvent √™tre directement disponibles sous la forme de fichiers txt, csv ‚Ä¶ ou provenir de sites internet (scraping, API). Plus le travail sur la r√©cup√©ration de donn√©es est important (par exemple scraping sur plusieurs sites), plus la partie obtiendra de points. Si le jeu de donn√©es utilis√© est un t√©l√©chargement d‚Äôun jeu propre existant, il faudra chercher √† le compl√©ter d‚Äôune mani√®re ou d‚Äôune autre pour obtenir des points sur cette partie.\nVous obtiendrez vraisemblablement des donn√©es qui ne sont pas ¬´ propres ¬ª du premier coup : mettez en place des protocoles de nettoyage pour obtenir √† la fin de cette √©tape un ou des jeux de donn√©es fiable et robuste pour mener ensuite votre analyse. C‚Äôest √©galement le moment de cr√©er des variables plus appr√©hendables, mieux identifi√©es etc.\n\n\nL‚Äôanalyse descriptive et la repr√©sentation graphique\nLa pr√©sence de statistiques descriptives est indispensable dans le projet. De la description de la base aux premi√®res grandes tendances des donn√©es, cette partie permet d‚Äôavoir une vision globale des donn√©es : le lien avec la probl√©matique, comment elle permet d‚Äôy r√©pondre, quels sont les premiers √©l√©ments de r√©ponse‚Ä¶ Chaque r√©sultat doit √™tre interpr√©t√© : pas la peine de faire un describe et de ne pas le commenter.\nEn termes de repr√©sentation graphique, plusieurs niveaux sont envisageables. Vous pouvez simplement repr√©senter vos donn√©es en utilisant matplotlib, aller plus loin avec seaborn ou scikit-plot, (voire D3.js pour les plus motiv√©s). La base d‚Äôune bonne visualisation est de trouver le type de graphique ad√©quat pour ce que vous voulez montrer (faut-il un scatter ou un line pour repr√©senter une √©volution ?) et de le rendre visible : une l√©gende qui a du sens, des axes avec des noms etc. Encore une fois, il faudra commenter votre graphique, qu‚Äôest ce qu‚Äôil montre, en quoi cela valide / contredit votre argumentaire ?\n\n\nLa mod√©lisation\nVient ensuite la phase de mod√©lisation : un mod√®le peut √™tre le bienvenu quand des statistiques descriptives ne suffisent pas √† apporter une solution compl√®te √† votre probl√©matique ou pour compl√©ter / renforcer l‚Äôanalyse descriptive. Le mod√®le importe peu (r√©gression lin√©aire, random forest ou autre) : il doit √™tre appropri√© (r√©pondre √† votre probl√©matique) et justifi√©.\nVous pouvez aussi confronter plusieurs mod√®les qui n‚Äôont pas la m√™me vocation : par exemple une CAH pour cat√©goriser et cr√©er des nouvelles variables / faire des groupes puis une r√©gression.\nM√™me si le projet n‚Äôest pas celui du cours de stats, il faut que la d√©marche soit scientifique et que les r√©sultats soient interpr√©t√©s."
  },
  {
    "objectID": "content/annexes/evaluation.html#format-du-rendu",
    "href": "content/annexes/evaluation.html#format-du-rendu",
    "title": "Evaluation",
    "section": "Format du rendu",
    "text": "Format du rendu\nSur le format du rendu, vous devrez :\n\n√âcrire un rapport sous forme de Notebook (quelques exceptions √† cette r√®gle peuvent exister, par exemple si vous d√©velopper une appli Dash)\nAvoir un r√©pertoire Github avec le rapport. Les donn√©es utilis√©es doivent √™tre accessibles √©galement, dans le d√©p√¥t ou sur internet.\nLes d√©p√¥ts Github o√π seul un upload du projet a √©t√© r√©alis√© seront p√©nalis√©s. A l‚Äôinverse, les d√©p√¥ts dans lequels le contr√¥le de version et le travail collaboratif ont √©t√© activement pratiqu√©s (commits fr√©quents, pull requests, ..) seront valoris√©s.\nLe code contenu dans le rapport devra √™tre un maximum propre (pas de copier coller de cellule, pr√©f√©rez des fonctions)\n\nCe post donne\nquelques conseils pour avoir des notebooks agr√©ables √† lire. N‚Äôoubliez pas cette r√®gle :\n\ncode is read much more often than written\n\nLors de l‚Äô√©valuation, une attention particuli√®re sera donn√©e √† la reproductibilit√© de votre projet.\nChaque √©tape (r√©cup√©ration et traitement des donn√©es, analyses descriptives, mod√©lisation) doit pouvoir √™tre reproduite √† partir du notebook final. Pour les op√©rations qui prennent du temps (ex : web scraping massif, requ√™tage d‚ÄôAPI avec des limites de nombre de requ√™tes, entra√Ænement de mod√®le, etc.), vous devez inclure l‚Äôoutput (base de donn√©es, mod√®le entra√Æn√©..) dans le d√©p√¥t, afin que les √©tapes suivantes puissent s‚Äô√©xecuter sans probl√®me.\nLe test √† r√©aliser : faire tourner toutes les cellules de votre notebook et ne pas avoir d‚Äôerreur est une condition sine qua non pour avoir la moyenne."
  },
  {
    "objectID": "content/annexes/evaluation.html#bar√™me-approximatif",
    "href": "content/annexes/evaluation.html#bar√™me-approximatif",
    "title": "Evaluation",
    "section": "Bar√™me approximatif",
    "text": "Bar√™me approximatif\n\nDonn√©es (collecte et nettoyage) : 4 points\nAnalyse descriptive : 4 points\nMod√©lisation : 2 points\nD√©marche scientifique et reproductibilit√© du projet : 4 points\nFormat du code (code propre et github) : 2 points\nSoutenance : 4 points\n\nLe projet doit √™tre r√©alis√© en groupe de trois, voire deux."
  },
  {
    "objectID": "content/annexes/evaluation.html#projets-men√©s-par-les-√©tudiants",
    "href": "content/annexes/evaluation.html#projets-men√©s-par-les-√©tudiants",
    "title": "Evaluation",
    "section": "Projets men√©s par les √©tudiants",
    "text": "Projets men√©s par les √©tudiants\n\n\n\n\n\n\n\n\n\nProjet\nAuteurs\nURL projet \nTags\n\n\n\n\nGPS v√©lo int√©grant les bornes V√©lib, les accidents, la congestion et la m√©t√©o\nVinciane Desbois ; Imane Fares ; Romane Gajdos\nhttps://github.com/ImaneFa/Projet_Python\nV√©lib ; Pistes cyclables ; Accidents ; Folium\n\n\nQuiz Generator\nAdrien Servi√®re ; M√©lissa Tamine\nhttps://github.com/taminemelissa/quiz-generator\nMachine Learning ; Natural Language Processing ; Question Generation ; Word2Vec\n\n\nAnalyse de sentiments sur les vaccins COVID administr√©s en France\nKOAGNE FONGUIENG Florette ; KONKOBO Idrissa\nhttps://github.com/kidrissa/projetpy\nAPI ; NLP ; Wordcloud ; Mod√©lisation pr√©dictive\n\n\nEstimation de l‚Äôempreinte carbone d‚Äôune recette de cuisine\nJean-Baptiste Laval ; Hadrien Lolivier ; Sirine Louati\nhttps://github.com/sirinelouati/Plat_CO2\nscraping ; Dashboard ; Empreinte carbone ; Alimentation\n\n\nLe ‚Äúbon sens du boucher-charcutier de Tourcoing vaut-il mieux que les enqu√™tes de victimation ?‚Äù\nConrad Thiounn ; Gaston Vermersch\nhttps://github.com/cthiounn/python-datascience-ENSAE-2A\nAPI ; Open-data ; ACP ; CAH ; LASSO\n\n\nPr√©diction du revenu g√©n√©r√© par un film en fonction de ses caract√©ristiques\nDmitri Lebrun ; Corentin Pernot ; Nina Stizi\nhttps://github.com/NinaStizi/Python_ENSAE_2A\nScrapping ; Cin√©ma ; Machine Learning\n\n\nAnalyse du r√©seau ferr√© de la SNCF: Comment expliquer les retards permanents de la compagnie fran√ßaise ?\nDiego Renaud ; Victor Parent ; Marion Chabrol\nhttps://github.com/NinaStizi/Python_ENSAE_2A\nAPI ; SNCF ; LASSO\n\n\nLe ‚Äúbon sens du boucher-charcutier de Tourcoing vaut-il mieux que les enqu√™tes de victimation ?‚Äù\nConrad Thiounn ; Gaston Vermersch\nhttps://github.com/cthiounn/python-datascience-ENSAE-2A\nAPI ; Open-data ; ACP ; CAH ; LASSO\n\n\nPr√©diction du revenu g√©n√©r√© par un film en fonction de ses caract√©ristiques\nDmitri Lebrun ; Corentin Pernot ; Nina Stizi\nhttps://github.com/NinaStizi/Python_ENSAE_2A\nScrapping ; Cin√©ma ; Machine Learning\n\n\nAnalyse du r√©seau ferr√© de la SNCF: Comment expliquer les retards permanents de la compagnie fran√ßaise ?\nDiego Renaud ; Victor Parent ; Marion Chabrol\nhttps://github.com/NinaStizi/Python_ENSAE_2A\nAPI ; SNCF ; LASSO"
  }
]