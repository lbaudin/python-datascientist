[
  {
    "objectID": "content/manipulation/04c_API_TP.html",
    "href": "content/manipulation/04c_API_TP.html",
    "title": "Récupérer des données avec des API depuis Python",
    "section": "",
    "text": "La partie utilisant l’API DVF n’est plus à jour, elle sera mise à jour prochainement"
  },
  {
    "objectID": "content/manipulation/04c_API_TP.html#introduction-quest-ce-quune-api",
    "href": "content/manipulation/04c_API_TP.html#introduction-quest-ce-quune-api",
    "title": "Récupérer des données avec des API depuis Python",
    "section": "Introduction : Qu’est-ce qu’une API ?",
    "text": "Introduction : Qu’est-ce qu’une API ?\n\nDéfinition\nPour expliquer le principe d’une API, je vais reprendre le début de\nla fiche dédiée dans la documentation collaborative\nutilitR que je recommande de lire :\n\nUne Application Programming Interface (ou API) est une interface de programmation qui permet d’utiliser une application existante pour restituer des données. Le terme d’API peut être paraître intimidant, mais il s’agit simplement d’une façon de mettre à disposition des données : plutôt que de laisser l’utilisateur consulter directement des bases de données (souvent volumineuses et complexes), l’API lui propose de formuler une requête qui est traitée par le serveur hébergeant la base de données, puis de recevoir des données en réponse à sa requête.\nD’un point de vue informatique, une API est une porte d’entrée clairement identifiée par laquelle un logiciel offre des services à d’autres logiciels (ou utilisateurs). L’objectif d’une API est de fournir un point d’accès à une fonctionnalité qui soit facile à utiliser et qui masque les détails de la mise en oeuvre. Par exemple, l’API Sirene permet de récupérer la raison sociale d’une entreprise à partir de son identifiant Siren en interrogeant le référentiel disponible sur Internet directement depuis un script R, sans avoir à connaître tous les détails du répertoire Sirene.\nÀ l’Insee comme ailleurs, la connexion entre les bases de données pour les nouveaux projets tend à se réaliser par des API. L’accès à des données par des API devient ainsi de plus en plus commun et est amené à devenir une compétence de base de tout utilisateur de données.\nutilitR\n\n\n\nAvantages des API\nA nouveau, citons la documentation utilitR\nLes API présentent de multiples avantages :\n\n\nLes API rendent les programmes plus reproductibles. En effet, grâce aux API, il est possible de mettre à jour facilement les données utilisées par un programme si celles-ci évoluent. Cette flexibilité accrue pour l’utilisateur évite au producteur de données d’avoir à réaliser de multiples extractions, et réduit le problème de la coexistence de versions différentes des données.\nGrâce aux API, l’utilisateur peut extraire facilement une petite partie d’une base de données plus conséquente.\nLes API permettent de mettre à disposition des données tout en limitant le nombre de personnes ayant accès aux bases de données elles-mêmes.\nGrâce aux API, il est possible de proposer des services sur mesure pour les utilisateurs (par exemple, un accès spécifique pour les gros utilisateurs).\n\nutilitR\n\nL’utilisation accrue d’API dans le cadre de stratégies open-data est l’un\ndes piliers des 15 feuilles de route ministérielles\nen matière d’ouverture, de circulation et de valorisation des données publiques.\n\n\nUtilisation des API\nCitons encore une fois\nla documentation utilitR\n\nUne API peut souvent être utilisée de deux façons : par une interface Web, et par l’intermédiaire d’un logiciel (R, Python…). Par ailleurs, les API peuvent être proposées avec un niveau de liberté variable pour l’utilisateur :\n\nsoit en libre accès (l’utilisation n’est pas contrôlée et l’utilisateur peut utiliser le service comme bon lui semble) ;\nsoit via la génération d’un compte et d’un jeton d’accès qui permettent de sécuriser l’utilisation de l’API et de limiter le nombre de requêtes.\n\nutilitR\n\nDe nombreuses API nécessitent une authentification, c’est-à-dire un\ncompte utilisateur afin de pouvoir accéder aux données.\nDans un premier temps,\nnous regarderons exclusivement les API ouvertes sans restriction d’accès.\nCertains exercices et exemples permettront néanmoins d’essayer des API\navec restrictions d’accès."
  },
  {
    "objectID": "content/manipulation/04c_API_TP.html#requêter-une-api",
    "href": "content/manipulation/04c_API_TP.html#requêter-une-api",
    "title": "Récupérer des données avec des API depuis Python",
    "section": "Requêter une API",
    "text": "Requêter une API\n\nPrincipe général\n\nL’utilisation de l’interface Web est utile dans une démarche exploratoire mais trouve rapidement ses limites, notamment lorsqu’on consulte régulièrement l’API. L’utilisateur va rapidement se rendre compte qu’il est beaucoup plus commode d’utiliser une API via un logiciel de traitement pour automatiser la consultation ou pour réaliser du téléchargement de masse. De plus, l’interface Web n’existe pas systématiquement pour toutes les API.\nLe mode principal de consultation d’une API consiste à adresser une requête à cette API via un logiciel adapté (R, Python, Java…). Comme pour l’utilisation d’une fonction, l’appel d’une API comprend des paramètres qui sont détaillées dans la documentation de l’API.\nutilitR\n\nVoici les éléments importants à avoir en tête sur les requêtes (j’emprunte encore\nà utilitR) :\n\nLe point d’entrée d’un service offert par une API se présente sous la forme d’une URL (adresse web).\nChaque service proposé par une API a sa propre URL. Par exemple, dans le cas de l’OpenFood Facts,\nl’URL à utiliser pour obtenir des informations sur un produit particulier (l’identifiant 737628064502)\nest https://world.openfoodfacts.org/api/v0/product/737628064502.json\nCette URL doit être complétée avec différents paramètres qui précisent la requête (par exemple l’identifiant Siren). Ces paramètres viennent s’ajouter à l’URL, souvent à la suite de ?. Chaque service proposé par une API a ses propres paramètres, détaillés dans la documentation.\nLorsque l’utilisateur soumet sa requête, l’API lui renvoie une réponse structurée contenant l’ensemble des informations demandées. Le résultat envoyé par une API est majoritairement aux formats JSON ou XML (deux formats dans lesquels les informations sont hiérarchisées de manière emboitée). Plus rarement, certains services proposent une information sous forme plate (de type csv).\n\nDu fait de la dimension hiérarchique des formats JSON ou XML,\nle résultat n’est pas toujours facile à récupérer mais\nPython propose d’excellents outils pour cela (meilleurs que ceux de R).\nCertains packages, notamment json, facilitent l’extraction de champs d’une sortie d’API.\nDans certains cas, des packages spécifiques à une API ont été créés pour simplifier l’écriture d’une requête ou la récupération du résultat. Par exemple, le package\npynsee\npropose des options qui seront retranscrites automatiquement dans l’URL de\nrequête pour faciliter le travail sur les données Insee.\n\n\nIllustration avec une API de l’Ademe pour obtenir des diagnostics energétiques\nLe diagnostic de performance énergétique (DPE)\nrenseigne sur la performance énergétique d’un logement ou d’un bâtiment,\nen évaluant sa consommation d’énergie et son impact en terme d’émissions de gaz à effet de serre.\nLes données des performances énergétiques des bâtiments sont\nmises à disposition par l’Ademe.\nComme ces données sont relativement\nvolumineuses, une API peut être utile lorsqu’on ne s’intéresse\nqu’à un sous-champ des données.\nUne documentation et un espace de test de l’API sont disponibles\nsur le site API GOUV1.\nSupposons qu’on désire récupérer une centaine de valeurs pour la commune\nde Villieu-Loyes-Mollon dans l’Ain (code Insee 01450).\nL’API comporte plusieurs points d’entrée. Globalement, la racine\ncommune est:\n\nhttps://koumoul.com/data-fair/api/v1/datasets/dpe-france\n\nEnsuite, en fonction de l’API désirée, on va ajouter des éléments\nà cette racine. En l’occurrence, on va utiliser\nl’API field qui permet de récupérer des lignes en fonction d’un\nou plusieurs critères (pour nous, la localisation géographique):\nL’exemple donné dans la documentation technique est\n\nGET https://koumoul.com/data-fair/api/v1/datasets/dpe-france/values/{field}\n\nce qui en Python se traduira par l’utilisation de la méthode get du\npackage Request\nsur un url dont la structure est la suivante :\n\nil commencera par https://koumoul.com/data-fair/api/v1/datasets/dpe-france/values/ ;\nil sera ensuite suivi par des paramètres de recherche. Le champ {field}\ncommence ainsi généralement par un ? qui permet ensuite de spécifier des paramètres\nsous la forme nom_parameter=value\n\nA la lecture de la documentation, les premiers paramètres qu’on désire :\n\nLe nombre de pages, ce qui nous permet d’obtenir un certain nombre d’échos. On\nva seulement récupérer 10 pages ce qui correspond à une centaine d’échos. On va\nnéanmoins préciser qu’on veut 100 échos\nLe format de sortie. On va privilégier le JSON qui est un format standard dans le\nmonde des API. Python offre beaucoup de flexibilité grâce à l’un de\nses objets de base, à savoir le dictionnaire (type dict), pour manipuler de tels\nfichiers\nLe code commune des données qu’on désire obtenir. Comme on l’a évoqué,\non va récupérer les données dont le code commune est 01450. D’après la doc,\nil convient de passer le code commune sous le format:\ncode_insee_commune_actualise:{code_commune}. Pour éviter tout risque de\nmauvais formatage, on va utiliser %3A pour signifier :, %2A pour signifier * et\n%22 pour signifier \".\nD’autres paramètres annexes, suggérés par la documentation\n\nCela nous donne ainsi un URL dont la structure est la suivante :\n\ncode_commune = \"01450\"\nsize = 100\napi_root = \"https://koumoul.com/data-fair/api/v1/datasets/dpe-france/lines\"\nurl_api = f\"{api_root}?page=1&after=10&format=json&q_mode=simple&qs=code_insee_commune_actualise\" + \"%3A%22\" + f\"{code_commune}\" + \"%22\" + f\"&size={size}&select=\" + \"%2A&sampling=neighbors\"\n\nSi vous introduisez cet URL dans votre navigateur, vous devriez aboutir\nsur un JSON non formaté2. En Python,\non peut utiliser requests pour récupérer les données3 :\n\nimport requests\nimport pandas as pd\n\nreq = requests.get(url_api)\nwb = req.json()\n\nPrenons par exemple les 1000 premiers caractères du résultat, pour se donner\nune idée du résultat et se convaincre que notre filtre au niveau\ncommunal est bien passé :\nprint(req.content[:1000])\nb’{“total”: 121,“next”: “https://koumoul.com/data-fair/api/v1/datasets/dpe-france/lines?after=102721&format=json&q_mode=simple&qs=code_insee_commune_actualise%3A%2201450%22&size=100&select=*&sampling=neighbors”,“results”: [\\n {“classe_consommation_energie”: “D”,“tr001_modele_dpe_type_libelle”: “Vente”,“annee_construction”: 1947,“_geopoint”: “45.925922,5.229964”,“latitude”: 45.925922,“surface_thermique_lot”: 117.16,“_i”: 487,“tr002_type_batiment_description”: “Maison Individuelle”,“geo_adresse”: “Rue de la Brugni8re 01800 Villieu-Loyes-Mollon”,“_rand”: 23215,“code_insee_commune_actualise”: “01450”,“estimation_ges”: 53,“geo_score”: 0.4,“classe_estimation_ges”: “E”,“nom_methode_dpe”: “M9thode Facture”,“tv016_departement_code”: “01”,“consommation_energie”: 178,“date_etablissement_dpe”: “2013-06-13”,“longitude”: 5.229964,“_score”: null,’\nIci, il n’est même pas nécessaire en première approche\nd’utiliser le package json, l’information\nétant déjà tabulée dans l’écho renvoyé (on a la même information pour tous les pays):\nOn peut donc se contenter de Pandas pour transformer nos données en\nDataFrame et Geopandas pour convertir en données\ngéographiques :\n\nimport pandas as pandas\nimport geopandas as gpd\n\ndef get_dpe_from_url(url):\n\n    req = requests.get(url)\n    wb = req.json()\n    df = pd.json_normalize(wb[\"results\"])\n\n    dpe = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude), crs = 4326)\n    dpe = dpe.dropna(subset = ['longitude', 'latitude'])\n\n    return dpe\n\ndpe = get_dpe_from_url(url_api)\ndpe.head(2)\n\n/opt/mamba/lib/python3.9/site-packages/geopandas/_compat.py:123: UserWarning:\n\nThe Shapely GEOS version (3.12.0-CAPI-1.18.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n\n/tmp/ipykernel_220/2008334648.py:2: UserWarning:\n\nShapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n\nimport os\nos.environ['USE_PYGEOS'] = '0'\nimport geopandas\n\nIn a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n\n\n\n\n\n\n\n\n\n\nclasse_consommation_energie\ntr001_modele_dpe_type_libelle\nannee_construction\n_geopoint\nlatitude\nsurface_thermique_lot\n_i\ntr002_type_batiment_description\ngeo_adresse\n_rand\n...\nclasse_estimation_ges\nnom_methode_dpe\ntv016_departement_code\nconsommation_energie\ndate_etablissement_dpe\nlongitude\n_score\n_id\nversion_methode_dpe\ngeometry\n\n\n\n\n0\nD\nVente\n1947\n45.925922,5.229964\n45.925922\n117.16\n487\nMaison Individuelle\nRue de la Brugnière 01800 Villieu-Loyes-Mollon\n23215\n...\nE\nMéthode Facture\n01\n178.00\n2013-06-13\n5.229964\nNone\n04JZNel3WCJYcfsHpCcHv\nNaN\nPOINT (5.22996 45.92592)\n\n\n2\nD\nNeuf\n2006\n45.923421,5.223777\n45.923421\n90.53\n689\nMaison Individuelle\nChemin du Pont-vieux 01800 Villieu-Loyes-Mollon\n401672\n...\nC\nFACTURE - DPE\n01\n227.99\n2013-06-11\n5.223777\nNone\nrkdV2lJn2wxaidVBaHBFY\nV2012\nPOINT (5.22378 45.92342)\n\n\n\n\n2 rows × 23 columns\n\n\n\nEssayons de représenter sur une carte ces DPE avec les\nannées de construction des logements.\nAvec Folium, on obtient la carte interactive suivante :\n\nimport seaborn as sns\nimport folium\n\npalette = sns.color_palette(\"coolwarm\", 8)\n\ndef interactive_map_dpe(dpe):\n\n    # convert in number\n    dpe['color'] = [ord(dpe.iloc[i]['classe_consommation_energie'].lower()) - 96 for i in range(len(dpe))]\n    dpe = dpe.loc[dpe['color']&lt;=7]\n    dpe['color'] = [palette.as_hex()[x] for x in dpe['color']]\n\n\n    center = dpe[['latitude', 'longitude']].mean().values.tolist()\n    sw = dpe[['latitude', 'longitude']].min().values.tolist()\n    ne = dpe[['latitude', 'longitude']].max().values.tolist()\n\n    m = folium.Map(location = center, tiles='OpenStreetMap')\n\n    # I can add marker one by one on the map\n    for i in range(0,len(dpe)):\n        folium.Marker([dpe.iloc[i]['latitude'], dpe.iloc[i]['longitude']],\n                    popup=f\"Année de construction: {dpe.iloc[i]['annee_construction']}, &lt;br&gt;DPE: {dpe.iloc[i]['classe_consommation_energie']}\",\n                    icon=folium.Icon(color=\"black\", icon=\"home\", icon_color = dpe.iloc[i]['color'])).add_to(m)\n\n    m.fit_bounds([sw, ne])\n\n    return m\n\nm = interactive_map_dpe(dpe)\n\n/opt/mamba/lib/python3.9/site-packages/geopandas/geodataframe.py:1443: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n# Afficher la carte\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nUn catalogue incomplet d’API existantes\nDe plus en plus de sites mettent des API à disposition des développeurs et autres curieux.\nPour en citer quelques-unes très connues :\n\nTwitter  : https://dev.twitter.com/rest/public\nFacebook  : https://developers.facebook.com/\nInstagram  : https://www.instagram.com/developer/\nSpotify  : https://developer.spotify.com/web-api/\n\nCependant, il est intéressant de ne pas se restreindre à celles-ci dont les\ndonnées ne sont pas toujours les plus intéressantes. Beaucoup\nde producteurs de données, privés comme publics, mettent à disposition\nleurs données sous forme d’API\n\nAPI.gouv: beaucoup d’API officielles de l’Etat français\net accès à de la documentation\nInsee: https://api.insee.fr/catalogue/ et pynsee\nPole Emploi : https://www.emploi-store-dev.fr/portail-developpeur-cms/home.html\nSNCF : https://data.sncf.com/api\nBanque Mondiale : https://datahelpdesk.worldbank.org/knowledgebase/topics/125589"
  },
  {
    "objectID": "content/manipulation/04c_API_TP.html#lapi-dvf-accéder-à-des-données-de-transactions-immobilières-simplement",
    "href": "content/manipulation/04c_API_TP.html#lapi-dvf-accéder-à-des-données-de-transactions-immobilières-simplement",
    "title": "Récupérer des données avec des API depuis Python",
    "section": "L’API DVF : accéder à des données de transactions immobilières simplement",
    "text": "L’API DVF : accéder à des données de transactions immobilières simplement\n⚠️ Cette partie nécessite une mise à jour pour privilégier l’API DVF du Cerema\nLe site DVF (demandes de valeurs foncières) permet de visualiser toutes les données relatives aux mutations à titre onéreux (ventes de maisons, appartements, garages…) réalisées durant les 5 dernières années.\nUn site de visualisation est disponible sur https://app.dvf.etalab.gouv.fr/.\nCe site est très complet quand il s’agit de connaître le prix moyen au mètre\ncarré d’un quartier ou de comparer des régions entre elles.\nL’API DVF permet d’aller plus loin afin de récupérer les résultats dans\nun logiciel de traitement de données. Elle a été réalisée par\nChristian Quest et le code\nsource est disponible sur Github .\nLes critères de recherche sont les suivants :\n- code_commune = code INSEE de la commune (ex: 94068)\n- section = section cadastrale (ex: 94068000CQ)\n- numero_plan = identifiant de la parcelle, (ex: 94068000CQ0110)\n- lat + lon + dist (optionnel): pour une recherche géographique, dist est par défaut un rayon de 500m\n- code_postal\nLes filtres de sélection complémentaires :\n- nature_mutation (Vente, etc)\n- type_local (Maison, Appartement, Local, Dépendance)\nLes requêtes sont de la forme : http://api.cquest.org/dvf?code_commune=29168.\n\n\n Exercice 1 : Exploiter l'API DVF\n\nRechercher toutes les transactions existantes dans DVF à Plogoff (code commune 29168, en Bretagne).\nAfficher les clés du JSON et en déduire le nombre de transactions répertoriées.\nN’afficher que les transactions portant sur des maisons.\nUtiliser l’API geo pour\nrécupérer le découpage communal de la ville de Plogoff\nReprésenter l’histogramme des prix de vente\n\nN’hésitez pas à aller plus loin en jouant sur des variables de\ngroupes par exemple\n\n\nLe résultat de la question 2 devrait\nressembler au DataFrame suivant:\nL’histogramme des prix de vente (question 4) aura l’aspect suivant:\nOn va faire une carte des ventes en affichant le prix de l’achat.\nLa cartographie réactive sera présentée dans les chapitres\nconsacrés à la visualisation de données.\nSupposons que le DataFrame des ventes s’appelle ventes. Il faut d’abord le\nconvertir\nen objet geopandas.\nAvant de faire une carte, on va convertir\nles limites de la commune de Plogoff en geoJSON pour faciliter\nsa représentation avec folium\n(voir la doc geopandas à ce propos):\nPour représenter graphiquement, on peut utiliser le code suivant (essayez de\nle comprendre et pas uniquement de l’exécuter).\n\n# Afficher la carte\nm"
  },
  {
    "objectID": "content/manipulation/04c_API_TP.html#géocoder-des-données-grâce-aux-api-officielles",
    "href": "content/manipulation/04c_API_TP.html#géocoder-des-données-grâce-aux-api-officielles",
    "title": "Récupérer des données avec des API depuis Python",
    "section": "Géocoder des données grâce aux API officielles",
    "text": "Géocoder des données grâce aux API officielles\nPour pouvoir faire cet exercice\n\n!pip install xlrd\n\nJusqu’à présent, nous avons travaillé sur des données où la dimension\ngéographique était déjà présente ou relativement facile à intégrer.\nCe cas idéal ne se rencontre pas nécessairement dans la pratique.\nOn dispose parfois de localisations plus ou moins précises et plus ou\nmoins bien formattées pour déterminer la localisation de certains\nlieux.\nDepuis quelques années, un service officiel de géocodage a été mis en place.\nCelui-ci est gratuit et permet de manière efficace de coder des adresses\nà partir d’une API. Cette API, connue sous le\nnom de la Base d’Adresses Nationale (BAN) a bénéficié de la mise en commun de données de plusieurs\nacteurs (collectivités locales, Poste) et de compétences d’acteurs\ncomme Etalab. La documentation de celle-ci est disponible à l’adresse\nhttps://api.gouv.fr/les-api/base-adresse-nationale.\nPour illustrer la manière de géocoder des données avec Python, nous\nallons partir de la base\ndes résultats des auto-écoles à l’examen du permis sur l’année 2018.\nCes données nécessitent un petit peu de travail pour être propres à une\nanalyse statistique.\nAprès avoir renommé les colonnes, nous n’allons conserver que\nles informations relatives au permis B (permis voiture classique) et\nles auto-écoles ayant présenté au moins 20 personnes à l’examen.\n\nimport pandas as pd\nimport xlrd\nimport geopandas as gpd\n\ndf = pd.read_excel(\"https://www.data.gouv.fr/fr/datasets/r/d4b6b072-8a7d-4e04-a029-8cdbdbaf36a5\", header = [0,1])\n\n# Le Excel a des noms de colonne emboitées, \n# on nettoie\nindex_0 = [\"\" if df.columns[i][0].startswith(\"Unnamed\") else df.columns[i][0] for i in range(len(df.columns))]\nindex_1 = [df.columns[i][1] for i in range(len(df.columns))]\nkeep_index = [True if el in ('', \"B\") else False for el in index_0] \ncols = [index_0[i] + \" \" + index_1[i].replace(\"+\", \"_\") for i in range(len(df.columns))]\ndf.columns = cols\ndf = df.loc[:, keep_index]\ndf.columns = df.columns.str.replace(\"(^ |°)\", \"\", regex = True).str.replace(\" \", \"_\")\n\n# On garde le sous-échantillon d'intérêt\ndf = df.dropna(subset = ['B_NB'])\ndf = df.loc[~df[\"B_NB\"].astype(str).str.contains(\"(\\%|\\.)\"),:]\ndf['B_NB'] = df['B_NB'].astype(int)\ndf['B_TR'] = df['B_TR'].str.replace(\",\", \".\").str.replace(\"%\",\"\").astype(float)\ndf = df.loc[df[\"B_NB\"]&gt;20]\n\n/tmp/ipykernel_220/3216706257.py:19: UserWarning:\n\nThis pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n\n\n\nSur cet échantillon, le taux de réussite moyen était, en 2018, de 58.02%\nNos informations géographiques prennent la forme suivante :\n\ndf.loc[:,['Adresse','CP','Ville']].head(5)\n\n\n\n\n\n\n\n\nAdresse\nCP\nVille\n\n\n\n\n0\n56 RUE CHARLES ROBIN\n01000\nBOURG EN BRESSE\n\n\n2\n7, avenue Revermont\n01250\nCeyzeriat\n\n\n3\n72 PLACE DE LA MAIRIE\n01000\nSAINT-DENIS LES BOURG\n\n\n4\n6 RUE DU LYCEE\n01000\nBOURG EN BRESSE\n\n\n5\n9 place Edgard Quinet\n01000\nBOURG EN BRESSE\n\n\n\n\n\n\n\nAutrement dit, nous disposons d’une adresse, d’un code postal et d’un nom\nde ville. Ces informations peuvent servir à faire une recherche\nsur la localisation d’une auto-école puis, éventuellement, de se restreindre\nà un sous-échantillon.\n\nUtiliser l’API BAN\nLa documentation officielle de l’API\npropose un certain nombre d’exemples de manière de géolocaliser des données.\nDans notre situation, deux points d’entrée paraissent intéressants:\n\nL’API /search/ qui représente un point d’entrée avec des URL de la forme\nhttps://api-adresse.data.gouv.fr/search/?q=\\&lt;adresse\\&gt;&postcode=\\&lt;codepostal\\&gt;&limit=1\nL’API /search/csv qui prend un CSV en entrée et retourne ce même CSV avec\nles observations géocodées. La requête prend la forme suivante, en apparence\nmoins simple à mettre en oeuvre :\ncurl -X POST -F data=@search.csv -F columns=adresse -F columns=postcode https://api-adresse.data.gouv.fr/search/csv/\n\nLa tentation serait forte d’utiliser la première méthode avec une boucle sur les\nlignes de notre DataFrame pour géocoder l’ensemble de notre jeu de données.\nCela serait néanmoins une mauvaise idée car les communications entre notre\nsession Python et les serveurs de l’API seraient beaucoup trop nombreuses\npour offrir des performances satisfaisantes.\nPour vous en convaincre, vous pouvez exécuter le code suivant sur un petit\néchantillon de données (par exemple 100 comme ici) et remarquer que le temps\nd’exécution est assez important\n\nimport time\n\ndfgeoloc = df.loc[:, ['Adresse','CP','Ville']].apply(lambda s: s.str.lower().str.replace(\",\",\" \"))\ndfgeoloc['url'] = (dfgeoloc['Adresse'] + \"+\" + dfgeoloc['Ville'].str.replace(\"-\",'+')).str.replace(\" \",\"+\")\ndfgeoloc['url'] = 'https://api-adresse.data.gouv.fr/search/?q=' + dfgeoloc['url'] + \"&postcode=\" + df['CP'] + \"&limit=1\"\ndfgeoloc = dfgeoloc.dropna()\n\nstart_time = time.time()\n\ndef get_geoloc(i):\n    print(i)\n    return gpd.GeoDataFrame.from_features(requests.get(dfgeoloc['url'].iloc[i]).json()['features'])\n\nlocal = [get_geoloc(i) for i in range(len(dfgeoloc.head(10)))]\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\nComme l’indique la documentation, si on désire industrialiser notre processus\nde géocodage, on va privilégier l’API CSV.\nPour obtenir une requête CURL cohérente avec le format désiré par l’API\non va à nouveau utiliser Requests mais cette fois avec des paramètres\nsupplémentaires:\n\ndata va nous permettre de passer des paramètres à CURL (équivalents aux -F\nde la requête CURL) :\n\ncolumns: Les colonnes utilisées pour localiser une donnée. En l’occurrence,\non utilise l’adresse et la ville (car les codes postaux n’étant pas uniques,\nun même nom de voirie peut se trouver dans plusieurs villes partageant le même\ncode postal) ;\npostcode: Le code postal de la ville. Idéalement nous aurions utilisé\nle code Insee mais nous ne l’avons pas dans nos données ;\nresult_columns: on restreint les données échangées avec l’API aux\ncolonnes qui nous intéressent. Cela permet d’accélérer les processus (on\néchange moins de données) et de réduire l’impact carbone de notre activité\n(moins de transferts = moins d’énergie dépensée). En l’occurrence, on ne ressort\nque les données géolocalisées et un score de confiance en la géolocalisation ;\n\nfiles: permet d’envoyer un fichier via CURL.\n\nLes données sont récupérées avec request.post. Comme il s’agit d’une\nchaîne de caractère, nous pouvons directement la lire avec Pandas en\nutilisant io.StringIO pour éviter d’écrire des données intermédiaires.\nLe nombre d’échos semblant être limité, il\nest proposé de procéder par morceaux\n(ici, le jeu de données est découpé en 5 morceaux).\n\nimport requests\nimport io   \nimport numpy as np\nimport time\n\nparams = {\n    'columns': ['Adresse', 'Ville'],\n    'postcode': 'CP',\n    'result_columns': ['result_score', 'latitude', 'longitude'],\n}\n\ndf[['Adresse','CP','Ville']] = df.loc[:, ['Adresse','CP','Ville']].apply(lambda s: s.str.lower().str.replace(\",\",\" \"))\n\ndef geoloc_chunk(x):\n    dfgeoloc = x.loc[:, ['Adresse','CP','Ville']]\n    dfgeoloc.to_csv(\"datageocodage.csv\", index=False)\n    response = requests.post('https://api-adresse.data.gouv.fr/search/csv/', data=params, files={'data': ('datageocodage.csv', open('datageocodage.csv', 'rb'))})\n    geoloc = pd.read_csv(io.StringIO(response.text), dtype = {'CP': 'str'})\n    return geoloc\n    \nstart_time = time.time()\ngeodata = [geoloc_chunk(dd) for dd in np.array_split(df, 10)]\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\nCette méthode est beaucoup plus rapide et permet ainsi, une fois retourné à nos\ndonnées initiales, d’avoir un jeu de données géolocalisé.\n\n# Retour aux données initiales\ngeodata = pd.concat(geodata, ignore_index = True)\ndf_xy = df.merge(geodata, on = ['Adresse','CP','Ville'])\ndf_xy = df_xy.dropna(subset = ['latitude','longitude'])\n\n# Mise en forme pour le tooltip\ndf_xy['text'] = (\n    df_xy['Raison_Sociale'] + '&lt;br&gt;' +\n    df_xy['Adresse'] + '&lt;br&gt;' +\n    df_xy['Ville'] + '&lt;br&gt;Nombre de candidats:' + df_xy['B_NB'].astype(str)\n)\ndf_xy.filter(\n    ['Raison_Sociale','Adresse','CP','Ville','latitude','longitude'],\n    axis = \"columns\"\n).sample(10)\n\n\n\n\n\n\n\n\nRaison_Sociale\nAdresse\nCP\nVille\nlatitude\nlongitude\n\n\n\n\n2259\nLUISANT MAIRIE\n10 rue de la république\n28600\nluisant\n48.427830\n1.473003\n\n\n7427\nFRESNOISE\nzone commerciale super u rue abbé lelièvre\n72130\nfresnay sur sarthe\n48.285915\n0.022954\n\n\n10047\nZZ-SCEAUX AUTO-ECOLE\n53 rue fontenay\n92330\nsceaux\n48.783695\n2.293347\n\n\n2757\nCONDOMOISE\n33 bd de la liberation\n32100\ncondom\n43.957580\n0.375704\n\n\n907\nAUTO ECOLE PLANET CONDUITE\n173 avenue de la rose\n13013\nmarseille\n43.328926\n5.426850\n\n\n7481\nCRENEAU -LE-\n51 place du forum\n73000\nchambery le haut\n45.593017\n5.919606\n\n\n8933\nTRAJECTOIRE\n49 av de la republique\n83210\nla farlede\n43.165488\n6.042610\n\n\n5758\nAUTO ECOLE BRUNO GODBILLE\n25 ville basse\n59550\nlandrecies\n50.122185\n3.694102\n\n\n4234\nDEGREZ / MACHECOUL CONDUITE\n20 rue de pornic\n44270\nmachecoul\n46.998920\n-1.828172\n\n\n9619\nRIVE DROITE\n19 quai gal leclerc\n89300\njoigny\n47.982360\n3.392759\n\n\n\n\n\n\n\nIl ne reste plus qu’à utiliser Geopandas\net nous serons en mesure de faire une carte des localisations des auto-écoles :\n\n# Transforme en geopandas pour les cartes\nimport geopandas as gpd\ndfgeo = gpd.GeoDataFrame(\n    df_xy,\n    geometry = gpd.points_from_xy(df_xy.longitude, df_xy.latitude)\n)\n\nNous allons représenter les stations dans l’Essonne avec un zoom initialement\nsur les villes de Massy et Palaiseau. Le code est le suivant:\n\nimport folium\n\n# Représenter toutes les autoécoles de l'Essonne\ndf_91 = df_xy.loc[df_xy[\"Dept\"] == \"091\"]\n\n# Centrer la vue initiale sur Massy-Palaiseau\ndf_pal = df_xy.loc[df_xy['Ville'].isin([\"massy\", \"palaiseau\"])]\ncenter = df_pal[['latitude', 'longitude']].mean().values.tolist()\nsw = df_pal[['latitude', 'longitude']].min().values.tolist()\nne = df_pal[['latitude', 'longitude']].max().values.tolist()\n\nm = folium.Map(location = center, tiles='Stamen Toner')\n\n# I can add marker one by one on the map\nfor i in range(0,len(df_91)):\n    folium.Marker([df_91.iloc[i]['latitude'], df_91.iloc[i]['longitude']],\n                  popup=df_91.iloc[i]['text'],\n                  icon=folium.Icon(icon='car', prefix='fa')).add_to(m)\n\nm.fit_bounds([sw, ne])\n\nCe qui permet d’obtenir la carte:\n\n# Afficher la carte\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nVous pouvez aller plus loin avec l’exercice suivant.\n\n\n Exercice 2 : Quelles sont les auto-écoles les plus proches de chez moi ?\nOn va supposer que vous cherchez, dans un rayon donné autour d’un centre ville,\nles auto-écoles disponibles.\n\n\nFonction nécessaire pour cet exercice\n\nCet exercice nécessite une fonction pour créer un cercle\nautour d’un point\n(source ici).\nLa voici :\nfrom functools import partial\nimport pyproj\nfrom shapely.ops import transform\nfrom shapely.geometry import Point\n\nproj_wgs84 = pyproj.Proj('+proj=longlat +datum=WGS84')\n\n\ndef geodesic_point_buffer(lat, lon, km):\n    # Azimuthal equidistant projection\n    aeqd_proj = '+proj=aeqd +lat_0={lat} +lon_0={lon} +x_0=0 +y_0=0'\n    project = partial(\n        pyproj.transform,\n        pyproj.Proj(aeqd_proj.format(lat=lat, lon=lon)),\n        proj_wgs84)\n    buf = Point(0, 0).buffer(km * 1000)  # distance in metres\n    return transform(project, buf).exterior.coords[:]\n\n\nPour commencer, utiliser l’API Geo\npour la ville de Palaiseau.\nAppliquer la fonction geodesic_point_buffer au centre ville de Palaiseau\nNe conserver que les auto-écoles dans ce cercle et les ordonner\n\nSi vous avez la réponse à la question 3, n’hésitez pas à la soumettre sur Github afin que je complète la correction 😉 !\n\n\n\n\nERROR 1: PROJ: proj_create_from_database: Open of /opt/mamba/share/proj failed\n\n\n\n\n/opt/mamba/lib/python3.9/site-packages/shapely/ops.py:276: FutureWarning:\n\nThis function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n\n\n\nPour se convaincre, de notre cercle constitué lors de\nla question 2, on peut représenter une carte.\nOn a bien un cercle centré autour de Palaiseau:\n\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "content/manipulation/04c_API_TP.html#exercices-supplémentaires",
    "href": "content/manipulation/04c_API_TP.html#exercices-supplémentaires",
    "title": "Récupérer des données avec des API depuis Python",
    "section": "Exercices supplémentaires",
    "text": "Exercices supplémentaires\n\nDécouvrir l’API d’OpenFoodFacts\nPour vous aidez, vous pouvez regarder une exemple de structure du JSON ici :\nhttps://world.openfoodfacts.org/api/v0/product/3274080005003.json en particulier la catégorie nutriments.\n\n\n Exercice 3 : Retrouver des produits dans l'openfood facts 🍕\nVoici une liste de code-barres:\n3274080005003,  5449000000996, 8002270014901, 3228857000906, 3017620421006, 8712100325953\nUtiliser l’API d’openfoodfacts\n(l’API, pas depuis le CSV !)\npour retrouver les produits correspondants\net leurs caractéristiques nutritionnelles.\nLe panier paraît-il équilibré ? 🍫\nRécupérer l’URL d’une des images et l’afficher dans votre navigateur.\n\n\nVoici par exemple la photo du produit ayant le code-barre 5449000000996. Vous le reconnaissez ?"
  },
  {
    "objectID": "content/manipulation/04c_API_TP.html#footnotes",
    "href": "content/manipulation/04c_API_TP.html#footnotes",
    "title": "Récupérer des données avec des API depuis Python",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLa documentation est également disponible ici↩︎\nLe JSON est un format très apprécié dans le domaine du big data\ncar il permet d’empiler des données\nqui ne sont pas complètes. Il\ns’agit d’un des formats privilégiés du paradigme No-SQL pour lequel\ncet excellent cours propose plus de détails.↩︎\nSuivant les API, nous avons soit besoin de rien de plus si nous parvenons directement à obtenir un json, soit devoir utiliser un parser comme BeautifulSoup dans le cas contraire. Ici, le JSON peut être formaté relativement aisément.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Thèmes en vrac",
    "section": "",
    "text": "Python pour la data science \n\n\nLino Galiana\n\nStar this website on Github\n\nCe site web rend public le contenu du cours de deuxième année (Master 1) de l’ENSAE:\nPython pour la data science\n\nTout est présent sur ce site web ! Des Notebooks Jupyter peuvent être récupérés pour s’exercer. L’ensemble des codes sources est stocké sur Github\n\n\n\n\n\n\n\n\n\n\n\n\n\nPour découvrir Python  de manière thématique\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRécupérer des données avec des API depuis Python\n\n\n\nExercice\n\n\nManipulation\n\n\n\nLes API (Application Programming Interface) sont un mode d’accès aux\ndonnées en expansion. Grâce aux API, l’automatisation de scripts\nest facilitée puisqu’il n’est…\n\n\n\nLino Galiana\n\n\nJul 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n Back to topCitationBibTeX citation:@book{galiana2023,\n  author = {Galiana, Lino},\n  title = {Python Pour La Data Science},\n  date = {2023},\n  url = {https://pythonds.linogaliana.fr/},\n  doi = {10.5281/zenodo.8229676},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGaliana, Lino. 2023. Python Pour La Data Science. https://doi.org/10.5281/zenodo.8229676."
  }
]