[
  {
    "objectID": "index.html#thèmes-en-vrac",
    "href": "index.html#thèmes-en-vrac",
    "title": "Python pour la data science",
    "section": "Thèmes en vrac",
    "text": "Thèmes en vrac\nPour découvrir Python  de manière thématique\n\n\n\n\n\n\n\n\n\n\nQuelques éléments pour comprendre les enjeux du NLP\n\n\n\nNLP\n\n\nTutoriel\n\n\n\nLes corpus textuels étant des objets de très grande dimension\noù le ratio signal/bruit est faible, il est nécessaire de mettre\nen oeuvre une série d’étapes de nettoyage de…\n\n\n\nLino Galiana\n\n\nJul 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNettoyer un texte: des exercices pour découvrir l’approche bag-of-words\n\n\n\nNLP\n\n\nExercice\n\n\n\nCe chapitre continue de présenter l’approche de nettoyage de données\ndu NLP en s’appuyant sur le corpus de trois auteurs\nanglo-saxons : Mary Shelley, Edgar Allan Poe…\n\n\n\nLino Galiana\n\n\nOct 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLatent Dirichlet Allocation (LDA)\n\n\n\nTutoriel\n\n\nNLP\n\n\n\nLa Latent Dirichlet Allocation (LDA)\nest un modèle probabiliste génératif qui permet\nde décrire des…\n\n\n\nLino Galiana\n\n\nOct 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMéthodes de vectorisation : comptages et word embeddings\n\n\n\nTutoriel\n\n\nNLP\n\n\n\nPour pouvoir utiliser des données textuelles dans des algorithmes\nde machine learning, il faut les vectoriser, c’est à dire transformer\nle texte en données numériques.…\n\n\n\nLino Galiana\n\n\nOct 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercices supplémentaires\n\n\n\nExercice\n\n\nNLP\n\n\n\nDes exercices supplémentaires pour pratiquer les concepts du NLP\n\n\n\nLino Galiana\n\n\nApr 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPartie 4 : Natural Language Processing (NLP)\n\n\n\nIntroduction\n\n\nNLP\n\n\n\nL’un des grands avantages comparatifs de Python par rapport aux\nlangages concurrents (R notamment) est dans\nla richesse des librairies de Traitement du Langage Naturel…\n\n\n\nLino Galiana\n\n\nOct 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfiguration de Python\n\n\n\nTutoriel\n\n\nRappels\n\n\n\nL’environnement que propose Python pour la data-science\nest très riche. Afin de bénéficier du meilleur environnement\npour tirer parti du langage, ce chapitre…\n\n\n\nLino Galiana\n\n\nJul 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL’environnement Python pour la data-science\n\n\n\nTutoriel\n\n\nRappels\n\n\n\nPython propose un écosystème très riche pour la\ndata-science. Ce chapitre fait un tour\nd’horizon de celui-ci en présentant les principaux\npackages qui seront présentés…\n\n\n\nLino Galiana\n\n\nJul 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComment aborder un jeu de données\n\n\n\nTutoriel\n\n\nRappels\n\n\n\nQuelques éléments pour adopter une démarche\nscientifique et éthique face à un\njeu de données.\n\n\n\nLino Galiana\n\n\nJul 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBonne pratique de Python\n\n\n\nTutoriel\n\n\nRappels\n\n\n\nLes normes communautaires du monde de\nl’open-source ont permis une\nharmonisation de la structure des projets\nPython et des scripts. Ce chapitre\névoque quelques unes de…\n\n\n\nLino Galiana\n\n\nJul 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuelques rappels sur les principes de base de Python\n\n\n\nTutoriel\n\n\nRappels\n\n\n\nRappels d’éléments essentiels en Python: les règles de syntaxes, les classes,\nles méthodes, etc.\n\n\n\nLino Galiana\n\n\nJul 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModules, tests, boucles, fonctions\n\n\n\nTutoriel\n\n\nRappels\n\n\n\nLes fonctions permettent de généraliser des\ninstructions. Il s’agit ainsi d’un outil privilégié\npour automatiser des tâches répétitives ou réduire\nla complexité d’une chaîne…\n\n\n\nLino Galiana\n\n\nJul 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLes classes en Python\n\n\n\nTutoriel\n\n\nRappels\n\n\n\nLa programmation orientée objet (POO) est\nl’un des atouts de Python. Elle permet\nd’adapter des…\n\n\n\nLino Galiana\n\n\nJul 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\nCette introduction propose quelques éléments de\nrévision des concepts de base en Python et\nprésente l’écosystème Python que nous allons\ndécouvrir tout au long de ce…\n\n\n\nLino Galiana\n\n\nJun 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumpy, la brique de base de la data science\n\n\n\nTutoriel\n\n\nManipulation\n\n\n\nNumpy constitue la brique de base de l’écosystème de la data-science en\nPython. Toutes les librairies de manipulation de données, de modélisation\net de visualisation…\n\n\n\nLino Galiana\n\n\nJul 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction à Pandas\n\n\n\nTutoriel\n\n\nManipulation\n\n\n\nPandas est l’élément central de l’écosystème Python pour la data-science.\nLe succès récent de Python dans l’analyse de données tient beaucoup à pandas qui a permis…\n\n\n\nLino Galiana\n\n\nJul 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPratique de pandas: un exemple complet\n\n\n\nManipulation\n\n\nExercice\n\n\n\nAprès avoir présenté la logique de Pandas dans le chapitre précédent,\nce chapitre vise à illustrer les fonctionalités du package\nà partir de données d’émissions de gaz à…\n\n\n\nLino Galiana\n\n\nJul 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPratique de geopandas avec les données vélib\n\n\n\nManipulation\n\n\nExercice\n\n\n\nCe chapitre illustre les fonctionalités de GeoPandas à partir des\ndécomptes de vélo fournis par la ville de Paris\nen…\n\n\n\nLino Galiana\n\n\nJul 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDonnées spatiales: découverte de geopandas\n\n\n\nTutoriel\n\n\nManipulation\n\n\n\nLes données géolocalisées se sont multipliées depuis quelques années, qu’il\ns’agisse de données open-data ou de traces numériques géolocalisées de\ntype big-data. Pour les…\n\n\n\nLino Galiana\n\n\nJul 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWebscraping avec python\n\n\n\nExercice\n\n\nManipulation\n\n\n\nPython permet de facilement récupérer une page web pour en extraire des\ndonnées à restructurer. Le webscraping, que les Canadiens nomment\n“moissonnage du web”, est une…\n\n\n\nLino Galiana\n\n\nSep 8, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaîtriser les expressions régulières\n\n\n\nTutoriel\n\n\nManipulation\n\n\n\nLes expressions régulières fournissent un cadre très pratique pour manipuler\nde manière flexible des données textuelles. Elles sont très utiles\nnotamment pour les tâches de…\n\n\n\nLino Galiana\n\n\nJul 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRécupérer des données avec des API depuis Python\n\n\n\nExercice\n\n\nManipulation\n\n\n\nLes API (Application Programming Interface) sont un mode d’accès aux\ndonnées en expansion. Grâce aux API, l’automatisation de scripts\nest facilitée puisqu’il n’est…\n\n\n\nLino Galiana\n\n\nJul 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercices supplémentaires de webscraping\n\n\n\nExercice\n\n\nManipulation\n\n\n\n\n\n\n\nLino Galiana\n\n\nJul 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction à dask grâce aux données DVF\n\n\n\nTutoriel\n\n\nManipulation\n\n\n\n\n\n\n\nLino Galiana\n\n\nAug 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPartie 1: manipuler des données\n\n\n\nManipulation\n\n\nIntroduction\n\n\n\nPython s’est imposé comme une alternative très crédible à R dans\nla manipulation de données. L’écosystème Pandas a permis de démocratiser\nl’utilisation des DataFrames…\n\n\n\nLino Galiana\n\n\nJul 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPréparation des données pour construire un modèle\n\n\n\nModélisation\n\n\nExercice\n\n\n\nAfin d’avoir des données cohérentes avec les hypothèses de modélisation,\nil est absolument fondamental de prendre le temps de\npréparer les données à fournir à un modèle. La…\n\n\n\nLino Galiana\n\n\nJul 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluer la qualité d’un modèle\n\n\n\nModélisation\n\n\nExercice\n\n\n\nFaire preuve de méthode pour évaluer la qualité d’un modèle\npermet de proposer des prédictions plus robustes, ayant\nde meilleures performances sur un nouveau jeu de…\n\n\n\nLino Galiana\n\n\nJul 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification: premier modèle avec les SVM\n\n\n\nModélisation\n\n\nExercice\n\n\n\nLa classification permet d’attribuer une classe d’appartenance (label\ndans la terminologie du machine learning)\ndiscrète à des données à partir de certaines variables…\n\n\n\nLino Galiana\n\n\nJul 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRégression : une introduction\n\n\n\nModélisation\n\n\nExercice\n\n\n\nLa régression linéaire est la première modélisation statistique\nqu’on découvre dans un cursus quantitatif. Il s’agit en effet d’une\nméthode très intuitive et très riche. Le…\n\n\n\nLino Galiana\n\n\nJul 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSélection de variables : une introduction\n\n\n\nModélisation\n\n\nExercice\n\n\n\nL’accès à des bases de données de plus en plus riches permet\ndes modélisations de plus en plus raffinées. Cependant,\nles modèles parcimonieux sont généralement…\n\n\n\nLino Galiana\n\n\nJul 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustering\n\n\n\nModélisation\n\n\nExercice\n\n\n\nLe clustering consiste à répartir des observations dans des groupes,\ngénéralement non observés,\nen fonction de caractéristiques observables. Il s’agit d’une\napplication…\n\n\n\nLino Galiana\n\n\nJul 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPremier pas vers l’industrialisation avec les pipelines scikit\n\n\n\nModélisation\n\n\nTutoriel\n\n\n\nLes pipelines scikit permettent d’intégrer de manière très flexible\nun ensemble d’opérations de pre-processing et d’entraînement de modèles\ndans une chaîne d’opérations.…\n\n\n\nLino Galiana\n\n\nOct 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPartie 3: modéliser\n\n\n\nIntroduction\n\n\nModélisation\n\n\n\nLa facilité à modéliser des processus très diverses a grandement\nparticipé au succès de Python. La librairie scikit offre une\ngrande variété de modèles et permet ainsi…\n\n\n\nLino Galiana\n\n\nOct 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nIntégration continue avec Python\n\n\n\n\n\n\nLino Galiana\n\n\nJul 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nGénération d’images avec Python, DALL-E et StableDiffusion\n\n\n\nTutoriel\n\n\nAvancé\n\n\n\nLa hype autour du\nmodèle de génération d’image Dall-E a amené\nune grande attention sur les modèles\nautogénératifs de contenu. Dall-E est, à l’heure\nactuelle, le modèle…\n\n\n\nLino Galiana\n\n\nAug 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nApprofondissement ElasticSearch pour des recherches de proximité géographique\n\n\n\nTutoriel\n\n\nAvancé\n\n\n\nUn chapitre plus approfondi sur ElasticSearch\n\n\n\nLino Galiana\n\n\nSep 3, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction à ElasticSearch pour la recherche textuelle\n\n\n\nTutoriel\n\n\nAvancé\n\n\n\nElasticSearch est un moteur de recherche extrêmement rapide et flexible.\nCette technologie s’est imposée dans le domaine du traitement des\ndonnées textuelles. L’API…\n\n\n\nLino Galiana\n\n\nSep 3, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPartie 5: Introduction aux outils et méthodes à l’état de l’art\n\n\n\nIntroduction\n\n\nAvancé\n\n\n\nAprès avoir abordé les différents champs de la\ndata-science, nous pouvons maintenant\nintroduire à quelques outils et méthodes plus avancés\nqui correspondent à des aspects…\n\n\n\nLino Galiana\n\n\nOct 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nLes nouveaux modes d’accès aux données: le format parquet et les données sur le cloud\n\n\n\nTutoriel\n\n\nAvancé\n\n\n\n\n\n\n\nLino Galiana\n\n\nAug 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPartie 2: visualiser les données\n\n\n\nIntroduction\n\n\nVisualisation\n\n\n\nCette partie présente les outils pour visualiser des\ndonnées avec Python, qu’il s’agisse de graphiques\nfigés (matplotlib, seaborn, geoplot…) ou de\nvisualisation…\n\n\n\nLino Galiana\n\n\nJul 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDe belles cartes avec python: mise en pratique\n\n\n\nVisualisation\n\n\nExercice\n\n\n\nLa cartographie est un excellent moyen de diffuser\nune connaissance, y compris à des publics peu\nfamiliers de la statistique. Ce chapitre permet\nde découvrir la manière dont…\n\n\n\nLino Galiana\n\n\nJul 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDe beaux graphiques avec python: mise en pratique\n\n\n\nVisualisation\n\n\nExercice\n\n\n\nUne partie essentielle du travail du\ndata-scientist est d’être en mesure\nde synthétiser une information dans des\nreprésentations graphiques percutantes. Ce\nchapitre permet…\n\n\n\nLino Galiana\n\n\nJul 14, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/course/getting-started/index.html",
    "href": "content/course/getting-started/index.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nCe cours rassemble l’ensemble du contenu du cours\nPython  pour la data-science que je donne\nà l’ENSAE\ndepuis 2018.\nCe cours était auparavant donné par Xavier Dupré.\nQuelques éléments supplémentaires sont disponibles dans\nles slides d’introduction.\nDes éléments plus avancés sont présents dans un autre cours consacré\nà la mise en production de projets data science\nque je donne avec Romain Avouac\nà l’ENSAE (ensae-reproductibilite.github.io/website)\nPython est un langage qui a déjà plus de trente ans\nmais qui a connu, au cours de la décennie 2010, une\nnouvelle jeunesse du fait de l’engouement pour\nla data science.\nPython, plus que tout autre\nlangage informatique, réunit des communautés aussi\ndiverses que des statisticiens, des développeurs,\ndes gestionnaires\nd’applications ou d’infrastructures informatiques,\ndes lycées - Python est au programme du bac français\ndepuis quelques années - ou des chercheurs\ndans des champs à la fois théoriques et appliqués. Contrairement\nà beaucoup de langages informatiques qui fédèrent\nune communauté assez homogène, Python est parvenu à réunir\nlargement grâce à quelques principes centraux: la lisibilité\ndu langage, la simplicité à utiliser des modules,\nla simplicité à l’associer à des langages plus performants\npour certaines tâches données, l’énorme volume de documentation\ndisponible en ligne…\nÊtre le deuxième meilleur langage pour réaliser telle ou telle\ntâche\npeut ainsi être une source de succès lorsque la concurrence ne dispose\npas d’un éventail aussi large d’avantages.\nLe succès de Python, de par sa nature de\nlangage couteau-suisse, est indissociable\nde l’émergence du profil du data-scientist, individu\ncapable de s’intégrer à différents niveaux dans la valorisation\nde données.\n@davenport2012data, dans la Harvard Business Review,\nont ainsi pu parler du “boulot le plus sexy du 21e siècle”\net ont pu, dix ans plus tard, faire un panorama complet de l’évolution\ndes compétences attendues d’un data-scientist dans\nla même revue [@davenport2022data].\nLa richesse de Python permet de l’utiliser dans toutes les phases\ndu traitement de la donnée, de sa récupération et structuration à partir de\nsources diverses à sa valorisation.\nPar le prisme de la data science, nous verrons que Python est\nun très bon candidat pour assister les data scientists dans tous\nles aspects du travail de données.\nCe cours introduit différents outils qui permettent de mettre en relation\ndes données et des théories grâce à Python. Néanmoins, ce cours\nva au-delà d’une simple introduction au langage et propose\ndes éléments plus approfondis, notamment sur les dernières\ninnovations permises par la data-science dans les méthodes de travail."
  },
  {
    "objectID": "content/course/getting-started/index.html#pourquoi-faire-du-python-pour-lanalyse-de-données",
    "href": "content/course/getting-started/index.html#pourquoi-faire-du-python-pour-lanalyse-de-données",
    "title": "Introduction",
    "section": "Pourquoi faire du Python pour l’analyse de données ?",
    "text": "Pourquoi faire du Python pour l’analyse de données ?\nLe succès de scikit-learn et\nde Tensorflow dans la communauté\nde la Data-Science ont beaucoup contribué à l’adoption de Python. Cependant,\nrésumer Python à ces quelques librairies serait réducteur tant il s’agit\nd’un véritable couteau-suisse pour les data-scientists,\nles social scientists ou les économistes.\nL’intérêt de Python pour un data scientist ou data economist\nva au-delà du champ du Machine Learning.\nComme pour R, l’intérêt de Python est son rôle central dans un\nécosystème plus large autour d’outils puissants, flexibles et open-source.\nPython concurrence très bien R dans son domaine de prédilection, à\nsavoir l’analyse statistique sur des bases de données structurées.\nComme dans R, les dataframes sont un concept central de Python.\nPython est néanmoins bien plus complet dans certains domaines.\nOutre le Machine Learning,\nPython est mieux adapté aux données volumineuses que\nR. Python est également meilleur que R pour faire\ndu webscraping ou accéder à des données par le biais d’API.\nDans le domaine de l’économétrie, Python offre\nl’avantage de la simplicité avec un nombre restreint de packages (scikit et\nstatsmodels) permettant d’avoir des modèles très généraux\n(les generalized estimating equations)\nalors qu’il faut\nchoisir parmi une grande variété de packages en R pour obtenir les\nmodèles équivalents. Dans le domaine du Deep Learning, Python écrase\nla concurrence.\nAu contraire, dans certains domaines, R reste meilleur, même si les\névolutions très récentes de certains outils peuvent amener à réviser\nce constant. Historiquement,\nR était très bien intégré au langage de publication Markdown ce qui,\npermet la construction de documents reproductibles très raffinés.\nL’émergence récente de Quarto, héritier de R Markdown développé par\nla société Posit permet aux utilisateur de Python de bénéficier\négalement de la richesse de cette approche pour leur langage de prédilection.\nCe site web, à l’arborescence relativement complexe, est ainsi\nconstruit grâce à cet outil qui permet à la fois de tester les blocs\nde code présentés mais aussi de produire de manière automatisée les\ntableaux et graphiques présentés. S’il fallait trouver un point faible\nà Python par rapport à R dans le domaine de la data-science\nc’est sur la production de graphiques. matplotlib et seaborn, qui sont\nprésentés dans la partie visualisation, sont d’excellents outils. Néanmoins,\nggplot2, l’équivalent en R est plus facile de prise en main et\npropose une syntaxe extrêmement flexible, qu’il est difficile de ne pas\napprécier. Cependant, l’écosystème de la\nvisualisation de données est en pleine révolution avec le succès\nd’Observable qui\nrapproche l’écosystème JavaScript des développeurs web\nde la communauté des analystes de données.\nUn des avantages comparatifs de Python par rapport à d’autres\nlangages (notamment R et Julia) est sa dynamique,\nce que montre l’explosion du nombre de questions\nsur Stack Overflow.\nCependant, il ne s’agit pas bêtement d’enterrer R.\nAu contraire, outre leur logique très proche,\nles deux langages sont dans une phase de convergence avec des initiatives comme\nreticulate,\nquarto ou\nsnakemake qui\npermettent, de manière différente, de créer des chaînes de traitement\nmélangeant R et Python.\nUne autre raison pour laquelle cette guéguerre R/Python n’a pas\nde sens est que les bonnes\npratiques peuvent être transposées de manière presque transparente d’un\nlangage à l’autre. Il s’agit d’un point qui est développé plus amplement\ndans le cours plus avancé que je donne avec Romain Avouac en dernière année\nd’ENSAE: ensae-reproductibilite.github.io/website.\nA terme, les data-scientists et chercheurs en sciences sociales ou\néconomie utiliseront\nde manière presque indifférente, et en alternance, Python et R. Ce cours\nprésentera ainsi régulièrement des analogies avec R pour aider les\npersonnes découvrant Python, mais connaissant déjà bien R, à\nmieux comprendre certains messages."
  },
  {
    "objectID": "content/course/getting-started/index.html#objectif-du-cours",
    "href": "content/course/getting-started/index.html#objectif-du-cours",
    "title": "Introduction",
    "section": "Objectif du cours",
    "text": "Objectif du cours\nLe but de ce cours est de rendre autonome sur\nl’utilisation de Python\ndans un contexte de travail de data scientist ou de\nsocial scientist (économie, sociologie, géographie…).\nAutrement dit,\nil présuppose qu’on désire faire un usage intense\nde données dans un cadre statistique rigoureux.\nLa data-science est un ensemble de techniques\nvisant à donner du sens à des sources de données\ndiverses. Selon les organisations,\nles data-scientists peuvent ainsi être à\nl’interface de projets nécessitant un\nlarge spectre de compétences\n(analyse\nde données textuelles, représentation\ngraphique interactive…),\navoir des interactions avec des profils\ntrès différents (experts métiers,\ndéveloppeurs, data architect,\ndata engineer…) voire adopter\nun peu tous ces rôles.\nLes innovations\nrécentes de la data-science ne se réduisent\nnéanmoins\npas qu’à des découvertes méthodologiques.\nLa data-science propose un ensemble de\ntechniques et de méthodes de travail\npour réduire les coûts de passage\nd’un protype à une chaine\nde production pérenne.\nCe cours introduit à quelques notions\nsur le sujet, notamment les\npipelines scikit, pour adopter\ndès l’apprentissage du langage\nquelques bons réflexes.\nJe donne également un cours,\nplus avancé,\nsur ce sujet à l’ENSAE avec\nRomain Avouac:\nhttps://ensae-reproductibilite.netlify.app/."
  },
  {
    "objectID": "content/course/getting-started/index.html#public-cible",
    "href": "content/course/getting-started/index.html#public-cible",
    "title": "Introduction",
    "section": "Public cible",
    "text": "Public cible\nCe cours ne revient que de manière secondaire\nsur les fondements statistiques ou algorithmiques\nderrière certaines des techniques évoquées.\nNe pas connaître ces notions n’empêche néanmoins pas de comprendre\nle contenu de ce site web. En effet, la facilité d’usage de Python\névite de devoir programmer soi-même un modèle, ce qui rend\npossible l’application\nde modèles dont on n’est pas expert. La connaissance des modèles sera\nplutôt nécessaire dans l’interprétation des résultats.\nCependant, la facilité avec laquelle il est possible de construire des modèles complexes\navec Python peut laisser apparaître que connaître les spécifités de chaque\nmodèle est inutile. Il\ns’agirait d’une grave erreur: même si l’implémentation de modèles est aisée, il\nest nécessaire de bien comprendre la structure des données et leur adéquation\navec les hypothèses d’un modèle."
  },
  {
    "objectID": "content/course/getting-started/index.html#reproductibilité",
    "href": "content/course/getting-started/index.html#reproductibilité",
    "title": "Introduction",
    "section": "Reproductibilité",
    "text": "Reproductibilité\nCe cours donne une place centrale à\nla notion de reproductibilité. Cette exigence se traduit de diverses\nmanières dans cet enseignement, en particulier en insistant sur un\noutil indispensable pour favoriser le partage de codes informatiques,\nà savoir Git.\nL’ensemble du contenu du site web est reproductible dans des environnements\ninformatiques divers. Il est bien-sûr possible de copier-coller les morceaux\nde code présents dans ce site. Cette méthode montrant rapidement ses limites,\nle site présente un certain nombre de boutons disponibles pour\nouvrir la page sous un format Jupyter Notebook sur divers\npages web:\n\nSur l’ensemble du site web,\nil est possible de cliquer\nsur la petite icone \npour être redirigé vers le dépôt Github associé à ce cours.\nUn certain nombre de boutons permettent de transformer chaque\npage web en Jupyter Notebooks s’il est nécessaire de\nvisualiser ou exécuter du code Python.\n\nVoici, par exemple, ces boutons pour le tutoriel Numpy\n\n\n\n\n\n\n\n\n\n\nPour les agents de la fonction publique, ou\nles élèves des écoles partenaires, il est recommandé\nde privilégier le bouton SSPCloud qui est\nune infrastructure cloud moderne, puissante et flexible\ndéveloppée par l’Insee et accessible à l’url\nhttps://datalab.sspcloud.fr1.\nL’ensemble du contenu de ce site s’appuie sur des données\nouvertes, qu’il s’agisse de données françaises (principalement\nissues de la plateforme\ncentralisatrice data.gouv ou du site\nweb de l’Insee) ou de données\naméricaines. Les résultats sont donc reproductibles pour quelqu’un\ndisposant d’un environnement identique."
  },
  {
    "objectID": "content/course/getting-started/index.html#architecture-du-site-web",
    "href": "content/course/getting-started/index.html#architecture-du-site-web",
    "title": "Introduction",
    "section": "Architecture du site web",
    "text": "Architecture du site web\nCe cours présente\ndes tutoriels et des exercices complets.\nChaque page est structurée sous la forme\nd’un problème concret et présente la\ndémarche générique pour résoudre ce problème général.\nVous pouvez naviguer dans l’architecture du site via la table des matières\nou par les liens vers le contenu antérieur ou postérieur à la fin de chaque\npage. Certaines parties, notamment celle consacrée à la modélisation,\nproposent des exemples fil-rouge pour illustrer la démarche de manière\nplus extensive."
  },
  {
    "objectID": "content/course/getting-started/index.html#evaluation",
    "href": "content/course/getting-started/index.html#evaluation",
    "title": "Introduction",
    "section": "Evaluation",
    "text": "Evaluation\nLes élèves de l’ENSAE valident le cours grâce à\nun projet approfondi.\nLes éléments relatifs à l’évaluation du cours, ainsi qu’une\nliste des projets déjà effectués, sont disponibles dans la\nSection Evaluation."
  },
  {
    "objectID": "content/course/getting-started/index.html#références",
    "href": "content/course/getting-started/index.html#références",
    "title": "Introduction",
    "section": "Références",
    "text": "Références"
  },
  {
    "objectID": "content/course/getting-started/index.html#contenu-général",
    "href": "content/course/getting-started/index.html#contenu-général",
    "title": "Introduction",
    "section": "Contenu général",
    "text": "Contenu général"
  },
  {
    "objectID": "content/course/getting-started/index.html#eléments-supplémentaires",
    "href": "content/course/getting-started/index.html#eléments-supplémentaires",
    "title": "Introduction",
    "section": "Eléments supplémentaires",
    "text": "Eléments supplémentaires"
  },
  {
    "objectID": "content/course/getting-started/index.html#structuration-de-cette-partie",
    "href": "content/course/getting-started/index.html#structuration-de-cette-partie",
    "title": "Introduction",
    "section": "Structuration de cette partie",
    "text": "Structuration de cette partie"
  },
  {
    "objectID": "content/course/getting-started/01_installation/index.html#local",
    "href": "content/course/getting-started/01_installation/index.html#local",
    "title": "1  Configuration de Python",
    "section": "1.1 Installer un environnement adapté à la data-science sur son ordinateur personnel",
    "text": "1.1 Installer un environnement adapté à la data-science sur son ordinateur personnel\nCette partie présente plusieurs éléments de configuration d’un environnement\nen local. Cependant, cette approche est de moins en moins fréquente. En effet,\nplusieurs facteurs conjoints ont amené à privilégier des\nserveurs plutôt que des installations locales (évolutions dans les technologies cloud,\nbesoins accrus de ressources, besoins de plus de contrôle sur la confidentialité\ndes données en limitant leur prolifération…). Au sein des administrations et\ndes entreprises, les approches cloud, où l’utilisateur se voit mis à disposition\nune interface graphique alors que les calculs sont déportés sur un serveur\ndistant, est de plus en plus fréquent.\n\n1.1.1 Installer Python en local\nPour installer Python, il est recommandé d’utiliser\nla distribution Anaconda\nqui permet d’installer une distribution minimale de Python ainsi qu’éventuellement\nun environnement plus complet :\n\nSous Windows, il suffit de télécharger l’exécutable puis\nl’exécuter (cf. la doc officielle\nou ce site).\nSous Mac, se reporter à la doc officielle\nSous Linux, suivre les instructions de la doc officielle selon sa distribution\n\nPasser par Anaconda permet:\n\nd’installer Python ;\nd’installer par défaut une multitude de packages utiles\n(liste ici) ;\nde pouvoir utiliser un gestionnaire de package nommé conda.\n\nAnaconda permet de créer des environnements isolés et facilite l’installation\nde certaines librairies qui nécessitent l’usage de langages externes (par exemple\ndu C++).\n\n\n1.1.2 Installer un environnement de développement\nLes notebooks Jupyter (extension .ipynb)\nsont très utilisés en data science. Ils sont en\nparticulier très adaptés à la réalisation d’analyses exploratoires.\nLes notebooks permettent de mêler du code, du texte et des sorties\ngraphiques ou des tableaux. L’intérêt principal des notebooks est qu’ils\npermettent d’exécuter du code très facilement dans un environnement\nPython donné (le kernel Jupyter). Ils sont particulièrement pratiques\npour ajouter du code ou du texte à un document déjà existant, d’où le\nterme de notebook.\nNéanmoins, passé l’étape d’exploration, il est recommandé de plutôt recourir à des\nscripts au format .py. L’utilisation du format .py est l’un des premiers\ngestes pour favoriser la reproductibilité des analyses.\nCes scripts peuvent être édités à l’aide d’éditeurs de texte adaptés au code, comme\nVisual Studio\n(mon préféré),\nSublime Text,\nou PyCharm (privilégier Pycharm Community Edition)\nentre autres.\nCes éditeurs\noffrent des fonctionalités supplémentaires pratiques :\n\nnombreux plugins pour une pleine utilisation de l’écosystème Python: éditeur de Markdown,\ninterface Git, etc.\nfonctionalités classiques d’un IDE dont manque Jupyter: autocomplétion, diagnostic du code, etc.\nintégration avec les environnements Conda\n\n\n\n1.1.3 Installation de Git\nLe principe de Git ainsi que son usage avec Python sont présentés dans\nune partie dédiée. Cette partie se concentre ainsi sur la question\nde la configuration de Git.\nGit est un langage dont la fonction est de tracer l’historique de modification\nd’un fichier. Pour disposer de ce langage, il est nécessaire d’installer\nle logiciel Git Bash. Grâce à lui, Git sera disponible et des outils\nexternes, notamment les interfaces de développement comme\nVisual Studio, pourront l’utiliser."
  },
  {
    "objectID": "content/course/getting-started/01_installation/index.html#exécution-dans-un-environnement-temporaire-sur-un-serveur-distant",
    "href": "content/course/getting-started/01_installation/index.html#exécution-dans-un-environnement-temporaire-sur-un-serveur-distant",
    "title": "1  Configuration de Python",
    "section": "1.2 Exécution dans un environnement temporaire sur un serveur distant",
    "text": "1.2 Exécution dans un environnement temporaire sur un serveur distant\nComme évoqué précedemment, les technologies dominantes dans\nle domaine du traitement des données ont amené à une évolution des pratiques\ndepuis quelques années.\nLa multiplication de données volumineuses qui dépassent les capacités en RAM\nvoire en stockage des machines personnelles,\nles progrès dans les technologies de stockage type cloud,\nl’adhésion de la communauté aux outils de versioning\n(le plus connu étant Git) sont autant de facteurs\nayant amené à repenser la manière de traiter des données.\nLes infrastructures à l’état de l’art permettent ainsi de découpler stockage\ndes données, stockage du code et exécution des traitements sur les données.\nL’exécution des traitements s’effectue ainsi sur des machines à la durée de vie\ncourte qui stockent temporairement données et code ensembles pour tester\nles traitements.\nAvec les dépôts sur Github ou Gitlab,\non dissocie environnement de stockage des codes et\nd’exécution de ceux-ci. Un système de stockage S3, présenté dans un\nchapitre ultérieur, permet en supplément de dissocier l’environnement\nde stockage des données de ces deux premiers environnements.\nSur le\ndépôt github de ce cours , on peut\nnaviguer dans les fichiers\n(et voir tout l’historique de modification de ceux-ci). Mais,\ncomment exécuter les scripts sans passer par un poste local ?\nDepuis quelques années, des services en ligne permettant de\nlancer une instance Jupyter à distance (analogue à celle que vous pouvez\nlancer en local en utilisant Anaconda) ont émergé. Parmi celles-ci :\n\nLe SSP Cloud  plateforme développée par l’Insee qui fournit des environnements bac à sable basés sur des technologie de conteneurisation\nGoogle collaboratory\n\n;\nGithub Visual Studio Editor  ;\nBinder  ;\n\nIl est également possible d’exécuter des codes sur les services d’intégration continue de\nGitlab (service Gitlab CI)\nou de Github (via Github Actions). Il s’agit d’une approche\nbash, c’est-à-dire que les scripts sont exécutés par une console à chaque interaction avec le dépôt\ndistant Gitlab/Github, sans session ouverte pour les éditer.\nCette approche est très appropriée\npour assurer la reproductibilité d’une chaîne de traitement (on peut aller\njusqu’au\ndéploiement de visualisations automatiques[^2]) mais n’est pas très pratique pour\nle griffonnage.\n[^2] A cet égard, il est recommandé de consulter le cours de dernière année\nde l’ENSAE déjà cité: https://ensae-reproductibilite.netlify.app/\nKaggle \npropose des compétitions de code mais\ndonne également la possibilité d’exécuter des notebooks,\ncomme les solutions précédentes.\nIl existe une API Kaggle pour\naccéder à des données Kaggle hors du système Kaggle\n{{% box status=“warning” title=“Warning” icon=“fa fa-exclamation-triangle” %}}\nLes performances de ces solutions peuvent être variables.\nLes serveurs publics mis à disposition\nne sont pas forcément des foudres de guerre. Avec ceux-ci,\non vérifie plutôt la reproductibilité des scripts avec des jeux d’exemples.\nIl est bien-sûr interdit de mettre des données confidentielles dessus: ces\ndernières doivent rester dans des infrastructures où elles sont autorisées.\nQuand on est dans une entreprise ou administration,\nqui dispose de serveurs propres,\non peut aller plus loin en utilisant ces outils\npour automatiser l’ensemble de la chaîne de traitement.\nAttention: il n’y a pas de garantie de perennité de service\n(notamment avec Binder où\n10 minutes d’inactivité mènent à l’extinction du service). Il s’agit plus d’un service pour griffoner\ndans le même environnement que celui du dépôt Git que de solutions durables.\nLes sessions sur l’environnement SSPCloud sont plus durables mais il convient\nde garder à l’esprit qu’elles sont également temporaires.\n{{% /box %}}\n\n1.2.1 SSP-Cloud \nOnyxia, l’autre petit nom du SSP-Cloud,\nest une plateforme libre service mutualisée de traitement\nde données statistiques et de datascience.\nCe cloud met à disposition aux statisticiens et aux data scientists\nde l’État un catalogue de services et un environnement de travail simple, rapide et collaboratif, permettant de lancer facilement ces outils et d’y connecter ses données et son code.\nAu-delà des ressources techniques, cette plateforme\nreprésente une opportunité pour les statisticiens publics et les\nétudiants de découvrir\net d’adopter de nouvelles méthodes de travail.\nElle est aussi utilisé à des fins de formations et d’auto-formations.\nDans cet environnement, Jupyter et Visual Studio sont tous deux\ndisponibles.\n\n\n1.2.2 Google collaboratory \nGoogle met à disposition une plateforme de calculs basée sur le format Jupyter Notebook.\nUn grand avantage de cette solution est la mise à disposition gratuite de\nGPUs de qualité raisonnable,\noutil quasi-indispensable dans les projets basés sur des méthodes de deep learning.\nIl est possible de connecter les notebooks ouverts à Google Drive ou à\ngithub. L’icone\n\nfournit un raccourci pour lancer le notebook dans un environnement dédié.\n\n\n1.2.3 Github Visual Studio Editor \nMicrosoft qui possède à la fois Github et Visual Studio a récemment\nlancé une offre Github dev qui permet d’ouvrir et lancer un notebook\nJupyter depuis un navigateur web.\nEn plus des fonctionalités attendues du logiciel Visual Studio\nCette interface permet également de gérer les issues et pull request\nd’un dépôt Github.\n\n\n1.2.4 La technologie en arrière-plan: Docker \nDocker est l’outil open-source de référence\nen matière de création d’environnements isolés et auto-suffisants (\nles conteneurs.\nEn pratique, une application codée en Python ne repose que rarement seulement sur\ndu code produit par son développeur, elle fait généralement intervenir des dépendances :\nd’autres librairies Python, ainsi que des librairies liées au système d’exploitation\nsur laquelle elle est développée. Docker va permettre d’empaqueter l’application ainsi\nque toutes ses dépendances et rendre son exécution portable, c’est à dire indépendante\ndu système sur laquelle elle est éxécutée.\nDocker  est utilisé dans\nle cadre de cours afin d’assurer la reproductibilité des exemples.\nPlus de détails sont disponibles dans le cours de dernière année d’ENSAE\ndédié à la mise en production de projets data-science\n(ensae-reproductibilite.netlify.app/).\nIl est possible d’utiliser les images Docker sur lesquelles reposent\nl’environnement de reproductibilité du cours. Celles-ci sont mises à\ndisposition sur DockerHub, le principal réseau de mise à disposition\nd’images Docker. Il existe une image minimale\nqui intègre Python et Quarto.\nPour utiliser l’image Visual Studio:\ndocker pull linogaliana/python-datascientist-vstudio\ndocker run --rm -p 8787:8787 -e PASSWORD=test linogaliana/python-datascientist-vstudio\nEn se rendant depuis un navigateur sur localhost:8887/, et en rentrant\nle mot de passe test (défini plus haut), on peut ainsi accéder\nà l’interface désirée (attention il s’agit d’un environnement temporaire, pas\npérenne)."
  },
  {
    "objectID": "content/course/getting-started/01_installation/index.html#installer-des-packages-supplémentaires",
    "href": "content/course/getting-started/01_installation/index.html#installer-des-packages-supplémentaires",
    "title": "1  Configuration de Python",
    "section": "1.3 Installer des packages supplémentaires",
    "text": "1.3 Installer des packages supplémentaires\nUn module est un script qui a vocation à définir des objets utilisés\npostérieurement par un interpréteur. C’est un script .py autosuffisant,\ndéfinissant des objets et des relations entre eux et le monde extérieur\n(d’autres modules). Un package est un ensemble cohérent de modules. Par exemple\nscikit-learn propose de nombreux modules utiles pour le machine learning.\nPython, sans ajout de briques supplémentaires,\ntrouvera rapidement ses limites.\nMême dans les scripts les plus simples, on a généralement besoin de packages qui\névitent de réinventer la roue.\nLes packages sont les éléments qui font la richesse des\nlangages open-source.\nIls sont l’équivalent des packages R ou Stata.\nLe monde de développeurs Python est très prolifique :\ndes mises à jour sont très souvent disponibles,\nles bibliothèques de packages sont très nombreuses. Un data-scientist\nprendra l’habitude de jongler avec des dizaines de packages dont il connaîtra\nquelques fonctions et où, surtout, il saura aller chercher de l’information.\nLe rythme des mises à jour et des ajouts de fonctionalités\ns’est accéléré ces dernières années. Les grandes compagnies du\nnumérique ont elles-mêmes opensourcées des librairies\ndevenues centrales dans l’écosystème de la data-science\n(TensorFlow par Google, PyTorch par Facebook…)\nLes forums, notamment StackOverflow\nregorgent de bons conseils.\nLes deux meilleurs conseils qu’on puisse donner :\n\nregarder la documentation officielle d’un package. Les bons packages sont\ngénéralement très bien documentés et beaucoup d’erreurs peuvent être évitées\nen apprenant à chercher dans la documentation ;\nen cas d’erreur : copiez-collez l’erreur sur votre moteur de recherche préféré. Quelqu’un aura déjà posé la question, sans doute sur stackoverflow. Néanmoins, ne copiez-collez\npas la réponse sans comprendre la solution.\n\n\n1.3.1 Les gestionnaires de packages\nLes packages d’un langage open-source sont mis à disposition sur\ndes dépôts. Le CTAN est ainsi le dépôt \\(\\LaTeX\\) le plus connu, le\nCRAN celui du langage R.\nEn Python, il existe deux gestionnaires de packages qu’on utilise\nassociés à deux dépôts différents :\n\npip associé au dépôt PyPi\nconda associé au dépôt Anaconda\n\nAnaconda a permis, il y a quelques années, de faciliter grandement\nl’installation de librairies dépendants d’autres langages\nque Python (notamment des librairies C pour améliorer\nla performance des calculs). Ces dernières sont\ncompliquées à installer, notamment sur Windows.\nLe fait de proposer des librairies pré-compilées sur une grande\nvariété de systèmes d’exploitation a été une avancée\nd’anaconda. PyPi a adopté ce même principe avec les\nwheels ce qui finalement, rend les installations\navec pip à nouveau intéressantes (sauf pour certaines\nlibrairies en Windows).\nAnaconda a deux défauts par rapport à pip :\n\nl’installation de packages via pip est plus rapide que via\nconda. conda est en effet plus précautionneux sur l’interaction\nentre les différentes versions des packages installés.\nmamba a récemment\nété développé pour accélérer l’installation de packages dans un\nenvironnement conda1\nles versions disponibles sur PyPi sont plus récentes\nque celles sur le canal par défaut d’Anaconda. En effet,\npour un développeur de packages, il est possible de publier\nun package de manière automatique sur PyPi\nL’utilisation\ndu canal alternatif qu’est la conda forge permet de disposer de versions plus récentes des packages et limite l’écart avec les versions\ndisponibles sur PyPi.\n\n{{% box status=“note” title=“Note” icon=“fa fa-comment” %}}\nLes conditions d’utilisation du canal par défaut d’Anaconda sont\nassez restrictives. L’utilisation d’Anaconda dans un cadre commercial est ainsi, depuis 2020,\nsoumis à l’achat de licences commerciales d’Anaconda pour réduire le problème de\npassager clandestin.\nIl est ainsi recommandé, notamment lorsqu’on travaille dans le\nsecteur privé où du code Python peut être utilisé,\nde ne pas ignorer ces conditions pour ne pas se mettre en faute juridiquement.\nLa conda forge n’est pas soumise à ces conditions et est ainsi préférable\ndans les entreprises.\n{{% /box %}}\n\n\n1.3.2 Comment installer des packages\nAvec Anaconda, il faut passer par la ligne de commande et taper\nconda install &lt;nom_module&gt;\nPar exemple conda install geopandas. Depuis une cellule de notebook\nJupyter, on ajoute un point d’exclamation pour indiquer à Jupyter\nque la commande doit être interprétée comme une commande shell\net non une commande Python\n!conda install &lt;nom_module&gt; -y\nL’option -y permet d’éviter que conda nous demande confirmation\nsur l’installation du package. Pour mettre à jour un package, on fera\nconda upgrade plutôt que conda install\nAvec pip, on va cette fois taper\npip install &lt;nom_module&gt;\npip permet également d’installer des librairies directement depuis\nGithub à condition que Anaconda et Git sachent\ncommuniquer (ce qui implique en général que Git soit dans le PATH\ndu système d’exploitation). Par exemple, pour installer le package\npynsee\npip install git+https://github.com/InseeFrLab/Py-Insee-Data.git#egg=pynsee\nLa partie dédiée aux environnement virtuels du cours de dernière année de\nl’ENSAE présente plus d’éléments sur les différences moins évidentes\nentre pip et conda."
  },
  {
    "objectID": "content/course/getting-started/02_DS_environment/index.html#les-packages-python-essentiels-pour-le-cours-et-la-vie-des-data-scientists",
    "href": "content/course/getting-started/02_DS_environment/index.html#les-packages-python-essentiels-pour-le-cours-et-la-vie-des-data-scientists",
    "title": "2  L’environnement Python pour la data-science",
    "section": "2.1 Les packages python essentiels pour le cours et la vie des data-scientists",
    "text": "2.1 Les packages python essentiels pour le cours et la vie des data-scientists\n\n\n\n\n\nCe\npost,\ndont l’image ci-dessus est tirée, résume la plupart des packages utiles\npour un data-scientist ou un économiste/sociologue. Nous nous bornerons\nici à évoquer ceux utilisés quotidiennement.\n\n2.1.1 numpy\nnumpy gère tout ce qui est calcul matriciel.\nLe langage Python est un des langages les plus lents qui soient1.\nTous les calculs rapides ne sont pas écrits en Python mais en C++, voire Fortran.\nC’est le cas du package numpy. Celui-ci est incontournable\ndès qu’on veut être rapide. Le package\nscipy est une extension où l’on peut trouver\ndes fonctions statistiques, d’optimisation.\nLa Cheat Sheet de numpy est pratique:\nhttps://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf\nComme numpy est la brique de base de l’analyse de données, un chapitre\nde ce cours lui est consacré.\n\n\n2.1.2 pandas\nAvant tout, un bon data-scientist doit être capable de\ns’approprier et manipuler des données rapidement. Pour cette raison,\npandas est incontournable.\nIl gère la plupart des formats de données. Pour être efficace,\nil est lui aussi implémenté en C++.\nLe package est rapide si on utilise les méthodes pré-implémentées sur\ndes données d’une taille raisonnable (par rapport à la RAM disponible). Il faut\nnéanmoins s’en méfier avec des données volumineuses.\nEn règle générale, un jeu de données nécessite\ntrois fois plus d’espace en mémoire que les\ndonnées n’en prennent sur le disque.\nLa Cheat Sheet de pandas :\nhttps://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_Pandas_Cheat_Sheet_2.pdf\npandas étant un élément incontournable, deux chapitres y sont consacrés.\n\n\n2.1.3 matplotlib et seaborn\nmatplotlib existe depuis une vingtaine d’années pour doter Python de\nfonctionalités graphiques. Il s’agit d’un package très flexible, offrant\nde nombreuses fonctionalités. Néanmoins, ces dernières années,\nseaborn a émergé pour simplifier la création de certains graphiques\nstandards de l’analyse de données (histogrammes, diagramme en barre, etc. ).\nLe succès de seaborn n’éclipse néanmoins pas matplotlib puisque ce\ndernier est souvent nécessaire pour finaliser la customisation d’un\ngraphique produit par seaborn2\n\n\n2.1.4 scikit-learn\nscikit-learn est le module de machine learning le plus populaire pour\ntrois raisons:\n\nil s’appuie sur une API extrêmement consistante (méthodes fit, transform\net predict, respectivement pour apprendre des données, appliquer des transformations et prédire sur de nouvelles données) ;\nil permet de construire\ndes analyses reproductibles en construisant des pipelines de données ;\nsa documentation est un modèle à suivre.\n\nL’INRIA, institution française, est l’un des éléments moteurs dans\nla création et la maintenance de scikit-learn\n\n\n2.1.5 TensorFlow, PyTorch et Keras\nLes librairies essentielles pour implémenter et utiliser des modèles\nde deep learning en Python ont été développées par des acteurs du\nnumérique.\nTensorFlow est la librairie la plus mature, mais pas nécessairement la plus facile à prendre en main. D’ailleurs, Google semble l’abandonner en usage interne pour lui\npréférer JAX.\nKeras propose une interface high-level,\ndonc plus facile d’utilisation,\nmais qui n’en reste pas moins suffisante pour une grande variété d’usages.\nLa documentation de Keras est très bien faite.\nPyTorch est un framework plus récent mais très complet,\ndont la syntaxe plaira aux amateurs de programmation orienté-objet.\nDéveloppé par Facebook,\nil est très utilisé dans certains domaines de recherche, comme le NLP.\nIl s’agit du framework dont la dynamique récente a été la plus\nascensionnelle.\n\n\n2.1.6 statsmodels\nstatsmodels plaira plus aux statisticiens, il implémente des modèles\néconométriques similaires à scikit-learn.\nPar rapport à scikit-learn,\nstatsmodels est plus orienté économétrie. La présentation des\nrésultats est très proche de ce qu’on trouve en R.\n\n\n2.1.7 requests et beautifulsoup\nrequests est l’une des librairies de base de Python, dédiée\nà gérer la connexion avec internet. Les amateurs d’API\nseront des utilisateurs fréquents de celle-ci. Les\npersonnes plus spécialistes de webscraping lui préféreront\nbeautifulsoup qui offre une syntaxe extrêmement puissante\npour récupérer automatiquement du contenu de pages web.\n\n\n2.1.8 nltk et spaCy\nDans le domaine du traitement automisé du langage, plus connu\nsous son acronyme anglais NLP, les deux packages phares sont\nnltk et spaCy.\nnltk est le package historique. Il existe depuis les années\n1990 et propose de nombreuses ressources utiles pour l’analyse\ntextuelle. Néanmoins, ces dernières années, spaCy est venu\nmoderniser l’approche en proposant une approche permettant\nde mieux intégrer les différentes phases du traitement de données\ntextuelles, une excellente documentation et un meilleur support\ndes langues non anglo-saxonnes, comme le Français.\nMais Python est également un outil privilégié pour communiquer:\n\nUne bonne intégration de python à Markdown (grâce notamment à … R Markdown) qui facilite la construction de documents HTML ou PDF (via Latex)\nSphynx et JupyterBook proposent des modèles de documentation\ntrès complets\nbokeh ou streamlit comme alternative à shiny (R)\nDjango et Flask permettent de construire des applications web en Python\nLes librairies dynamiques, notamment\nfolium ou\nplotly, sont très appréciées pour construire des\nvisualisations dynamiques qui sont pratiques dans une analyse exploratoire\nmais également lorsqu’il faut valoriser ses travaux auprès de\npublics non experts de la donnée.\n\nL’un des nouveaux arrivants dans cet écosystème déjà riche\nest FastAPI). Avec ce package,\nil est très facile de transformer un code Python en API ce qui facilite\nla mise à disposition de données mais aussi de productions par Python (comme\nla mise à disposition d’une API pour permettre à des personnes de tester\nles résultats d’un modèle de machine learning).\nCe n’est qu’une petite partie de l’écosystème Python, d’une richesse rare."
  },
  {
    "objectID": "content/course/getting-started/02_DS_environment/index.html#environnement-autour-de-python",
    "href": "content/course/getting-started/02_DS_environment/index.html#environnement-autour-de-python",
    "title": "2  L’environnement Python pour la data-science",
    "section": "2.2 Environnement autour de Python",
    "text": "2.2 Environnement autour de Python\nPython est un langage très riche, grâce à sa logique open-source. Mais l’un\ndes principaux intérêts réside dans le riche écosystème avec lequel Python\ns’intègre. On peut donner quelques éléments, dans un inventaire à la Prévert non exaustif.\nEn premier lieu, des éléments reliés au traitement des données:\n\nSpark,\nle framework dominant dans le domaine du traitement des big-data, très bien\ninterfacé avec Python (grâce à l’API pyspark), qui facilite le traitement des données volumineuses. Son utilisation nécessite cependant d’avoir accès à une\ninfrastructure de calculs distribuée.\nCython permet d’intégrer facilement du code C, très\nefficace avec Python (équivalent de Rcpp pour R).\nJulia est un langage récent, qui propose une syntaxe familière aux utilisateurs de languages scientifiques (Python, R, MATLAB), tout en permettant des performances proches du C grâce à une compilation à la volée.\n\nEnfin, des éléments permettant un déploiement de résultats ou d’applications\nen continu :\n* Les images Docker de Jupyterhub facilitent l’usage de l’intégration continue\npour construire des modules, les tester et déployer des site web.\n* Les services type Binder, Google Colab et Kaggle proposent des kernels\nPython"
  },
  {
    "objectID": "content/course/getting-started/02_DS_environment/index.html#rester-au-courant-des-évolutions",
    "href": "content/course/getting-started/02_DS_environment/index.html#rester-au-courant-des-évolutions",
    "title": "2  L’environnement Python pour la data-science",
    "section": "2.3 Rester au courant des évolutions",
    "text": "2.3 Rester au courant des évolutions\nL’écosystème riche et foisonnant de Python a comme contrepartie\nqu’il faut rester attentif à ses évolutions pour ne pas\nvoir son capital humain vieillir et ainsi devenir has-been.\nAlors qu’avec des langages\nmonolithiques comme\nSAS ou Stata on pouvait se permettre de ne faire de vieille technique\nmais seulement consulter la documentation officielle, avec Python\nou R c’est impossible. Ce cours lui-même est en évolution continue, ce\nqui est assez exigeant :sweating:, pour épouser les évolutions\nde l’écosystème.\nTwitter est une excellente source d’information pour être rapidement\nau courant des évolutions du monde de la data-science. Les agrégateurs\nde contenu comme medium ou towarddatascience proposent des posts\nde qualité hétérogène mais il peut être utile de recevoir par mail\nle feed des nouveaux posts: au bout d’un certain temps, cela peut\npermettre de dégager les nouvelles tendances. Le site\nrealpython propose généralement de très bon posts, complets et\npédagogiques.\nEn ce qui concerne les ouvrages papiers, certains sont de très bonne qualité.\nCependant, il convient de faire attention à la date de mise à jour de ceux-ci:\nla vitesse d’évolution de certains éléments de l’écosystème peut les\npérimer très rapidement."
  },
  {
    "objectID": "content/course/getting-started/03_data_analysis/index.html",
    "href": "content/course/getting-started/03_data_analysis/index.html",
    "title": "3  Comment aborder un jeu de données",
    "section": "",
    "text": "4 Démarche à adopter face à un jeu de données\nPour bien débuter des travaux sur une base de données,\nil est nécessaire de se poser quelques questions de bon sens\net de suivre une démarche assez simple."
  },
  {
    "objectID": "content/course/getting-started/03_data_analysis/index.html#une-démarche-scientifique",
    "href": "content/course/getting-started/03_data_analysis/index.html#une-démarche-scientifique",
    "title": "3  Comment aborder un jeu de données",
    "section": "4.1 Une démarche scientifique",
    "text": "4.1 Une démarche scientifique\nDans un projet sur des jeux de données, on peut schématiquement\nséparer les étapes en quatre grandes parties :\n\nla récupération et structuration des données;\nleur analyse (notamment descriptive) ;\nla modélisation ;\nla valorisation finale des étapes précédentes."
  },
  {
    "objectID": "content/course/getting-started/03_data_analysis/index.html#lors-de-la-récupération-des-données",
    "href": "content/course/getting-started/03_data_analysis/index.html#lors-de-la-récupération-des-données",
    "title": "3  Comment aborder un jeu de données",
    "section": "4.2 Lors de la récupération des données",
    "text": "4.2 Lors de la récupération des données\n\n4.2.1 Réflexions à mener en amont\nLa phase de constitution de son jeu de données sous-tend tout le projet qui suit.\nLa première question à se poser est\n“de quelles données ai-je besoin pour répondre à ma problématique ?”.\nCette problématique pourra éventuellement\nêtre affinée en fonction des besoins mais les travaux sont généralement\nde meilleure qualité lorsque la problématique amène à la réflexion sur les données\ndisponibles plutôt que l’inverse.\nEnsuite, “qui produit et met à disposition ces données” ?\nLes sources disponibles sur internet sont-elles fiables ?\nPar exemple, les sites d’open data gouvernementaux sont par exemple assez fiables mais autorisent parfois l’archivage de données restructurées par des tiers et non des producteurs officiels. A l’inverse, sur Kaggle ou sur Github la source de certains jeux de données n’est pas tracée ce qui rend compliquée la confiance sur la qualité de la donnée\nUne fois identifié une ou plusieurs sources de données,\nest-ce que je peux les compléter avec d’autres données ?\n(dans ce cas, faire attention à avoir des niveaux de granularité adéquats).\n\n\n4.2.2 Structuration des données\nVient ensuite la phase de mise en forme et nettoyage des jeux de données récupérés.\nCette étape est primordiale et est généralement celle qui mobilise le plus\nde temps. Pendant quelques années, on parlait de data-cleaning. Cependant,\ncela a pu, implicitement, laisser penser qu’il s’agissait d’une tâche\nsubalterne. On commence à lui préférer le concept de feature engineering\nqui souligne bien qu’il s’agit d’une compétence qui nécessite beaucoup\nde compétences.\nUn jeu de données propre est un jeu de données dont la structure est\nadéquate et n’entraînera pas d’erreur, visible ou non,\nlors de la phase d’analyse. Voici quelques éléments structurants\nd’un jeu de données propre:\n\nles informations manquantes sont bien comprises et traitées. numpy et\npandas proposent un certain formalisme sur le sujet qu’il est utile\nd’adopter en remplaçant par NaN les observations manquantes. Cela\nimplique de faire attention à la manière dont certains producteurs\ncodent les valeurs manquantes: certains ont la facheuse tendance à\nêtre imaginatifs sur les codes pour valeurs manquantes: “-999”, “XXX”, “NA”\nles variables servant d’identifiants sont bien les mêmes d’une table à l’autre (notamment dans le cas de jointure) : même format, même modalités\npour des variables textuelles, qui peuvent etre mal saisies, avoir corrigé les éventuelles fautes (ex “Rolland Garros” &gt; “Roland Garros”)\ncréer des variables qui synthétisent l’information dont vous avez besoin\nsupprimer les éléments inutiles (colonne ou ligne vide)\nrenommer les colonnes avec des noms compréhensibles"
  },
  {
    "objectID": "content/course/getting-started/03_data_analysis/index.html#lors-de-lanalyse-descriptive",
    "href": "content/course/getting-started/03_data_analysis/index.html#lors-de-lanalyse-descriptive",
    "title": "3  Comment aborder un jeu de données",
    "section": "4.3 Lors de l’analyse descriptive",
    "text": "4.3 Lors de l’analyse descriptive\nUne fois les jeux de données nettoyés, vous pouvez plus sereinement\nétudier l’information présente dans les données.\nCette phase et celle du nettoyage ne sont pas séquentielles,\nen réalité vous devrez régulièrement passer de votre nettoyage à quelques statistiques descriptives qui vous montreront un problème, retourner au nettoyage etc.\nLes questions à se poser pour “challenger” le jeu de données :\n\nest-ce que mon échantillon est bien représentatif de ce qui m’intéresse ? N’avoir que 2000 communes sur les 35000 n’est pas nécessairement un problème mais il est bon de s’être posé la question.\nest-ce que les ordres de grandeur sont bons ? pour cela, confronter vos premieres stats desc à vos recherches internet. Par exemple trouver que les maisons vendues en France en 2020 font en moyenne 400 m² n’est pas un ordre de grandeur réaliste.\nest-ce que je comprends toutes les variables de mon jeu de données ? est-ce qu’elles se “comportent” de la bonne façon ? à ce stade, il est parfois utile de se faire un dictionnaire de variable (qui explique comment elles sont construites ou calculées). On peut également mener des études de corrélation entre nos variables.\nest-ce que j’ai des outliers, i.e. des valeurs aberrantes pour certains individus ? Dans ce cas, il faut décider quel traitement on leur apporte (les supprimer, appliquer une transformation logarithmique, les laisser tel quel) et surtout bien le justifier.\nest-ce que j’ai des premiers grands messages sortis de mon jeu de données ? est-ce que j’ai des résultats surprenants ? Si oui, les ai-je creusé suffisamment pour voir si les résultats tiennent toujours ou si c’est à cause d’un souci dans la construction du jeu de données (mal nettoyées, mauvaise variable…)"
  },
  {
    "objectID": "content/course/getting-started/03_data_analysis/index.html#lors-de-la-modélisation",
    "href": "content/course/getting-started/03_data_analysis/index.html#lors-de-la-modélisation",
    "title": "3  Comment aborder un jeu de données",
    "section": "4.4 Lors de la modélisation",
    "text": "4.4 Lors de la modélisation\nA cette étape, l’analyse descriptive doit voir avoir donné quelques premières pistes pour savoir dans quelle direction vous voulez mener votre modèle.\nUne erreur de débutant est de se lancer directement dans la modélisation parce\nqu’il s’agirait d’une compétence plus poussée. Cela amène généralement\nà des analyses de pauvre qualité: la modélisation tend généralement à confirmer\nles intuitions issues de l’analyse descriptive. Sans cette dernière,\nl’interprétation des résultats d’un modèle peu s’avérer inutilement complexe.\nVous devrez plonger dans vos autres cours (Econométrie 1, Series Temporelles, Sondages, Analyse des données etc.) pour trouver le modèle le plus adapté à votre question.\nLa méthode sera guidée par l’objectif.\n\nEst-ce que vous voulez expliquer ou prédire ? https://hal-cnam.archives-ouvertes.fr/hal-02507348/document\nEst-ce que vous voulez classer un élément dans une catégorie (classification ou clustering) ou prédire une valeur numérique (régression) ?\n\nEn fonction des modèles que vous aurez déjà vu en cours et des questions que vous souhaiterez résoudre sur votre jeu de données, le choix du modèle sera souvent assez direct.\nVous pouvez également vous référez à la démarche proposée par Xavier Dupré\nhttp://www.xavierdupre.fr/app/ensae_teaching_cs/helpsphinx3/debutermlprojet.html#l-debutermlprojet\nPour aller plus loin (mais de manière simplifiée) sur les algorithmes de Machine Learning :\nhttps://datakeen.co/8-machine-learning-algorithms-explained-in-human-language/\n\n4.4.1 Valorisation des travaux\nLa mise à disposition de code sur Github ou Gitlab est une incitation\ntrès forte pour produire du code de qualité. Il est ainsi recommandé de\nsystématiquement utiliser ces plateformes pour la mise à disposition de\ncode. Cependant, il ne s’agit que d’une petite partie des gains à\nl’utiliser.\nLe cours que je donne avec Romain Avouac en troisième année d’ENSAE\n(https://ensae-reproductibilite.netlify.app/) évoque\nl’un des principaux gains à utiliser ces plateformes, à savoir\nla possibilité de mettre à disposition automatiquement différents livrables\npour valoriser son travail auprès de différents publics.\nSelon le public visé, la communication ne sera pas identique. Le code peut\nintéresser les personnes désirant avoir des détails sur la méthodologie mise\nen oeuvre en pratique mais il peut s’agir d’un format rebutant pour d’autres\npublics. Une visualisation de données dynamiques parlera à des publics\nmoins experts de la donnée mais est plus dure à mettre en oeuvre\nqu’un graphique standard.\n{{% box status=“hint” title=“Conseil” icon=“fa fa-lightbulb” %}}\nLes Notebooks Jupyter ont eu beaucoup de succès dans le monde de\nla data-science pour valoriser des travaux. Pourtant il ne s’agit\npas forcément toujours du meilleur format. En effet, beaucoup\nde notebooks tentent à empiler des pavés de code et du texte, ce\nqui les rend difficilement lisibles.\nSur un projet conséquent, il vaut mieux reporter le plus de code\npossible dans des scripts bien structurés et avoir un notebook\nqui appelle ces scripts pour produire des outputs. Ou alors ne\npas utiliser un notebook et privilégier un autre format (un\ntableau de bord, un site web, une appli réactive…)\n{{% /box %}}"
  },
  {
    "objectID": "content/course/getting-started/03_data_analysis/index.html#la-reproductibilité-est-importante",
    "href": "content/course/getting-started/03_data_analysis/index.html#la-reproductibilité-est-importante",
    "title": "3  Comment aborder un jeu de données",
    "section": "5.1 La reproductibilité est importante",
    "text": "5.1 La reproductibilité est importante\nLes données sont une représentation synthétique de la vie des gens et les\nconclusions de certaines analyses peuvent avoir un vrai impact sur\nleur vie. Les chiffres erronés de\nReinhart et Rogoff ont ainsi pu servir de justification théorique à des\npolitiques d’austérité qui ont pu avoir des conséquences violentes\npour certains citoyens de\npays en crise1. En Grande-Bretagne, le recensement des personnes\ncontaminées par le Covid en 2020, et donc de leurs proches pour le\nsuivi de l’épidémie,\na été incomplet à cause de\ntroncatures dues à l’utilisation d’un format non adapté de stockage\ndes données (tableur Excel)2.\nDernier exemple avec le credit scoring mis en oeuvre aux Etats-Unis.\nLa citation ci-dessous, issue de l’article de Hurley and Adebayo (2016),\nillustre très bien les conséquences et les aspects problématiques\nd’un système de construction automatisée d’un score de crédit :\n\nConsumers have limited ability to identify and contest unfair credit\ndecisions, and little chance to understand what steps they\nshould take to improve their credit. Recent studies have also\nquestioned the accuracy of the data used by these tools, in some\ncases identifying serious flaws that have a substantial bearing\non lending decisions. Big-data tools may also risk creating a\nsystem of “creditworthinessby association” in which consumers’\nfamilial, religious, social, and other affiliations determine their\neligibility for an affordable loan.\nHurley and Adebayo (2016)"
  },
  {
    "objectID": "content/course/getting-started/03_data_analysis/index.html#lutter-contre-les-biais-cognitifs",
    "href": "content/course/getting-started/03_data_analysis/index.html#lutter-contre-les-biais-cognitifs",
    "title": "3  Comment aborder un jeu de données",
    "section": "5.2 Lutter contre les biais cognitifs",
    "text": "5.2 Lutter contre les biais cognitifs\nLa transparence sur les intérêts et limites d’une méthode mise en oeuvre\nest donc importante.\nCette exigence de la recherche, parfois oubliée à cause de la course\naux résultats novateurs, mérite également d’être appliquée\nen entreprise ou administration.\nMême sans intention manifeste de la part de la personne qui analyse des données,\nune mauvaise interprétation est toujours possible. Tout en valorisant un\nrésultat, il est possible d’alerter sur certaines limites. Il est important,\ndans ses recherches comme dans les discussions avec d’autres interlocuteurs,\nde faire attention au biais de confirmation qui consiste\nà ne retenir que l’information qui correspond à nos conceptions a priori et\nà ne pas considérer celles qui pourraient aller à l’encontre de celles-ci:\n\n\n\n\n\nCertaines représentations de données sont à exclure car des biais cognitifs\npeuvent amener à des interprétations erronées3. Dans le domaine de la\nvisualisation de données, les camemberts (pie chart) ou les diagrammes\nradar sont par exemple\nà exclure car l’oeil humain perçoit mal ces formes circulaires. Pour une raison\nsimilaire, les cartes avec aplat de couleur (cartes\nchoroplèthes) sont trompeuses."
  },
  {
    "objectID": "content/course/getting-started/03_data_analysis/index.html#réglementation",
    "href": "content/course/getting-started/03_data_analysis/index.html#réglementation",
    "title": "3  Comment aborder un jeu de données",
    "section": "5.3 Réglementation",
    "text": "5.3 Réglementation\nLe cadre réglementaire de protection des données a évolué ces dernières\nannées avec le RGPD. Cette réglementation a permis de mieux faire\nsaisir le fait que la collecte de données se justifie au nom\nde finalités plus ou moins bien identifiées. Prendre conscience que\nla confidentialité des données se justifie pour éviter la dissémination\nnon contrôlée d’informations sur une personne est important.\nDes données particulièrement sensibles, notamment les données de santé,\npeuvent être plus contraignantes à traiter que des données peu sensibles.\nEn Europe, par exemple, les agents du service statistique public\n(Insee ou services statistiques ministériels) sont tenus au secret professionnel\n(article L121-6 du Code général de la fonction publique),\nqui leur interdit la communication des informations confidentielles\ndont ils sont dépositaires au titre de leurs missions ou fonctions,\nsous peine des sanctions prévues par l’article 226-13 du Code pénal\n(jusqu’à un an d’emprisonnement et 15 000 € d’amende).\nLe secret statistique, défini dans une loi de 1951,\nrenforce cette obligation dans le cas de données détenues pour des usages statistiques.\nIl interdit strictement la communication de données individuelles\nou susceptibles d’identifier les personnes,\nissues de traitements à finalités statistiques,\nque ces traitements proviennent d’enquêtes ou de bases de données.\nLe secret statistique exclut par principe de diffuser des données\nqui permettraient l’identification des personnes concernées,\npersonnes physiques comme personnes morales.\nCette obligation limite la finesse des informations disponibles en diffusion\nCe cadre contraignant s’explique par l’héritage de la Seconde Guerre Mondiale\net le désir de ne plus revivre une situation où la collecte d’information\nsert une action publique basée sur la discrimination entre catégories\nde la population."
  },
  {
    "objectID": "content/course/getting-started/03_data_analysis/index.html#partager-les-moyens-de-reproduire-une-analyse",
    "href": "content/course/getting-started/03_data_analysis/index.html#partager-les-moyens-de-reproduire-une-analyse",
    "title": "3  Comment aborder un jeu de données",
    "section": "5.4 Partager les moyens de reproduire une analyse",
    "text": "5.4 Partager les moyens de reproduire une analyse\nUn article récent de Nature,\nqui reprend les travaux d’une équipe d’épidémiologistes (Gabelica, Bojčić, and Puljak 2022)\névoque le problème de l’accès aux données pour des chercheurs désirant reproduire\nune étude. Même dans les articles scientifiques où il est mentionné que les\ndonnées peuvent être mises à disposition d’autres chercheurs, le partage\nde celles-ci est rare :\n\nGraphique issu de l’article de Nature\nAfin de partager les moyens de reproduire des publications sans diffuser des\ndonnées potentiellement confidentielles, les jeux de données synthétiques\nsont de plus en plus utilisés. Par le biais de modèles de deep learning,\nil est ainsi possible de générer des jeux de données synthétiques complexes\nqui permettent de reproduire les principales caractéristiques d’un jeu de données\ntout en évitant, si le modèle a été bien calibré, de diffuser une information\nindividuelle."
  },
  {
    "objectID": "content/course/getting-started/03_data_analysis/index.html#adopter-une-approche-écologique",
    "href": "content/course/getting-started/03_data_analysis/index.html#adopter-une-approche-écologique",
    "title": "3  Comment aborder un jeu de données",
    "section": "5.5 Adopter une approche écologique",
    "text": "5.5 Adopter une approche écologique\nLe numérique constitue une part croissante des\némissions de gaz à effet de serre.\nReprésentant aujourd’hui 4 % des émissions mondiales\nde CO2, cette part devrait encore croître (Arcep 2019).\nLe monde de la data-science est également\nconcerné.\nL’utilisation de données de plus en\nplus massives, notamment la constitution\nde corpus monumentaux de textes,\nrécupérés par scraping, est une première source\nde dépense d’énergie. De même, la récupération\nen continue de nouvelles traces numériques\nnécessite d’avoir des serveurs fonctionnels\nen continu. A cette première source de\ndépense d’énergie, s’ajoute l’entraînement\ndes modèles qui peut prendre des jours,\ny compris sur des architectures très\npuissantes. Strubell, Ganesh, and McCallum (2019)\nestime que l’entraînement d’un modèle à\nl’état de l’art dans le domaine du\nNLP nécessite autant d’énergie que ce que\nconsommeraient cinq voitures, en moyenne,\nau cours de l’ensemble de leur\ncycle de vie.\nL’utilisation accrue de l’intégration\ncontinue, qui permet de mettre en oeuvre de manière\nautomatisée l’exécution de certains scripts ou\nla production de livrables en continu,\namène également à une dépense d’énergie importante.\nIl convient donc d’essayer de limiter l’intégration\ncontinue à la production d’output vraiment nouveaux.\nPar exemple, cet ouvrage utilise de manière intensive\ncette approche. Néanmoins, pour essayer de limiter\nles effets pervers de la production en continu d’un\nouvrage extensif, seuls les chapitres modifiés\nsont produits lors des prévisualisations mises en\noeuvre à chaque pull request sur le dépôt\n.\nLes data-scientists doivent être conscients\ndes implications de leur usage intensif de\nressources et essayer de minimiser leur\nimpact. Par exemple, plutôt que ré-estimer\nun modèle de NLP,\nla méthode de l’apprentissage par transfert,\nqui permet de transférer les poids d’apprentissage\nd’un modèle à une nouvelle source, permet\nde réduire les besoins computationnels.\nDe même, il peut être utile, pour prendre\nconscience de l’effet d’un code trop long,\nde convertir le temps de calcul en\némissions de gaz à effet de serre.\nLe package codecarbon\npropose cette solution en adaptant l’estimation\nen fonction du mix énergétique du pays\nen question. Mesurer étant le\nprérequis pour prendre conscience puis comprendre,\nce type d’initiatives peut amener à responsabiliser\nles data-scientists et ainsi permettre un\nmeilleur partage des ressources."
  },
  {
    "objectID": "content/course/getting-started/04_python_practice/index.html#structure-dun-projet-en-python",
    "href": "content/course/getting-started/04_python_practice/index.html#structure-dun-projet-en-python",
    "title": "4  Bonne pratique de Python",
    "section": "4.1 Structure d’un projet en python",
    "text": "4.1 Structure d’un projet en python\nLa structure basique d’un projet développé en Python est la suivante, qu’on peut retrouver dans\nce dépôt:\nREADME.md\nLICENSE\nsetup.py\nrequirements.txt\nmonmodule/__init__.py\nmonmodule/core.py\nmonmodule/helpers.py\ndocs/conf.py\ndocs/index.rst\ntests/context.py\ntests/test_basic.py\ntests/test_advanced.py\nQuelques explications et parallèles avec les packages R1 :\n\nLe code python est stocké dans un module nommé monmodule. C’est le coeur du code dans le projet. Contrairement\nà R, il est possible d’avoir une arborescence avec plusieurs modules dans un seul package. Un bon exemple\nde package dont le fonctionnement adopte une arborescence à plusieurs niveaux est scikit\nLe fichier setup.py sert à construire le package monmodule pour en faire un code utilisable. Il n’est pas\nobligatoire quand le projet n’a pas vocation à être sur PyPi mais il est assez facile à créer en suivant ce\ntemplate. C’est l’équivalent\ndu fichier Description dans un package R\n(exemple)\nLe fichier requirements.txt permet de contrôler les dépendances du projet. Il s’agit des\ndépendances nécessaires pour faire tourner les fonctions (par exemple numpy), les tester et\nconstruire automatiquement la documentation (par exemple sphinx). Dans un package R, le fichier qui contrôle\nl’environnement est le NAMESPACE.\nLe dossier docs stocke la documentation du package. Le mieux est de le générer à partir de\nsphinx et non de l’éditer\nmanuellement. (cf. plus tard).\nLes éléments qui s’en rapprochent dans un package R sont les vignettes.\nLes tests génériques des fonctions. Ce n’est pas obligatoire mais c’est recommandé: ça évite de découvrir deux jours\navant un rendu de projet que la fonction ne produit pas le résultat espéré.\nLe README.md permet de créer une présentation du package qui s’affiche automatiquement sur\ngithub/gitlab et le fichier LICENSE vise à protéger la propriété intellectuelle. Un certain nombre de licences\nstandards existent et peuvent être utilisées comme template grâce au site https://choosealicense.com/\n\n2 La structure nécessaire des projets nécessaire pour pouvoir construire un package R est plus contrainte.\nLes packages devtools, usethis et testthat ont grandement facilité l’élaboration d’un package R. A cet égard,\nil est recommandé de lire l’incontournable livre d’Hadley Wickham"
  },
  {
    "objectID": "content/course/getting-started/04_python_practice/index.html#style-de-programmation-et-de-documentation",
    "href": "content/course/getting-started/04_python_practice/index.html#style-de-programmation-et-de-documentation",
    "title": "4  Bonne pratique de Python",
    "section": "4.2 Style de programmation et de documentation",
    "text": "4.2 Style de programmation et de documentation\n\nThe code is read much more often than it is written.\nGuido Van Rossum [créateur de Python]\n\nPython est un langage très lisible. Avec un peu d’effort sur le nom des objets, sur la gestion\ndes dépendances et sur la structure du programme, on peut\ntrès bien comprendre un script sans avoir besoin de l’exécuter. La communauté Python a abouti à un certain\nnombre de normes, dites PEP (Python Enhancement Proposal), qui constituent un standard\ndans l’écosystème Python. Les deux normes les plus connues sont\nla norme PEP8 (code) et la norme PEP257 (documentation).\nLa plupart de ces recommandations ne sont pas propres à Python, on les retrouve aussi dans R\n(cf. ici).\nOn retrouve de nombreux conseils dans cet ouvrage qu’il est\nrecommandé de suivre. La suite se concentrera sur des éléments complémentaires.\n\n4.2.1 Import des modules\nLes éléments suivants concernent plutôt les scripts finaux, qui appellent de multiples fonctions, que des\nscripts qui définissent des fonctions.\nUn module est un ensemble de fonctions stockées dans un fichier .py. Lorsqu’on écrit dans un script\nimport modu\nPython commence par chercher le fichier modu.py dans le dossier de travail. Il n’est donc pas une bonne\nidée d’appeler un fichier du nom d’un module standard de python, par exemple math.py ou os.py. Si le fichier\nmodu.py n’est pas trouvé dans le dossier de travail, Python va chercher dans le chemin et s’il ne le trouve pas\nretournera une erreur.\nUne fois que modu.py est trouvé, il sera exécuté dans un environnement isolé (relié de manière cohérente\naux dépendances renseignées) et le résultat rendu disponible à l’interpréteur Python pour un usage\ndans la session via le namespace (espace où python associe les noms donnés aux objets).\nEn premier lieu, ne jamais utiliser la syntaxe suivante:\n# A NE PAS UTILISER\nfrom modu import *\nx = sqrt(4)  # Is sqrt part of modu? A builtin? Defined above?\nL’utilisation de la syntaxe import * créé une ambiguité sur les fonctions disponibles dans l’environnement. Le code\nest ainsi moins clair, moins compartimenté et ainsi moins robuste. La syntaxe à privilégier est la suivante:\nimport modu\nx = modu.sqrt(4)  # Is sqrt part of modu? A builtin? Defined above?"
  },
  {
    "objectID": "content/course/getting-started/04_python_practice/index.html#structuration-du-code",
    "href": "content/course/getting-started/04_python_practice/index.html#structuration-du-code",
    "title": "4  Bonne pratique de Python",
    "section": "4.3 Structuration du code",
    "text": "4.3 Structuration du code\nIl est commun de trouver sur internet des codes très longs, généralement dans un fichier __init__.py\n(méthode pour passer d’un module à un package, qui est un ensemble plus structuré de fonctions).\nContrairement à la légende, avoir des scripts longs est peu désirable et est même mauvais ;\ncela rend le code difficilement à s’approprier et à faire évoluer. Mieux vaut avoir des scripts relativement courts\n(sans l’être à l’excès…) qui font éventuellement appels à des fonctions définies dans d’autres scripts.\nPour la même raison, la multiplication de conditions logiques if…else if…else est généralement très mauvais\nsigne (on parle de code spaghetti) ; mieux vaut\nutiliser des méthodes génériques dans ce type de circonstances."
  },
  {
    "objectID": "content/course/getting-started/04_python_practice/index.html#ecrire-des-fonctions",
    "href": "content/course/getting-started/04_python_practice/index.html#ecrire-des-fonctions",
    "title": "4  Bonne pratique de Python",
    "section": "4.4 Ecrire des fonctions",
    "text": "4.4 Ecrire des fonctions\nLes fonctions sont un objet central en Python.\nLa fonction idéale est une fonction qui agit de manière compartimentée:\nelle prend un certain nombre d’inputs et est reliée au monde extérieur uniquement par les dépendances,\nelle effectue des opérations sans interaction avec le monde extérieur et retourne un résultat.\nCette définition assez consensuelle masque un certain nombre d’enjeux:\n\nUne bonne gestion des dépendances nécessite d’avoir appliqué les recommandations évoquées précédemment\nIsoler du monde extérieur nécessite de ne pas faire appel à un objet extérieur à l’environnement de la fonction.\nAutrement dit, aucun objet hors de la portée (scope) de la fonction ne doit être altéré ou utilisé.\n\nPar exemple, le script suivant est mauvais au sens où il utilise un objet y hors du scope de la fonction add\ndef add(x):\n    return x + y\nIl faudrait revoir la fonction pour y ajouter un élément y:\ndef add(x, y):\n    return x + y\nPycharm offre des outils de diagnostics très pratiques pour détecter et corriger ce type d’erreur.\n\n4.4.1 ⚠️ aux arguments optionnels\nLa fonction la plus lisible (mais la plus contraignante) est celle\nqui utilise exclusivement des arguments positionnels avec des noms explicites.\nDans le cadre d’une utilisation avancée des fonctions (par exemple un gros modèle de microsimulation), il est\ndifficile d’anticiper tous les objets qui seront nécessaires à l’utilisateur. Dans ce cas, on retrouve généralement\ndans la définition d’une fonction le mot-clé **kwargs (équivalent du ... en R) qui capture les\narguments supplémentaires et les stocke sous forme de dictionnaire. Il s’agit d’une technique avancée de\nprogrammation qui est à utiliser avec parcimonie."
  },
  {
    "objectID": "content/course/getting-started/04_python_practice/index.html#documenter-les-fonctions",
    "href": "content/course/getting-started/04_python_practice/index.html#documenter-les-fonctions",
    "title": "4  Bonne pratique de Python",
    "section": "4.5 Documenter les fonctions",
    "text": "4.5 Documenter les fonctions\nLa documentation d’une fonction s’appelle le docstring. Elle prend la forme suivante:\ndef square_and_rooter(x):\n    \"\"\"Return the square root of self times self.\"\"\"\n    ...\nAvec PyCharm, lorsqu’on utilise trois guillemets sous la définition d’une fonction, un template minimal à\ncompleter est automatiquement généré. Les normes à suivre pour que la docstrings soit reconnue par le package\nsphinx sont présentées dans la PEP257. Néanmoins,\nelles ont été enrichies par le style de docstrings NumPy qui est plus riche et permet ainsi des documentations\nplus explicites\n(voir ici et\nici).\nSuivre ces canons formels permet une lecture simplifiée du code source de la documentation. Mais cela a surtout\nl’avantage, lors de la génération d’un package, de permettre une mise en forme automatique des fichiers\nhelp d’une fonction à partir de la docstrings. L’outil canonique pour ce type de construction automatique est\nsphinx (dont l’équivalent R est Roxygen)"
  },
  {
    "objectID": "content/course/getting-started/04_python_practice/index.html#les-tests",
    "href": "content/course/getting-started/04_python_practice/index.html#les-tests",
    "title": "4  Bonne pratique de Python",
    "section": "4.6 Les tests",
    "text": "4.6 Les tests\nTester ses fonctions peut apparaître formaliste mais c’est, en fait, souvent d’un grand secours car cela permet de\ndétecter et corriger des bugs précoces (ou au moins d’être conscient de leur existence).\nAu-delà de la correction de bug, cela permet de vérifier que\nla fonction produit bien un résultat espéré dans une expérience contrôlée.\nEn fait, il existe deux types de tests:\n\ntests unitaires: on teste seulement une fonctionalité ou propriété\ntests d’intégration: on teste l’intégration de la fonction dans un ensemble plus large de fonctionalité\n\nIci, on va plutôt se focaliser sur la notion de test unitaire ; la notion de\ntest d’intégration nécessitant d’avoir une chaîne plus complète de fonctions (mais il ne faut\npas la négliger)\nOn peut partir du principe suivant:\n\ntoute fonctionnalité non testée comporte un bug\n\nLe fichier tests/context.py sert à définir le contexte dans lequel le test de la fonction s’exécute, de manière\nisolée. On peut adopter le modèle suivant, en changeant import monmodule par le nom de module adéquat\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nimport monmodule\nChaque fichier du dossier de test\n(par exemple test_basic.py et test_advanced.py) incorpore ensuite la ligne suivante,\nen début de script\nfrom .context import sample\nPour automatiser les tests, on peut utiliser le package unittest\n(doc ici). L’idée est que dans un cadre contrôlé\n(on connaît l’input et en tant que concepteur de la fonction on connaît l’output ou, a minima\nles propriétés de l’output) on peut tester la sortie d’une fonction.\nLa structure canonique de test est la suivante3\nimport unittest\n\ndef fun(x):\n    return x + 1\n\nclass MyTest(unittest.TestCase):\n    def test(self):\n        self.assertEqual(fun(3), 4)\n4 Le code équivalent avec R serait testthat::expect_equal(fun(3),4)\nParler de codecov"
  },
  {
    "objectID": "content/course/getting-started/04_python_practice/index.html#partager",
    "href": "content/course/getting-started/04_python_practice/index.html#partager",
    "title": "4  Bonne pratique de Python",
    "section": "4.7 Partager",
    "text": "4.7 Partager\nCe point est ici évoqué en dernier mais, en fait, il est essentiel et mérite d’être une réflexion prioritaire.\nTout travail n’a pas vocation à être public\nou à dépasser le cadre d’une équipe. Cependant, les mêmes exigences qui s’appliquent lorsqu’un code est public méritent\nde s’appliquer avec un projet personnel. Avant de partager un code avec d’autres, on le partage avec le “futur moi”.\nReprendre un code écrit il y a plusieurs semaines est coûteux et mérite d’anticiper en adoptant des bonnes pratiques qui\nrendront quasi-indolore la ré-appropriation du code.\nL’intégration d’un projet avec git fiabilise grandement le processus d’écriture du code mais aussi, grâce aux\noutils d’intégration continue, la production de contenu (par exemple des visualisations html ou des rapports\nfinaux écrits avec markdown). Il est recommandé d’immédiatement connecter un projet à git, même avec un\ndépôt qui aura vocation à être personnel. Les instructions d’utilisation de git sont détaillées ici."
  },
  {
    "objectID": "content/course/getting-started/05_rappels_types/index.html#les-quelques-règles-de-python",
    "href": "content/course/getting-started/05_rappels_types/index.html#les-quelques-règles-de-python",
    "title": "5  Quelques rappels sur les principes de base de Python",
    "section": "5.1 Les quelques règles de Python",
    "text": "5.1 Les quelques règles de Python\nPython est un peu susceptible et protocolaire, plus formaliste que ne l’est R.\nIl y a ainsi quelques règles à respecter :\nRègle 1: L’indentation est primordiale : un code mal indenté provoque une erreur.\nL’indentation indique à l’interpréteur où se trouvent les\nséparations entre des blocs d’instructions. Un peu comme des points dans un\ntexte.\nSi les lignes ne sont pas bien alignées, l’interpréteur ne sait plus à quel\nbloc associer la ligne. Par exemple, le corps d’une fonction doit être indenté\nd’un niveau ; les éléments dans une clause logique (if, else, etc.) également\nRègle 2: On commence à compter à 0, comme dans beaucoup de langages\n(C++, java…). Python diffère dans ce domaine de R où on commence\nà compter à 1.\nLe premier élément d’une liste est ainsi, en Python, le 0-ème.\nRègle 3: Comme dans une langue naturelle, les marques de\nponctuation sont importantes :\n\nPour une liste : []\nPour un dictionnaire : {}\nPour un tuple : ()\nPour séparer des éléments : ,\nPour commenter un bout de code : #\nPour aller à la ligne dans un bloc d’instructions : \\\nLes majuscules et minuscules sont importantes\nPar contre l’usage des ' ou des \" est indifférent.\nIl faut juste avoir les mêmes début et fin.\nPour documenter une fonction ou une classe ““” mon texte de documentation “““"
  },
  {
    "objectID": "content/course/getting-started/05_rappels_types/index.html#les-outputs-de-python-lopération-le-print-et-le-return",
    "href": "content/course/getting-started/05_rappels_types/index.html#les-outputs-de-python-lopération-le-print-et-le-return",
    "title": "5  Quelques rappels sur les principes de base de Python",
    "section": "5.2 Les outputs de Python : l’opération, le print et le return",
    "text": "5.2 Les outputs de Python : l’opération, le print et le return\nQuand Python réalise des opérations, il faut lui préciser ce qu’il doit en faire :\n\nest-ce qu’il doit juste faire l’opération,\nafficher le résultat de l’opération,\ncréer un objet avec le résultat de l’opération ?\n\nContrairement à R, par défaut, Python ne renvoie pas le résultat de la\ndernière opération effectuée.\n{{% box status=“note” title=“Remarque” icon=“fas fa-pencil-alt” %}}\nDans l’environnement Jupyter Notebook, le dernier élement d’une cellule\nest automatiquement affiché (print), qu’on lui demande ou non de le faire.\nCe comportement est particulièrement pratique pour afficher des figures\ngénérées via matplotlib ou seaborn.\nCe comportement\nn’est pas le cas dans un éditeur classique comme VisualStudio,\nSpyder ou PyCharm. Pour afficher un résultat dans la console,\nil faut utiliser\nprint ou la commande consacrée (par exemple plt.show()\npour afficher la dernière figure générée par matplotlib)\n{{% /box %}}\n\n5.2.1 Le print\n\n# on calcule : dans le cas d'une opération par exemple une somme\n2+3 # Python calcule le résultat mais n'affiche rien dans la sortie\n\n# le print : on affiche\n\nprint(2+3) # Python calcule et on lui demande juste de l'afficher\n# le résultat est en dessous du code\n\n5\n\n\n\n# le print dans une fonction \n\ndef addition_v1(a,b) : \n    print(a+b)\n\nresultat_print = addition_v1(2,0) \nprint(type(resultat_print))\n\n# dans la sortie on a l'affichage du résultat, car la sortie de la fonction est un print \n# en plus on lui demande quel est le type du résultat. Un print ne renvoie aucun type, ce n'est ni un numérique,\n# ni une chaine de charactères, le résultat d'un print n'est pas un format utilisable\n\n2\n&lt;class 'NoneType'&gt;\n\n\nLe résultat de l’addition est affiché\ncar la fonction addition_v1 effectue un print\nPar contre, l’objet créé n’a pas de type, il n’est pas un chiffre,\nce n’est qu’un affichage.\n\n\n5.2.2 Le return\nPour créer un objet avec le résultat de la fonction, il faut utiliser return\n\n# le return dans une fonction\ndef addition_v2(a,b) : \n    return a+b\n\nresultat_return = addition_v2(2,5) # \nprint(type(resultat_return))\n## là on a bien un résultat qui est du type \"entier\"\n\n&lt;class 'int'&gt;\n\n\nLe résultat de addition_v2 n’est pas affiché comme dans addition_v1\nPar contre, la fonction addition_v2 permet d’avoir un objet de type int,\nun entier donc."
  },
  {
    "objectID": "content/course/getting-started/05_rappels_types/index.html#les-types-de-base-variables-listes-dictionnaires",
    "href": "content/course/getting-started/05_rappels_types/index.html#les-types-de-base-variables-listes-dictionnaires",
    "title": "5  Quelques rappels sur les principes de base de Python",
    "section": "5.3 Les types de base : variables, listes, dictionnaires …",
    "text": "5.3 Les types de base : variables, listes, dictionnaires …\nPython permet de manipuler différents types de base. Nous en\nverrons des extensions dans la suite du cours (np.array par exemple)\nqui, d’une manière ou d’une autre, s’appuient sur ces types de base.\nOn distingue deux types de variables : les immuables (immutables)\nqui ne peuvent être\nmodifiés et les modifiables (mutables)\n\n5.3.1 Les variables immuables\nLes variables immuables ne peuvent être modifiées\n\nNone : ce type est une convention de programmation pour dire que la valeur n’est pas calculée\nbool : un booléen\nint : un entier\nfloat : un réel\nstr : une chaine de caractères\ntuple : un vecteur\n\n\ni = 3         # entier = type numérique (type int)\nr = 3.3       # réel   = type numérique (type float)\ns = \"exemple\" # chaîne de caractères = type str \nn = None      # None signifie que la variable existe mais qu'elle ne contient rien\n              # elle est souvent utilisée pour signifier qu'il n'y a pas de résultat\na = (1,2)     # tuple\n\nprint(i,r,s,n,a)         \n\n3 3.3 exemple None (1, 2)\n\n\nSi on essaie de changer le premier élément de la chaine de caractères s on va avoir un peu de mal.\nPar exemple si on voulait mettre une majuscule à “exemple”,\non aurait envie d’écrire que le premier élément de la chaine s est “E” majuscule\nMais Python ne va pas nous laisser faire, il nous dit que les objets “chaine de caractère” ne peuvent être modifiés\n\ns[0] = \"E\"  # déclenche une exception\n\nTypeError: 'str' object does not support item assignment\n\n\nTout ce qu’on peut faire avec une variable immuable,\nc’est la réaffecter à une autre valeur : elle ne peut pas être modifiée.\nPour s’en convaincre, utilisons la fonction id() qui donne un identifiant à chaque objet.\n\nprint(s)\nid(s)\n\nexemple\n\n\n140589188114416\n\n\n\ns = \"autre_mot\"\nid(s)\n\n140588001360048\n\n\nOn voit bien que s a changé d’identifiant : il peut avoir le même nom, ce n’est plus le même objet\n\n\n5.3.2 Les types modifiable : listes et dictionnaires\nHeureusement, il existe des variables modifiables comme les listes et les dictionnaires.\n\n5.3.2.1 Les listes - elles s’écrivent entre [ ]\nLes listes sont des élements très utiles, notamment quand vous souhaitez faire des boucles.\nPour faire appel aux élements d’une liste, on donne leur position dans la liste : le 1er est le 0, le 2ème est le 1 …\n\nma_liste = [1,2,3,4]\n\n\nprint(\"La longueur de ma liste est de\", len(ma_liste))\nprint(\"Le premier élément de ma liste est :\", ma_liste[0])\nprint(\"Le dernier élément de ma liste est :\", ma_liste[3])\nprint(\"Le dernier élément de ma liste est :\", ma_liste[-1])\n\nLa longueur de ma liste est de 4\nLe premier élément de ma liste est : 1\nLe dernier élément de ma liste est : 4\nLe dernier élément de ma liste est : 4\n\n\n{{% box status=“exercise” title=“Exercice 1” icon=“fas fa-pencil-alt” %}}\nPour effectuer des boucles sur les listes, la méthode la plus lisible\nest d’utiliser les list comprehension. Cette approche consiste\nà itérer les éléments d’une liste à la volée.\nPar exemple, si on reprend cet exemple,\nun code qui repose sur les list comprehension sera le suivant:\n\nfruits = [\"apple\", \"banana\", \"cherry\", \"kiwi\", \"mango\"]\nnewlist = [x for x in fruits if \"a\" in x]\nprint(newlist)\n\n['apple', 'banana', 'mango']\n\n\nLe même code, ne reposant pas sur les compréhensions de liste, sera beaucoup\nmoins concis et ainsi inutilement verbeux:\n\nfruits = [\"apple\", \"banana\", \"cherry\", \"kiwi\", \"mango\"]\nnewlist = []\n\nfor x in fruits:\n  if \"a\" in x:\n    newlist.append(x)\n\nprint(newlist) \n\n['apple', 'banana', 'mango']\n\n\n{{% /box %}}\n\n\n5.3.2.2 Les dictionnaires - ils s’écrivent entre accolades {}\nUn dictionnaire associe à une clé un autre élément, appelé une valeur : un chiffre, un nom, une liste, un autre dictionnaire etc.\nLe format d’un dictionnaire est le suivant: {Clé : valeur}. Il s’agit\nd’un objet très pratique pour la recherche, beaucoup plus que les listes\nqui ne permettent pas de stocker de l’information diverse de manière\nhiérarchisée.\n\n\n5.3.2.3 Dictionnaire avec des valeurs int\nOn peut par exemple associer à un nom, un nombre\n\nmon_dictionnaire_notes = { 'Nicolas' : 18 , 'Pimprenelle' : 15} \n# un dictionnaire qui à chaque nom associe un nombre\n# à Nicolas, on associe 18\n\nprint(mon_dictionnaire_notes) \n\n{'Nicolas': 18, 'Pimprenelle': 15}\n\n\n\n\n5.3.2.4 Dictionnaire avec des valeurs qui sont des listes\nPour chaque clé d’un dictionnaire, il ne faut pas forcément garder la même forme de valeur\nDans l’exemple, la valeur de la clé “Nicolas” est une liste, alors que celle de “Philou” est une liste de liste\n\nmon_dictionnaire_loisirs =  \\\n{ 'Nicolas' : ['Rugby','Pastis','Belote'] , \n  'Pimprenelle' : ['Gin Rami','Tisane','Tara Jarmon','Barcelone','Mickey Mouse'],\n  'Philou' : [['Maths','Jeux'],['Guillaume','Jeanne','Thimothée','Adrien']]}\n\nPour accéder à un élément du dictionnaire, on fait appel à la clé et non plus à la position, comme c’était le cas dans les listes.\nC’est beaucoup plus pratique pour rechercher de l’information:\n\nprint(mon_dictionnaire_loisirs['Nicolas']) # on affiche une liste\n\n['Rugby', 'Pastis', 'Belote']\n\n\n\nprint(mon_dictionnaire_loisirs['Philou']) # on affiche une liste de listes\n\n[['Maths', 'Jeux'], ['Guillaume', 'Jeanne', 'Thimothée', 'Adrien']]\n\n\nSi on ne veut avoir que la première liste des loisirs de Philou, on demande le premier élément de la liste\n\nprint(mon_dictionnaire_loisirs['Philou'][0]) # on affiche alors juste la première liste\n\n['Maths', 'Jeux']\n\n\nOn peut aussi avoir des valeurs qui sont des int et des list\n\nmon_dictionnaire_patchwork_good = \\\n{ 'Nicolas' : ['Rugby','Pastis','Belote'] ,\n  'Pimprenelle' : 18 }\n\n\n\n\n5.3.3 A retenir\n\nL’indentation du code est importante (4 espaces et pas une tabulation)\nUne liste est entre [] et on peut appeler les positions par leur place\nUn dictionnaire, clé x valeur, s’écrit entre {} et on appelle un élément en fonction de la clé"
  },
  {
    "objectID": "content/course/getting-started/05_rappels_types/index.html#questions-pratiques",
    "href": "content/course/getting-started/05_rappels_types/index.html#questions-pratiques",
    "title": "5  Quelques rappels sur les principes de base de Python",
    "section": "5.4 Questions pratiques :",
    "text": "5.4 Questions pratiques :\n{{% box status=“exercise” title=“Exercice 1” icon=“fas fa-pencil-alt” %}}\n\nQuelle est la position de 7 dans la liste suivante\n\n\nliste_nombres = [1,2,7,5,3]\n\n\nCombien de clés a ce dictionnaire ?\n\n\ndictionnaire_evangile = {\"Marc\" : \"Lion\", \"Matthieu\" : [\"Ange\",\"Homme ailé\"] , \n                          \"Jean\" : \"Aigle\" , \"Luc\" : \"Taureau\"}\n\n\nQue faut-il écrire pour obtenir “Ange” en résultat à partir du dictionnaire_evangile ?\n\n{{% /box %}}"
  },
  {
    "objectID": "content/course/getting-started/05_rappels_types/index.html#effectuer-des-opérations-sur-les-objets-de-base-python",
    "href": "content/course/getting-started/05_rappels_types/index.html#effectuer-des-opérations-sur-les-objets-de-base-python",
    "title": "5  Quelques rappels sur les principes de base de Python",
    "section": "5.5 Effectuer des opérations sur les objets de base Python",
    "text": "5.5 Effectuer des opérations sur les objets de base Python\nMaintenant qu’on a vu quels objets existent en Python,\nnous allons\nvoir comment nous en servir.\nPour comprendre comment modifier un objet, il convient\nde distinguer deux concepts, développés plus amplement\ndans le chapitre dédié: les attributs et les méthodes:\n\nLes attributs décrivent la structure interne d’un objet. Par exemple,\nla taille d’un objet, sa langue, etc.\nNous n’allons pas trop développer ce concept ici. Le chapitre dédié au sujet\npermettra de plus développer ce concept.\nLes méthodes correspondent à des actions qui s’appliqueront à l’objet et s’adaptent à sa structure.\nLa même méthode (par exemple append) fonctionnera ainsi de manière différente selon le type d’objet.\n\n\n5.5.1 Premiers exemples de méthodes\nAvec les éléments définis dans la partie 1\n(les listes, les dictionnaires) on peut faire appel à des méthodes qui sont directement liées à ces objets.\n\n5.5.1.1 Une méthode pour les listes\nPour ajouter un élément (item) dans une liste : on va utiliser la méthode .append()\n\nma_liste = [\"Nicolas\",\"Michel\",\"Bernard\"]\n\nma_liste.append(\"Philippe\")\n\nprint(ma_liste)\n\n['Nicolas', 'Michel', 'Bernard', 'Philippe']\n\n\n\n\n5.5.1.2 Une méthode pour les dictionnaires\nPour connaitre l’ensemble des clés d’un dictionnaire, on appelle la méthode .keys()\n\nmon_dictionnaire = {\"Marc\" : \"Lion\", \"Matthieu\" : [\"Ange\",\"Homme ailé\"] , \n                          \"Jean\" : \"Aigle\" , \"Luc\" : \"Taureau\"}\n\nprint(mon_dictionnaire.keys())\n\ndict_keys(['Marc', 'Matthieu', 'Jean', 'Luc'])\n\n\n\n\n\n5.5.2 Connaitre les méthodes d’un objet\nPour savoir quelles sont les méthodes d’un objet vous pouvez :\n\ntaper help(mon_objet) ou mon_objet? dans la console Python\ntaper mon_objet. + touche tabulation dans la console Python ou dans le Notebook.\nPython permet la complétion, c’est-à-dire que vous pouvez faire appaître la liste\ndes méthodes possibles."
  },
  {
    "objectID": "content/course/getting-started/05_rappels_types/index.html#les-opérations-et-méthodes-classiques-des-listes",
    "href": "content/course/getting-started/05_rappels_types/index.html#les-opérations-et-méthodes-classiques-des-listes",
    "title": "5  Quelques rappels sur les principes de base de Python",
    "section": "5.6 Les opérations et méthodes classiques des listes",
    "text": "5.6 Les opérations et méthodes classiques des listes\n\n5.6.1 Créer une liste\nPour créer un objet de la classe list, il suffit de le déclarer. Ici on affecte à x une liste\n\nx = [4, 5] # création d’une liste composée de deux entiers\nx = [\"un\", 1, \"deux\", 2] # création d’une liste composée de 2 chaînes de caractères\n# et de deux entiers, l’ordre d’écriture est important\nx = [3] # création d’une liste d’un élément, sans la virgule,\nx = [ ] # crée une liste vide\nx = list () # crée une liste vide\n\n\n\n5.6.2 Un premier test sur les listes\nSi on veut tester la présence d’un élément dans une liste, on l’écrit de la manière suivante :\n\n# Exemple \n\nx = \"Marcel\"\n\nl = [\"Marcel\",\"Edith\",\"Maurice\",\"Jean\"]\n\nprint(x in l)\n\n#vrai si x est un des éléments de l\n\nTrue\n\n\n\n\n5.6.3 +: une méthode pour concaténer deux listes\nOn utilise le symbole +\n\nt = [\"Antoine\",\"David\"]\nprint(l + t) #concaténation de l et t\n\n['Marcel', 'Edith', 'Maurice', 'Jean', 'Antoine', 'David']\n\n\n\n\n5.6.4 Pour trouver certains éléments d’une liste\nPour chercher des élements dans une liste, on utilise la position dans la liste.\n\nl[1] # donne l'élément qui est en 2ème position de la liste\n\n'Edith'\n\n\n\nl[1:3] # donne les éléments de la 2ème position de la liste à la 4ème exclue\n\n['Edith', 'Maurice']\n\n\n\n\n5.6.5 Quelques fonctions des listes\nLes listes embarquent ainsi nativement un certain nombre de méthodes\nqui sont pratiques. Cependant, pour avoir certaines informations\nsur une liste, il faut parfois plutôt passer par\ndes fonctions natives comme les suivantes:\n\nlongueur = len(l) # nombre d’éléments de l\nminimum = min(l) # plus petit élément de l, ici par ordre alphabétique\nmaximum = max(l) # plus grand élément de l, ici par ordre alphabétique\nprint(longueur,minimum,maximum)\n\n4 Edith Maurice\n\n\n\ndel l[0 : 2] # supprime les éléments entre la position 0 et 2 exclue\nprint(l)\n\n['Maurice', 'Jean']\n\n\n\n\n5.6.6 Les méthodes des listes\nOn les trouve dans l’aide de la liste.\nOn distingue les méthodes et les méthodes spéciales : visuellement,\nles méthodes spéciales sont celles qui précédées et suivis de deux caractères de soulignement,\nles autres sont des méthodes classiques.\n\nhelp(l)\n\nHelp on list object:\n\nclass list(object)\n |  list(iterable=(), /)\n |  \n |  Built-in mutable sequence.\n |  \n |  If no argument is given, the constructor creates a new empty list.\n |  The argument must be an iterable if specified.\n |  \n |  Methods defined here:\n |  \n |  __add__(self, value, /)\n |      Return self+value.\n |  \n |  __contains__(self, key, /)\n |      Return key in self.\n |  \n |  __delitem__(self, key, /)\n |      Delete self[key].\n |  \n |  __eq__(self, value, /)\n |      Return self==value.\n |  \n |  __ge__(self, value, /)\n |      Return self&gt;=value.\n |  \n |  __getattribute__(self, name, /)\n |      Return getattr(self, name).\n |  \n |  __getitem__(...)\n |      x.__getitem__(y) &lt;==&gt; x[y]\n |  \n |  __gt__(self, value, /)\n |      Return self&gt;value.\n |  \n |  __iadd__(self, value, /)\n |      Implement self+=value.\n |  \n |  __imul__(self, value, /)\n |      Implement self*=value.\n |  \n |  __init__(self, /, *args, **kwargs)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __iter__(self, /)\n |      Implement iter(self).\n |  \n |  __le__(self, value, /)\n |      Return self&lt;=value.\n |  \n |  __len__(self, /)\n |      Return len(self).\n |  \n |  __lt__(self, value, /)\n |      Return self&lt;value.\n |  \n |  __mul__(self, value, /)\n |      Return self*value.\n |  \n |  __ne__(self, value, /)\n |      Return self!=value.\n |  \n |  __repr__(self, /)\n |      Return repr(self).\n |  \n |  __reversed__(self, /)\n |      Return a reverse iterator over the list.\n |  \n |  __rmul__(self, value, /)\n |      Return value*self.\n |  \n |  __setitem__(self, key, value, /)\n |      Set self[key] to value.\n |  \n |  __sizeof__(self, /)\n |      Return the size of the list in memory, in bytes.\n |  \n |  append(self, object, /)\n |      Append object to the end of the list.\n |  \n |  clear(self, /)\n |      Remove all items from list.\n |  \n |  copy(self, /)\n |      Return a shallow copy of the list.\n |  \n |  count(self, value, /)\n |      Return number of occurrences of value.\n |  \n |  extend(self, iterable, /)\n |      Extend list by appending elements from the iterable.\n |  \n |  index(self, value, start=0, stop=9223372036854775807, /)\n |      Return first index of value.\n |      \n |      Raises ValueError if the value is not present.\n |  \n |  insert(self, index, object, /)\n |      Insert object before index.\n |  \n |  pop(self, index=-1, /)\n |      Remove and return item at index (default last).\n |      \n |      Raises IndexError if list is empty or index is out of range.\n |  \n |  remove(self, value, /)\n |      Remove first occurrence of value.\n |      \n |      Raises ValueError if the value is not present.\n |  \n |  reverse(self, /)\n |      Reverse *IN PLACE*.\n |  \n |  sort(self, /, *, key=None, reverse=False)\n |      Sort the list in ascending order and return None.\n |      \n |      The sort is in-place (i.e. the list itself is modified) and stable (i.e. the\n |      order of two equal elements is maintained).\n |      \n |      If a key function is given, apply it once to each list item and sort them,\n |      ascending or descending, according to their function values.\n |      \n |      The reverse flag can be set to sort in descending order.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods defined here:\n |  \n |  __class_getitem__(...) from builtins.type\n |      See PEP 585\n |  \n |  ----------------------------------------------------------------------\n |  Static methods defined here:\n |  \n |  __new__(*args, **kwargs) from builtins.type\n |      Create and return a new object.  See help(type) for accurate signature.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __hash__ = None\n\n\n\n\n\n5.6.7 A retenir et questions\nA retenir :\n\nChaque objet Python a des attributs et des méthodes\nVous pouvez créer des classes avec des attributs et des méthodes\nLes méthodes des listes et des dictionnaires qui sont les plus utilisées :\n\nlist.count()\nlist.sort()\nlist.append()\ndict.keys()\ndict.items()\ndict.values()\n\n\n{{% box status=“exercise” title=“Exercice 2” icon=“fas fa-pencil-alt” %}}\n\nDéfinir la liste allant de 1 à 10, puis effectuez les actions suivantes :\n\n\ntriez et affichez la liste\najoutez l’élément 11 à la liste et affichez la liste\nrenversez et affichez la liste\naffichez l’élément d’indice 7\nenlevez l’élément 9 et affichez la liste\naffichez la sous-liste du 2e au 3e éléments inclus ;\naffichez la sous-liste du début au 2e élément inclus ;\naffichez la sous-liste du 3e élément à la fin de la liste ;\n\n\nConstruire le dictionnaire des 6 premiers mois de l’année avec comme valeurs le nombre de jours respectif.\n\n\nRenvoyer la liste des mois\nRenvoyer la liste des jours\nAjoutez la clé du mois de Juillet\n\n{{% /box %}}"
  },
  {
    "objectID": "content/course/getting-started/05_rappels_types/index.html#passer-des-listes-dictionnaires-à-pandas",
    "href": "content/course/getting-started/05_rappels_types/index.html#passer-des-listes-dictionnaires-à-pandas",
    "title": "5  Quelques rappels sur les principes de base de Python",
    "section": "5.7 Passer des listes, dictionnaires à pandas",
    "text": "5.7 Passer des listes, dictionnaires à pandas\nSupposons que la variable ‘data’ est une liste qui contient nos données.\nUne observation correspond à un dictionnaire qui contient le nom, le type, l’ambiance et la note d’un restaurant.\nIl est aisé de transformer cette liste en dataframe grâce à la fonction ‘DataFrame’.\n\nimport pandas \n\ndata = [{\"nom\": \"Little Pub\", \"type\" : \"Bar\", \"ambiance\": 9, \"note\": 7},\n     {\"nom\": \"Le Corse\", \"type\" : \"Sandwicherie\", \"ambiance\": 2, \"note\": 8},\n     {\"nom\": \"Café Caumartin\", \"type\" : \"Bar\", \"ambiance\": 1}]\n\ndf = pandas.DataFrame(data)\n\nprint(data)\ndf\n\n[{'nom': 'Little Pub', 'type': 'Bar', 'ambiance': 9, 'note': 7}, {'nom': 'Le Corse', 'type': 'Sandwicherie', 'ambiance': 2, 'note': 8}, {'nom': 'Café Caumartin', 'type': 'Bar', 'ambiance': 1}]\n\n\n\n\n\n\n\n\n\nnom\ntype\nambiance\nnote\n\n\n\n\n0\nLittle Pub\nBar\n9\n7.0\n\n\n1\nLe Corse\nSandwicherie\n2\n8.0\n\n\n2\nCafé Caumartin\nBar\n1\nNaN"
  },
  {
    "objectID": "content/course/getting-started/06_rappels_fonctions/index.html#utilisation-dun-module-installé",
    "href": "content/course/getting-started/06_rappels_fonctions/index.html#utilisation-dun-module-installé",
    "title": "6  Modules, tests, boucles, fonctions",
    "section": "6.1 Utilisation d’un module installé",
    "text": "6.1 Utilisation d’un module installé\nPython, comme R, sont des langages\nconstruits sur le principe de briques.\nCes briques sont ce qu’on appelle des packages.\nAu contraire de Stata mais comme pour R,\nil\nfaut toujours préciser les packages que vous utilisez au début du code,\nsinon Python ne reconnait pas les fonctions appelées.\n\n6.1.1 Import module\nOn charge un module grâce à la commande import.\nPour chaque code que vous exécutez,\nil faut charger les modules en introduction.\nUne fois qu’on a chargé le module,\non peut faire appel aux commandes qui en dépendent en les appelant\naprès avoir tapé le nom du module.\nSi vous ne précisez pas le nom du module avant celui de la fonction,\nil ne la trouvera pas forcément.\nVoici un exemple avec le module numpy\nqui est très courant et permet de faire des\ncalculs matriciels sous Python.\n\nimport numpy\nprint(numpy.arange(5))\n\n[0 1 2 3 4]\n\n\n\n\n6.1.2 Import module as md - donner un nom au module\nOn peut aussi donner un pseudonyme au module pour\néviter de taper un nom trop long à chaque fois\nqu’on utilise une fonction.\nClassiquement le nom raccourci de numpy est np,\ncelui de pandas est pd.\n\nimport pandas as pd\nimport numpy as np\nsmall_array = np.array([[1, 2], [3, 4]])\ndata = pd.DataFrame(small_array)\ndata.head()\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n1\n2\n\n\n1\n3\n4\n\n\n\n\n\n\n\n\n\n6.1.3 from Module Import fonction - seulement une partie du module\nSi on ne veut pas être obligé de donner\nle nom du module avant d’appeler\nla fonction,\nil y a toujours la possibilité de n’importer qu’une fonction du module.\nDans le cas de l’exemple, Python sait que la fonction arrange est celle de numpy.\nMais attention : si deux fonctions de modules différents\nont le même nom,\nc’est toujours la dernière importée qui gagne.\nOn voit souvent from _module_ import *.\nC’est-à-dire qu’on importe toutes\nles fonctions du module\nmais on n’a pas besoin de spécifier le nom du module avant les méthodes.\n{{% box status=“warning” title=“Warning” icon=“fa fa-exclamation-triangle” %}}\nLa méthode from _module_ import * n’est pas recommandée car elle rend le code moins intelligible.\nEn effet, d’où vient la fonction floor ? De maths ou de numpy ?\nElle risque\naussi de créer des conflits de fonction, qui malgré un nom commun peuvent ne\npas attendre les mêmes arguments ou objets.\n{{% /box %}}\n\nfrom numpy import array\nprint(array(5))\n\n5"
  },
  {
    "objectID": "content/course/getting-started/06_rappels_fonctions/index.html#les-tests",
    "href": "content/course/getting-started/06_rappels_fonctions/index.html#les-tests",
    "title": "6  Modules, tests, boucles, fonctions",
    "section": "6.2 Les tests",
    "text": "6.2 Les tests\nLes tests permettent d’exécuter telle ou telle instruction\nselon la valeur d’une condition.\nPour faire un test avec un bloc d’instructions, il faut toujours :\n\nque l’expression à vérifier soit suivie de :\nque le bloc d’instruction soit forcément indenté\n\n\n6.2.1 Test avec contrepartie : if et else\nComme dans les autres langages,\non teste une condition. Si elle est vérifiée,\nalors une instruction suit et sinon, une autre instruction est exécutée.\nIl est conseillé de toujours indiquer une contrepartie afin d’éviter les surprises.\n\n6.2.1.1 Test d’une égalité ou inégalité\n\nx = 6\n\nif x &gt; 5 :\n    print(\"x est plus grand que 5\")\nelse : # la contrepartie si la condition if n'est pas réalisée\n    print(\"x est plus petit que 5\")\n\nx est plus grand que 5\n\n\n\n\n6.2.1.2 Test dans un intervalle\n\n# on peut avoir des intervalles directement\nx = 6\nif 5 &lt; x &lt; 10 : \n    print(\"x est entre 5 et 10\")\nelse : \n    print(\"x est plus grand que 10\")\n\nx est entre 5 et 10\n\n\n\n# tester plusieurs valeurs avec l'opérateur logique \"or\"\nx = 5\nif x == 5 or x == 10 : \n    print(\"x vaut 5 ou 10\")    \nelse : \n    print(\"x est différent de 5 et 10\")\n\nx vaut 5 ou 10\n\n\n\n\n\n6.2.2 Tests successifs : if, elif et else\nAvec if et elif,\ndès qu’on rencontre une condition qui est réalisée,\non n’en cherche pas d’autres potentiellement vérifiées.\nPlusieurs if à la suite peuvent quant à eux être vérifiés.\nSuivant ce que vous souhaitez faire, les opérateurs ne sont pas substituables.\nNotez la différence entre ces deux bouts de code :\n\n#code 1\nx = 5\n\nif x != 10 : \n    print(\"x ne vaut pas 10\")   \nelif x &gt;= 5 : \n    print(\"x est égal ou supérieur à 5\")\n\nx ne vaut pas 10\n\n\nDans le cas de elif, on s’arrête à la première condition vérifiée et dans le cas suivant, on continue à chaque condition vérifiée\n\n#code 2\nx = 5\n\nif x != 10 : \n    print(\"x ne vaut pas 10\")   \nif x &gt;= 5 : \n    print(\"x est égal ou supérieur à 5\")\n\nx ne vaut pas 10\nx est égal ou supérieur à 5"
  },
  {
    "objectID": "content/course/getting-started/06_rappels_fonctions/index.html#boucles",
    "href": "content/course/getting-started/06_rappels_fonctions/index.html#boucles",
    "title": "6  Modules, tests, boucles, fonctions",
    "section": "6.3 Boucles",
    "text": "6.3 Boucles\nIl existe deux types de boucles : les boucles for et les boucles while\nLa boucle for parcourt un ensemble, tandis que la boucle while\ncontinue tant qu’une condition est vraie.\n\n6.3.1 Boucle for\n\n6.3.1.1 Parcourir une liste croissantes d’entiers\n\n# parcourt les entiers de 0 à n-1 inclus\nfor i in range(0,3) : \n    print(i)\n\n0\n1\n2\n\n\n\n\n6.3.1.2 Parcourir une liste décroissante d’entiers\n\n# parcourt les entiers de 3 à n+1 inclus\nfor i in range(3,0,-1) : \n    print(i)\n\n3\n2\n1\n\n\n\n\n6.3.1.3 Parcourir une liste de chaines de caractères\nOn va faire une boucle sur les éléments d’une liste\n\n\n6.3.1.4 Boucle sur les éléments d’une liste\n\nliste_elements = ['Nicolas','Romain','Florimond']\n\n# pour avoir l'ensemble des éléments de la liste\nfor item in liste_elements : \n    print(item)\n\nNicolas\nRomain\nFlorimond\n\n\n\n\n6.3.1.5 Boucle sur les éléments d’une liste dans une autre liste\n\n# pour avoir la place des éléments de la première liste dans la seconde liste  \n\nliste_globale = ['Violette','Nicolas','Mathilde','Romain','Florimond','Helene'] \n\nfor item in liste_elements : \n    print(item,liste_globale.index(item))\n\nNicolas 1\nRomain 3\nFlorimond 4\n\n\n\n\n\n6.3.2 Bonus : les list comprehension\nAvec les listes, il existe aussi un moyen très élégant de condenser son code pour éviter de faire apparaitre des boucles sans arrêt. Comme les boucles doivent etre indentées, le code peut rapidement devenir illisible.\nGrace aux list comprehension, vous pouvez en une ligne faire ce qu’une boucle vous permettait de faire en 3 lignes.\nPar exemple, imaginez que vous vouliez faire la liste de toutes les lettres contenues dans un mot, avec un boucle vous devrez d’abord créer une liste vide, puis ajouter à cette liste toutes les lettres en question avec un .append()\n\nliste_lettres = []\n\nfor lettre in 'ENSAE':\n    liste_lettres.append(lettre)\n\nprint(liste_lettres)\n\n['E', 'N', 'S', 'A', 'E']\n\n\navec une list comprehension, on condense la syntaxe de la manière suivante :\n\nh_letters = [ letter for letter in 'ENSAE' ]\nprint(h_letters)\n\n['E', 'N', 'S', 'A', 'E']\n\n\nAvec une list comprehension\n[ expression for item in list if conditional ]\nest équivalent à\nfor item in list:\n    if conditional:\n        expression  \n\n6.3.2.1 Mise en application\nMettez sous forme de list comprehension le bout de code suivant\n\nsquares = []\n\nfor x in range(10):\n    squares.append(x**2)\nprint(squares)\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\n\n# Correction\nsquares = [x**2 for x in range(10)]\nsquares\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\n\n\n\n6.3.3 Boucle while\nLe bloc d’instruction d’une boucle while est exécuté tant que la condition est vérifiée.\nLe piège de ces boucles : la boucle while infinie ! Il faut toujours vérifier que votre boucle s’arrêtera un jour, il faut qu’à un moment ou à un autre, il y ait un élément qui s’incrémente ou qui soit modifié.\n\nx = 10\ny = 8\n# tant que y est plus petit que 10, je continue de lui ajouter 1\nwhile y &lt;= x : \n    print(\"y n'est pas encore plus grand que x\")\n    y += 1 # l'incrément\nelse : \n    print(\"y est plus grand que x et vaut\",y)\n\ny n'est pas encore plus grand que x\ny n'est pas encore plus grand que x\ny n'est pas encore plus grand que x\ny est plus grand que x et vaut 11\n\n\n\n\n6.3.4 Break et continue\nDans les boucles for ou while on peut avoir besoin d’ignorer ou de ne pas effectuer certaines itérations. 2 instructions utiles :\n\nl’instruction break : permet de sortir de la boucle\nl’instruction continue : permet de passer à l’itération suivante sans exécuter les instructions qui suivent\n\n\n# utilisation de break\nfor x in range(5) : \n    if x == 2 : \n        break\n    else :\n        print(x)\n\n0\n1\n\n\n\n# utilisation de continue\nfor x in range(5) : \n    if x == 2 : \n        continue\n    else :\n        print(x)\n\n0\n1\n3\n4"
  },
  {
    "objectID": "content/course/getting-started/06_rappels_fonctions/index.html#créer-ses-fonctions",
    "href": "content/course/getting-started/06_rappels_fonctions/index.html#créer-ses-fonctions",
    "title": "6  Modules, tests, boucles, fonctions",
    "section": "6.4 Créer ses fonctions",
    "text": "6.4 Créer ses fonctions\nLes fonctions permettent de faire la même chose sans avoir à recopier le code plusieurs fois dans le même programme. Dès que vous le pouvez, faites des fonctions : le copier-coller est trop dangereux.\n\nElles peuvent prendre plusieurs paramètres (ou aucun) elles peuvent retourner plusieurs résultats (ou aucun)\nPour mettre une aide à la fonction, on écrit au début entre ““” ““” (en rouge dans l’exemple)\n\n\n# 1er exemple de fonction\n\ndef ma_fonction_increment(parametre) : \n    \"\"\"Cette fonction ajoute 1 au paramètre qu'on lui donne\"\"\"\n    x = parametre + 1 \n    return x\n\n# pour documenter la fonction, on écrit son aide\nhelp(ma_fonction_increment)\n\nHelp on function ma_fonction_increment in module __main__:\n\nma_fonction_increment(parametre)\n    Cette fonction ajoute 1 au paramètre qu'on lui donne\n\n\n\nOn peut également :\n\navoir des paramètres facultatifs, mais ils doivent toujours être placés à la fin des paramètres\nen cas de paramètre facultatif, il faut lui donner une valeur par défaut\nretourner plus d’un élément à la fin d’une fonction\navoir des paramètres de tailles différentes\n\n\ndef ma_fonction(p,q = 2) :\n    y1 = p + q\n    y2 = y1%3 #reste de la division euclidienne\n    return y1,y2\n\nx = ma_fonction(11) \n# ici, on n'a pas de 2nd paramètre\n#, par défaut, x = ma_fonction(10,2)\nprint(\"x=\", x)\n\nz = ma_fonction(10,-1)\nprint(\"z =\",z)\n\nx= (13, 1)\nz = (9, 0)\n\n\nUne fonction peut également s’appeler elle même : c’est ce qu’on appelle une fonction récursive.\nDans cet exemple, somme_recursion() est une fonction que nous avons définie de sorte à ce qu’elle s’appelle elle-même (récursif).\nOn utilise l’argument k, qui décroit (-1) chaque fois qu’on fait appel à la fonction.\nLa récursion s’arrête quand k est nul.\nDans cet exemple, on va donc appeler 6 fois la fonction récursive.\n\ndef somme_recursion(k):\n    if(k &gt; 0):\n        result = k + somme_recursion(k - 1)\n        print(k,result)\n    else:\n        result = 0\n    return result\nsomme_recursion(6)\n\n1 1\n2 3\n3 6\n4 10\n5 15\n6 21\n\n\n21\n\n\nLes fonctions sont très utiles et nous vous invitons à les utiliser dès que vous le pouvez car elles permettent d’avoir un code clair et structuré, plutôt que des bouts de code éparpillés."
  },
  {
    "objectID": "content/course/getting-started/06_rappels_fonctions/index.html#lever-des-exceptions",
    "href": "content/course/getting-started/06_rappels_fonctions/index.html#lever-des-exceptions",
    "title": "6  Modules, tests, boucles, fonctions",
    "section": "6.5 Lever des exceptions",
    "text": "6.5 Lever des exceptions\nPython peut rencontrer des erreurs en exécutant votre programme.\nCes erreurs peuvent être interceptées très facilement et c’est même, dans certains cas, indispensable. Par exemple, si vous voulez faire une boucle mais que vous savez que l’instruction ne marchera pas toujours : au lieu de lister les cas où une opération n’est pas possible, on peut indiquer directement quelle erreur doit être ignorée.\nCependant, il ne faut pas tout intercepter non plus : si Python envoie une erreur, c’est qu’il y a une raison. Si vous ignorez une erreur, vous risquez d’avoir des résultats très étranges dans votre programme.\n\n# éviter une division par 0, c'est une bonne idée : \n\ndef inverse(x) :\n    '''Cette fonction renvoie l inverse de l argument'''\n    y = 1/x\n    return y\n\ndiv = inverse(0)\n\nZeroDivisionError: division by zero\n\n\nL’erreur est écrite noir sur blanc : ZeroDivisionError\nDans l’idéal on aimerait éviter que notre code bloque sur ce problème. On pourrait passer par un test if et vérifier que x est différent de 0. Mais on se rend vite compte que dans certains cas, on ne peut lister tous les tests en fonction de valeurs.\nAlors on va lui précisier ce qu’il doit faire en fonction de l’erreur retournée.\nSyntaxe\nTry :\ninstruction\nexcept TypeErreur :\nautre instruction\n\n\ndef inverse(x) : \n    try :\n        y = 1/x\n    except ZeroDivisionError :\n        y = None\n    return y\n    \nprint(inverse(10))\nprint(inverse(0))\n\n0.1\nNone\n\n\nIl est recommandé de toujours préciser le type d’erreur qu’on rencontre. Si on met uniquement “except” sans préciser le type, on peut passer à côté d’erreurs pour lesquelles la solution n’est pas universelle."
  },
  {
    "objectID": "content/course/getting-started/06_rappels_fonctions/index.html#a-retenir-et-questions",
    "href": "content/course/getting-started/06_rappels_fonctions/index.html#a-retenir-et-questions",
    "title": "6  Modules, tests, boucles, fonctions",
    "section": "6.6 A retenir et questions",
    "text": "6.6 A retenir et questions\n\n6.6.1 A retenir\n\nToujours mettre “:” avant un bloc d’instructions\nIndenter avant un bloc d’instructions (avec 4 espaces et non une tabulation !)\nIndiquer les modules nécessaires à l’exécution en début de code\nDocumenter les fonctions créées\nPréciser le type d’erreur pour les exceptions et potentiellement différencier les blocs d’instructions en fonction de l’erreur\n\n\n\n6.6.2 Questions\n\nQue fait ce programme ?\n\n\ndef inverse(x) : \n    try :\n        y = 1/x\n    except ZeroDivisionError :\n        y = None\n        return y\n\n\nEcrivez un programme qui peut trouver tous les nombres divisibles par 7 et non multiples de 5 entre 6523 et 8463 (inclus)\nEcrivez un programme qui prend une phrase en entrée et qui calcule le nombre de voyelles en Majuscules et de consonnes en minuscules :\n\nphrase = “Vous savez, moi je ne crois pas qu’il y ait de bonne ou de mauvaise situation. Moi, si je devais résumer ma vie aujourd’hui avec vous, je dirais que c’est d’abord des rencontres. Des gens qui m’ont tendu la main, peut-être à un moment où je ne pouvais pas, où j’étais seul chez moi. Et c’est assez curieux de se dire que les hasards, les rencontres forgent une destinée… Parce que quand on a le goût de la chose, quand on a le goût de la chose bien faite, le beau geste, parfois on ne trouve pas l’interlocuteur en face je dirais, le miroir qui vous aide à avancer. Alors ça n’est pas mon cas, comme je disais là, puisque moi au contraire, j’ai pu : et je dis merci à la vie, je lui dis merci, je chante la vie, je danse la vie… je ne suis qu’amour ! Et finalement, quand beaucoup de gens aujourd’hui me disent ‘Mais comment fais-tu pour avoir cette humanité ?’, et bien je leur réponds très simplement, je leur dis que c’est ce goût de l’amour ce goût donc qui m’a poussé aujourd’hui à entreprendre une construction mécanique, mais demain qui sait ? Peut-être simplement à me mettre au service de la communauté, à faire le don, le don de soi…”"
  },
  {
    "objectID": "content/course/getting-started/07_rappels_classes/index.html#quest-ce-que-la-programmation-orientée-objet",
    "href": "content/course/getting-started/07_rappels_classes/index.html#quest-ce-que-la-programmation-orientée-objet",
    "title": "7  Les classes en Python",
    "section": "7.1 Qu’est-ce que la programmation orientée objet ?",
    "text": "7.1 Qu’est-ce que la programmation orientée objet ?\nLe langage Python se base sur des objets et définit pour eux des actions.\nSelon le type d’objet, les actions seront différentes.\nOn parle à ce propos de langage orienté objet ce qui signifie\nque la syntaxe du langage Python permet de définir de manière conceptuelle\ndes objets et appliquer des traitements cohérents avec leur structure interne.\nPar exemple,\npour manipuler des données textuelles ou numériques, on aura\nbesoin d’appliquer des méthodes différentes. Prenons l’exemple\nde l’opération +. Pour des données numériques, il s’agit\nde l’addition. Pour des données textuelles, l’addition n’a pas de sens\nmais on peut envisager d’appliquer cette opération pour faire de la\nconcaténation.\nChaque type d’objet se verra donc appliquer des actions\nadaptées. Cela offre une grande flexibilité au langage Python car on\npeut définir une méthode générique (par exemple l’addition) et l’adapter\nà différents types d’objets.\nLe fait que Python soit un langage orienté objet a une influence sur la\nsyntaxe. On retrouvera régulière la syntaxe objet.method qui est au coeur\nde Python. Par exemple pd.DataFrame.mean se traduit par\nappliquer la méthode mean a un objet de type pd.DataFrame.\n\n7.1.1 Quand utilise-t-on cela dans le domaine de la data-science ?\nLes réseaux de neurone programmés avec Keras ou PyTorch fonctionnent de\ncette manière. On part d’une structure de base et modifie les attributs (par\nexemple le nombre de couches) ou les méthodes."
  },
  {
    "objectID": "content/course/getting-started/07_rappels_classes/index.html#la-définition-dun-objet",
    "href": "content/course/getting-started/07_rappels_classes/index.html#la-définition-dun-objet",
    "title": "7  Les classes en Python",
    "section": "7.2 La définition d’un objet",
    "text": "7.2 La définition d’un objet\nPour définir un objet, il faut lui donner des caractéristiques et des actions, ce qu’il est, ce qu’il peut faire.\nAvec une liste, on peut ajouter des éléments par exemple avec l’action .append(). On peut créer autant d’objets “liste” qu’on le souhaite.\nUne classe regroupe des fonctions et des attributs qui définissent un objet.\nUn objet est une instance d’une classe, c’est-à-dire un exemplaire issu de la classe. L’objet avec un comportement et un état, tous deux définis par la classe. On peut créer autant d’objets que l’on désire avec une classe donnée.\nIci nous allons essayer de créer une classe chat, avec des attributs pour caractériser le chat et des actions, pour voir ce qu’il peut faire avec un objet de la classe chat."
  },
  {
    "objectID": "content/course/getting-started/07_rappels_classes/index.html#exemple-la-classe-chat",
    "href": "content/course/getting-started/07_rappels_classes/index.html#exemple-la-classe-chat",
    "title": "7  Les classes en Python",
    "section": "7.3 Exemple : la Classe chat()",
    "text": "7.3 Exemple : la Classe chat()\n\n7.3.1 Les attributs de la classe chat\n\n7.3.1.1 Classe chat version 1 - premiers attributs\nOn veut pouvoir créer un objet chat() qui nous permettra à terme de créer une colonie de chats (on sait\njamais ca peut servir …).\nPour commencer, on va définir un chat avec des attributs de base : une couleur et un nom.\n\nclass chat: # Définition de notre classe chat\n    \"\"\"Classe définissant un chat caractérisé par :\n    - son nom\n    - sa couleur \"\"\"\n    \n    def __init__(self): # Notre méthode constructeur - \n        # self c'est notre objet qu'on est en train de créer\n        \"\"\"Pour l'instant, on ne va définir que deux attributs - nom et couleur \"\"\"\n        self.couleur = \"Noir\"   \n        self.nom = \"Aucun nom\"\n\n\nmon_chat = chat()\n\nprint(type(mon_chat), mon_chat.couleur ,\",\", mon_chat.nom) \n\n&lt;class '__main__.chat'&gt; Noir , Aucun nom\n\n\nOn nous dit bien que Mon chat est défini à partir de la classe chat,\nc’est ce que nous apprend la fonction type.\nPour l’instant il n’a pas de nom\n\n\n7.3.1.2 Classe chat version 2 - autres attributs\nAvec un nom et une couleur, on ne va pas loin. On peut continuer à définir des attributs pour la classe chat\nde la même façon que précédemment.\n\nclass chat: # Définition de notre classe chat\n    \"\"\"Classe définissant un chat caractérisé par :\n    - sa couleur\n    - son âge\n    - son caractère\n    - son poids\n    - son maitre\n    - son nom \"\"\"\n\n    \n    def __init__(self): # Notre méthode constructeur - \n        #self c'est notre objet qu'on est en train de créer\n        self.couleur = \"Noir\"    \n        self.age = 10\n        self.caractere = \"Joueur\"\n        self.poids = 3\n        self.maitre = \"Jeanne\"\n        self.nom = \"Aucun nom\"\n\n\nhelp(chat) \n# si on veut savoir ce que fait la classe \"chat\" on appelle l'aide\n\nHelp on class chat in module __main__:\n\nclass chat(builtins.object)\n |  Classe définissant un chat caractérisé par :\n |  - sa couleur\n |  - son âge\n |  - son caractère\n |  - son poids\n |  - son maitre\n |  - son nom\n |  \n |  Methods defined here:\n |  \n |  __init__(self)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n\n\n\nmon_chat = chat()\nprint(\"L'âge du chat est\", mon_chat.age,\"ans\") \n# on avait défini l'attribut age de la classe chat comme étant égal à 10\n#, si on demande l'attribut age de notre Martin on obtient 10\n\nL'âge du chat est 10 ans\n\n\nPar défaut, les attributs de la classe Chat seront toujours les mêmes à chaque création de chat à partir\nde la classe Chat.\nMais une fois qu’une instance de classe est créée (ici mon chat est une instance de classe) on peut décider\nde changer la valeur de ses attributs.\n\n\n7.3.1.3 Un nouveau poids\n\nprint(mon_chat.poids)\n# si on veut changer le poids de mon chat, parce qu'il a un peu grossi après les fêtes\nmon_chat.poids = 3.5\nprint(mon_chat.poids) # maintenant le poids est 3.5\n\n3\n3.5\n\n\n\n\n7.3.1.4 Un nouveau nom\n\n# on veut aussi lui donner un nom \nmon_chat.nom = \"Martin\"\nmon_chat.nom\n\n'Martin'\n\n\n\n\n7.3.1.5 Une autre instance de la classe Chat\nOn peut aussi créer d’autres objets chat à partir de la classe chat :\n\n# on appelle la classe\nl_autre_chat = chat()\n# on change les attributs qui nous intéressent\nl_autre_chat.nom = \"Ginette\"\nl_autre_chat.maitre = \"Roger\"\n# les attributs inchangés donnent la même chose \n# que ceux définis par défaut pour la classe\nprint(l_autre_chat.couleur)\n\nNoir\n\n\n\n\n\n7.3.2 Les méthodes de la classe chat\nLes attributs sont des variables propres à notre objet, qui servent à le caractériser.\nLes méthodes sont plutôt des actions, comme nous l’avons vu dans la partie précédente, agissant sur l’objet.\nPar exemple, la méthode append de la classe list permet d’ajouter un élément dans l’objet list manipulé.\n\n7.3.2.1 Classe chat version 3 - première méthode\nOn peut définir une première méthode : nourrir\n\nclass chat: # Définition de notre classe chat\n    \"\"\"Classe définissant un chat caractérisé par :\n    - sa couleur\n    - son âge\n    - son caractère\n    - son poids\n    - son maitre\n    - son nom \n    \n    L'objet chat a une méthode : nourrir \"\"\"\n\n    \n    def __init__(self): # Notre méthode constructeur - \n        #self c'est notre objet qu'on est en train de créer\n        self.couleur = \"Noir\"    \n        self.age = 10\n        self.caractere = \"Joueur\"\n        self.poids = 3\n        self.maitre = \"Jeanne\"\n        self.nom = \"Aucun nom\"\n        \n        \"\"\"Par défaut, notre ventre est vide\"\"\"\n        self.ventre = \"\"\n        \n    def nourrir(self, nourriture):\n        \"\"\"Méthode permettant de donner à manger au chat.\n        Si le ventre n'est pas vide, on met une virgule avant de rajouter\n        la nourriture\"\"\"       \n        if self.ventre != \"\":\n            self.ventre += \",\"\n        self.ventre += nourriture\n\n\nmon_chat = chat()\nmon_chat.nom = \"Martin\"\nmon_chat.ventre # On n'a rien donné à Martin, son ventre est vide\n\n''\n\n\n\n# on appelle la méthode \"nourrir\" de la classe chat, \n# on lui donne un élément, ici des croquettes\nmon_chat.nourrir('Croquettes')\nprint(\"Le contenu du ventre de martin : \",mon_chat.ventre)\n\nLe contenu du ventre de martin :  Croquettes\n\n\n\nmon_chat.nourrir('Saumon')\nprint(\"Le contenu du ventre de martin : \",mon_chat.ventre)\n\nLe contenu du ventre de martin :  Croquettes,Saumon\n\n\n\n\n7.3.2.2 Classe chat version 4 - autre méthode\nAvec un chat, on peut imaginer plein de méthodes. Ici on va définir une action “nourrir” et une autre action\n“litiere”, qui consiste à vider l’estomac du chat.\n\nclass chat: # Définition de notre classe Personne\n    \"\"\"Classe définissant un chat caractérisé par :\n    - sa couleur\n    - son âge\n    - son caractère\n    - son poids\n    - son maitre\n    - son nom \n    \n    L'objet chat a deux méthodes : nourrir et litiere \"\"\"\n\n    \n    def __init__(self): # Notre méthode constructeur - \n        #self c'est notre objet qu'on est en train de créer\n        self.nom = \"\"\n        self.couleur = \"Roux\"    \n        self.age = 10\n        self.caractere = \"Joueur\"\n        self.poids = 3\n        self.maitre = \"Jeanne\"\n        \"\"\"Par défaut, notre ventre est vide\"\"\"\n        self.ventre = \"\"\n        \n    def nourrir(self, nourriture):\n        \"\"\"Méthode permettant de donner à manger au chat.\n        Si le ventre n'est pas vide, on met une virgule avant de rajouter\n        la nourriture\"\"\"       \n        if self.ventre != \"\":\n            self.ventre += \",\"\n        self.ventre += nourriture\n\n    def litiere(self) : \n        \"\"\" Méthode permettant au chat d'aller à sa litière : \n        en conséquence son ventre est vide \"\"\"       \n        self.ventre = \"\"\n        print(self.nom,\"a le ventre vide\")\n\n\n# on définit Martin le chat\nmon_chat = chat()\nmon_chat.nom = \"Martin\"\n# on le nourrit avec des croquettes\nmon_chat.nourrir('croquettes')\nprint(\"Le contenu du ventre de martin\", mon_chat.ventre)\n\n\n# Il va dans sa litiere\nmon_chat.litiere()\n\nLe contenu du ventre de martin croquettes\nMartin a le ventre vide\n\n\n\nhelp(mon_chat.nourrir)\nhelp(mon_chat.litiere)\n\nHelp on method nourrir in module __main__:\n\nnourrir(nourriture) method of __main__.chat instance\n    Méthode permettant de donner à manger au chat.\n    Si le ventre n'est pas vide, on met une virgule avant de rajouter\n    la nourriture\n\nHelp on method litiere in module __main__:\n\nlitiere() method of __main__.chat instance\n    Méthode permettant au chat d'aller à sa litière : \n    en conséquence son ventre est vide\n\n\n\n\n\n7.3.2.3 Les méthodes spéciales (facultatif)\nSi on reprend notre classe chat, il y a en réalité des méthodes spéciales que nous n’avons pas définies mais\nqui sont implicites.\nPython comprend seul ce que doivent faire ces méthodes. Il a une idée préconcue de ce qu’elles doivent\neffectuer comme opération. Si vous ne redéfinissez par une méthode spéciale pour qu’elle fasse ce que vous\nsouhaitez, ca peut donner des r\u0013esultats inattendus.\nElles servent à plusieurs choses :\n\nà initialiser l’objet instancié : __init__\nà modifier son affichage : __repr__\n\n\n\n# pour avoir la valeur de l'attribut \"nom\"\n\nprint(mon_chat.__getattribute__(\"nom\"))\n# on aurait aussi pu faire plus simple :\nprint(mon_chat.nom)\n\nMartin\nMartin\n\n\n# si l'attribut n'existe pas : on a une erreur\n# Python recherche l'attribut et, s'il ne le trouve pas dans l'objet et si une méthode __getattr__ est spécifiée, \n# il va l'appeler en lui passant en paramètre le nom de l'attribut recherché, sous la forme d'une chaîne de caractères.\n\nprint(mon_chat.origine)\n## Error in py_call_impl(callable, dots$args, dots$keywords): AttributeError: 'chat' object has no attribute 'origine'\n## \n## Detailed traceback: \n##   File \"&lt;string&gt;\", line 1, in &lt;module&gt;\nMais on peut modifier les méthodes spéciales de notre classe chat pour éviter d’avoir des erreurs d’attributs. On va aussi en profiter pour modifier la représentation de l’instance chat qui pour l’instant donne &lt;_main_.chat object at 0x0000000005AB4C50&gt;\n\n\n7.3.2.4 Classe chat version 5 - méthode spéciale\n\nclass chat: # Définition de notre classe Personne\n    \"\"\"Classe définissant un chat caractérisé par :\n    - sa couleur\n    - son âge\n    - son caractère\n    - son poids\n    - son maitre\n    - son nom \n    \n    L'objet chat a deux méthodes : nourrir et litiere \"\"\"\n\n    \n    def __init__(self): # Notre méthode constructeur - \n        #self c'est notre objet qu'on est en train de créer\n        self.nom = \"\"\n        self.couleur = \"Roux\"    \n        self.age = 10\n        self.caractere = \"Joueur\"\n        self.poids = 3\n        self.maitre = \"Jeanne\"\n        \"\"\"Par défaut, notre ventre est vide\"\"\"\n        self.ventre = \"\"\n        \n    def nourrir(self, nourriture):\n        \"\"\"Méthode permettant de donner à manger au chat.\n        Si le ventre n'est pas vide, on met une virgule avant de rajouter\n        la nourriture\"\"\"       \n        if self.ventre != \"\":\n            self.ventre += \",\"\n        self.ventre += nourriture\n\n    def litiere(self) : \n        \"\"\" Méthode permettant au chat d'aller à sa litière : \n        en conséquence son ventre est vide \"\"\"       \n        self.ventre = \"\"\n        print(self.nom,\"a le ventre vide\")\n    \n    def __getattribute__(self, key):\n            return print(key,\"n'est pas un attribut de la classe chat\")   \n            \n    def __repr__(self):\n            return \"Je suis une instance de la classe chat\"\n\n\n# j'ai gardé l'exemple chat défini selon la classe version 4\n# Martin, le chat\n# on a vu précédemment qu'il n'avait pas d'attribut origine\n# et que cela levait une erreur AttributeError\nprint(mon_chat.nom)\n\n\n# on va définir un nouveau chat avec la version 5\n# on appelle à nouveau un attribut qui n'existe pas \"origine\"\n# on a bien le message défini par la méthode spéciale _gettattribute\n\nmon_chat_nouvelle_version = chat()\nmon_chat_nouvelle_version.origine\n\n# Maintenant on a aussi une définition de l'objet plus clair\nprint(mon_chat)\nprint(mon_chat_nouvelle_version)\n\nMartin\norigine n'est pas un attribut de la classe chat\n&lt;__main__.chat object at 0x7f7970186e80&gt;\nJe suis une instance de la classe chat\n\n\n\n\n\n\n7.3.3 Conclusion sur les classes : ce qu’on retient\n\nLes méthodes se définissent comme des fonctions, sauf qu’elles se trouvent dans le corps de la classe.\nOn définit les attributs d’une instance dans le constructeur de sa classe, en suivant cette syntaxe : self.nom_attribut = valeur.\nfacultatif : Les méthodes d’instance prennent en premier paramètre “self”, l’instance de l’objet manipulé.\nfacultatif : On construit une instance de classe en appelant son constructeur, une méthode d’instance appelée init."
  },
  {
    "objectID": "content/course/manipulation/index.html#résumé-de-cette-partie",
    "href": "content/course/manipulation/index.html#résumé-de-cette-partie",
    "title": "Partie 1: manipuler des données",
    "section": "Résumé de cette partie",
    "text": "Résumé de cette partie\nPandas est devenu incontournable dans l’écosystème Python pour la data science.\nPandas est lui-même construit à partir du package Numpy, qu’il est utile de comprendre\npour être à l’aise avec Pandas. Numpy est une librairie bas-niveau\npour stocker et manipuler des données.\nNumpy est au coeur de l’écosystème de la data science car la plupart des librairies, même celles\nqui manient des objets destructurés,\nutilisent des objets construits à partir de Numpy1.\nL’approche Pandas, qui offre un point d’entrée harmonisé pour manipuler\ndes jeux de données de nature très différente,\na été étendue aux objets géographiques avec Geopandas.\nIl est ainsi possible de manipuler des données géographiques comme s’il\ns’agissait de données structurées classiques. Les données géographiques et\nla représentation cartographique deviennent de plus en plus commun avec\nla multiplication de données ouvertes localisées et de big-data géolocalisées.\nCependant, les données structurées, importées depuis des fichiers plats\nne représentent pas l’unique source de données. Les API et le webscraping\npermettent de télécharger ou d’extraire\ndes données de manière très flexible depuis des pages web ou des guichets\nspécialisés. Ces données, notamment\ncelles obtenues par webscraping nécessitent souvent un peu plus de travail\nde nettoyage de données, notamment des chaînes de caractère.\nL’écosystème Pandas représente donc un couteau-suisse\npour l’analyse de données. C’est pour cette raison que ce cours\ndéveloppera beaucoup de contenu dessus.\nAvant d’essayer de mettre en oeuvre une solution ad hoc, il est\nsouvent utile de se poser la question suivante: “ne pourrais-je pas le faire\navec les fonctionalités de base de Pandas ?” Se poser cette question peut\néviter des chemins ardus et faire économiser beaucoup de temps.\nNéanmoins, Pandas n’est pas\nadapté à des données ayant une volumétrie\nimportante. Pour traiter de telles\ndonnées, il est plutôt recommander\nde privilégier Polars ou Dask qui reprennent la logique de Pandas mais\noptimisent son fonctionnement, Spark si on a une infrastructure adaptée, généralement dans\ndes environnements big data, ou\nDuckDB si on est prêt à utiliser des requêtes SQL plutôt qu’une librairie haut-niveau."
  },
  {
    "objectID": "content/course/manipulation/index.html#exercices",
    "href": "content/course/manipulation/index.html#exercices",
    "title": "Partie 1: manipuler des données",
    "section": "Exercices",
    "text": "Exercices\nCette partie présente à la fois des tutoriels détaillés\net des exercices guidés.\nIl est\npossible de les consulter sur ce site ou d’utiliser l’un des\nbadges présents en début de chapitre, par exemple\nceux-ci pour ouvrir\nle chapitre d’exercices sur Pandas:"
  },
  {
    "objectID": "content/course/manipulation/index.html#pour-aller-plus-loin",
    "href": "content/course/manipulation/index.html#pour-aller-plus-loin",
    "title": "Partie 1: manipuler des données",
    "section": "Pour aller plus loin",
    "text": "Pour aller plus loin\nCe cours n’aborde pas vraiment les questions de volumétrie ou de vitesse de\ncalcul.\nPandas peut montrer ses limites dans ce domaine sur des jeux de données\nd’une volumétrie conséquente (plusieurs Gigas).\nIl est ainsi intéressant de porter attention à:\n\nLe livre Modern Pandas\npour obtenir des éléments supplémentaires sur la question de la performance\navec Pandas ;\nLa question des\nobjets sparse ;\nLes packages Dask ou Polars pour accélérer les calculs ;\nDuckDB pour effectuer de manière très efficace des requêtes SQL ;\nPySpark pour des données très volumineuses.\n\n\nRéférences\nVoici une bibliographie sélective des ouvrages\nintéressants en complément des chapitres de la partie “Manipulation” de ce cours:\n\n\nMcKinney, Wes. 2012. Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython. \" O’Reilly Media, Inc.\".\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook: Essential Tools for Working with Data. \" O’Reilly Media, Inc.\".\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O’Reilly Media, Inc.\"."
  },
  {
    "objectID": "content/course/manipulation/01_numpy/index.html#le-concept-darray",
    "href": "content/course/manipulation/01_numpy/index.html#le-concept-darray",
    "title": "8  Numpy, la brique de base de la data science",
    "section": "8.1 Le concept d’array",
    "text": "8.1 Le concept d’array\nLe concept central de NumPy (Numerical Python) est\nl’array qui est un tableau de données multidimensionnel.\nL’array numpy peut être unidimensionnel et s’apparenter à un\nvecteur (1d-array),\nbidimensionnel et ainsi s’apparenter à une matrice (2d-array) ou,\nde manière plus générale,\nprendre la forme d’un objet\nmultidimensionnel (Nd-array).\nLes tableaux simples (uni ou bi-dimensionnels) sont faciles à se représenter et seront particulièrement\nutilisés dans le paradigme des DataFrames mais\nla possibilité d’avoir des objets multidimensionnels permettra d’exploiter des\nstructures très complexes.\nUn DataFrame sera construit à partir d’une collection\nd’array uni-dimensionnels (les variables de la table), ce qui permettra d’effectuer des opérations cohérentes\n(et optimisées) avec le type de la variable.\nPar rapport à une liste,\n\nun array ne peut contenir qu’un type de données (integer, string, etc.),\ncontrairement à une liste.\nles opérations implémentées par numpy seront plus efficaces et demanderont moins\nde mémoire\n\nLes données géographiques constitueront une construction un peu plus complexe qu’un DataFrame traditionnel.\nLa dimension géographique prend la forme d’un tableau plus profond, au moins bidimensionnel\n(coordonnées d’un point)."
  },
  {
    "objectID": "content/course/manipulation/01_numpy/index.html#créer-un-array",
    "href": "content/course/manipulation/01_numpy/index.html#créer-un-array",
    "title": "8  Numpy, la brique de base de la data science",
    "section": "8.2 Créer un array",
    "text": "8.2 Créer un array\nOn peut créer un array de plusieurs manières. Pour créer un array à partir d’une liste,\nil suffit d’utiliser la méthode array:\n\nnp.array([1,2,5])\n\narray([1, 2, 5])\n\n\nIl est possible d’ajouter un argument dtype pour contraindre le type du array:\n\nnp.array([[\"a\",\"z\",\"e\"],[\"r\",\"t\"],[\"y\"]], dtype=\"object\")\n\narray([list(['a', 'z', 'e']), list(['r', 't']), list(['y'])], dtype=object)\n\n\nIl existe aussi des méthodes pratiques pour créer des array:\n\nséquences logiques : np.arange (suite) ou np.linspace (interpolation linéaire entre deux bornes)\nséquences ordonnées: array rempli de zéros, de 1 ou d’un nombre désiré : np.zeros, np.ones ou np.full\nséquences aléatoires: fonctions de génération de nombres aléatoires: np.rand.uniform, np.rand.normal, etc.\ntableau sous forme de matrice identité: np.eye\n\n\nnp.arange(0,10)\nnp.arange(0,10,3)\nnp.linspace(0, 1, 5)\nnp.zeros(10, dtype=int)\nnp.ones((3, 5), dtype=float)\nnp.full((3, 5), 3.14)\nnp.eye(3)\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\n\n\n Exercice 1\nGénérer:\n\n\\(X\\) une variable aléatoire, 1000 répétitions d’une loi \\(U(0,1)\\)\n\\(Y\\) une variable aléatoire, 1000 répétitions d’une loi normale de moyenne nulle et de variance égale à 2\nVérifier la variance de \\(Y\\) avec np.var"
  },
  {
    "objectID": "content/course/manipulation/01_numpy/index.html#indexation-et-slicing",
    "href": "content/course/manipulation/01_numpy/index.html#indexation-et-slicing",
    "title": "8  Numpy, la brique de base de la data science",
    "section": "8.3 Indexation et slicing",
    "text": "8.3 Indexation et slicing\n\n8.3.1 Logique dans le cas d’un array unidimensionnel\nLa structure la plus simple est l’array unidimensionnel:\n\nx = np.arange(10)\nprint(x)\n\n[0 1 2 3 4 5 6 7 8 9]\n\n\nL’indexation est dans ce cas similaire à celle d’une liste:\n\nle premier élément est 0\nle énième élément est accessible à la position \\(n-1\\)\n\nLa logique d’accès aux éléments est ainsi la suivante:\nx[start:stop:step]\nAvec un array unidimensionnel, l’opération de slicing (garder une coupe du array) est très simple.\nPar exemple, pour garder les K premiers éléments d’un array, on fera:\nx[:(K-1)]\nEn l’occurrence, on sélectionne le K\\(^{eme}\\) élément en utilisant\nx[K-1]\nPour sélectionner uniquement un élément, on fera ainsi:\n\nx = np.arange(10)\nx[2]\n\n2\n\n\nLes syntaxes qui permettent de sélectionner des indices particuliers d’une liste fonctionnent également\navec les arrays.\n\n\n Exercice 2\n\nSélectionner les éléments 0,3,5\nSélectionner les éléments pairs\nSélectionner tous les éléments sauf le premier\nSélectionner les 5 premiers éléments\n\n\n\n\n\n8.3.2 Sur la performance\nUn élément déterminant dans la performance de numpy par rapport aux listes,\nlorsqu’il est question de\nslicing est qu’un array ne renvoie pas une\ncopie de l’élément en question (copie qui coûte de la mémoire et du temps)\nmais simplement une vue de celui-ci.\nLorsqu’il est nécessaire d’effectuer une copie,\npar exemple pour ne pas altérer l’array sous-jacent, on peut\nutiliser la méthode copy:\nx_sub_copy = x[:2, :2].copy()\n\n\n\n8.3.3 Filtres logiques\nIl est également possible, et plus pratique, de sélectionner des données à partir de conditions logiques\n(opération qu’on appelle un boolean mask).\nCette fonctionalité servira principalement à\neffectuer des opérations de filtre sur les données.\nPour des opérations de comparaison simples, les comparateurs logiques peuvent être suffisants.\nCes comparaisons fonctionnent aussi sur les tableaux multidimensionnels grâce au\nbroadcasting sur lequel nous reviendrons :\n\nx = np.arange(10)\nx2 = np.array([[-1,1,-2],[-3,2,0]])\nprint(x)\nprint(x2)\n\n[0 1 2 3 4 5 6 7 8 9]\n[[-1  1 -2]\n [-3  2  0]]\n\n\n\nx==2\nx2&lt;0\n\narray([[ True, False,  True],\n       [ True, False, False]])\n\n\nPour sélectionner les observations relatives à la condition logique,\nil suffit d’utiliser la logique de slicing de numpy qui fonctionne avec les conditions logiques\n\n\n Exercice 3\nSoit\nx = np.random.normal(size=10000)\n\nNe conserver que les valeurs dont la valeur absolue est supérieure à 1.96\nCompter le nombre de valeurs supérieures à 1.96 en valeur absolue et leur proportion dans l’ensemble\nSommer les valeurs absolues de toutes les observations supérieures (en valeur absolue) à 1.96\net rapportez les à la somme des valeurs de x (en valeur absolue)\n\n\n\nLorsque c’est possible, il est recommandé d’utiliser les fonctions logiques de numpy (optimisées et\nqui gèrent bien la dimension).\nParmi elles, on peut retrouver:\n\ncount_nonzero\nisnan\nany ; all ; notamment avec l’argument axis\nnp.array_equal pour vérifier, élément par élément, l’égalité\n\nSoit\n\nx = np.random.normal(0, size=(3, 4))\n\nun array multidimensionnel et\n\ny = np.array([np.nan, 0, 1])\n\nun array unidimensionnel présentant une valeur manquante.\n\n\n Exercice 4\n\nUtiliser count_nonzero sur y\nUtiliser isnan sur y et compter le nombre de valeurs non NaN\nVérifier que x comporte au moins une valeur positive dans son ensemble, en parcourant les lignes puis les colonnes.\n\nNote : Jetez un oeil à ce que correspond le paramètre axis dans numpy en vous documentant sur internet. Par exemple ici."
  },
  {
    "objectID": "content/course/manipulation/01_numpy/index.html#manipuler-un-array",
    "href": "content/course/manipulation/01_numpy/index.html#manipuler-un-array",
    "title": "8  Numpy, la brique de base de la data science",
    "section": "8.4 Manipuler un array",
    "text": "8.4 Manipuler un array\nDans cette section, on utilisera un array multidimensionnel:\n\nx = np.random.normal(0, size=(3, 4))\n\n\n8.4.1 Statistiques sur un array\nPour les statistiques descriptives classiques,\nnumpy propose un certain nombre de fonctions déjà implémentées,\nqui peuvent être combinées avec l’argument axis\n\n\n Exercice 5\n\nFaire la somme de tous les éléments d’un array, des éléments en ligne et des éléments en colonne. Vérifier\nla cohérence\nEcrire une fonction statdesc pour renvoyer les valeurs suivantes : moyenne, médiane, écart-type, minimum et maximum.\nL’appliquer sur x en jouant avec l’argument axis\n\n\n\n\n\n8.4.2 Fonctions de manipulation\nVoici quelques fonctions pour modifier un array,\n\n\n\n\n\n\n\nOpération\nImplémentation\n\n\n\n\nApplatir un array\nx.flatten() (méthode)\n\n\nTransposer un array\nx.T (méthode) ou np.transpose(x) (fonction)\n\n\nAjouter des éléments à la fin\nnp.append(x, [1,2])\n\n\nAjouter des éléments à un endroit donné (aux positions 1 et 2)\nnp.insert(x, [1,2], 3)\n\n\nSupprimer des éléments (aux positions 0 et 3)\nnp.delete(x, [0,3])\n\n\n\nPour combiner des array, on peut utiliser, selon les cas,\nles fonctions np.concatenate, np.vstack ou la méthode .r_ (concaténation rowwise).\nnp.hstack ou la méthode .column_stack ou .c_ (concaténation column-wise)\n\nx = np.random.normal(size = 10)\n\nPour ordonner un array, on utilise np.sort\n\nx = np.array([7, 2, 3, 1, 6, 5, 4])\n\nnp.sort(x)\n\narray([1, 2, 3, 4, 5, 6, 7])\n\n\nSi on désire faire un ré-ordonnement partiel pour trouver les k valeurs les plus petites d’un array sans les ordonner, on utilise partition:\n\nnp.partition(x, 3)\n\narray([2, 1, 3, 4, 6, 5, 7])"
  },
  {
    "objectID": "content/course/manipulation/01_numpy/index.html#broadcasting",
    "href": "content/course/manipulation/01_numpy/index.html#broadcasting",
    "title": "8  Numpy, la brique de base de la data science",
    "section": "8.5 Broadcasting",
    "text": "8.5 Broadcasting\nLe broadcasting désigne un ensemble de règles permettant\nd’appliquer des opérations sur des tableaux de dimensions différentes. En pratique,\ncela consiste généralement à appliquer une seule opération à l’ensemble des membres d’un tableau numpy.\nLa différence peut être comprise à partir de l’exemple suivant. Le broadcasting permet\nde transformer le scalaire 5 en array de dimension 3:\n\na = np.array([0, 1, 2])\n\nb = np.array([5, 5, 5])\n\na + b\na + 5\n\narray([5, 6, 7])\n\n\nLe broadcasting peut être très pratique pour effectuer de manière efficace des opérations sur des données à\nla structure complexe. Pour plus de détails, se rendre\nici ou ici."
  },
  {
    "objectID": "content/course/manipulation/01_numpy/index.html#une-application-programmer-ses-propres-k-nearest-neighbors",
    "href": "content/course/manipulation/01_numpy/index.html#une-application-programmer-ses-propres-k-nearest-neighbors",
    "title": "8  Numpy, la brique de base de la data science",
    "section": "8.6 Une application: programmer ses propres k-nearest neighbors",
    "text": "8.6 Une application: programmer ses propres k-nearest neighbors\n\n\n\n Exercice (un peu plus corsé)\n\nCréer X un tableau à deux dimensions (i.e. une matrice) comportant 10 lignes\net 2 colonnes. Les nombres dans le tableau sont aléatoires.\nImporter le module matplotlib.pyplot sous le nom plt. Utiliser\nplt.scatter pour représenter les données sous forme de nuage de points.\nConstuire une matrice 10x10 stockant, à l’élément \\((i,j)\\), la distance euclidienne entre les points \\(X[i,]\\) et \\(X[j,]\\). Pour cela, il va falloir jouer avec les dimensions en créant des tableaux emboîtés à partir par des appels à np.newaxis :\n\n\nEn premier lieu, utiliser X1 = X[:, np.newaxis, :] pour transformer la matrice en tableau emboîté. Vérifier les dimensions\nCréer X2 de dimension (1, 10, 2) à partir de la même logique\nEn déduire, pour chaque point, la distance avec les autres points pour chaque coordonnées. Elever celle-ci au carré\nA ce stade, vous devriez avoir un tableau de dimension (10, 10, 2). La réduction à une matrice s’obtient en sommant sur le dernier axe. Regarder dans l’aide de np.sum comme effectuer une somme sur le dernier axe.\nEnfin, appliquer la racine carrée pour obtenir une distance euclidienne en bonne et due forme.\n\n\nVérifier que les termes diagonaux sont bien nuls (distance d’un point à lui-même…)\nIl s’agit maintenant de classer, pour chaque point, les points dont les valeurs sont les plus similaires. Utiliser np.argsort pour obtenir, pour chaque ligne, le classement des points les plus proches\nOn va s’intéresser aux k-plus proches voisins. Pour le moment, fixons k=2. Utiliser argpartition pour réordonner chaque ligne de manière à avoir les 2 plus proches voisins de chaque point d’abord et le reste de la ligne ensuite\nUtiliser le morceau de code ci-dessous\n\n\n\n\nUn indice pour représenter graphiquement les plus proches voisins\nplt.scatter(X[:, 0], X[:, 1], s=100)\n\n# draw lines from each point to its two nearest neighbors\nK = 2\n\nfor i in range(X.shape[0]):\n    for j in nearest_partition[i, :K+1]:\n        # plot a line from X[i] to X[j]\n        # use some zip magic to make it happen:\n        plt.plot(*zip(X[j], X[i]), color='black')\n\n\nPour la question 2, vous devriez obtenir un graphique ayant cet aspect :\n\n\n\n\n\nLe résultat de la question 7 est le suivant:\n\n\n\n\n\nAi-je inventé cet exercice corsé ? Pas du tout, il vient de l’ouvrage Python Data Science Handbook. Mais, si je vous l’avais indiqué immédiatement, auriez-vous cherché à répondre aux questions ?\nPar ailleurs, il ne serait pas une bonne idée de généraliser cet algorithme à de grosses données. La complexité de notre approche est \\(O(N^2)\\). L’algorithme implémenté par Scikit-Learn est\nen \\(O[NlogN]\\).\nDe plus, le calcul de distances matricielles en utilisant la puissance des cartes graphiques serait plus rapide. A cet égard, la librairie faiss offre des performances beaucoup plus satisfaisantes que celles que permettraient numpy sur ce problème précis."
  },
  {
    "objectID": "content/course/manipulation/01_numpy/index.html#exercices-supplémentaires",
    "href": "content/course/manipulation/01_numpy/index.html#exercices-supplémentaires",
    "title": "8  Numpy, la brique de base de la data science",
    "section": "8.7 Exercices supplémentaires",
    "text": "8.7 Exercices supplémentaires\n\nSimulations de variables aléatoires ;\nTCL ;\n\n\n\n Comprendre le principe de l'algorithme PageRank\nGoogle est devenu célèbre grâce à son algorithme PageRank. Celui-ci permet, à partir\nde liens entre sites web, de donner un score d’importance à un site web qui va\nêtre utilisé pour évaluer sa centralité dans un réseau.\nL’objectif de cet exercice est d’utiliser Numpy pour mettre en oeuvre un tel\nalgorithme à partir d’une matrice d’adjacence qui relie les sites entre eux.\n\nCréer la matrice suivante avec numpy. L’appeler M:\n\n\\[\n\\begin{bmatrix}\n0 & 0 & 0 & 0 & 1 \\\\\n0.5 & 0 & 0 & 0 & 0 \\\\\n0.5 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0.5 & 0 & 0 \\\\\n0 & 0 & 0.5 & 1 & 0\n\\end{bmatrix}\n\\]\n\nPour représenter visuellement ce web minimaliste,\nconvertir en objet networkx (une librairie spécialisée\ndans l’analyse de réseau) et utiliser la fonction draw\nde ce package.\n\nIl s’agit de la transposée de la matrice d’adjacence\nqui permet de relier les sites entre eux. Par exemple,\nle site 1 (première colonne) est référencé par\nles sites 2 et 3. Celui-ci ne référence que le site 5.\n\nA partir de la page wikipedia anglaise de PageRank, tester\nsur votre matrice.\n\n\n\n\n\n\n\n\n\n\n\n\nLe site 1 est assez central car il est référencé 2 fois. Le site\n5 est lui également central puisqu’il est référencé par le site 1.\n\n\narray([[0.25419178],\n       [0.13803151],\n       [0.13803151],\n       [0.20599017],\n       [0.26375504]])"
  },
  {
    "objectID": "content/course/manipulation/02a_pandas_tutorial/index.html#logique-de-pandas",
    "href": "content/course/manipulation/02a_pandas_tutorial/index.html#logique-de-pandas",
    "title": "9  Introduction à Pandas",
    "section": "9.1 Logique de pandas",
    "text": "9.1 Logique de pandas\nL’objet central dans la logique pandas est le DataFrame.\nIl s’agit d’une structure particulière de données\nà deux dimensions, structurées en alignant des lignes et colonnes. Les colonnes\npeuvent être de type différent.\nUn DataFrame est composé des éléments suivants:\n\nl’indice de la ligne ;\nle nom de la colonne ;\nla valeur de la donnée ;\n\nStructuration d’un DataFrame pandas,\nempruntée à https://medium.com/epfl-extension-school/selecting-data-from-a-pandas-dataframe-53917dc39953:\n\n\n\n\n\nLe concept de tidy data, popularisé par Hadley Wickham via ses packages R,\nest parfaitement pertinent pour décrire la structure d’un DataFrame pandas.\nLes trois règles sont les suivantes:\n\nChaque variable possède sa propre colonne ;\nChaque observation possède sa propre ligne ;\nUne valeur, matérialisant une observation d’une variable,\nse trouve sur une unique cellule.\n\n\n\n\nConcept de tidy data (emprunté à H. Wickham)\n\n\n\n\n Hint\nLes DataFrames sont assez rapides en Python[^2] et permettent de traiter en local de manière efficace des tables de\ndonnées comportant plusieurs millions d’observations (en fonction de la configuration de l’ordinateur)\net dont la volumétrie peut être conséquente (plusieurs centaines\nde Mo). Néanmoins, passé un certain seuil, qui dépend de la puissance de la machine mais aussi de la complexité\nde l’opération effectuée, le DataFrame pandas peut montrer certaines limites. Dans ce cas, il existe différentes\nsolutions: Dask (dataframe aux opérations parallélisés), SQL (notamment Postgres),\nSpark (solution big data). Un chapitre spécial de ce cours est consacré à Dask.\n\n\n::: {.cell .markdown}\n[^2]: En R, les deux formes de dataframes qui se sont imposées récemment sont les tibbles (package dplyr)\net les data.tables (package data.table). dplyr reprend la syntaxe SQL de manière relativement\ntransparente ce qui rend la syntaxe très proche de celle de pandas. Cependant,\nalors que dplyr supporte très mal les données dont la volumétrie dépasse 1Go, pandas s’en\naccomode bien. Les performances de pandas sont plus proches de celles de data.table, qui est\nconnu pour être une approche efficace avec des données de taille importante.\n:::\nConcernant la syntaxe, une partie des commandes python est inspirée par la logique SQL. On retrouvera ainsi\ndes instructions relativement transparentes.\nIl est vivement recommandé, avant de se lancer dans l’écriture d’une\nfonction, de se poser la question de son implémentation native dans numpy, pandas, etc.\nEn particulier, la plupart du temps, s’il existe une solution implémentée dans une librairie, il convient\nde l’utiliser."
  },
  {
    "objectID": "content/course/manipulation/02a_pandas_tutorial/index.html#les-series",
    "href": "content/course/manipulation/02a_pandas_tutorial/index.html#les-series",
    "title": "9  Introduction à Pandas",
    "section": "9.2 Les Series",
    "text": "9.2 Les Series\nEn fait, un DataFrame est une collection d’objets appelés pandas.Series.\nCes Series sont des objets d’une dimension qui sont des extensions des\narray-unidimensionnels numpy. En particulier, pour faciliter le traitement\nde données catégorielles ou temporelles, des types de variables\nsupplémentaires sont disponibles dans pandas par rapport à\nnumpy (categorical, datetime64 et timedelta64). Ces\ntypes sont associés à des méthodes optimisées pour faciliter le traitement\nde ces données.\nIl ne faut pas négliger l’attribut dtype d’un objet\npandas.Series car cela a une influence déterminante sur les méthodes\net fonctions pouvant être utilisées (on ne fait pas les mêmes opérations\nsur une donnée temporelle et une donnée catégorielle) et le volume en\nmémoire d’une variable (le type de la variable détermine le volume\nd’information stocké pour chaque élément ; être trop précis est parfois\nnéfaste).\nIl existe plusieurs types possibles pour un pandas.Series.\nLe type object correspond aux types Python str ou mixed.\nIl existe un type particulier pour les variables dont le nombre de valeurs\nest une liste finie et relativement courte, le type category.\nIl faut bien examiner les types de son DataFrame, et convertir éventuellement\nles types lors de l’étape de data cleaning.\n\n9.2.1 Indexation\nLa différence essentielle entre une Series et un objet numpy est l’indexation.\nDans numpy,\nl’indexation est implicite ; elle permet d’accéder à une donnée (celle à\nl’index situé à la position i).\nAvec une Series, on peut bien-sûr utiliser un indice de position mais on peut\nsurtout faire appel à des indices plus explicites.\nPar exemple,\n\ntaille = pd.Series(\n    [1.,1.5,1],\n    index = ['chat', 'chien', 'koala']\n)\n\ntaille.head()\n\nchat     1.0\nchien    1.5\nkoala    1.0\ndtype: float64\n\n\nCette indexation permet d’accéder à des valeurs de la Series\nvia une valeur de l’indice. Par\nexemple, taille['koala']:\n\ntaille['koala']\n\n1.0\n\n\nL’existence d’indice rend le subsetting particulièrement aisé, ce que vous\npouvez expérimenter dans les TP\n\n\n\n\n\n\n\n\n\n\n\n\nPour transformer un objet pandas.Series en array numpy,\non utilise la méthode values. Par exemple, taille.values:\n\ntaille.values\n\narray([1. , 1.5, 1. ])\n\n\nUn avantage des Series par rapport à un array numpy est que\nles opérations sur les Series alignent\nautomatiquement les données à partir des labels.\nAvec des Series labélisées, il n’est ainsi pas nécessaire\nde se poser la question de l’ordre des lignes.\nL’exemple dans la partie suivante permettra de s’en assurer.\n\n\n9.2.2 Valeurs manquantes\nPar défaut, les valeurs manquantes sont affichées NaN et sont de type np.nan (pour\nles valeurs temporelles, i.e. de type datatime64, les valeurs manquantes sont\nNaT).\nOn a un comportement cohérent d’agrégation lorsqu’on combine deux DataFrames (ou deux colonnes).\nPar exemple,\n\nx = pd.DataFrame(\n    {'prix': np.random.uniform(size = 5),\n     'quantite': [i+1 for i in range(5)]\n    },\n    index = ['yaourt','pates','riz','tomates','gateaux']\n)\nx\n\n\n\n\n\n\n\n\nprix\nquantite\n\n\n\n\nyaourt\n0.696469\n1\n\n\npates\n0.286139\n2\n\n\nriz\n0.226851\n3\n\n\ntomates\n0.551315\n4\n\n\ngateaux\n0.719469\n5\n\n\n\n\n\n\n\n\ny = pd.DataFrame(\n    {'prix': [np.nan, 0, 1, 2, 3],\n     'quantite': [i+1 for i in range(5)]\n    },\n    index = ['tomates','yaourt','gateaux','pates','riz']\n)\ny\n\n\n\n\n\n\n\n\nprix\nquantite\n\n\n\n\ntomates\nNaN\n1\n\n\nyaourt\n0.0\n2\n\n\ngateaux\n1.0\n3\n\n\npates\n2.0\n4\n\n\nriz\n3.0\n5\n\n\n\n\n\n\n\n\nx + y\n\n\n\n\n\n\n\n\nprix\nquantite\n\n\n\n\ngateaux\n1.719469\n8\n\n\npates\n2.286139\n6\n\n\nriz\n3.226851\n8\n\n\ntomates\nNaN\n5\n\n\nyaourt\n0.696469\n3\n\n\n\n\n\n\n\ndonne bien une valeur manquante pour la ligne tomates. Au passage, on peut remarquer que l’agrégation\na tenu compte des index.\nIl est possible de supprimer les valeurs manquantes grâce à dropna().\nCette méthode va supprimer toutes les lignes où il y a au moins une valeur manquante.\nIl est aussi possible de supprimer seulement les colonnes où il y a des valeurs manquantes\ndans un DataFrame avec dropna() avec le paramètre axis=1 (par défaut égal à 0).\nIl est également possible de remplir les valeurs manquantes grâce à la méthode fillna()."
  },
  {
    "objectID": "content/course/manipulation/02a_pandas_tutorial/index.html#le-dataframe-pandas",
    "href": "content/course/manipulation/02a_pandas_tutorial/index.html#le-dataframe-pandas",
    "title": "9  Introduction à Pandas",
    "section": "9.3 Le DataFrame Pandas",
    "text": "9.3 Le DataFrame Pandas\nLe DataFrame est l’objet central de la librairie pandas.\nIl s’agit d’une collection de pandas.Series (colonnes) alignées par les index.\nLes types des variables peuvent différer.\nUn DataFrame non-indexé a la structure suivante:\n\n\n\n\n\n\n\n\n\nindex\ntaille\npoids\n\n\n\n\n0\nchat\n1.0\n3.0\n\n\n1\nchien\n1.5\n5.0\n\n\n2\nkoala\n1.0\n2.5\n\n\n\n\n\n\n\nAlors que le même DataFrame indexé aura la structure suivante:\n\n\n\n\n\n\n\n\n\ntaille\npoids\n\n\n\n\nchat\n1.0\n3.0\n\n\nchien\n1.5\n5.0\n\n\nkoala\n1.0\n2.5\n\n\n\n\n\n\n\n\n9.3.1 Les attributs et méthodes utiles\nPour présenter les méthodes les plus pratiques pour l’analyse de données,\non peut partir de l’exemple des consommations de CO2 communales issues\ndes données de l’Ademe. Cette base de données est exploitée plus intensément\ndans le TP.\n\n\n\n\n\n\n\n\n\n\n\n\nL’import de données depuis un fichier plat se fait avec la fonction read_csv:\n\ndf = pd.read_csv(\"https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert\")\ndf\n\n\n\n\n\n\n\n\nINSEE commune\nCommune\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nDéchets\nEnergie\nIndustrie hors-énergie\nRésidentiel\nRoutier\nTertiaire\n\n\n\n\n0\n01001\nL'ABERGEMENT-CLEMENCIAT\n3711.425991\nNaN\nNaN\n432.751835\n101.430476\n2.354558\n6.911213\n309.358195\n793.156501\n367.036172\n\n\n1\n01002\nL'ABERGEMENT-DE-VAREY\n475.330205\nNaN\nNaN\n140.741660\n140.675439\n2.354558\n6.911213\n104.866444\n348.997893\n112.934207\n\n\n2\n01004\nAMBERIEU-EN-BUGEY\n499.043526\n212.577908\nNaN\n10313.446515\n5314.314445\n998.332482\n2930.354461\n16616.822534\n15642.420313\n10732.376934\n\n\n3\n01005\nAMBERIEUX-EN-DOMBES\n1859.160954\nNaN\nNaN\n1144.429311\n216.217508\n94.182310\n276.448534\n663.683146\n1756.341319\n782.404357\n\n\n4\n01006\nAMBLEON\n448.966808\nNaN\nNaN\n77.033834\n48.401549\nNaN\nNaN\n43.714019\n398.786800\n51.681756\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n35793\n95676\nVILLERS-EN-ARTHIES\n1628.065094\nNaN\nNaN\n165.045396\n65.063617\n11.772789\n34.556067\n176.098160\n309.627908\n235.439109\n\n\n35794\n95678\nVILLIERS-ADAM\n698.630772\nNaN\nNaN\n1331.126598\n111.480954\n2.354558\n6.911213\n1395.529811\n18759.370071\n403.404815\n\n\n35795\n95680\nVILLIERS-LE-BEL\n107.564967\nNaN\nNaN\n8367.174532\n225.622903\n534.484607\n1568.845431\n22613.830247\n12217.122402\n13849.512001\n\n\n35796\n95682\nVILLIERS-LE-SEC\n1090.890170\nNaN\nNaN\n326.748418\n108.969749\n2.354558\n6.911213\n67.235487\n4663.232127\n85.657725\n\n\n35797\n95690\nWY-DIT-JOLI-VILLAGE\n1495.103542\nNaN\nNaN\n125.236417\n97.728612\n4.709115\n13.822427\n117.450851\n504.400972\n147.867245\n\n\n\n\n35798 rows × 12 columns\n\n\n\n\n\n Note\nDans un processus de production, où normalement on connait les types des variables du DataFrame qu’on va importer,\nil convient de préciser les types avec lesquels on souhaite importer les données\n(argument dtype, sous la forme d’un dictionnaire).\nCela est particulièrement important lorsqu’on désire utiliser une colonne\ncomme une variable textuelle mais qu’elle comporte des attributs proches d’un nombre\nqui vont inciter pandas à l’importer sous forme de variable numérique.\nPar exemple, une colonne [00001,00002,...] risque d’être importée comme une variable numérique, ignorant l’information des premiers 0 (qui peuvent pourtant la distinguer de la séquence 1, 2, etc.). Pour s’assurer que pandas importe sous forme textuelle la variable, on peut utiliser dtype = {\"code\": \"str\"}\nSinon, on peut importer le csv, et modifier les types avec astype().\nAvec astype, on peut gérer les erreurs de conversion avec le paramètre errors.\n\n\nL’affichage des DataFrames est très ergonomique. On obtiendrait le même output\navec display(df)2. Les premières et dernières lignes s’affichent\nautomatiquement. Autrement, on peut aussi faire:\n\nhead qui permet, comme son\nnom l’indique, de n’afficher que les premières lignes ;\ntail qui permet, comme son\nnom l’indique, de n’afficher que les dernières lignes\nsample qui permet d’afficher un échantillon aléatoire de n lignes.\nCette méthode propose de nombreuses options.\n\n\n\n\n Warning\nIl faut faire attention au display et aux\ncommandes qui révèlent des données (head, tail, etc.)\ndans un Notebook ou un Markdown qui exploite\ndes données confidentielles lorsqu’on utilise Git.\nEn effet, on peut se\nretrouver à partager des données, involontairement, dans l’historique\nGit. Avec un R Markdown, il suffit d’ajouter les sorties au fichier\n.gitignore (par exemple avec une balise de type *.html). Avec un\nNotebook Jupyter, la démarche est plus compliquée car les fichiers\n.ipynb intègrent dans le même document, texte, sorties et mise en forme.\nTechniquement, il est possible d’appliquer des filtres avec Git\n(voir\nici)\nmais c’est une démarche très complexe.\nCe post de l’équipe à l’origine de nbdev2\nrésume bien le problème du contrôle de version avec Git et des solutions qui\npeuvent y être apportées.\nUne solution est d’utiliser Quarto qui permet de générer les\n.ipynb en output d’un document texte, ce qui facilite le contrôle sur les\néléments présents dans le document.\n\n\n\n9.3.2 Dimensions et structure du DataFrame\nLes premières méthodes utiles permettent d’afficher quelques\nattributs d’un DataFrame.\n\ndf.axes\n\n[RangeIndex(start=0, stop=35798, step=1),\n Index(['INSEE commune', 'Commune', 'Agriculture', 'Autres transports',\n        'Autres transports international', 'CO2 biomasse hors-total', 'Déchets',\n        'Energie', 'Industrie hors-énergie', 'Résidentiel', 'Routier',\n        'Tertiaire'],\n       dtype='object')]\n\n\n\ndf.columns\n\nIndex(['INSEE commune', 'Commune', 'Agriculture', 'Autres transports',\n       'Autres transports international', 'CO2 biomasse hors-total', 'Déchets',\n       'Energie', 'Industrie hors-énergie', 'Résidentiel', 'Routier',\n       'Tertiaire'],\n      dtype='object')\n\n\n\ndf.index\n\nRangeIndex(start=0, stop=35798, step=1)\n\n\nPour connaître les dimensions d’un DataFrame, on peut utiliser quelques méthodes\npratiques:\n\ndf.ndim\n\n2\n\n\n\ndf.shape\n\n(35798, 12)\n\n\n\ndf.size\n\n429576\n\n\nPour déterminer le nombre de valeurs uniques d’une variable, plutôt que chercher à écrire soi-même une fonction,\non utilise la\nméthode nunique. Par exemple,\n\ndf['Commune'].nunique()\n\n33338\n\n\npandas propose énormément de méthodes utiles.\nVoici un premier résumé, accompagné d’un comparatif avec R\n\n\n\n\n\n\n\n\n\nOpération\npandas\ndplyr (R)\ndata.table (R)\n\n\n\n\nRécupérer le nom des colonnes\ndf.columns\ncolnames(df)\ncolnames(df)\n\n\nRécupérer les indices[^4]\ndf.index\n\nunique(df[,get(key(df))])\n\n\nRécupérer les dimensions\ndf.shape\nc(nrow(df), ncol(df))\nc(nrow(df), ncol(df))\n\n\nRécupérer le nombre de valeurs uniques d’une variable\ndf['myvar'].nunique()\ndf %&gt;%  summarise(distinct(myvar))\ndf[,uniqueN(myvar)]\n\n\n\n::: {.cell .markdown}\n[^4]: Le principe d’indice n’existe pas dans dplyr. Ce qui s’approche le plus des indices, au sens de\npandas, sont les clés en data.table.\n\n\n\n\n9.3.3 Statistiques agrégées\npandas propose une série de méthodes pour faire des statistiques\nagrégées de manière efficace.\nOn peut, par exemple, appliquer des méthodes pour compter le nombre de lignes,\nfaire une moyenne ou une somme de l’ensemble des lignes\n\ndf.count()\n\nINSEE commune                      35798\nCommune                            35798\nAgriculture                        35736\nAutres transports                   9979\nAutres transports international     2891\nCO2 biomasse hors-total            35798\nDéchets                            35792\nEnergie                            34490\nIndustrie hors-énergie             34490\nRésidentiel                        35792\nRoutier                            35778\nTertiaire                          35798\ndtype: int64\n\n\n\ndf.mean(numeric_only = True)\n\nAgriculture                        2459.975760\nAutres transports                   654.919940\nAutres transports international    7692.344960\nCO2 biomasse hors-total            1774.381550\nDéchets                             410.806329\nEnergie                             662.569846\nIndustrie hors-énergie             2423.127789\nRésidentiel                        1783.677872\nRoutier                            3535.501245\nTertiaire                          1105.165915\ndtype: float64\n\n\n\ndf.sum(numeric_only = True)\n\nAgriculture                        8.790969e+07\nAutres transports                  6.535446e+06\nAutres transports international    2.223857e+07\nCO2 biomasse hors-total            6.351931e+07\nDéchets                            1.470358e+07\nEnergie                            2.285203e+07\nIndustrie hors-énergie             8.357368e+07\nRésidentiel                        6.384140e+07\nRoutier                            1.264932e+08\nTertiaire                          3.956273e+07\ndtype: float64\n\n\n\ndf.nunique()\n\nINSEE commune                      35798\nCommune                            33338\nAgriculture                        35576\nAutres transports                   9963\nAutres transports international     2883\nCO2 biomasse hors-total            35798\nDéchets                            11016\nEnergie                             1453\nIndustrie hors-énergie              1889\nRésidentiel                        35791\nRoutier                            35749\nTertiaire                           8663\ndtype: int64\n\n\n\ndf.quantile(q = [0.1,0.25,0.5,0.75,0.9], numeric_only = True)\n\n\n\n\n\n\n\n\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nDéchets\nEnergie\nIndustrie hors-énergie\nRésidentiel\nRoutier\nTertiaire\n\n\n\n\n0.10\n382.620882\n25.034578\n4.430792\n109.152816\n14.811230\n2.354558\n6.911213\n50.180933\n199.765410\n49.289082\n\n\n0.25\n797.682631\n52.560412\n10.050967\n197.951108\n25.655166\n2.354558\n6.911213\n96.052911\n419.700460\n94.749885\n\n\n0.50\n1559.381285\n106.795928\n19.924343\n424.849988\n54.748653\n4.709115\n13.822427\n227.091193\n1070.895593\n216.297718\n\n\n0.75\n3007.883903\n237.341501\n32.983111\n1094.749825\n110.820941\n51.800270\n152.046694\n749.469293\n3098.612157\n576.155869\n\n\n0.90\n5442.727470\n528.349529\n59.999169\n3143.759029\n190.695774\n367.311008\n1154.172630\n2937.699671\n8151.047248\n1897.732565\n\n\n\n\n\n\n\n\n\n Warning\nLa version 2.0 de Pandas a introduit un changement\nde comportement dans les méthodes d’agrégation.\nIl est dorénavant nécessaire de préciser quand on désire\neffectuer des opérations si on désire ou non le faire\nexclusivement sur les colonnes numériques. C’est pour cette\nraison qu’on exlicite ici l’argument numeric_only = True.\nCe comportement\nétait par le passé implicite.\n\n\nIl faut toujours regarder les options de ces fonctions en termes de valeurs manquantes, car\nces options sont déterminantes dans le résultat obtenu.\nLes exercices de TD visent à démontrer l’intérêt de ces méthodes dans quelques cas précis.\n\n\n\n\n\n\n\n\n\n\n\n\nLe tableau suivant récapitule le code équivalent pour avoir des\nstatistiques sur toutes les colonnes d’un dataframe en R.\n\n\n\n\n\n\n\n\n\nOpération\npandas\ndplyr (R)\ndata.table (R)\n\n\n\n\nNombre de valeurs non manquantes\ndf.count()\ndf %&gt;% summarise_each(funs(sum(!is.na(.))))\ndf[, lapply(.SD, function(x) sum(!is.na(x)))]\n\n\nMoyenne de toutes les variables\ndf.mean()\ndf %&gt;% summarise_each(funs(mean((., na.rm = TRUE))))\ndf[,lapply(.SD, function(x) mean(x, na.rm = TRUE))]\n\n\n\nLa méthode describe permet de sortir un tableau de statistiques\nagrégées:\n\ndf.describe()\n\n\n\n\n\n\n\n\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nDéchets\nEnergie\nIndustrie hors-énergie\nRésidentiel\nRoutier\nTertiaire\n\n\n\n\ncount\n35736.000000\n9979.000000\n2.891000e+03\n35798.000000\n35792.000000\n3.449000e+04\n3.449000e+04\n35792.000000\n35778.000000\n35798.000000\n\n\nmean\n2459.975760\n654.919940\n7.692345e+03\n1774.381550\n410.806329\n6.625698e+02\n2.423128e+03\n1783.677872\n3535.501245\n1105.165915\n\n\nstd\n2926.957701\n9232.816833\n1.137643e+05\n7871.341922\n4122.472608\n2.645571e+04\n5.670374e+04\n8915.902379\n9663.156628\n5164.182507\n\n\nmin\n0.003432\n0.000204\n3.972950e-04\n3.758088\n0.132243\n2.354558e+00\n1.052998e+00\n1.027266\n0.555092\n0.000000\n\n\n25%\n797.682631\n52.560412\n1.005097e+01\n197.951108\n25.655166\n2.354558e+00\n6.911213e+00\n96.052911\n419.700460\n94.749885\n\n\n50%\n1559.381285\n106.795928\n1.992434e+01\n424.849988\n54.748653\n4.709115e+00\n1.382243e+01\n227.091193\n1070.895593\n216.297718\n\n\n75%\n3007.883903\n237.341501\n3.298311e+01\n1094.749825\n110.820941\n5.180027e+01\n1.520467e+02\n749.469293\n3098.612157\n576.155869\n\n\nmax\n98949.317760\n513140.971691\n3.303394e+06\n576394.181208\n275500.374439\n2.535858e+06\n6.765119e+06\n410675.902028\n586054.672836\n288175.400126\n\n\n\n\n\n\n\n\n\n9.3.4 Méthodes relatives aux valeurs manquantes\nLes méthodes relatives aux valeurs manquantes peuvent être mobilisées\nen conjonction des méthodes de statistiques agrégées. C’est utiles lorsqu’on\ndésire obtenir une idée de la part de valeurs manquantes dans un jeu de\ndonnées\n\ndf.isnull().sum()\n\nINSEE commune                          0\nCommune                                0\nAgriculture                           62\nAutres transports                  25819\nAutres transports international    32907\nCO2 biomasse hors-total                0\nDéchets                                6\nEnergie                             1308\nIndustrie hors-énergie              1308\nRésidentiel                            6\nRoutier                               20\nTertiaire                              0\ndtype: int64\n\n\nOn trouvera aussi la référence à isna() qui est la même méthode que isnull()."
  },
  {
    "objectID": "content/course/manipulation/02a_pandas_tutorial/index.html#graphiques-rapides",
    "href": "content/course/manipulation/02a_pandas_tutorial/index.html#graphiques-rapides",
    "title": "9  Introduction à Pandas",
    "section": "9.4 Graphiques rapides",
    "text": "9.4 Graphiques rapides\nLes méthodes par défaut de graphique\n(approfondies dans la partie visualisation)\nsont pratiques pour\nproduire rapidement un graphique, notamment après des opérations\ncomplexes de maniement de données.\nEn effet, on peut appliquer la méthode plot() directement à une pandas.Series:\n\ndf['Déchets'].plot()\ndf['Déchets'].hist()\ndf['Déchets'].plot(kind = 'hist', logy = True)\n\n\nplt.figure()\nfig = df['Déchets'].plot()\nfig\n#plt.savefig('plot_base.png', bbox_inches='tight')\n\nplt.figure()\nfig = df['Déchets'].hist()\nfig\n#plt.savefig('plot_hist.png', bbox_inches='tight')\n\nplt.figure()\nfig = df['Déchets'].plot(kind = 'hist', logy = True)\nfig\n#plt.show()\n#plt.savefig('plot_hist_log.png', bbox_inches='tight')\n\n&lt;Axes: ylabel='Frequency'&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa sortie est un objet matplotlib. La customisation de ces\nfigures est ainsi\npossible (et même désirable car les graphiques matplotlib\nsont, par défaut, assez rudimentaires), nous en verrons quelques exemples."
  },
  {
    "objectID": "content/course/manipulation/02a_pandas_tutorial/index.html#accéder-à-des-éléments-dun-dataframe",
    "href": "content/course/manipulation/02a_pandas_tutorial/index.html#accéder-à-des-éléments-dun-dataframe",
    "title": "9  Introduction à Pandas",
    "section": "9.5 Accéder à des éléments d’un DataFrame",
    "text": "9.5 Accéder à des éléments d’un DataFrame\n\n9.5.1 Sélectionner des colonnes\nEn SQL, effectuer des opérations sur les colonnes se fait avec la commande\nSELECT. Avec pandas,\npour accéder à une colonne dans son ensemble on peut\nutiliser plusieurs approches:\n\ndataframe.variable, par exemple df.Energie.\nCette méthode requiert néanmoins d’avoir des\nnoms de colonnes sans espace.\ndataframe[['variable']] pour renvoyer la variable sous\nforme de DataFrame ou dataframe['variable'] pour\nla renvoyer sous forme de Series. Par exemple, df[['Autres transports']]\nou df['Autres transports']. C’est une manière préférable de procéder.\n\n\n\n9.5.2 Accéder à des lignes\nPour accéder à une ou plusieurs valeurs d’un DataFrame,\nil existe deux manières conseillées de procéder, selon la\nforme des indices de lignes ou colonnes utilisés:\n\ndf.loc: utilise les labels\ndf.iloc: utilise les indices\n\n\n\n Warning\nLes bouts de code utilisant la structure df.ix\nsont à bannir car la fonction est deprecated et peut\nainsi disparaître à tout moment.\n\n\niloc va se référer à l’indexation de 0 à N où N est égal à df.shape[0] d’un\npandas.DataFrame. loc va se référer aux valeurs de l’index\nde df.\nPar exemple, avec le pandas.DataFrame df_example:\n\ndf_example = pd.DataFrame(\n    {'year': [2012, 2014, 2013, 2014], 'sale': [55, 40, 84, 31]})\ndf_example\n\n\n\n\n\n\n\n\nyear\nsale\n\n\n\n\n0\n2012\n55\n\n\n1\n2014\n40\n\n\n2\n2013\n84\n\n\n3\n2014\n31\n\n\n\n\n\n\n\n\ndf_example.loc[1, :] donnera la première ligne de df (ligne où l’indice month est égal à 1) ;\ndf_example.iloc[1, :] donnera la deuxième ligne (puisque l’indexation en Python commence à 0) ;\ndf_example.iloc[:, 1] donnera la deuxième colonne, suivant le même principe."
  },
  {
    "objectID": "content/course/manipulation/02a_pandas_tutorial/index.html#principales-manipulation-de-données",
    "href": "content/course/manipulation/02a_pandas_tutorial/index.html#principales-manipulation-de-données",
    "title": "9  Introduction à Pandas",
    "section": "9.6 Principales manipulation de données",
    "text": "9.6 Principales manipulation de données\nL’objectif du TP pandas est de se familiariser plus avec ces\ncommandes à travers l’exemple des données des émissions de C02.\nLes opérations les plus fréquentes en SQL sont résumées par le tableau suivant.\nIl est utile de les connaître (beaucoup de syntaxes de maniement de données\nreprennent ces termes) car, d’une\nmanière ou d’une autre, elles couvrent la plupart\ndes usages de manipulation des données\n\n\n\n\n\n\n\n\n\n\nOpération\nSQL\npandas\ndplyr (R)\ndata.table (R)\n\n\n\n\nSélectionner des variables par leur nom\nSELECT\ndf[['Autres transports','Energie']]\ndf %&gt;% select(Autres transports, Energie)\ndf[, c('Autres transports','Energie')]\n\n\nSélectionner des observations selon une ou plusieurs conditions;\nFILTER\ndf[df['Agriculture']&gt;2000]\ndf %&gt;% filter(Agriculture&gt;2000)\ndf[Agriculture&gt;2000]\n\n\nTrier la table selon une ou plusieurs variables\nSORT BY\ndf.sort_values(['Commune','Agriculture'])\ndf %&gt;% arrange(Commune, Agriculture)\ndf[order(Commune, Agriculture)]\n\n\nAjouter des variables qui sont fonction d’autres variables;\nSELECT *, LOG(Agriculture) AS x FROM df\ndf['x'] = np.log(df['Agriculture'])\ndf %&gt;% mutate(x = log(Agriculture))\ndf[,x := log(Agriculture)]\n\n\nEffectuer une opération par groupe\nGROUP BY\ndf.groupby('Commune').mean()\ndf %&gt;% group_by(Commune) %&gt;% summarise(m = mean)\ndf[,mean(Commune), by = Commune]\n\n\nJoindre deux bases de données (inner join)\nSELECT * FROM table1 INNER JOIN table2 ON table1.id = table2.x\ntable1.merge(table2, left_on = 'id', right_on = 'x')\ntable1 %&gt;% inner_join(table2, by = c('id'='x'))\nmerge(table1, table2, by.x = 'id', by.y = 'x')\n\n\n\n\n9.6.1 Opérations sur les colonnes: select, mutate, drop\nLes DataFrames pandas sont des objets mutables en langage Python,\nc’est-à-dire qu’il est possible de faire évoluer le DataFrame au grès\ndes opérations. L’opération la plus classique consiste à ajouter ou retirer\ndes variables à la table de données.\n\ndf_new = df.copy()\n\n\n\n Warning\nAttention au comportement de pandas lorsqu’on crée une duplication\nd’un DataFrame.\nPar défaut, pandas effectue une copie par référence. Dans ce\ncas, les deux objets (la copie et l’objet copié) restent reliés. Les colonnes\ncrées sur l’un vont être répercutées sur l’autre. Ce comportement permet de\nlimiter l’inflation en mémoire de Python. En faisant ça, le deuxième\nobjet prend le même espace mémoire que le premier. Le package data.table\nen R adopte le même comportement, contrairement à dplyr.\nCela peut amener à quelques surprises si ce comportement d’optimisation\nn’est pas anticipé. Si vous voulez, par sécurité, conserver intact le\npremier DataFrame, faites appel à une copie profonde (deep copy) en\nutilisant la méthode copy, comme ci-dessus.\nAttention toutefois, cela a un coût mémoire.\nAvec des données volumineuses, c’est une pratique à utiliser avec précaution.\n\n\nLa manière la plus simple d’opérer pour ajouter des colonnes est\nd’utiliser la réassignation. Par exemple, pour créer une variable\nx qui est le log de la\nvariable Agriculture:\n\ndf_new['x'] = np.log(df_new['Agriculture'])\n\nIl est possible d’appliquer cette approche sur plusieurs colonnes. Un des\nintérêts de cette approche est qu’elle permet de recycler le nom de colonnes.\n\nvars = ['Agriculture', 'Déchets', 'Energie']\n\ndf_new[[v + \"_log\" for v in vars]] = np.log(df_new[vars])\ndf_new\n\n\n\n\n\n\n\n\nINSEE commune\nCommune\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nDéchets\nEnergie\nIndustrie hors-énergie\nRésidentiel\nRoutier\nTertiaire\nx\nAgriculture_log\nDéchets_log\nEnergie_log\n\n\n\n\n0\n01001\nL'ABERGEMENT-CLEMENCIAT\n3711.425991\nNaN\nNaN\n432.751835\n101.430476\n2.354558\n6.911213\n309.358195\n793.156501\n367.036172\n8.219171\n8.219171\n4.619374\n0.856353\n\n\n1\n01002\nL'ABERGEMENT-DE-VAREY\n475.330205\nNaN\nNaN\n140.741660\n140.675439\n2.354558\n6.911213\n104.866444\n348.997893\n112.934207\n6.164010\n6.164010\n4.946455\n0.856353\n\n\n2\n01004\nAMBERIEU-EN-BUGEY\n499.043526\n212.577908\nNaN\n10313.446515\n5314.314445\n998.332482\n2930.354461\n16616.822534\n15642.420313\n10732.376934\n6.212693\n6.212693\n8.578159\n6.906086\n\n\n3\n01005\nAMBERIEUX-EN-DOMBES\n1859.160954\nNaN\nNaN\n1144.429311\n216.217508\n94.182310\n276.448534\n663.683146\n1756.341319\n782.404357\n7.527881\n7.527881\n5.376285\n4.545232\n\n\n4\n01006\nAMBLEON\n448.966808\nNaN\nNaN\n77.033834\n48.401549\nNaN\nNaN\n43.714019\n398.786800\n51.681756\n6.106949\n6.106949\n3.879532\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n35793\n95676\nVILLERS-EN-ARTHIES\n1628.065094\nNaN\nNaN\n165.045396\n65.063617\n11.772789\n34.556067\n176.098160\n309.627908\n235.439109\n7.395148\n7.395148\n4.175366\n2.465791\n\n\n35794\n95678\nVILLIERS-ADAM\n698.630772\nNaN\nNaN\n1331.126598\n111.480954\n2.354558\n6.911213\n1395.529811\n18759.370071\n403.404815\n6.549122\n6.549122\n4.713854\n0.856353\n\n\n35795\n95680\nVILLIERS-LE-BEL\n107.564967\nNaN\nNaN\n8367.174532\n225.622903\n534.484607\n1568.845431\n22613.830247\n12217.122402\n13849.512001\n4.678095\n4.678095\n5.418865\n6.281303\n\n\n35796\n95682\nVILLIERS-LE-SEC\n1090.890170\nNaN\nNaN\n326.748418\n108.969749\n2.354558\n6.911213\n67.235487\n4663.232127\n85.657725\n6.994749\n6.994749\n4.691070\n0.856353\n\n\n35797\n95690\nWY-DIT-JOLI-VILLAGE\n1495.103542\nNaN\nNaN\n125.236417\n97.728612\n4.709115\n13.822427\n117.450851\n504.400972\n147.867245\n7.309951\n7.309951\n4.582194\n1.549500\n\n\n\n\n35798 rows × 16 columns\n\n\n\nIl est également possible d’utiliser la méthode assign. Pour des opérations\nvectorisées, comme le sont les opérateurs de numpy, cela n’a pas d’intérêt.\nCela permet notamment d’enchainer les opérations sur un même DataFrame (notamment grâce au pipe que\nnous verrons plus loin).\nCette approche utilise généralement\ndes lambda functions. Par exemple le code précédent (celui concernant une\nseule variable) prendrait la forme:\n\ndf_new.assign(Energie_log = lambda x: np.log(x['Energie']))\n\n\n\n\n\n\n\n\nINSEE commune\nCommune\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nDéchets\nEnergie\nIndustrie hors-énergie\nRésidentiel\nRoutier\nTertiaire\nx\nAgriculture_log\nDéchets_log\nEnergie_log\n\n\n\n\n0\n01001\nL'ABERGEMENT-CLEMENCIAT\n3711.425991\nNaN\nNaN\n432.751835\n101.430476\n2.354558\n6.911213\n309.358195\n793.156501\n367.036172\n8.219171\n8.219171\n4.619374\n0.856353\n\n\n1\n01002\nL'ABERGEMENT-DE-VAREY\n475.330205\nNaN\nNaN\n140.741660\n140.675439\n2.354558\n6.911213\n104.866444\n348.997893\n112.934207\n6.164010\n6.164010\n4.946455\n0.856353\n\n\n2\n01004\nAMBERIEU-EN-BUGEY\n499.043526\n212.577908\nNaN\n10313.446515\n5314.314445\n998.332482\n2930.354461\n16616.822534\n15642.420313\n10732.376934\n6.212693\n6.212693\n8.578159\n6.906086\n\n\n3\n01005\nAMBERIEUX-EN-DOMBES\n1859.160954\nNaN\nNaN\n1144.429311\n216.217508\n94.182310\n276.448534\n663.683146\n1756.341319\n782.404357\n7.527881\n7.527881\n5.376285\n4.545232\n\n\n4\n01006\nAMBLEON\n448.966808\nNaN\nNaN\n77.033834\n48.401549\nNaN\nNaN\n43.714019\n398.786800\n51.681756\n6.106949\n6.106949\n3.879532\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n35793\n95676\nVILLERS-EN-ARTHIES\n1628.065094\nNaN\nNaN\n165.045396\n65.063617\n11.772789\n34.556067\n176.098160\n309.627908\n235.439109\n7.395148\n7.395148\n4.175366\n2.465791\n\n\n35794\n95678\nVILLIERS-ADAM\n698.630772\nNaN\nNaN\n1331.126598\n111.480954\n2.354558\n6.911213\n1395.529811\n18759.370071\n403.404815\n6.549122\n6.549122\n4.713854\n0.856353\n\n\n35795\n95680\nVILLIERS-LE-BEL\n107.564967\nNaN\nNaN\n8367.174532\n225.622903\n534.484607\n1568.845431\n22613.830247\n12217.122402\n13849.512001\n4.678095\n4.678095\n5.418865\n6.281303\n\n\n35796\n95682\nVILLIERS-LE-SEC\n1090.890170\nNaN\nNaN\n326.748418\n108.969749\n2.354558\n6.911213\n67.235487\n4663.232127\n85.657725\n6.994749\n6.994749\n4.691070\n0.856353\n\n\n35797\n95690\nWY-DIT-JOLI-VILLAGE\n1495.103542\nNaN\nNaN\n125.236417\n97.728612\n4.709115\n13.822427\n117.450851\n504.400972\n147.867245\n7.309951\n7.309951\n4.582194\n1.549500\n\n\n\n\n35798 rows × 16 columns\n\n\n\nDans les méthodes suivantes, il est possible de modifier le pandas.DataFrame\nen place, c’est à dire en ne le réassignant pas, avec le paramètre inplace = True.\nPar défaut, inplace est égal à False et pour modifier le pandas.DataFrame,\nil convient de le réassigner.\nOn peut facilement renommer des variables avec la méthode rename qui\nfonctionne bien avec des dictionnaires (pour renommer des colonnes il faut\npréciser le paramètre axis = 1):\n\ndf_new = df_new.rename({\"Energie\": \"eneg\", \"Agriculture\": \"agr\"}, axis=1)\n\nEnfin, pour effacer des colonnes, on utilise la méthode drop avec l’argument\ncolumns:\n\ndf_new = df_new.drop(columns = [\"eneg\", \"agr\"])\n\n\n\n9.6.2 Réordonner\nLa méthode sort_values permet de réordonner un DataFrame. Par exemple,\nsi on désire classer par ordre décroissant de consommation de CO2 du secteur\nrésidentiel, on fera\n\ndf = df.sort_values(\"Résidentiel\", ascending = False)\n\nAinsi, en une ligne de code, on identifie les villes où le secteur\nrésidentiel consomme le plus.\n\n\n9.6.3 Filtrer\nL’opération de sélection de lignes s’appelle FILTER en SQL. Elle s’utilise\nen fonction d’une condition logique (clause WHERE). On sélectionne les\ndonnées sur une condition logique. Il existe plusieurs méthodes en pandas.\nLa plus simple est d’utiliser les boolean mask, déjà vus dans le chapitre\nnumpy.\nPar exemple, pour sélectionner les communes dans les Hauts-de-Seine, on\npeut utiliser le résultat de la méthode str.startswith (qui renvoie\nTrue ou False) directement dans les crochets:\n\ndf[df['INSEE commune'].str.startswith(\"92\")].head(2)\n\n\n\n\n\n\n\n\nINSEE commune\nCommune\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nDéchets\nEnergie\nIndustrie hors-énergie\nRésidentiel\nRoutier\nTertiaire\n\n\n\n\n35494\n92012\nBOULOGNE-BILLANCOURT\nNaN\n1250.483441\n34.234669\n51730.704250\n964.828694\n8817.818741\n25882.493998\n92216.971456\n64985.280901\n60349.109482\n\n\n35501\n92025\nCOLOMBES\nNaN\n411.371588\n14.220061\n53923.847088\n698.685861\n12855.885267\n50244.664227\n87469.549463\n52070.927943\n41526.600867\n\n\n\n\n\n\n\nPour remplacer des valeurs spécifiques, on utilise la méthode where ou une\nréassignation couplée à la méthode précédente.\nPar exemple, pour assigner des valeurs manquantes aux départements du 92,\non peut faire cela\n\ndf_copy = df.copy()\ndf_copy = df_copy.where(~df['INSEE commune'].str.startswith(\"92\"))\n\net vérifier les résultats:\n\ndf_copy[df['INSEE commune'].str.startswith(\"92\")].head(2)\ndf_copy[~df['INSEE commune'].str.startswith(\"92\")].head(2)\n\n\n\n\n\n\n\n\nINSEE commune\nCommune\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nDéchets\nEnergie\nIndustrie hors-énergie\nRésidentiel\nRoutier\nTertiaire\n\n\n\n\n12167\n31555\nTOULOUSE\n1434.045233\n4482.980062\n130.792683\n576394.181208\n88863.732538\n91549.914092\n277062.573234\n410675.902028\n586054.672836\n288175.400126\n\n\n16774\n44109\nNANTES\n248.019465\n138738.544337\n250814.701179\n193478.248177\n18162.261628\n17461.400209\n77897.138554\n354259.013785\n221068.632724\n173447.582779\n\n\n\n\n\n\n\nou alors utiliser une réassignation plus classique:\n\ndf_copy = df.copy()\ndf_copy[df_copy['INSEE commune'].str.startswith(\"92\")] = np.nan\n\nIl est conseillé de filtrer avec loc en utilisant un masque.\nEn effet, contrairement à df[mask], df.loc[mask, :] permet d’indiquer clairement\nà Python que l’on souhaite appliquer le masque aux labels de l’index.\nCe n’est pas le cas avec df[mask].\nD’ailleurs, lorsqu’on utilise la syntaxe df[mask], pandas renvoie généralement un warning\n\n\n9.6.4 Opérations par groupe\nEn SQL, il est très simple de découper des données pour\neffectuer des opérations sur des blocs cohérents et recollecter des résultats\ndans la dimension appropriée.\nLa logique sous-jacente est celle du split-apply-combine qui est repris\npar les langages de manipulation de données, auxquels pandas\nne fait pas exception.\nL’image suivante, issue de\nce site\nreprésente bien la manière dont fonctionne l’approche\nsplit-apply-combine\n\n\n\nSplit-apply-combine\n\n\nCe tutoriel sur le sujet\nest particulièrement utile.\nPour donner quelques exemples, on peut créer une variable départementale qui\nservira de critère de groupe.\n\ndf['dep'] = df['INSEE commune'].str[:2]\n\nEn pandas, on utilise groupby pour découper les données selon un ou\nplusieurs axes. Techniquement, cette opération consiste à créer une association\nentre des labels (valeurs des variables de groupe) et des\nobservations.\nPar exemple, pour compter le nombre de communes par département en SQL, on\nutiliserait la requête suivante:\nSELECT dep, count(INSEE commune)\nFROM df\nGROUP BY dep;\nCe qui, en pandas, donne:\n\ndf.groupby('dep')[\"INSEE commune\"].count()\n\ndep\n01    410\n02    805\n03    318\n04    199\n05    168\n     ... \n91    196\n92     36\n93     40\n94     47\n95    185\nName: INSEE commune, Length: 96, dtype: int64\n\n\nLa syntaxe est quasiment transparente. On peut bien-sûr effectuer des opérations\npar groupe sur plusieurs colonnes. Par exemple,\n\ndf.groupby('dep').mean(numeric_only = True)\n\n\n\n\n\n\n\n\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nDéchets\nEnergie\nIndustrie hors-énergie\nRésidentiel\nRoutier\nTertiaire\n\n\ndep\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n01\n1974.535382\n100.307344\n8.900375\n1736.353087\n671.743966\n280.485435\n1744.567552\n1346.982227\n3988.658995\n1021.089078\n\n\n02\n1585.417729\n202.878748\n17.390638\n767.072924\n223.907551\n76.316247\n932.135611\n793.615867\n1722.240298\n403.744266\n\n\n03\n6132.029417\n240.076499\n45.429978\n1779.630883\n349.746819\n326.904841\n1452.423506\n1401.650215\n3662.773062\n705.937016\n\n\n04\n1825.455590\n177.321816\nNaN\n583.198128\n253.975910\n62.808435\n313.913553\n587.116013\n1962.654370\n493.609329\n\n\n05\n1847.508592\n141.272766\nNaN\n502.012857\n132.548068\n34.971220\n102.649239\n728.734494\n2071.010178\n463.604908\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n91\n802.793163\n10114.998156\n73976.107892\n3716.906101\n1496.516194\n538.761253\n1880.810170\n6532.123033\n10578.452789\n3866.757200\n\n\n92\n8.309835\n362.964554\n13.132461\n29663.579634\n7347.163353\n6745.611611\n19627.706224\n40744.279029\n33289.456629\n23222.587595\n\n\n93\n50.461775\n1753.443710\n61188.896632\n18148.789684\n6304.173594\n2570.941598\n10830.409025\n32911.305703\n35818.236459\n21575.444794\n\n\n94\n48.072971\n5474.808839\n16559.384091\n14710.744314\n4545.099181\n1624.281505\n9940.192318\n28444.561597\n24881.531613\n16247.876321\n\n\n95\n609.172047\n682.143912\n37984.576873\n3408.871963\n1334.032970\n463.860672\n1729.692179\n6684.181989\n8325.948748\n4014.985843\n\n\n\n\n96 rows × 10 columns\n\n\n\nA noter que la variable de groupe, ici dep, devient, par défaut, l’index\ndu DataFrame de sortie. Si on avait utilisé plusieurs variables de groupe,\non obtiendrait un objet multi-indexé. Sur la gestion des multi-index, on\npourra se référer à l’ouvrage Modern Pandas dont la référence est\ndonnée en fin de cours.\nTant qu’on n’appelle pas une action sur un DataFrame par groupe, du type\nhead ou display, pandas n’effectue aucune opération. On parle de\nlazy evaluation. Par exemple, le résultat de df.groupby('dep') est\nune transformation qui n’est pas encore évaluée:\n\ndf.groupby('dep')\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f33611d1a00&gt;\n\n\nIl est possible d’appliquer plus d’une opération à la fois grâce à la méthode\nagg. Par exemple, pour obtenir à la fois le minimum, la médiane et le maximum\nde chaque département, on peut faire:\n\nnumeric_columns = df.select_dtypes(['number']).columns\ndf.loc[:, numeric_columns.tolist() + [\"dep\"] ].groupby('dep').agg(['min',\"median\",\"max\"], numeric_only = True)\n\n\n\n\n\n\n\n\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\n...\nIndustrie hors-énergie\nRésidentiel\nRoutier\nTertiaire\n\n\n\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\n...\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\n\n\ndep\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n01\n0.003432\n1304.519570\n14402.057335\n3.307596\n75.686090\n617.281080\n0.297256\n6.985161\n2.209492e+01\n30.571400\n...\n175185.892467\n9.607822\n351.182294\n57689.832901\n20.848982\n1598.934149\n45258.256406\n10.049230\n401.490676\n30847.366865\n\n\n02\n0.391926\n1205.725078\n13257.716591\n0.326963\n130.054615\n1126.961565\n0.517437\n15.492120\n5.799402e+01\n28.294993\n...\n220963.067245\n7.849347\n138.819865\n99038.124236\n22.936184\n700.826152\n49245.101730\n6.220952\n130.639994\n34159.345750\n\n\n03\n5.041238\n5382.194339\n24912.249269\n24.158870\n144.403590\n1433.217868\n29.958027\n42.762328\n8.269019e+01\n44.825515\n...\n154061.446374\n19.441088\n217.959697\n75793.882483\n120.667614\n1426.905646\n40957.845304\n17.705787\n191.892445\n31099.772884\n\n\n04\n30.985972\n1404.752852\n11423.535554\n33.513854\n158.780500\n362.637639\nNaN\nNaN\nNaN\n7.162928\n...\n16889.531061\n1.708652\n133.130946\n18088.189529\n30.206298\n687.390045\n31438.078325\n0.957070\n122.504902\n16478.024806\n\n\n05\n38.651727\n1520.896526\n13143.465812\n0.299734\n139.754980\n456.042002\nNaN\nNaN\nNaN\n20.931602\n...\n4271.129851\n6.871678\n211.945147\n46486.555748\n57.132270\n958.506314\n37846.651181\n4.785348\n151.695524\n23666.235898\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n91\n0.400740\n516.908303\n5965.349174\n25.785594\n177.177127\n513140.971691\n1.651873\n14.762210\n7.858782e+05\n41.661474\n...\n50288.560827\n15.886514\n2580.902085\n48464.979708\n20.260110\n3610.009634\n72288.020125\n36.368643\n1428.426303\n38296.204729\n\n\n92\n0.073468\n6.505185\n32.986132\n7.468879\n297.529178\n1250.483441\n1.104401\n11.482381\n3.423467e+01\n2173.614704\n...\n95840.512400\n4122.277198\n33667.904692\n92216.971456\n4968.382962\n23516.458236\n113716.853033\n800.588678\n18086.633085\n65043.364499\n\n\n93\n3.308495\n3.308495\n1362.351634\n24.188172\n320.755486\n45251.869710\n0.171075\n12.449476\n1.101146e+06\n899.762120\n...\n89135.302368\n4364.038661\n31428.227282\n87927.730552\n1632.496185\n22506.758771\n193039.792609\n2257.370945\n20864.923339\n71918.163984\n\n\n94\n1.781885\n1.781885\n556.939161\n6.249609\n294.204166\n103252.271268\n0.390223\n14.944807\n1.571965e+05\n928.232154\n...\n96716.055178\n2668.358896\n24372.900300\n100948.169898\n1266.101605\n19088.651049\n97625.957714\n1190.115985\n14054.223449\n58528.623477\n\n\n95\n8.779506\n445.279844\n2987.287417\n1.749091\n80.838639\n44883.982753\n0.201508\n13.149987\n1.101131e+06\n13.490977\n...\n66216.914749\n11.585833\n1434.343631\n104543.465908\n2.619451\n3417.197938\n147040.904455\n11.484835\n725.467969\n61497.821477\n\n\n\n\n96 rows × 30 columns\n\n\n\nLa première ligne est présente pour nous faciliter la récupération des noms de colonnes des variables\nnumériques\n\n\n9.6.5 Appliquer des fonctions\npandas est, comme on a pu le voir, un package très flexible, qui\npropose une grande variété de méthodes optimisées. Cependant, il est fréquent\nd’avoir besoin de méthodes non implémentées.\nDans ce cas, on recourt souvent aux lambda functions. Par exemple, si\non désire connaître les communes dont le nom fait plus de 40 caractères,\non peut appliquer la fonction len de manière itérative:\n\n# Noms de communes superieurs à 40 caracteres\ndf[df['Commune'].apply(lambda s: len(s)&gt;40)]\n\n\n\n\n\n\n\n\nINSEE commune\nCommune\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nDéchets\nEnergie\nIndustrie hors-énergie\nRésidentiel\nRoutier\nTertiaire\ndep\n\n\n\n\n28082\n70058\nBEAUJEU-SAINT-VALLIER-PIERREJUX-ET-QUITTEUR\n4024.909815\n736.948351\n41.943384\n1253.135313\n125.101996\n2.354558\n6.911213\n549.734302\n1288.215480\n452.693897\n70\n\n\n4984\n14621\nSAINT-MARTIN-DE-BIENFAITE-LA-CRESSONNIERE\n1213.333523\nNaN\nNaN\n677.571743\n72.072503\n63.573059\n186.602760\n298.261044\n1396.353375\n260.801452\n14\n\n\n19276\n51513\nSAINT-REMY-EN-BOUZEMONT-SAINT-GENEST-ET-ISSON\n1927.401921\nNaN\nNaN\n595.583152\n71.675773\n4.709115\n13.822427\n273.826687\n521.864748\n259.365848\n51\n\n\n5402\n16053\nBORS (CANTON DE BAIGNES-SAINTE-RADEGONDE)\n1919.249545\nNaN\nNaN\n165.443226\n16.265904\n2.354558\n6.911213\n54.561623\n719.293151\n58.859777\n16\n\n\n\n\n\n\n\nCependant, toutes les lambda functions ne se justifient pas.\nPar exemple, prenons\nle résultat d’agrégation précédent. Imaginons qu’on désire avoir les résultats\nen milliers de tonnes. Dans ce cas, le premier réflexe est d’utiliser\nla lambda function suivante:\n\nnumeric_columns = df.select_dtypes(['number']).columns\n(df\n    .loc[:, numeric_columns.tolist() + [\"dep\"] ]\n    .groupby('dep')\n    .agg(['min',\"median\",\"max\"])\n    .apply(lambda s: s/1000)\n)\n\n\n\n\n\n\n\n\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\n...\nIndustrie hors-énergie\nRésidentiel\nRoutier\nTertiaire\n\n\n\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\n...\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\n\n\ndep\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n01\n0.000003\n1.304520\n14.402057\n0.003308\n0.075686\n0.617281\n0.000297\n0.006985\n0.022095\n0.030571\n...\n175.185892\n0.009608\n0.351182\n57.689833\n0.020849\n1.598934\n45.258256\n0.010049\n0.401491\n30.847367\n\n\n02\n0.000392\n1.205725\n13.257717\n0.000327\n0.130055\n1.126962\n0.000517\n0.015492\n0.057994\n0.028295\n...\n220.963067\n0.007849\n0.138820\n99.038124\n0.022936\n0.700826\n49.245102\n0.006221\n0.130640\n34.159346\n\n\n03\n0.005041\n5.382194\n24.912249\n0.024159\n0.144404\n1.433218\n0.029958\n0.042762\n0.082690\n0.044826\n...\n154.061446\n0.019441\n0.217960\n75.793882\n0.120668\n1.426906\n40.957845\n0.017706\n0.191892\n31.099773\n\n\n04\n0.030986\n1.404753\n11.423536\n0.033514\n0.158781\n0.362638\nNaN\nNaN\nNaN\n0.007163\n...\n16.889531\n0.001709\n0.133131\n18.088190\n0.030206\n0.687390\n31.438078\n0.000957\n0.122505\n16.478025\n\n\n05\n0.038652\n1.520897\n13.143466\n0.000300\n0.139755\n0.456042\nNaN\nNaN\nNaN\n0.020932\n...\n4.271130\n0.006872\n0.211945\n46.486556\n0.057132\n0.958506\n37.846651\n0.004785\n0.151696\n23.666236\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n91\n0.000401\n0.516908\n5.965349\n0.025786\n0.177177\n513.140972\n0.001652\n0.014762\n785.878155\n0.041661\n...\n50.288561\n0.015887\n2.580902\n48.464980\n0.020260\n3.610010\n72.288020\n0.036369\n1.428426\n38.296205\n\n\n92\n0.000073\n0.006505\n0.032986\n0.007469\n0.297529\n1.250483\n0.001104\n0.011482\n0.034235\n2.173615\n...\n95.840512\n4.122277\n33.667905\n92.216971\n4.968383\n23.516458\n113.716853\n0.800589\n18.086633\n65.043364\n\n\n93\n0.003308\n0.003308\n1.362352\n0.024188\n0.320755\n45.251870\n0.000171\n0.012449\n1101.145545\n0.899762\n...\n89.135302\n4.364039\n31.428227\n87.927731\n1.632496\n22.506759\n193.039793\n2.257371\n20.864923\n71.918164\n\n\n94\n0.001782\n0.001782\n0.556939\n0.006250\n0.294204\n103.252271\n0.000390\n0.014945\n157.196520\n0.928232\n...\n96.716055\n2.668359\n24.372900\n100.948170\n1.266102\n19.088651\n97.625958\n1.190116\n14.054223\n58.528623\n\n\n95\n0.008780\n0.445280\n2.987287\n0.001749\n0.080839\n44.883983\n0.000202\n0.013150\n1101.131222\n0.013491\n...\n66.216915\n0.011586\n1.434344\n104.543466\n0.002619\n3.417198\n147.040904\n0.011485\n0.725468\n61.497821\n\n\n\n\n96 rows × 30 columns\n\n\n\nEn effet, cela effectue le résultat désiré. Cependant, il y a mieux: utiliser\nla méthode div:\n\nimport timeit\ndf_numeric = df.loc[:, numeric_columns.tolist() + [\"dep\"] ]\n%timeit df_numeric.groupby('dep').agg(['min',\"median\",\"max\"]).div(1000)\n%timeit df_numeric.groupby('dep').agg(['min',\"median\",\"max\"]).apply(lambda s: s/1000)\n\nLa méthode div est en moyenne plus rapide et a un temps d’exécution\nmoins variable. Dans ce cas, on pourrait même utiliser le principe\ndu broadcasting de numpy (cf. chapitre numpy) qui offre\ndes performances équivalentes:\n\n%timeit df_numeric.groupby('dep').agg(['min',\"median\",\"max\"])/1000\n\napply est plus rapide qu’une boucle (en interne, apply utilise Cython\npour itérer) mais reste moins rapide qu’une solution vectorisée quand\nelle existe. Ce site\npropose des solutions, par exemple les méthodes isin ou digitize, pour\néviter de manuellement créer des boucles lentes.\nEn particulier, il faut noter que apply avec le paramètre axis=1 est en générale lente.\n\n\n9.6.6 Joindre des données\nIl est commun de devoir combiner des données issues de sources différentes.\nNous allons ici nous focaliser sur le cas le plus favorable qui est la situation\noù une information permet d’apparier de manière exacte deux bases de données (autrement nous\nserions dans une situation, beaucoup plus complexe, d’appariement flou3).\nLa situation typique est l’appariement entre deux sources de données selon un identifiant\nindividuel. Ici, il s’agit d’un identifiant de code commune.\nIl est recommandé de lire ce guide assez complet sur la question des jointures avec R\nqui donne des recommandations également utiles pour un utilisateur de Python.\n\n\n\n\n\nOn utilise de manière indifférente les termes merge ou join.\nLe deuxième terme provient de la syntaxe SQL.\nEn Pandas, dans la plupart des cas, on peut utiliser indifféremment df.join et df.merge\n\n\n\n\n\nIl est aussi possible de réaliser un merge en utilisant la fonction pandas.concat() avec axis=1.\nSe référer à la documentation de concat pour voir les options possibles.\n\n\n9.6.7 Restructurer des données (reshape)\nOn présente généralement deux types de données:\n\nformat wide: les données comportent des observations répétées, pour un même individu (ou groupe), dans des colonnes différentes\nformat long: les données comportent des observations répétées, pour un même individu, dans des lignes différentes avec une colonne permettant de distinguer les niveaux d’observations\n\nUn exemple de la distinction entre les deux peut être emprunté à l’ouvrage de référence d’Hadley Wickham, R for Data Science:\n\n\n\n\n\nL’aide mémoire suivante aidera à se rappeler les fonctions à appliquer si besoin:\n\n\n\n\n\nLe fait de passer d’un format wide au format long (ou vice-versa) peut être extrêmement pratique car\ncertaines fonctions sont plus adéquates sur une forme de données ou sur l’autre.\nEn règle générale, avec Python comme avec R, les formats long sont souvent préférables.\nLe chapitre suivant, qui fait office de TP, proposera des applications de ces principes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.6.8 Les pipe\nEn général, dans un projet, le nettoyage de données va consister en un ensemble de\nméthodes appliquées à un pandas.DataFrame.\nOn a vu que assign permettait de créer une variable dans un DataFrame.\nIl est également possible d’appliquer une fonction, appelée par exemple my_udf au\nDataFrame grâce à pipe:\ndf = (pd.read_csv(path2data)\n            .pipe(my_udf))\nL’utilisation des pipe rend le code très lisible et peut être très\npratique lorsqu’on enchaine des opérations sur le même\ndataset."
  },
  {
    "objectID": "content/course/manipulation/02a_pandas_tutorial/index.html#quelques-enjeux-de-performance",
    "href": "content/course/manipulation/02a_pandas_tutorial/index.html#quelques-enjeux-de-performance",
    "title": "9  Introduction à Pandas",
    "section": "9.7 Quelques enjeux de performance",
    "text": "9.7 Quelques enjeux de performance\nLa librairie Dask intègre la structure de numpy, pandas et sklearn.\nElle a vocation à traiter de données en grande dimension, ainsi elle ne sera pas\noptimale pour des données qui tiennent très bien en RAM.\nIl s’agit d’une librairie construite sur la parallélisation.\nUn chapitre dans ce cours lui est consacré\nPour aller plus loin, se référer à la documentation de Dask."
  },
  {
    "objectID": "content/course/manipulation/02a_pandas_tutorial/index.html#références",
    "href": "content/course/manipulation/02a_pandas_tutorial/index.html#références",
    "title": "9  Introduction à Pandas",
    "section": "9.8 Références",
    "text": "9.8 Références\n\nLe site\npandas.pydata\nfait office de référence\nLe livre Modern Pandas de Tom Augspurger: https://tomaugspurger.github.io/modern-1-intro.html\n\n\n\nMcKinney, Wes. 2012. Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython. \" O’Reilly Media, Inc.\"."
  },
  {
    "objectID": "content/course/manipulation/02b_pandas_TP/index.html#exploration-de-la-structure-des-données",
    "href": "content/course/manipulation/02b_pandas_TP/index.html#exploration-de-la-structure-des-données",
    "title": "10  Pratique de pandas: un exemple complet",
    "section": "10.1 Exploration de la structure des données",
    "text": "10.1 Exploration de la structure des données\nCommencer par importer les données de l’Ademe à l’aide du package pandas. Vous pouvez nommer le DataFrame obtenu df.\n\ndf = pd.read_csv(\"https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert\", sep=\",\")\n\nPour les données de cadrage au niveau communal (source Insee), le package pynsee facilite grandement la vie.\nLa liste des données disponibles est ici.\nEn l’occurrence, on va utiliser les données Filosofi (données de revenus) au niveau communal de 2016.\nLe point d’entrée principal de la fonction pynsee est la fonction download_file.\nLe code pour télécharger les données est le suivant :\n\ndf_city = pynsee.download.download_file(\"FILOSOFI_COM_2016\")\n\n\n\n Note\nLa fonction download_file attend un identifiant unique\npour savoir quelle base de données aller chercher et\nrestructurer depuis le\nsite insee.fr.\nPour connaître la liste des bases disponibles, vous\npouvez utiliser la fonction meta = pynsee.get_file_list().\nCelle-ci renvoie un DataFrame dans lequel on peut\nrechercher, par exemple grâce à une recherche\nde mot clé:\n\nmeta = pynsee.get_file_list()\nmeta.loc[meta['label'].str.contains(r\"Filosofi.*2016\")]\n\nIci, meta['label'].str.contains(r\"Filosofi.*2016\") signifie:\n“pandas trouve moi tous les labels où sont contenus les termes Filosofi et 2016.”\n(.* signifiant “peu m’importe le nombre de mots ou caractères entre”)\n\n\n\n\n Exercice 1: Afficher des données\nL’objectif de cet exercice est de vous amener à afficher des informations sur les données dans un bloc de code (notebook) ou dans la console\nCommencer sur df:\n\nUtiliser les méthodes adéquates pour les 10 premières valeurs, les 15 dernières et un échantillon aléatoire de 10 valeurs\nTirer 5 pourcent de l’échantillon sans remise\nNe conserver que les 10 premières lignes et tirer aléatoirement dans celles-ci pour obtenir un DataFrame de 100 données.\nFaire 100 tirages à partir des 6 premières lignes avec une probabilité de 1/2 pour la première observation et une probabilité uniforme pour les autres\nFaire la même chose sur df_city.\n\n\n\nCette première approche exploratoire donne une idée assez précise de la manière dont les données sont organisées.\nOn remarque ainsi une différence entre df et df_city quant aux valeurs manquantes :\nla première base est relativement complète, la seconde comporte beaucoup de valeurs manquantes.\nAutrement dit, si on désire exploiter df_city, il faut faire attention à la variable choisie.\n\n\n Exercice 2: structure des données\nLa première chose à vérifier est le format des données,\nafin d’identifier des types de variables qui ne conviennent pas\nIci, comme c’est pandas qui a géré automatiquement les types de variables,\nil y a peu de chances que les types ne soient pas adéquats mais une vérification ne fait pas de mal.\n\nVérifier les types des variables.\nS’assurer que les types des variables communes aux deux bases sont cohérents.\nPour les variables qui ne sont pas en type float alors qu’elles devraient l’être, modifier leur type.\n\nEnsuite, on vérifie les dimensions des DataFrames et la structure de certaines variables clés.\nEn l’occurrence, les variables fondamentales pour lier nos données sont les variables communales.\nIci, on a deux variables géographiques: un code commune et un nom de commune.\n\nVérifier les dimensions des DataFrames\nVérifier le nombre de valeurs uniques des variables géographiques dans chaque base. Les résultats apparaissent-ils cohérents ?\nIdentifier dans df_city les noms de communes qui correspondent à plusieurs codes communes et sélectionner leurs codes. En d’autres termes, identifier les CODGEO tels qu’il existe des doublons de LIBGEO et les stocker dans un vecteur x (conseil: faire attention à l’index de x)\n\nOn se focalise temporairement sur les observations où le libellé comporte plus de deux codes communes différents\n\nRegarder dans df_city ces observations\nPour mieux y voir, réordonner la base obtenue par order alphabétique\nDéterminer la taille moyenne (variable nombre de personnes: NBPERSMENFISC16) et quelques statistiques descriptives de ces données.\nComparer aux mêmes statistiques sur les données où libellés et codes communes coïncident\nVérifier les grandes villes (plus de 100 000 personnes),\nla proportion de villes pour lesquelles un même nom est associé à différents codes commune.\nVérifier dans df_city les villes dont le libellé est égal à Montreuil.\nVérifier également celles qui contiennent le terme ‘Saint-Denis’\n\n\n\nCe petit exercice permet de se rassurer car les libellés dupliqués\nsont en fait des noms de commune identiques mais qui ne sont pas dans le même département.\nIl ne s’agit donc pas d’observations dupliquées.\nOn se fiera ainsi aux codes communes, qui eux sont uniques."
  },
  {
    "objectID": "content/course/manipulation/02b_pandas_TP/index.html#les-indices",
    "href": "content/course/manipulation/02b_pandas_TP/index.html#les-indices",
    "title": "10  Pratique de pandas: un exemple complet",
    "section": "10.2 Les indices",
    "text": "10.2 Les indices\nLes indices sont des éléments spéciaux d’un DataFrame puisqu’ils permettent d’identifier certaines observations.\nIl est tout à fait possible d’utiliser plusieurs indices, par exemple si on a des niveaux imbriqués.\n\n\n Exercice 3: Les indices\nA partir de l’exercice précédent, on peut se fier aux codes communes.\n\nFixer comme indice la variable de code commune dans les deux bases.\nRegarder le changement que cela induit sur le display du DataFrame\nLes deux premiers chiffres des codes communes sont le numéro de département.\nCréer une variable de département dep dans df et dans df_city\nCalculer les émissions totales par secteur pour chaque département.\nMettre en log ces résultats dans un objet df_log.\nGarder 5 départements et produire un barplot\nRepartir de df.\nCalculer les émissions totales par département et sortir la liste\ndes 10 principaux émetteurs de CO2 et des 5 départements les moins émetteurs.\nSans faire de merge,\nregarder les caractéristiques de ces départements (population et niveau de vie)\n\n\n\n\n\n Exercice 4: performance des indices\nUn des intérêts des indices est qu’ils permettent des agrégations efficaces.\n\nRepartir de df et créer une copie df_copy = df.copy() et df_copy2 = df.copy() afin de ne pas écraser le DataFrame df\nUtiliser la variable dep comme indice pour df_copy et retirer tout index pour df_copy2\nImporter le module timeit et comparer le temps d’exécution de la somme par secteur, pour chaque département, des émissions de CO2"
  },
  {
    "objectID": "content/course/manipulation/02b_pandas_TP/index.html#restructurer-les-données",
    "href": "content/course/manipulation/02b_pandas_TP/index.html#restructurer-les-données",
    "title": "10  Pratique de pandas: un exemple complet",
    "section": "10.3 Restructurer les données",
    "text": "10.3 Restructurer les données\nOn présente généralement deux types de données :\n\nformat wide: les données comportent des observations répétées, pour un même individu (ou groupe), dans des colonnes différentes\nformat long: les données comportent des observations répétées, pour un même individu, dans des lignes différentes avec une colonne permettant de distinguer les niveaux d’observations\n\nUn exemple de la distinction entre les deux peut être pris à l’ouvrage de référence d’Hadley Wickham, R for Data Science:\n\n\n\n\n\nL’aide mémoire suivante aidera à se rappeler les fonctions à appliquer si besoin:\n\n\n\n\n\nLe fait de passer d’un format wide au format long (ou vice-versa)\npeut être extrêmement pratique car certaines fonctions sont plus adéquates sur une forme de données ou sur l’autre.\nEn règle générale, avec Python comme avec R, les formats long sont souvent préférables.\n\n\n Exercice 5: Restructurer les données: wide to long\n\nCréer une copie des données de l’ADEME en faisant df_wide = df.copy()\nRestructurer les données au format long pour avoir des données d’émissions par secteur en gardant comme niveau d’analyse la commune (attention aux autres variables identifiantes).\nFaire la somme par secteur et représenter graphiquement\nGarder, pour chaque département, le secteur le plus polluant\n\n\n\n\n\n Exercice 6: long to wide\nCette transformation est moins fréquente car appliquer des fonctions par groupe, comme nous le verrons par la suite, est très simple.\n\nRepartir de `df_wide = df.copy()\nReconstruire le DataFrame, au format long, des données d’émissions par secteur en gardant comme niveau d’analyse la commune puis faire la somme par département et secteur\nPasser au format wide pour avoir une ligne par secteur et une colonne par département\nCalculer, pour chaque secteur, la place du département dans la hiérarchie des émissions nationales\nA partir de là, en déduire le rang médian de chaque département dans la hiérarchie des émissions et regarder les 10 plus mauvais élèves, selon ce critère."
  },
  {
    "objectID": "content/course/manipulation/02b_pandas_TP/index.html#combiner-les-données",
    "href": "content/course/manipulation/02b_pandas_TP/index.html#combiner-les-données",
    "title": "10  Pratique de pandas: un exemple complet",
    "section": "10.4 Combiner les données",
    "text": "10.4 Combiner les données\nUne information que l’on cherche à obtenir s’obtient de moins en moins à partir d’une unique base de données. Il devient commun de devoir combiner des données issues de sources différentes. Nous allons ici nous focaliser sur le cas le plus favorable qui est la situation où une information permet d’apparier de manière exacte deux bases de données (autrement nous serions dans une situation, beaucoup plus complexe, d’appariement flou). La situation typique est l’appariement entre deux sources de données selon un identifiant individuel ou un identifiant de code commune, ce qui est notre cas.\nIl est recommandé de lire ce guide assez complet sur la question des jointures avec R qui donne des recommandations également utiles en python.\nOn utilise de manière indifférente les termes merge ou join. Le deuxième terme provient de la syntaxe SQL. En pandas, dans la plupart des cas, on peut utiliser indifféremment df.join et df.merge\n\n\n\n\n\n\n\n Exercice 7: Calculer l'empreinte carbone par habitant\n\nCréer une variable emissions qui correspond aux émissions totales d’une commune\nFaire une jointure à gauche entre les données d’émissions et les données de cadrage. Comparer les émissions moyennes des villes sans match (celles dont des variables bien choisies de la table de droite sont NaN) avec celles où on a bien une valeur correspondante dans la base Insee\nFaire un inner join puis calculer l’empreinte carbone (l’émission rapportée au nombre de ménages fiscaux) dans chaque commune. Sortir un histogramme en niveau puis en log et quelques statistiques descriptives sur le sujet.\nRegarder la corrélation entre les variables de cadrage et l’empreinte carbone. Certaines variables semblent-elles pouvoir potentiellement influer sur l’empreinte carbone ?"
  },
  {
    "objectID": "content/course/manipulation/02b_pandas_TP/index.html#exercices-bonus",
    "href": "content/course/manipulation/02b_pandas_TP/index.html#exercices-bonus",
    "title": "10  Pratique de pandas: un exemple complet",
    "section": "10.5 Exercices bonus",
    "text": "10.5 Exercices bonus\nLes plus rapides d’entre vous sont invités à aller un peu plus loin en s’entraînant avec des exercices bonus qui proviennent du site de Xavier Dupré. 3 notebooks en lien avec numpy et pandas vous y sont proposés :\n\nCalcul Matriciel, Optimisation : énoncé / corrigé\nDataFrame et Graphes : énoncé / corrigé\nPandas et itérateurs : énoncé / corrigé"
  },
  {
    "objectID": "content/course/manipulation/03_geopandas_tutorial/index.html#données-spatiales-quelle-différence-avec-des-données-traditionnelles",
    "href": "content/course/manipulation/03_geopandas_tutorial/index.html#données-spatiales-quelle-différence-avec-des-données-traditionnelles",
    "title": "11  Données spatiales: découverte de geopandas",
    "section": "11.1 Données spatiales: quelle différence avec des données traditionnelles ?",
    "text": "11.1 Données spatiales: quelle différence avec des données traditionnelles ?\nLe terme “données spatiales” désigne les données qui portent sur les caractéristiques géographiques des objets (localisation, contours, liens).\nLes caractéristiques géographiques des objets sont décrites à l’aide d’un système de coordonnées\nqui permettent une représentation dans un espace euclidien (\\((x,y)\\)).\nLe passage de l’espace réel (la Terre, qui est une sphère) à l’espace plan\nse fait grâce à un système de projection. Voici quelques exemples\nde données spatiales :\n\nUne table décrivant des bâtiments, avec les coordonnées géographiques de chaque bâtiment;\nLe découpage communal du territoire, avec le contour du territoire de chaque commune;\nLes routes terrestres, avec les coordonnées décrivant leur parcours.\n\nLes données spatiales rassemblent classiquement deux types de données :\n\ndes données géographiques (ou géométries): objets géométriques tels que des points, des vecteurs, des polygones, ou des maillages (raster). Exemple: la forme de chaque chaque commune, les coordonnées d’un bâtiment;\ndes données attributaires (ou attributs): des mesures et des caractéristiques associés aux objets géométriques. Exemple: la population de chaque commune, le nombre de fenêtres et le nombre d’étages d’un bâtiment.\n\nLes données spatiales sont fréquemment traitées à l’aide d’un système d’information géographique (SIG), c’est-à-dire un système d’information capable de stocker, d’organiser et de présenter des données alphanumériques spatialement référencées par des coordonnées dans un système de référence (CRS). Python dispose de fonctionnalités lui permettant de réaliser les mêmes tâches qu’un SIG (traitement de données spatiales, représentations cartographiques).\nLes systèmes de projection font l’objet de standards internationaux et sont souvent désignés par des codes dits codes EPSG. Ce site est un bon aide-mémoire. Les plus fréquents, pour les utilisateurs français, sont les suivants (plus d’infos ici):\n\n2154: système de projection Lambert 93. Il s’agit du système de projection officiel. La plupart des données diffusées par l’administration pour la métropole sont disponibles dans ce système de projection.\n27572: Lambert II étendu. Il s’agit de l’ancien système de projection officiel. Les données spatiales anciennes peuvent être dans ce format.\n4326: WGS 84 ou système de pseudo-Mercator. Attention, ce n’est en réalité pas un système de projection mais un système de coordonnées (longitude / latitude) qui permet simplement un repérage angulaire sur l’ellipsoïde. Il est utilisé pour les données GPS.\n\n\n11.1.1 De pandas à geopandas\nLe package geopandas est une boîte à outils conçue pour faciliter la manipulation de données spatiales. La grande force de geopandas est qu’il permet de manipuler des données spatiales comme s’il s’agissait de données traditionnelles, car il repose sur le standard ISO 19125 simple feature access défini conjointement par l’Open Geospatial Consortium (OGC) et l’International Organization for Standardization (ISO).\nPar rapport à un DataFrame standard, un objet geopandas comporte\nune colonne supplémentaire: geometry. Elle stocke les coordonnées des\nobjets géographiques (ou ensemble de coordonnées s’agissant de contours). Un objet geopandas hérite des propriétés d’un\nDataFrame pandas mais propose des méthodes adaptées au traitement des données spatiales.\nAinsi, grâce à Geopandas, on pourra effectuer des manipulations sur les attributs des données comme avec pandas mais on pourra également faire des manipulations sur la dimension spatiale des données. En particulier,\n\nCalculer des distances et des surfaces;\nAgréger rapidement des zonages (regrouper les communes en département par exemple);\nTrouver dans quelle commune se trouve un bâtiment à partir de ses coordonnées géographiques;\nRecalculer des coordonnées dans un autre système de projection.\nFaire une carte, rapidement et simplement\n\n\n\n Hint\nLes manipulations de données sur un objet Geopandas sont nettement plus lentes que sur\nun DataFrame traditionnel (car Python doit gérer les informations géographiques pendant la manipulation des données).\nLorsque vous manipulez des données de grandes dimensions,\nil peut être préférable d’effectuer les opérations sur les données avant de joindre une géométrie à celles-ci.\n\n\nPar rapport à un logiciel spécialisé comme QGIS, Python permettra\nd’automatiser le traitement et la représentation des données. D’ailleurs,\nQGIS utilise lui-même python…"
  },
  {
    "objectID": "content/course/manipulation/03_geopandas_tutorial/index.html#importer-des-données-spatiales",
    "href": "content/course/manipulation/03_geopandas_tutorial/index.html#importer-des-données-spatiales",
    "title": "11  Données spatiales: découverte de geopandas",
    "section": "11.2 Importer des données spatiales",
    "text": "11.2 Importer des données spatiales\nLes données spatiales sont plus riches que les données traditionnelles car elles\nincluent, habituellement, des éléments supplémentaires pour placer dans\nun espace cartésien les objets. Cette dimension supplémentaire peut être simple\n(un point comporte deux informations supplémentaire: \\(x\\) et \\(y\\)) ou\nassez complexe (polygones, lignes avec direction, etc.)\nLes formats les plus communs de données spatiales sont les suivants :\n\nshapefile (.shp): format (propriétaire) le plus commun de données géographiques.\nLa table de données (attributs) est stockée dans un fichier séparé des\ndonnées spatiales. En faisant geopandas.read_file(\"monfichier.shp\"), le\npackage fait lui-même le lien entre les observations et leur représentation spatiale ;\ngeopackage (.gpkg) : ce (relativement) nouveau format libre en un seul fichier également (lui recommandé par l’OGC) vise progressivement à se substituer au shapefile. Il est par exemple le format par défaut dans QGIS.\ngeojson (.json) : ce format, non préconisé par l’OGC est largement utilisé pour le développement web\ncomme dans la librairie leaflet.js.\nLa dimension spatiale est stockée dans le même fichier que les attributs.\nCes fichiers sont généralement beaucoup plus légers que les shapefiles mais possèdent des limites s’agissant de gros jeux de données.\n\nCette page compare plus en détail ces trois types de formats de données géographiques.\nL’aide de geopandas propose des bouts de code en fonction des différentes situations dans lesquelles on se trouve.\n\n11.2.1 Exemple: récupérer les découpages territoriaux\nL’un des fonds de carte les plus fréquents qu’on utilise est celui des\nlimites administratives des communes.\nCelui-ci peut être récupéré de plusieurs manières.\nEn premier lieu, pour récupérer\nle fond de carte officiel, produit par l’IGN, sous\nle nom d’AdminExpress1,\nil est possible de se rendre sur le site de l’IGN et de le télécharger depuis\nle serveur FTP.\nIl est également possible d’utiliser l’une des API de l’IGN\nmais ces dernières ne sont pas encore très documentées pour des utilisateurs\nde Python.\nLe package pynsee propose notamment un module dédié à la récupération de fonds de carte officiels pour valoriser des données\nd’open data. L’API sur laquelle il repose étant parfois lente, nous présentons le code dédié uniquement en annexe.\nNous proposons ici une méthode nouvelle de récupération de\nces données qui s’appuie sur le projet interministériel\ncartiflette.\nCe projet vise à faciliter la récupération des sources officielles, notamment\ncelles de l’IGN, et leur association à des jeux de données géographiques.\n\n\n Note\nLe package cartiflette est expérimental\net n’est disponible que sur\nGithub, pas sur PyPi.\nIl est amené à évoluer rapidement et cette page sera mise à jour\nquand de nouvelles fonctionalités (notamment l’utilisation d’API)\nseront disponibles pour encore simplifier la récupération de\ncontours géographiques.\nPour installer cartiflette, il est nécessaire d’utiliser les commandes suivantes\ndepuis un Jupyter Notebook (si vous utilisez la ligne de commande directement,\nvous pouvez retirer les ! et % en début de ligne):\n\n!pip install --upgrade botocore==1.23.26  #Sur colab, sinon bug\n!pip install --upgrade urllib3==1.22.0 #Sur colab, sinon bug\n!pip install py7zr #Sur colab, sinon bug\n!pip install s3fs #Sur colab, sinon bug\n!git clone https://github.com/InseeFrLab/cartogether.git\n%cd ./cartogether\n!pip install -r requirements.txt\n!pip install .\n\nCes commandes permettent de récupérer l’ensemble du code\nsource depuis Github \n\n\nIci, nous sommes intéressés par les contours des communes\nde la petite couronne. On pourrait désirer récupérer\nl’ensemble de la région Ile-de-France mais nous\nallons nous contenter de l’analyse de Paris intra-muros\net des départements limitrophes.\nC’est l’un des avantage de cartiflette que de faciliter\nla récupération de fonds de carte sur un ensemble de département.\nCela évite la récupération d’un fond de carte très\nvolumineux (plus de 500Mo) pour une analyse restreinte (quelques départements).\nUn autre avantage de cartiflette est de faciliter la récupération de fonds\nde carte consolidés comme celui dont on a besoin ici: arrondissements\ndans Paris, communes ailleurs. Comme cela est expliqué dans un encadré à part,\nil s’agirait d’une opération pénible à mettre en oeuvre sans cartiflette.\nLes contours de cet espace peuvent être récupérés de la manière suivante:\n\nimport cartiflette.s3 as s3\n\nshp_communes = s3.download_vectorfile_url_all(\n    crs = 4326,\n    values = [\"75\", \"92\", \"93\", \"94\"],\n    borders=\"COMMUNE\",\n    vectorfile_format=\"topojson\",\n    filter_by=\"DEPARTEMENT\",\n    source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n    year=2022)\n\nshp_communes.head()\n\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=75/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=92/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=93/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=94/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\n\n\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 12.0kiB [00:00, 98.5kiB/s]Downloading: : 14.5kiB [00:00, 117kiB/s] \nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 12.0kiB [00:00, 99.7kiB/s]Downloading: : 40.0kiB [00:00, 200kiB/s] Downloading: : 62.3kiB [00:00, 255kiB/s]\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 12.0kiB [00:00, 105kiB/s]Downloading: : 39.0kiB [00:00, 179kiB/s]Downloading: : 83.0kiB [00:00, 281kiB/s]Downloading: : 119kiB [00:00, 337kiB/s] \nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 12.0kiB [00:00, 96.2kiB/s]Downloading: : 44.0kiB [00:00, 186kiB/s] Downloading: : 103kiB [00:00, 311kiB/s] Downloading: : 110kiB [00:00, 285kiB/s]\n\n\n\n\n\n\n\n\n\nid\nID\nNOM\nNOM_M\nINSEE_COM\nSTATUT\nPOPULATION\nINSEE_CAN\nINSEE_ARR\nINSEE_DEP\nINSEE_REG\nSIREN_EPCI\nsource\ngeometry\n\n\n\n\n0\nCOMMUNE_0000000009736048\nNaN\nParis\nPARIS\n75056\nCapitale d'état\n2165423\nNR\n1\n75\n11\n200054781\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.36421 48.81640, 2.36333 48.81615, ...\n\n\n0\nCOMMUNE_0000000009736037\nNaN\nLevallois-Perret\nLEVALLOIS-PERRET\n92044\nCommune simple\n66082\n16\n2\n92\n11\n200054781/200057982\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.28739 48.90364, 2.28846 48.90302, ...\n\n\n1\nCOMMUNE_0000000009736055\nNaN\nBois-Colombes\nBOIS-COLOMBES\n92009\nCommune simple\n28841\n11\n2\n92\n11\n200054781/200057990\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.26639 48.90629, 2.26645 48.90615, ...\n\n\n2\nCOMMUNE_0000000009736538\nNaN\nMalakoff\nMALAKOFF\n92046\nCommune simple\n30950\n18\n1\n92\n11\n200054781/200057966\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.27818 48.81425, 2.28066 48.81469, ...\n\n\n3\nCOMMUNE_0000000009736038\nNaN\nClichy\nCLICHY\n92024\nCommune simple\n63089\n09\n2\n92\n11\n200054781/200057990\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.30377 48.89415, 2.30258 48.89487, ...\n\n\n\n\n\n\n\nOn reconnaît la structure d’un DataFrame Pandas. A cette structure s’ajoute\nune colonne geometry qui enregistre la position des limites des polygones de chaque\nobservation.\nComme vu précédemment, le système de projection est un élément important. Il permet à Python\nd’interpréter les valeurs des points (deux dimensions) en position sur\nla terre, qui n’est pas un espace plan.\n\nshp_communes.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nIci, les données sont dans le système WGS84 (code EPSG 4326).\nCe n’est pas le\nLambert-93 comme on pourrait s’y attendre, ce dernier\nétant le système légal de projection pour la France\nmétropolitaine.\nPour s’assurer qu’on a bien récupéré les contours voulus,\non peut représenter graphiquement\nles contours grâce à la méthode plot sur laquelle nous\nreviendrons :\n\nax = shp_communes.boundary.plot()\nax.set_axis_off()\n\n\n\n\n\n\n\n\n\n\n Note\nSi on ne désire pas utiliser le niveau COMMUNE_ARRONDISSEMENT,\nil est nécessaire de mettre en oeuvre une construction du fond de\ncarte en plusieurs phases.\nEn premier lieu, il est nécessaire de récupérer le niveau des communes.\n\nimport cartiflette.s3 as s3\n\nshp_communes = s3.download_vectorfile_url_all(\n    crs = 4326,\n    values = [\"75\", \"92\", \"93\", \"94\"],\n    borders=\"COMMUNE\",\n    vectorfile_format=\"topojson\",\n    filter_by=\"DEPARTEMENT\",\n    source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n    year=2022)\n\nshp_communes.head()\n\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=75/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=92/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=93/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=94/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\n\n\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 12.0kiB [00:00, 91.7kiB/s]Downloading: : 14.5kiB [00:00, 109kiB/s] \nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 12.0kiB [00:00, 95.7kiB/s]Downloading: : 44.0kiB [00:00, 187kiB/s] Downloading: : 62.3kiB [00:00, 241kiB/s]\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 13.0kiB [00:00, 103kiB/s]Downloading: : 38.0kiB [00:00, 158kiB/s]Downloading: : 82.0kiB [00:00, 243kiB/s]Downloading: : 119kiB [00:00, 308kiB/s] \nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 8.00kiB [00:00, 74.4kiB/s]Downloading: : 32.0kiB [00:00, 141kiB/s] Downloading: : 64.0kiB [00:00, 149kiB/s]Downloading: : 110kiB [00:00, 214kiB/s] \n\n\n\n\n\n\n\n\n\nid\nID\nNOM\nNOM_M\nINSEE_COM\nSTATUT\nPOPULATION\nINSEE_CAN\nINSEE_ARR\nINSEE_DEP\nINSEE_REG\nSIREN_EPCI\nsource\ngeometry\n\n\n\n\n0\nCOMMUNE_0000000009736048\nNaN\nParis\nPARIS\n75056\nCapitale d'état\n2165423\nNR\n1\n75\n11\n200054781\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.36421 48.81640, 2.36333 48.81615, ...\n\n\n0\nCOMMUNE_0000000009736037\nNaN\nLevallois-Perret\nLEVALLOIS-PERRET\n92044\nCommune simple\n66082\n16\n2\n92\n11\n200054781/200057982\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.28739 48.90364, 2.28846 48.90302, ...\n\n\n1\nCOMMUNE_0000000009736055\nNaN\nBois-Colombes\nBOIS-COLOMBES\n92009\nCommune simple\n28841\n11\n2\n92\n11\n200054781/200057990\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.26639 48.90629, 2.26645 48.90615, ...\n\n\n2\nCOMMUNE_0000000009736538\nNaN\nMalakoff\nMALAKOFF\n92046\nCommune simple\n30950\n18\n1\n92\n11\n200054781/200057966\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.27818 48.81425, 2.28066 48.81469, ...\n\n\n3\nCOMMUNE_0000000009736038\nNaN\nClichy\nCLICHY\n92024\nCommune simple\n63089\n09\n2\n92\n11\n200054781/200057990\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.30377 48.89415, 2.30258 48.89487, ...\n\n\n\n\n\n\n\n\nax = shp_communes.boundary.plot()\nax.set_axis_off()\n\n\n\n\n\n\n\n\nOn peut remarquer que la ville de Paris ne comporte pas d’arrondissements\nsur cette carte. Pour vous en convaincre, vous pouvez exécuter la\ncommande :\n\nax = shp_communes.loc[shp_communes['INSEE_DEP']==\"75\"].boundary.plot()\nax.set_axis_off()\n\n\n\n\n\n\n\n\nIl faut donc utiliser une source complémentaire.\nLe contour officiel des arrondissements est\nproduit par l’IGN séparemment des contours de communes.\nLes contours d’arrondissements sont également\ndisponibles\ngrâce à cartiflette:\n\narrondissements = s3.download_vectorfile_url_all(\n    crs = 4326,\n    values = [\"75\"],\n    borders=\"COMMUNE\",\n    vectorfile_format=\"topojson\",\n    filter_by=\"DEPARTEMENT\",\n    source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n    year=2022)\n\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=75/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\n\n\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 12.0kiB [00:00, 99.3kiB/s]Downloading: : 14.5kiB [00:00, 117kiB/s] \n\n\n\nax = arrondissements.plot(alpha = 0.8, edgecolor = \"k\")\nax.set_axis_off()\n\n\n\n\n\n\n\n\nIl ne reste plus qu’à remplacer Paris par\nses arrondissements dans shp_communes.\nPour cela, on peut utiliser les méthodes\nvues dans le chapitre Pandas relatives\naux filtres et à la concaténation\nde plusieurs DataFrames:\n\nimport pandas as pd\n\nshp_communes = pd.concat(\n  [\n    shp_communes.loc[shp_communes['INSEE_DEP'] != \"75\"].to_crs(2154),\n    arrondissements.to_crs(2154)\n  ])\n\nCette approche fonctionne mais elle nécessite un certain nombre\nde gestes, qui sont autant de risques d’erreurs. Il est\ndonc recommandé de privilégier le niveau COMMUNE_ARRONDISSEMENT\nqui fait exactement ceci mais de manière fiable."
  },
  {
    "objectID": "content/course/manipulation/03_geopandas_tutorial/index.html#opérations-sur-les-attributs-et-les-géométries",
    "href": "content/course/manipulation/03_geopandas_tutorial/index.html#opérations-sur-les-attributs-et-les-géométries",
    "title": "11  Données spatiales: découverte de geopandas",
    "section": "11.3 Opérations sur les attributs et les géométries",
    "text": "11.3 Opérations sur les attributs et les géométries\n\n11.3.1 Import des données velib\nSouvent, le découpage communal ne sert qu’en fond de cartes, pour donner des\nrepères. En complément de celui-ci, on peut désirer exploiter\nun autre jeu de données. On va partir des données de localisation des\nstations velib,\ndisponibles sur le site d’open data de la ville de Paris et\nrequêtables directement par l’url\nhttps://opendata.paris.fr/explore/dataset/velib-emplacement-des-stations/download/?format=geojson&timezone=Europe/Berlin&lang=fr\n\nvelib_data = 'https://opendata.paris.fr/explore/dataset/velib-emplacement-des-stations/download/?format=geojson&timezone=Europe/Berlin&lang=fr'\nstations = gpd.read_file(velib_data)\nstations.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nLes données sont dans le système de projection WGS84 qui est celui du\nsystème GPS. Celui-ci s’intègre bien avec les fonds de carte\nStamen, OpenStreetMap ou Google Maps. En toute rigueur, si on\ndésire effectuer certains calculs géométriques (mesurer des surfaces…), il est\nnécessaire de re-projeter les données dans un système qui préserve la géométrie\n(c’est le cas du Lambert 93).\nPour avoir une intuition de la localisation des stations, et notamment de la\ndensité hétérogène de celles-ci,\non peut afficher les données sur la carte des communes\nde la petite couronne. Il s’agit donc d’enrichir la carte\nprécédente d’une couche supplémentaire, à savoir la localisation\ndes stations. Au passage, on va utiliser un fond de carte\nplus esthétique:\n\nfig,ax = plt.subplots(figsize=(10, 10))\nstations.sample(200).to_crs(3857).plot(ax = ax, color = 'red', alpha = 0.4, zorder=2)\nshp_communes.to_crs(3857).plot(ax = ax, zorder=1, edgecolor = \"black\", facecolor=\"none\",\n                                                           color = None)\nctx.add_basemap(ax, source = ctx.providers.Stamen.Watercolor)\nax.set_axis_off()\n\n\n\n\n\n\n\n\nDécouvrez ci-dessous par étape les différentes lignes de commandes permettant d’afficher cette carte complète,\nétape par étape :\n:one:\nAfficher le nuage de points de 200 stations vélibs prises au hasard\n\nfig,ax = plt.subplots(figsize=(10, 10))\nstations.sample(200).to_crs(3857).plot(ax = ax, color = 'red', alpha = 0.4, zorder=2)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\n:two:\nAjouter à cette couche, en-dessous, les contours des communes\n\nfig,ax = plt.subplots(figsize=(10, 10))\nstations.sample(200).to_crs(3857).plot(ax = ax, color = 'red', alpha = 0.4, zorder=2)\nshp_communes.to_crs(3857).plot(ax = ax, zorder=1, edgecolor = \"black\", facecolor=\"none\",\n                                                           color = None)\n\n\n\n\n\n\n\n\n\n\n:three:\nAjouter un fond de carte de type open street map grâce au package\ncontextily\n\nfig,ax = plt.subplots(figsize=(10, 10))\nstations.sample(200).to_crs(3857).plot(ax = ax, color = 'red', alpha = 0.4, zorder=2)\nshp_communes.to_crs(3857).plot(ax = ax, zorder=1, edgecolor = \"black\", facecolor=\"none\",\n                                                           color = None)\nctx.add_basemap(ax, source = ctx.providers.Stamen.Watercolor)\n\n\n\n\n\n\n\n\n\n\n:four:\nIl ne reste plus qu’à retirer l’axe des coordonnées, qui n’est pas très\nesthétique.\n\nfig,ax = plt.subplots(figsize=(10, 10))\nstations.sample(200).to_crs(3857).plot(ax = ax, color = 'red', alpha = 0.4, zorder=2)\nshp_communes.to_crs(3857).plot(ax = ax, zorder=1, edgecolor = \"black\", facecolor=\"none\",\n                                                           color = None)\nctx.add_basemap(ax, source = ctx.providers.Stamen.Watercolor)\nax.set_axis_off()\nax\n\n\n\n\n\n\n\n\n\n\nIn fine, on obtient la carte désirée.\n\n\n11.3.2 Opérations sur les attributs\nToutes les opérations possibles sur un objet Pandas le sont également\nsur un objet GeoPandas. Pour manipuler les données, et non la géométrie,\non parlera d’opérations sur les attributs.\nPar exemple, si on désire\nconnaître quelques statistiques sur la taille des stations:\n\nstations.describe()\n\n\n\n\n\n\n\n\ncapacity\n\n\n\n\ncount\n1471.000000\n\n\nmean\n30.993882\n\n\nstd\n12.079131\n\n\nmin\n0.000000\n\n\n25%\n23.000000\n\n\n50%\n29.000000\n\n\n75%\n37.000000\n\n\nmax\n74.000000\n\n\n\n\n\n\n\nPour connaître les plus grands départements de France métropolitaine,\nprocédons en deux étapes:\n\nRécupérons le contour des communes de France métropolitaine dans son ensemble\ngrâce à cartiflette.\nNotons qu’on pourrait récupérer directement les contours départementaux mais\npour l’exercice, nous allons le créer nous-mêmes comme agrégation\ndes contours communaux\n(voir ce notebook observable pour la méthode plus\nlégère qui utilise pleinement les fonctionnalités de cartiflette).\nCalculons la surface (méthode area sur un objet GeoPandas.GeoDataFrame ramenée en km², attention néamoins au système de projection comme cela est expliqué plus bas)\n\n\nfrom cartiflette.download import get_vectorfile_ign\nfrance = get_vectorfile_ign(\n  borders = \"COMMUNE\",\n  field = \"metropole\",\n  source = \"EXPRESS-COG-CARTO-TERRITOIRE\",\n  provider=\"IGN\"\n)\n\n\nfrance['surface'] = france.area.div(10**6)\n\nLes plus grands départements s’obtiennent par une agrégation des\nsurfaces communales :\n\nfrance.groupby('INSEE_DEP').sum(numeric_only = True).sort_values('surface', ascending = False)\n\n\n\n\n\n\n\n\nPOPULATION\nsurface\n\n\nINSEE_DEP\n\n\n\n\n\n\n33\n1623749\n10367.786664\n\n\n40\n413690\n9354.208426\n\n\n24\n413223\n9210.826232\n\n\n21\n534124\n8787.757953\n\n\n12\n279595\n8770.152839\n\n\n...\n...\n...\n\n\n90\n141318\n610.126210\n\n\n94\n1407124\n244.811547\n\n\n93\n1644903\n236.867946\n\n\n92\n1624357\n175.570732\n\n\n75\n2165423\n105.431453\n\n\n\n\n96 rows × 2 columns\n\n\n\nSi on veut directement les plus\ngrandes communes de France métropolitaine :\n\nfrance.sort_values('surface', ascending = False).head(10)\n\n\n\n\n\n\n\n\nID\nNOM\nNOM_M\nINSEE_COM\nSTATUT\nPOPULATION\nINSEE_CAN\nINSEE_ARR\nINSEE_DEP\nINSEE_REG\nSIREN_EPCI\ngeometry\nsource\nsurface\n\n\n\n\n240\nCOMMUNE_0000000009760125\nArles\nARLES\n13004\nSous-préfecture\n50454\n04\n2\n13\n93\n241300417\nPOLYGON ((841785.400 6290116.800, 841787.600 6...\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n757.418542\n\n\n308\nCOMMUNE_0000000009753237\nVal-Cenis\nVAL-CENIS\n73290\nCommune simple\n2062\n10\n3\n73\n84\n200070340\nPOLYGON ((1017798.400 6466625.200, 1017573.700...\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n455.423629\n\n\n19074\nCOMMUNE_0000000009760352\nSaintes-Maries-de-la-Mer\nSAINTES-MARIES-DE-LA-MER\n13096\nCommune simple\n2144\n04\n2\n13\n93\n241300417\nMULTIPOLYGON (((813751.200 6261910.500, 813716...\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n371.214527\n\n\n274\nCOMMUNE_0000000009746086\nChemillé-en-Anjou\nCHEMILLE-EN-ANJOU\n49092\nCommune simple\n20828\n11\n2\n49\n52\n200060010\nPOLYGON ((429946.200 6682848.700, 429939.900 6...\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n320.816158\n\n\n436\nCOMMUNE_0000000009744893\nNoyant-Villages\nNOYANT-VILLAGES\n49228\nCommune simple\n5546\n08\n3\n49\n52\n244900882\nPOLYGON ((490764.000 6714964.800, 490615.800 6...\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n300.457150\n\n\n14079\nCOMMUNE_0000000009744622\nBaugé-en-Anjou\nBAUGE-EN-ANJOU\n49018\nCommune simple\n11829\n08\n3\n49\n52\n244900882\nPOLYGON ((456856.800 6728449.000, 456857.600 6...\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n270.223011\n\n\n1279\nCOMMUNE_0000000009762779\nLaruns\nLARUNS\n64320\nCommune simple\n1185\n15\n2\n64\n75\n246400337\nPOLYGON ((428969.700 6200124.400, 428966.500 6...\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n248.657511\n\n\n275\nCOMMUNE_0000000009750652\nChamonix-Mont-Blanc\nCHAMONIX-MONT-BLANC\n74056\nCommune simple\n8640\n10\n2\n74\n84\n200023372\nPOLYGON ((1000337.100 6532738.100, 1000258.500...\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n245.012587\n\n\n6206\nCOMMUNE_0000000009743670\nSegré-en-Anjou Bleu\nSEGRE-EN-ANJOU BLEU\n49331\nSous-préfecture\n17462\n20\n4\n49\n52\n244900809\nMULTIPOLYGON (((421207.400 6742529.000, 421215...\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n243.945747\n\n\n473\nCOMMUNE_0000000009761156\nMarseille\nMARSEILLE\n13055\nPréfecture de région\n870731\n98\n3\n13\n93\n200054807\nMULTIPOLYGON (((894824.800 6233030.600, 894821...\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n238.445688\n\n\n\n\n\n\n\nLors des étapes d’agrégation, groupby ne conserve pas les géométries. Autrement\ndit, si on effectue, par exemple, une somme en fonction d’une variable de groupe avec\nle combo groupby(...).sum(...) , on perd\nla dimension géographique.\nIl est néanmoins possible d’aggréger à la fois les géométries et les\nattribus avec la méthode dissolve:\n\nfig,ax = plt.subplots(figsize=(10, 10))\nfrance.dissolve(by='INSEE_DEP', aggfunc='sum').plot(ax = ax, column = \"surface\")\nax.set_axis_off()\nax\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nPour produire cette carte, il serait néanmoins plus simple de directement\nrécupérer les fonds officiels des départements plutôt que d’agréger les\ncontours des communes:\n\ndep = get_vectorfile_ign(\n  borders = \"DEPARTEMENT\",\n  field = \"metropole\",\n  source = \"EXPRESS-COG-CARTO-TERRITOIRE\",\n  provider=\"IGN\")\ndep[\"area\"] = dep.area\nax = dep.plot(column = \"area\")\nax.set_axis_off()\n\nhttps://wxs.ign.fr/x02uy2aiwjo9bm8ce5plwqmr/telechargement/prepackage/ADMINEXPRESS-COG_SHP_TERRITOIRES_PACK_2022-04-15$ADMIN-EXPRESS-COG_3-1__SHP_LAMB93_FXX_2022-04-15/file/ADMIN-EXPRESS-COG_3-1__SHP_LAMB93_FXX_2022-04-15.7z\nData have been previously downloaded and are still available in /tmp/EXPRESS-COG-CARTO-TERRITOIRE-2022/metropole\n\n\n\n\n\n\n\n\n\n\n\n11.3.3 Opérations sur les géométries\nOutre la représentation graphique simplifiée,\nsur laquelle nous reviendrons ultérieurement, l’intérêt principal d’utiliser\nGeoPandas est l’existence de méthodes efficaces pour\nmanipuler la dimension spatiale. Un certain nombre proviennent du\npackage\nShapely.\n\n\n Warning\nLes données sont en système de coordonnées WGS 84 ou pseudo-Mercator (epsg: 4326) et ne sont pas projetées.\nC’est un format approprié lorsqu’il s’agit d’utiliser un fonds\nde carte OpenStreetMap, Stamen, Google Maps, etc.\nMais ce n’est pas un\nformat sur lequel on désire faire des calculs car les distances sont faussées sans utiliser de projection. D’ailleurs, geopandas refusera certaines opérations\nsur des données dont le crs est 4326. On reprojete ainsi les données\ndans la projection officielle pour la métropole, le Lambert 93\n(epsg: 2154).\n\n\nComme indiqué ci-dessus, nous reprojetons les données\ndans le système Lambert 93 qui ne fausse pas les\ncalculs de distance et d’aires.\n\ncommunes = shp_communes.to_crs(2154)\nstations = stations.to_crs(2154)\n\nPar exemple, on peut recalculer la taille d’une commune ou d’arrondissement\navec la méthode area (et diviser par \\(10^6\\) pour avoir des \\(km^2\\) au lieu\ndes \\(m^2\\)):\n\ncommunes['superficie'] = communes.area.div(10**6)\ncommunes.head(3)\n\n\n\n\n\n\n\n\nid\nID\nNOM\nNOM_M\nINSEE_COM\nSTATUT\nPOPULATION\nINSEE_CAN\nINSEE_ARR\nINSEE_DEP\nINSEE_REG\nSIREN_EPCI\nsource\ngeometry\nsuperficie\n\n\n\n\n0\nCOMMUNE_0000000009736037\nNaN\nLevallois-Perret\nLEVALLOIS-PERRET\n92044\nCommune simple\n66082\n16\n2\n92\n11\n200054781/200057982\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((647761.341 6867306.988, 647839.223 6...\n2.417491\n\n\n1\nCOMMUNE_0000000009736055\nNaN\nBois-Colombes\nBOIS-COLOMBES\n92009\nCommune simple\n28841\n11\n2\n92\n11\n200054781/200057990\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((646224.655 6867615.810, 646228.990 6...\n1.926619\n\n\n2\nCOMMUNE_0000000009736538\nNaN\nMalakoff\nMALAKOFF\n92046\nCommune simple\n30950\n18\n1\n92\n11\n200054781/200057966\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((646995.323 6857373.499, 647177.485 6...\n2.070953\n\n\n\n\n\n\n\nUne méthode qu’on utilise régulièrement est centroid qui, comme son nom l’indique,\nrecherche le centroïde de chaque polygone et transforme ainsi des données\nsurfaciques en données ponctuelles. Par exemple, pour\nreprésenter approximativement les centres des villages de la\nHaute-Garonne (31), après avoir téléchargé le fonds de carte adapté,\nfera\n\ncommunes_31 = s3.download_vectorfile_url_all(\n      crs = 4326,\n      values = \"31\",\n      borders=\"COMMUNE\",\n      vectorfile_format=\"topojson\",\n      filter_by=\"DEPARTEMENT\",\n      source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n      year=2022)\n\n# on reprojete en 3857 pour le fond de carte\ncommunes_31 = communes_31.to_crs(3857)\n\n# on calcule le centroide\ndep_31 = communes_31.copy()\ncommunes_31['geometry'] = communes_31['geometry'].centroid\n\nax = communes_31.plot(figsize = (10,10), color = 'red', alpha = 0.4, zorder=2)\ndep_31.to_crs(3857).plot(ax = ax, zorder=1, edgecolor = \"black\", facecolor=\"none\",\n                                                           color = None)\nctx.add_basemap(ax, source = ctx.providers.Stamen.Toner)\nax.set_axis_off()\nax\n\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=31/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\n\n\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 12.0kiB [00:00, 101kiB/s]Downloading: : 40.0kiB [00:00, 178kiB/s]Downloading: : 96.0kiB [00:00, 302kiB/s]Downloading: : 208kiB [00:00, 546kiB/s] Downloading: : 428kiB [00:00, 1.00MiB/s]Downloading: : 872kiB [00:00, 1.90MiB/s]Downloading: : 1.60MiB [00:00, 2.08MiB/s]\n\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "content/course/manipulation/03_geopandas_tutorial/index.html#gérer-le-système-de-projection",
    "href": "content/course/manipulation/03_geopandas_tutorial/index.html#gérer-le-système-de-projection",
    "title": "11  Données spatiales: découverte de geopandas",
    "section": "11.4 Gérer le système de projection",
    "text": "11.4 Gérer le système de projection\nPrécédemment, nous avons appliqué une méthode to_crs pour reprojeter\nles données dans un système de projection différent de celui du fichier\nd’origine :\n\ncommunes = communes.to_crs(2154)\nstations = stations.to_crs(2154)\n\nLe système de projection est fondamental pour que la dimension\nspatiale soit bien interprétée par Python. Un mauvais système de représentation\nfausse l’appréciation visuelle mais peut aussi entraîner des erreurs dans\nles calculs sur la dimension spatiale.\nCe post propose de riches éléments sur le\nsujet, notamment l’image suivante qui montre bien le principe d’une projection :\n\n\n\nLes différents types de projection\n\n\nLa Terre peut ainsi être représentée de multiples manières, ce qui n’est pas neutre sur la manière de se représenter\ncertains continents.\nL’Afrique apparaît beaucoup moins vaste qu’elle ne l’est en réalité sur les cartes utilisant\ncette projection.\nL’une des déformations les mieux connue est celle provoquée par la projection Mercator.\nLe Groënland paraît avoir\nla même surface que l’Amérique du Sud. Pourtant, cette dernière est 8 fois\nplus grande.\nIl existe en fait de nombreuses représentations possibles du monde, plus ou moins\nalambiquées. Les projections sont très nombreuses et certaines peuvent avoir une forme suprenante.\nPar exemple,\nla projection de Spillhaus\npropose de centrer la vue sur les océans et non une terre. C’est pour\ncette raison qu’on parle parfois de monde tel que vu par les poissons\nà son propos.\n\nExemple de reprojection de pays depuis le site thetruesize.com\nConcernant la gestion des projections avec GeoPandas,\nla documentation officielle est très bien\nfaite. Elle fournit notamment l’avertissement suivant qu’il est\nbon d’avoir en tête:\n\nBe aware that most of the time you don’t have to set a projection. Data loaded from a reputable source (using the geopandas.read_file() command) should always include projection information. You can see an objects current CRS through the GeoSeries.crs attribute.\nFrom time to time, however, you may get data that does not include a projection. In this situation, you have to set the CRS so geopandas knows how to interpret the coordinates.\n\n\nImage empruntée à XKCD https://xkcd.com/2256/ qu’on peut également trouver sur https://blog.chrislansdown.com/2020/01/17/a-great-map-projection-joke/\nPour déterminer le système de projection d’une base de données, on peut vérifier l’attribut crs:\n\ncommunes.crs\n\n&lt;Projected CRS: EPSG:2154&gt;\nName: RGF93 v1 / Lambert-93\nAxis Info [cartesian]:\n- X[east]: Easting (metre)\n- Y[north]: Northing (metre)\nArea of Use:\n- name: France - onshore and offshore, mainland and Corsica (France métropolitaine including Corsica).\n- bounds: (-9.86, 41.15, 10.38, 51.56)\nCoordinate Operation:\n- name: Lambert-93\n- method: Lambert Conic Conformal (2SP)\nDatum: Reseau Geodesique Francais 1993 v1\n- Ellipsoid: GRS 1980\n- Prime Meridian: Greenwich\n\n\nLes deux principales méthodes pour définir le système de projection utilisé sont:\n\ndf.set_crs: cette commande sert à préciser quel est le système de projection utilisé, c’est-à-dire comment les coordonnées (x,y) sont reliées à la surface terrestre. Cette commande ne doit pas être utilisée pour transformer le système de coordonnées, seulement pour le définir.\ndf.to_crs: cette commande sert à projeter les points d’une géométrie dans une autre, c’est-à-dire à recalculer les coordonnées selon un autre système de projection.\n\nDans le cas particulier de production de carte avec un fond OpenStreetMaps ou une carte dynamique leaflet, il est nécessaire de dé-projeter les données (par exemple à partir du Lambert-93) pour atterrir dans le système non-projeté WGS 84 (code EPSG 4326). Ce site dédié aux projections géographiques peut-être utile pour retrouver le système de projection d’un fichier où il n’est pas indiqué.\nLa définition du système de projection se fait de la manière suivante (:warning: avant de le faire, se souvenir de l’avertissement !):\ncommunes = communes.set_crs(2154)\nAlors que la reprojection (projection Albers : 5070) s’obtient de la manière suivante :\n\nshp_region = get_vectorfile_ign(\n  borders = \"REGION\", field = \"metropole\",\n  source = \"EXPRESS-COG-CARTO-TERRITOIRE\", provider=\"IGN\")\n\nfig,ax = plt.subplots(figsize=(10, 10))\nshp_region.to_crs(5070).plot(ax = ax)\nax\n\nhttps://wxs.ign.fr/x02uy2aiwjo9bm8ce5plwqmr/telechargement/prepackage/ADMINEXPRESS-COG_SHP_TERRITOIRES_PACK_2022-04-15$ADMIN-EXPRESS-COG_3-1__SHP_LAMB93_FXX_2022-04-15/file/ADMIN-EXPRESS-COG_3-1__SHP_LAMB93_FXX_2022-04-15.7z\nData have been previously downloaded and are still available in /tmp/EXPRESS-COG-CARTO-TERRITOIRE-2022/metropole\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nOn le voit, cela modifie totalement la représentation de l’objet dans l’espace.\nClairement, cette projection n’est pas adaptée aux longitudes et latitudes françaises.\nC’est normal, il s’agit d’une projection adaptée au continent\nnord-américain (et encore, pas dans son ensemble !).\n\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n\nfig,ax = plt.subplots(figsize=(10, 10))\nworld[world.continent == \"North America\"].to_crs(5070).plot(alpha = 0.2, edgecolor = \"k\", ax = ax)\nax\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "content/course/manipulation/03_geopandas_tutorial/index.html#joindre-des-données",
    "href": "content/course/manipulation/03_geopandas_tutorial/index.html#joindre-des-données",
    "title": "11  Données spatiales: découverte de geopandas",
    "section": "11.5 Joindre des données",
    "text": "11.5 Joindre des données\n\n11.5.1 Joindre des données sur des attributs\nCe type de jointure se fait entre un objet géographique et un\ndeuxième objet, géographique ou non. A l’exception de la question\ndes géométries, il n’y a pas de différence par rapport à Pandas.\nLa seule différence avec Pandas est dans la dimension géographique.\nSi on désire conserver la dimension géographique, il faut faire\nattention à faire :\ngeopandas_object.merge(pandas_object)\nSi on utilise deux objets géographiques mais ne désire conserver qu’une seule\ndimension géographique2, on fera\ngeopandas_object1.merge(geopandas_object2)\nSeule la géométrie de l’objet de gauche\nsera conservée, même si on fait un right join.\n\n\n11.5.2 Prolongation possible : joindre des données sur dimension géographique\nLe chapitre suivant permettra de mettre en oeuvre des\njointures géographiques.\n\n\n Hint\nLes jointures spatiales peuvent être très gourmandes en ressources (car il peut être nécessaire de croiser toutes les géométries de x avec toutes les géométries de y). Voici deux conseils qui peuvent vous aider :\n\nIl est préférable de tester les jointures géographiques sur un petit échantillon de données, pour estimer le temps et les ressources nécessaires à la réalisation de la jointure.\nIl est parfois possible d’écrire une fonction qui réduit la taille du problème. Exemple: vous voulez déterminer dans quelle commune se situe un logement dont vous connaissez les coordonnées et le département; vous pouvez écrire une fonction qui réalise pour chaque département une jointure spatiale entre les logements situés dans ce département et les communes de ce département, puis empiler les 101 tables de sorties."
  },
  {
    "objectID": "content/course/manipulation/03_geopandas_tutorial/index.html#annexe",
    "href": "content/course/manipulation/03_geopandas_tutorial/index.html#annexe",
    "title": "11  Données spatiales: découverte de geopandas",
    "section": "11.6 Annexe",
    "text": "11.6 Annexe\n\n11.6.1 Récupération des données depuis data.gouv\nAvec cette méthode, les données des limites administratives demandent donc un peu de travail pour être\nimportées car elles sont zippées (mais c’est un bon exercice !).\nLe code suivant, dont les\ndétails apparaîtront plus clairs après la lecture de la partie\nwebscraping permet de :\n\nTélécharger les données avec requests dans un dossier temporaire\nLes dézipper avec le module zipfile\n\nLa fonction suivante automatise un peu le processus :\n\nimport requests\nimport tempfile\nimport zipfile\n\nurl = 'https://www.data.gouv.fr/fr/datasets/r/0e117c06-248f-45e5-8945-0e79d9136165'\ntemporary_location = tempfile.gettempdir()\n\ndef download_unzip(url, dirname = tempfile.gettempdir(), destname = \"borders\"):\n  myfile = requests.get(url)\n  open(\"{}/{}.zip\".format(dirname, destname), 'wb').write(myfile.content)\n  with zipfile.ZipFile(\"{}/{}.zip\".format(dirname, destname), 'r') as zip_ref:\n      zip_ref.extractall(dirname + '/' + destname)\n\n\ndownload_unzip(url)\n\n\nshp_communes = gpd.read_file(temporary_location + \"/borders/communes-20220101.shp\")\n\nIci, les données ne sont pas projetées puisqu’elles sont dans le\nsystème WSG84 (epsg: 4326) ce qui permet de facilement ajouter\nun fonds de carte Openstreetmap ou Stamen pour rendre une représentation\ngraphique plus esthétique.\nEn toute rigueur, pour faire une carte statique d’un pays en particulier,\nil faudrait reprojeter les données dans un système de projection adapté à la zone géographique étudiée\n(par exemple le Lambert-93 pour la France métropolitaine).\nOn peut ainsi représenter Paris pour se donner une idée de la nature\ndu shapefile utilisé :\n\nparis = shp_communes.loc[shp_communes['insee'].str.startswith(\"75\")]\n\nfig,ax = plt.subplots(figsize=(10, 10))\nparis.plot(ax = ax, alpha=0.5, edgecolor='blue')\nctx.add_basemap(ax, crs = paris.crs.to_string())\nax.set_axis_off()\nax\n\nOn voit ainsi que les données pour Paris ne comportent pas d’arrondissement,\nce qui est limitant pour une analyse focalisée sur Paris. On va donc les\nrécupérer sur le site d’open data de la ville de Paris et les substituer\nà Paris :\n\narrondissements = gpd.read_file(\"https://opendata.paris.fr/explore/dataset/arrondissements/download/?format=geojson&timezone=Europe/Berlin&lang=fr\")\narrondissements = arrondissements.rename(columns = {\"c_arinsee\": \"insee\"})\narrondissements['insee'] = arrondissements['insee'].astype(str)\nshp_communes = shp_communes[~shp_communes.insee.str.startswith(\"75\")].append(arrondissements)\n\nPour produire la carte, il faudrait faire:\n\nparis = shp_communes.loc[shp_communes.insee.str.startswith(\"75\")]\n\nfig,ax = plt.subplots(figsize=(10, 10))\n\nparis.plot(ax = ax, alpha=0.5, edgecolor='k')\nctx.add_basemap(ax, crs = paris.crs.to_string())\nax.set_axis_off()\nax\n\n\n\n11.6.2 Récupération des données depuis le package pynsee\nPour connaître les contraintes d’installation du package pynsee,\nse référer à la partie de cours dédiée à Pandas.\n\n#le téléchargement des données prend plusieurs minutes\nfrom pynsee.geodata import get_geodata\nshp_communes = gpd.GeoDataFrame(\n  get_geodata('ADMINEXPRESS-COG-CARTO.LATEST:commune')\n)\nshp_communes = shp_communes.rename({\"insee_com\": 'insee'}, axis = 'columns')\n#shp_communes = shp_communes.set_crs(3857)"
  },
  {
    "objectID": "content/course/manipulation/03_geopandas_TP/index.html#lire-et-enrichir-des-données-spatiales",
    "href": "content/course/manipulation/03_geopandas_TP/index.html#lire-et-enrichir-des-données-spatiales",
    "title": "12  Pratique de geopandas avec les données vélib",
    "section": "12.1 Lire et enrichir des données spatiales",
    "text": "12.1 Lire et enrichir des données spatiales\nDans cette partie,\nnous utiliserons\nle package cartiflette\nqui facilite la récupération de contours de cartes.\nUne version antérieure de cet exercice, présentée sous forme\nd’exercice supplémentaire 👇️, utilisait des fonds de carte issus\nde data.gouv.\n\n Exercice 1: lire et explorer la structure de fichiers géographiques\n\nS’inspirer des exemples de code présents dans le chapitre précédent, mobilisant\nle package cartiflette\npour télécharger les données communales des départements 75, 92, 93 et 94.\nVous pouvez nommer l’objet communes_borders\nRegarder les premières lignes des données. Identifier la différence avec\nun DataFrame standard.\nAfficher l’attribut crs de communes_borders. Ce dernier contrôle la\ntransformation de l’espace tridimensionnel terrestre en une surface plane.\nUtiliser to_crs pour transformer les données en Lambert 93 (code EPSG 2154).\nAfficher les communes des Hauts de Seine (département 92) et utiliser la méthode\nplot\nRéprésenter la carte de Paris : quel est le problème ?\n\n\n:::\n\n\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=75/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=92/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=93/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=94/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\n\n\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 12.0kiB [00:00, 99.0kiB/s]Downloading: : 14.5kiB [00:00, 117kiB/s] \nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 12.0kiB [00:00, 96.0kiB/s]Downloading: : 40.0kiB [00:00, 169kiB/s] Downloading: : 62.3kiB [00:00, 243kiB/s]\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 12.0kiB [00:00, 91.6kiB/s]Downloading: : 36.0kiB [00:00, 144kiB/s] Downloading: : 84.0kiB [00:00, 243kiB/s]Downloading: : 116kiB [00:00, 241kiB/s] Downloading: : 119kiB [00:00, 222kiB/s]\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 12.0kiB [00:00, 106kiB/s]Downloading: : 44.0kiB [00:00, 205kiB/s]Downloading: : 100kiB [00:00, 331kiB/s] Downloading: : 110kiB [00:00, 312kiB/s]\n\n\n\n\n\n\n\n\n\n\n\nid\nID\nNOM\nNOM_M\nINSEE_COM\nSTATUT\nPOPULATION\nINSEE_CAN\nINSEE_ARR\nINSEE_DEP\nINSEE_REG\nSIREN_EPCI\nsource\ngeometry\n\n\n\n\n0\nCOMMUNE_0000000009736048\nNaN\nParis\nPARIS\n75056\nCapitale d'état\n2165423\nNR\n1\n75\n11\n200054781\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.36421 48.81640, 2.36333 48.81615, ...\n\n\n0\nCOMMUNE_0000000009736037\nNaN\nLevallois-Perret\nLEVALLOIS-PERRET\n92044\nCommune simple\n66082\n16\n2\n92\n11\n200054781/200057982\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.28739 48.90364, 2.28846 48.90302, ...\n\n\n1\nCOMMUNE_0000000009736055\nNaN\nBois-Colombes\nBOIS-COLOMBES\n92009\nCommune simple\n28841\n11\n2\n92\n11\n200054781/200057990\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.26639 48.90629, 2.26645 48.90615, ...\n\n\n2\nCOMMUNE_0000000009736538\nNaN\nMalakoff\nMALAKOFF\n92046\nCommune simple\n30950\n18\n1\n92\n11\n200054781/200057966\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.27818 48.81425, 2.28066 48.81469, ...\n\n\n3\nCOMMUNE_0000000009736038\nNaN\nClichy\nCLICHY\n92024\nCommune simple\n63089\n09\n2\n92\n11\n200054781/200057990\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.30377 48.89415, 2.30258 48.89487, ...\n\n\n\n\n\n\n\n\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nLa carte du 92 est la suivante:\n\n\n\n\n\n\n\n\n\nQuant à Paris, à l’issue de la question 5, la carte\naura l’aspect suivant:\n\n\n\n\n\n\n\n\n\nEn effet, on ne dispose ainsi pas des limites des arrondissements parisiens, ce\nqui appauvrit grandement la carte de Paris.\nOn pourrait les récupérer directement\ndepuis le site d’open-data du Grand Paris, ce qui est proposé\nen exercice supplémentaire 👇️.\nOn propose ici d’utiliser à nouveau\ncartiflette pour cela afin de disposer du fonds de carte officiel.\n\n\n Exercice 2: compléter des données spatiales issues de sources différentes\n\nImporter les données de découpage des arrondissements parisiens à l’adresse à l’aide de cartiflette.\nVérifier sur une carte que les découpages des arrondissements sont bien présents.\nVérifier l’attribut crs. Est-il cohérent avec celui des données communales ?\nSi non, transformer en Lambert 93 (code EPSG 2154).\nRetirer Paris du jeu de données communales et utiliser les arrondissements\npour enrichir (nommer l’objet obtenu data_borders).\nReprésenter à nouveau les communes de la petite couronne parisienne (75, 92, 93, 94)\n\n\n\n\n\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=ARRONDISSEMENT_MUNICIPAL/crs=4326/DEPARTEMENT=75/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\n\n\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 12.0kiB [00:00, 99.8kiB/s]Downloading: : 37.0kiB [00:00, 161kiB/s] Downloading: : 38.0kiB [00:00, 154kiB/s]\n\n\nLa carte de Paris intra-muros est, après la\nrécupération des arrondissements avec\ncartiflette de ce type là:\n\n\n\n\n\n\n\n\n\n\n\nEPSG:4326\nEPSG:4326\nFalse\n\n\nLa carte obtenue à l’issue de la question 6, c’est-à-dire après\navoir consolidé les données, devrait avoir l’aspect suivant:"
  },
  {
    "objectID": "content/course/manipulation/03_geopandas_TP/index.html#utiliser-des-données-géographiques-comme-des-couches-graphiques",
    "href": "content/course/manipulation/03_geopandas_TP/index.html#utiliser-des-données-géographiques-comme-des-couches-graphiques",
    "title": "12  Pratique de geopandas avec les données vélib",
    "section": "12.2 Utiliser des données géographiques comme des couches graphiques",
    "text": "12.2 Utiliser des données géographiques comme des couches graphiques\nSouvent, le découpage communal ne sert qu’en fond de cartes, pour donner des\nrepères. En complément de celui-ci, on peut désirer exploiter\nun autre jeu de données.\nOn va partir des données de localisation des\nstations velib,\ndisponibles sur le site d’open data de la ville de Paris et\nrequêtables directement par l’url\nhttps://opendata.paris.fr/explore/dataset/velib-emplacement-des-stations/download/?format=geojson&timezone=Europe/Berlin&lang=fr\n\n\n Exercice 3: importer et explorer les données velib\n\nImporter les données velib sous le nom station\nVérifier la projection géographique de station (attribut crs). Si celle-ci est différente des données communales, reprojeter ces\ndernières dans le même système de projection que les stations de vélib\nReprésenter sur une carte les 50 stations les plus importantes (variable capacity). Vous pouvez également afficher le fonds de carte des arrondissements de Paris.\nCette page peut vous aider pour comprendre comment afficher plusieurs couches à la fois. Vous pouvez customiser la carte en retirant les axes grâce à la méthode set_axis_off et mettre un titre tel que “Les 50 principales stations de Vélib” avec la méthode set_title.\nAfficher également (trait bleu et épais) les réseaux de transport en communs, disponibles ici. L’url à requêter est\nhttps://data.iledefrance-mobilites.fr/explore/dataset/traces-du-reseau-ferre-idf/download/?format=geojson&timezone=Europe/Berlin&lang=fr\n\n\n\n\n\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 73.7kiB [00:00, 359kiB/s]Downloading: : 362kiB [00:00, 1.19MiB/s]\n\n\n\n\n\n\n\n\n\ncapacity\nname\nstationcode\ngeometry\n\n\n\n\n0\n23\nArtois - Berri\n8103\nPOINT (2.30643 48.87394)\n\n\n1\n25\nFélix Ziem - Armand Gauthier\n18111\nPOINT (2.33339 48.88950)\n\n\n2\n14\nBoudreau - Auber\n9106\nPOINT (2.32942 48.87219)\n\n\n3\n48\nPereire - Ternes\n17040\nPOINT (2.28801 48.87974)\n\n\n4\n40\nInstitut du Monde Arabe - Saint-Germain\n5020\nPOINT (2.35581 48.84906)\n\n\n\n\n\n\n\nLa carte attendu à l’issue de la question 3 a l’aspect suivant:\n\n\nText(0.5, 1.0, 'Les 50 principales stations de Vélib')\n\n\n\n\n\n\n\n\n\n\n\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 228kiB [00:00, 1.23MiB/s]Downloading: : 463kiB [00:00, 1.73MiB/s]Downloading: : 1.00MiB [00:00, 3.24MiB/s]Downloading: : 1.89MiB [00:00, 5.41MiB/s]Downloading: : 2.46MiB [00:01, 2.18MiB/s]Downloading: : 3.25MiB [00:01, 2.96MiB/s]\n\n\n['TRAIN' 'RER' 'METRO' 'TER' 'TRAMWAY' 'NAVETTE']\n\n\nL’ajout du réseau de métro permet d’obtenir une carte ressemblant à celle-ci:\n\n\nText(0.5, 1.0, 'Les 50 principales stations de Vélib')\n\n\n\n\n\n\n\n\n\nPour faire une belle carte, il faudrait couper les lignes de métro via une jointure spatiale ou\nutiliser un fonds de carte conceptuel.\nL’exercice suivant propose de mettre en oeuvre la deuxième méthode. La première\nest proposée en exercice supplémentaire 👇️.\n\n\n Exercice 4: ajouter un fond de carte\n\nRecréer par couche successive la carte précédente, que vous pouvez nommer base\nUtiliser add_basemap du package contextily\npour ajouter, en arrière plan, un fonds de carte\nJouer avec les fonds disponibles en utilisant l’argument source\n\n\n\nPar exemple, en utilisant le fond Stamen.Watercolor, on obtient la carte\nsuivante. Celle-ci permet déjà de mieux localiser les stations."
  },
  {
    "objectID": "content/course/manipulation/03_geopandas_TP/index.html#jointures-spatiales",
    "href": "content/course/manipulation/03_geopandas_TP/index.html#jointures-spatiales",
    "title": "12  Pratique de geopandas avec les données vélib",
    "section": "12.3 Jointures spatiales",
    "text": "12.3 Jointures spatiales\nLes jointures attributaires fonctionnent comme avec un DataFrame pandas.\nPour conserver un objet spatial in fine, il faut faire attention à utiliser en premier (base de gauche) l’objet GeoPandas.\nEn revanche, l’un des intérêts des objets geopandas est qu’on peut également faire une jointure sur la dimension spatiale grâce à sjoin.\nLa documentation à laquelle se référer est ici.\n\n\n Exercice 5 : Associer les stations aux communes et arrondissements auxquels elles appartiennent\n\nFaire une jointure spatiale pour enrichir les données de stations en y ajoutant des informations de data_paris. Appeler cet objet stations_info\nReprésenter la carte des stations du 19e arrondissement (s’aider de la variable c_ar). Vous pouvez mettre en fond de carte les arrondissements parisiens.\nCompter le nombre de stations velib et le nombre de places velib par arrondissement ou commune (pour vous aider, vous pouvez compléter vos connaissances avec ce tutoriel). Représenter sur une carte chacune des informations\nReprésenter les mêmes informations mais en densité (diviser par la surface de l’arrondissement ou commune en km2)\n(optionnel) Choisir une des cartes de densité et la nettoyer (retirer les axes, mettre les titres…)\n\n\n\n\n\n\n\n\n\n\n\n\ncapacity\nname\nstationcode\ngeometry\nindex_right\nid\nID\nNOM\nNOM_M\nINSEE_COM\nSTATUT\nPOPULATION\nINSEE_CAN\nINSEE_ARR\nINSEE_DEP\nINSEE_REG\nSIREN_EPCI\nsource\nINSEE_ARM\n\n\n\n\n0\n23\nArtois - Berri\n8103\nPOINT (2.30643 48.87394)\n9\nARR_MUNI0000000009736044\nNaN\nParis 8e Arrondissement\nPARIS 8E ARRONDISSEMENT\n75056\nNaN\n35655\nNaN\nNaN\n75\nNaN\nNaN\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n75108\n\n\n17\n36\nBienfaisance - Place de Narvik\n8035\nPOINT (2.31298 48.87660)\n9\nARR_MUNI0000000009736044\nNaN\nParis 8e Arrondissement\nPARIS 8E ARRONDISSEMENT\n75056\nNaN\n35655\nNaN\nNaN\n75\nNaN\nNaN\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n75108\n\n\n26\n29\nBeaujon - Wagram\n8056\nPOINT (2.29644 48.87535)\n9\nARR_MUNI0000000009736044\nNaN\nParis 8e Arrondissement\nPARIS 8E ARRONDISSEMENT\n75056\nNaN\n35655\nNaN\nNaN\n75\nNaN\nNaN\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n75108\n\n\n28\n37\nRoquépine - Malesherbes\n8015\nPOINT (2.32040 48.87345)\n9\nARR_MUNI0000000009736044\nNaN\nParis 8e Arrondissement\nPARIS 8E ARRONDISSEMENT\n75056\nNaN\n35655\nNaN\nNaN\n75\nNaN\nNaN\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n75108\n\n\n60\n32\nPierre Charron - Champs-Elysées\n8040\nPOINT (2.30435 48.87010)\n9\nARR_MUNI0000000009736044\nNaN\nParis 8e Arrondissement\nPARIS 8E ARRONDISSEMENT\n75056\nNaN\n35655\nNaN\nNaN\n75\nNaN\nNaN\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n75108\n\n\n\n\n\n\n\nPour la question 2,\nla première méthode consiste à afficher\ntoute la ville mais à ne représenter que\nles points des stations du 19e:\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nNéanmoins, il est préférable de se centrer sur\nle 19e en premier lieu, ce qui donne une\ncarte comme celle-ci:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nID\nNOM\nNOM_M\nINSEE_COM\nSTATUT\nPOPULATION\nINSEE_CAN\nINSEE_ARR\nINSEE_DEP\nINSEE_REG\nSIREN_EPCI\nsource\ngeometry\nINSEE_ARM\nstationcode\ncapacity\n\n\n\n\n0\nCOMMUNE_0000000009736037\nNaN\nLevallois-Perret\nLEVALLOIS-PERRET\n92044\nCommune simple\n66082\n16\n2\n92\n11\n200054781/200057982\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.28739 48.90364, 2.28846 48.90302, ...\nNaN\n11\n334\n\n\n1\nCOMMUNE_0000000009736055\nNaN\nBois-Colombes\nBOIS-COLOMBES\n92009\nCommune simple\n28841\n11\n2\n92\n11\n200054781/200057990\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.26639 48.90629, 2.26645 48.90615, ...\nNaN\n2\n60\n\n\n2\nCOMMUNE_0000000009736538\nNaN\nMalakoff\nMALAKOFF\n92046\nCommune simple\n30950\n18\n1\n92\n11\n200054781/200057966\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.27818 48.81425, 2.28066 48.81469, ...\nNaN\n8\n238\n\n\n3\nCOMMUNE_0000000009736038\nNaN\nClichy\nCLICHY\n92024\nCommune simple\n63089\n09\n2\n92\n11\n200054781/200057990\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.30377 48.89415, 2.30258 48.89487, ...\nNaN\n13\n422\n\n\n4\nCOMMUNE_0000000009736052\nNaN\nNanterre\nNANTERRE\n92050\nPréfecture\n96277\n99\n2\n92\n11\n200054781/200057982\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.22910 48.90603, 2.23037 48.90377, ...\nNaN\n8\n221\n\n\n\n\n\n\n\nLa carte des places disponibles est celle-ci:\n\n\nText(0.5, 1.0, 'Nombre de places disponibles')\n\n\n\n\n\n\n\n\n\nAlors que la carte des capacités de stations est\nplutôt celle-là:\n\n\nText(0.5, 1.0, 'Nombre de stations')\n\n\n\n\n\n\n\n\n\nPas vraiment de différence marquée entre les\ndeux, on peut se contenter de regarder la capacité.\nEnfin, dans la question 4,\nsi on représente plutôt la capacité\nsous forme de densité, pour tenir compte\nde la taille différente des arrondissements,\non obtient cette carte:\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nAvec une palette plasma_r, cela donne plutôt cette carte:\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nAvec un peu de travail sur l’esthétique, la carte\nque vous obtenez à l’issue de l’exercice\nressemble à celle-ci:"
  },
  {
    "objectID": "content/course/manipulation/03_geopandas_TP/index.html#trouver-les-toilettes-publiques-les-plus-proches",
    "href": "content/course/manipulation/03_geopandas_TP/index.html#trouver-les-toilettes-publiques-les-plus-proches",
    "title": "12  Pratique de geopandas avec les données vélib",
    "section": "12.4 Trouver les toilettes publiques les plus proches",
    "text": "12.4 Trouver les toilettes publiques les plus proches\n\n12.4.1 Objectif\nJusqu’à présent, nous nous sommes familiarisés avec\nla manipulation de données spatiales et la représentation\nrapide de celle-ci grâce aux fonctionalités de GeoPandas.\nA partir de maintenant, nous allons utiliser GeoPandas\npour des tâches de manipulation géométrique.\nCes opérations reposeront sur des tâches classiques\nde la géomatique qui sont facilitées par le fait que\nGeoPandas offre une surcouche au package Shapely\nde la même manière que Pandas était une sur-couche\nde Numpy pour les opérations numériques.\nL’exemple suivant permet d’illustrer\nle principe d’une des\nopérations que nous allons utiliser,\nà savoir la recherche de plus proche point:\n\nfrom shapely.ops import Polygon\nfrom shapely.ops import nearest_points\ntriangle = Polygon([(0, 0), (1, 0), (0.5, 1), (0, 0)])\nsquare = Polygon([(0, 2), (1, 2), (1, 3), (0, 3), (0, 2)])\n[o.wkt for o in nearest_points(triangle, square)]\n\n['POINT (0.5 1)', 'POINT (0.5 2)']\n\n\nGeoPandas va permettre de généraliser ce processus\nen utilisant non plus deux listes modifiées (les\npolygones de Shapely) mais des DataFrames géographiques.\nCela permettra, au passage, d’enrichir les\njointures spatiales avec les attributs des DataFrames\nconcernés.\nSur Shapely, vous pourrez trouver une aide ici.\nNéanmoins, à mesure que GeoPandas se développe, il\ndevient de moins en moins nécessaire d’utiliser directement\nShapely.\n\n\n12.4.2 Mise en application\nNous allons rechercher les toilettes publiques les\nplus proches de chaque station.\nSans les fonctionalités de GeoPandas,\ncette recherche serait assez pénible.\n\n\n Exercice 5 (optionnel) : Trouver les toilettes publiques les plus proches d'une station de vélib\n\nCharger la localisation des toilettes publiques présente ici : https://data.ratp.fr/explore/dataset/sanitaires-reseau-ratp/download/?format=geojson&timezone=Europe/Berlin&lang=fr. Appelez-la toilettes_publiques.\nConvertir les objets toilettes_publiques et stations en projection Lambert-93 (CRS 2154). Cette\nconversion permettra de mesurer en mètres les distances entre objets géographiques. Sans\ncelle-ci, nous ferions des distances entre coordonnées GPS, ce qui n’aide pas l’analyse\net l’interprétation.\nUtiliser la jointure spatiale par plus proche distance sjoin_nearest pour associer à chaque station les toilettes publiques les plus proches\nTrouver les toilettes publiques les plus proches des stations de vélib autour d’Edgard Quinet.\nReprésenter un histogramme des distances aux toilettes les plus proches\n\n\n\nLe jeu de données open-data des toilettes\npubliques présente l’aspect suivant:\n\n\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 27.1kiB [00:00, 9.99MiB/s]\n\n\n\n\n\n\n\n\n\naccessible_au_public\nacces_bouton_poussoir\ntarif_gratuit_payant\naccessibilite_pmr\ngestionnaire\nligne\nacces_passe_navigo_ou_ticket_t\nlocalisation\nen_zone_controlee\nstation\nhors_zone_controlee_station\nhors_zone_controlee_voie_publique\ngeometry\n\n\n\n\n0\nNaN\nNaN\nNaN\nNaN\nToilette publique RATP\n4\nNaN\nIl est accessible depuis le guichet au niveau ...\nNaN\nBagneux Lucie Aubrac\nNaN\nNaN\nPOINT (2.31740 48.80357)\n\n\n1\noui\nNaN\npayant\noui\nToilette publique RATP\n10\noui\nSortie 2 boulevard St-Germain, à proximité du ...\nNaN\nCluny - La Sorbonne\nNaN\nNaN\nPOINT (2.34503 48.85080)\n\n\n\n\n\n\n\nLes toilettes les plus proches\nd’Edgar Quinet sont les suivantes:\n\n\n\n\n\n\n\n\n\nlocalisation\nstation\nname\ndistance\n\n\n\n\n801\nSur le quai ligne B, en direction de Saint-Rém...\nDenfert-Rochereau\nGare Montparnasse - Edgar Quinet\n1177.501002\n\n\n1050\nSur le quai ligne B, en direction de Saint-Rém...\nDenfert-Rochereau\nEdgar Quinet - Raspail\n737.660246\n\n\n1060\nSur le quai ligne B, en direction de Saint-Rém...\nDenfert-Rochereau\nEdgar Quinet - Gaité\n1091.971078\n\n\n\n\n\n\n\nIl va donc falloir se\nretenir un peu car s’agit de toilettes situées\nà la station Denfert Rochereau !\nEnfin, de manière plus globale, voici la distribution\ndes distances aux toilettes les plus proches:\n\n\n&lt;Axes: ylabel='Frequency'&gt;\n\n\n\n\n\n\n\n\n\nLe mode de la distribution est entre 1 et 2 km, ce\nqui est une petite distance tout de même !\nC’est normal, il ne s’agit pas de l’ensemble des\ntoilettes publiques de la ville de Paris mais\nde celles gérées par la RATP. Rassurez-vous, au\nmoins dans Paris intra-muros, vous n’avez pas\nà systématiquement marcher (ou rouler) autant."
  },
  {
    "objectID": "content/course/manipulation/03_geopandas_TP/index.html#exo-supp",
    "href": "content/course/manipulation/03_geopandas_TP/index.html#exo-supp",
    "title": "12  Pratique de geopandas avec les données vélib",
    "section": "12.5 Exercices supplémentaires",
    "text": "12.5 Exercices supplémentaires\nVoici une fonction pour télécharger et dézipper\nfacilement un fonds de carte issu de data.gouv\n\nimport requests\nimport tempfile\nimport zipfile\n\ntemporary_location = tempfile.gettempdir()\n\ndef download_unzip(url, dirname = tempfile.gettempdir(), destname = \"borders\"):\n  myfile = requests.get(url)\n  open(\"{}/{}.zip\".format(dirname, destname), 'wb').write(myfile.content)\n  with zipfile.ZipFile(\"{}/{}.zip\".format(dirname, destname), 'r') as zip_ref:\n      zip_ref.extractall(dirname + '/' + destname)\n\n\n\n Exercice optionnel 1: télécharger et dézipper vous-même le fonds de carte\nImporter le fichier avec le package GeoPandas\n(si vous avez laissé les paramètres par défaut,\nle fichier devrait\nêtre à l’emplacement temporary_location + \"/borders/communes-20210101.shp\").\n\n\n\n\n Exercice optionnel 2 : Utiliser les arrondissements fournis par l'open data parisien\n\nImporter les données de découpage des arrondissements parisiens à l’adresse\nhttps://opendata.paris.fr/explore/dataset/arrondissements/download/?format=geojson&timezone=Europe/Berlin&lang=fr\nVérifier sur une carte que les découpages des arrondissements sont bien présents.\nVérifier l’attribut crs. Est-il cohérent avec celui des données communales ?\nRetirer Paris du jeu de données communales et utiliser les arrondissements\npour enrichir (nommer l’objet obtenu data_borders). Ici, on peut ne pas se\nsoucier de la variable commune de superficie aux niveaux différents car on\nva la recréer. En revanche, renommer la variable c_arinsee en insee avec\nla méthode rename et faire attention aux types des variables\n\n\n\n\n\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 119kiB [00:00, 668kiB/s]Downloading: : 206kiB [00:00, 1.11MiB/s]\n\n\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\n\n\nEPSG:4326\nFalse\n\n\n\n\n\n\n\n\n\n\n\nid\nID\nNOM\nNOM_M\nINSEE_COM\nSTATUT\nPOPULATION\nINSEE_CAN\nINSEE_ARR\nINSEE_DEP\n...\nSIREN_EPCI\nsource\ngeometry\nc_ar\nl_aroff\nsurface\nl_ar\nn_sq_co\nn_sq_ar\nperimetre\n\n\n\n\n141\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n75\n...\nNaN\nNaN\nPOLYGON ((650546.989 6863492.739, 650536.022 6...\n8.0\nÉlysée\n3.880036e+06\n8ème Ardt\n750001537.0\n750000008.0\n7880.533268\n\n\n142\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n75\n...\nNaN\nNaN\nPOLYGON ((648584.964 6861576.307, 648698.567 6...\n15.0\nVaugirard\n8.494994e+06\n15ème Ardt\n750001537.0\n750000015.0\n13678.798315\n\n\n\n\n2 rows × 21 columns\n\n\n\n\n12.5.1 Jointures spatiales\nL’objectif de cet exercice est de ne conserver que les\nlignes de transports à l’intérieur de Paris intra-muros.\nIl s’agit d’appliquer les jointures spatiales de manière\nun petit peu différente à précédemment.\n\n Exercice optionnel 3 : Les lignes de transport dans Paris\n\nUtiliser l’URL https://data.iledefrance-mobilites.fr/explore/dataset/traces-du-reseau-ferre-idf/download/?format=geojson&timezone=Europe/Berlin&lang=fr pour récupérer les lignes de transport\nde la RATP. L’appeler transports.\nA partir des arrondissements parisiens, utiliser unary_union pour créer un unique polygone parisien. Utiliser within pour ne conserver que les points de transports qui se trouvent\ndans Paris intra-muros\nReprésenter graphiquement\n\n\n:::\n\n\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 228kiB [00:00, 1.23MiB/s]Downloading: : 439kiB [00:00, 1.63MiB/s]Downloading: : 936kiB [00:00, 2.77MiB/s]Downloading: : 1.90MiB [00:01, 1.53MiB/s]Downloading: : 2.48MiB [00:01, 2.09MiB/s]Downloading: : 3.25MiB [00:01, 2.47MiB/s]\n\n\nLa carte obtenue aura l’aspect suivant:\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nCette fois, on a bien conservé que les lignes de transport dans\nParis. Un peu de travail sur le rendu serait nécessaire pour\nobtenir une belle carte. Vous pouvez le faire en exercice, après\navoir consulté le chapitre relatif à la cartographie dans\nla partie visualisation de données."
  },
  {
    "objectID": "content/course/manipulation/04a_webscraping_TP/index.html#enjeux",
    "href": "content/course/manipulation/04a_webscraping_TP/index.html#enjeux",
    "title": "13  Webscraping avec python",
    "section": "13.1 Enjeux",
    "text": "13.1 Enjeux\nUn certain nombre d’enjeux du webscraping ne seront évoqués\nque superficiellement dans le cadre de ce chapitre.\n\n13.1.1 La zone grise de la légalité du webscraping\nEn premier lieu, en ce qui concerne la question de la légalité\nde la récupération d’information par scraping, il existe\nune zone grise. Ce n’est pas parce qu’une information est\ndisponible sur internet, directement ou avec un peu de recherche,\nqu’elle peut être récupérée et réutilisée.\nL’excellent cours d’Antoine Palazzolo évoque un certain nombre de cas\nmédiatiques et judiciaires sur cette question.\nDans le champ français, la CNIL a publié en 2020\nde nouvelles directives sur le webscraping reprécisant\nque toute donnée ne peut être réutilisée à l’insu de la personne\nà laquelle ces données appartiennent. Autrement dit, en principe,\nles données collectées par webscraping sont soumises au\nRGPD, c’est-à-dire nécessitent le consentement des personnes\nà partir desquelles la réutilisation des données est faite.\nIl est donc recommandé d’être vigilant avec les données récupérées\npar webscraping pour ne pas se mettre en faute légalement.\n\n\n13.1.2 Stabilité et fiabilité des informations reçues\nLa récupération de données par webscraping\nest certes pratique mais elle ne correspond pas nécessairement\nà un usage pensé, ou désiré, par un fournisseur de données.\nLes données étant coûteuses à collecter et à mettre à disposition,\ncertains sites ne désirent pas nécessairement que celles-ci soient\nextraites gratuitement et facilement. A fortiori lorsque la donnée\npeut permettre à un concurrent de disposer d’une information\nutile d’un point de vue commercial (prix d’un produit concurrent, etc.).\nLes acteurs mettent donc souvent en oeuvre des stratégies pour bloquer ou\nlimiter la quantité de données scrappées. La méthode la plus\nclassique est la détection et le blocage\ndes requêtes faites par des robots plutôt que par des humains.\nPour des acteurs spécialisés, cette détection est très facile car\nde nombreuses preuves permettent d’identifier si une visite du site web\nprovient d’un utilisateur\nhumain derrière un navigateur ou d’un robot. Pour ne citer que quelques indices:\nvitesse de la navigation entre pages, rapidité à extraire la donnée,\nempreinte digitale du navigateur utilisé, capacité à répondre à des\nquestions aléatoires (captcha)…\nLes bonnes pratiques, évoquées par la suite, ont pour objectif de faire\nen sorte qu’un robot se comporte de manière civile en adoptant un comportement\nproche de celui de l’humain mais sans contrefaire le fait qu’il ne s’agit\npas d’un humain.\nIl convient d’ailleurs\nd’être prudent quant aux informations reçues par webscraping.\nLa donnée étant au coeur du modèle économique de certains acteurs, certains\nn’hésitent pas à renvoyer des données fausses aux robots\nplutôt que les bloquer. C’est de bonne guerre!\nUne autre technique piège s’appelle le honey pot. Il s’agit de pages qu’un humain\nn’irait jamais visiter - par exemple parce qu’elles n’apparaissent pas dans\nl’interface graphique - mais sur lesquelles un robot, en recherche automatique\nde contenu, va rester bloquer.\nSans aller jusqu’à la stratégie de blocage du webscraping, d’autres raisons\npeuvent expliquer qu’une récupération de données ait fonctionné par\nle passé mais ne fonctionne plus. La plus fréquente est un changement dans la structure\nd’un site web. Le webscraping présente en effet l’inconvénient d’aller chercher\nde l’information dans une structure très hiérarchisée. Un changement dans cette structure\npeut suffire à rendre un robot incapable de récupérer du contenu. Or, pour rester\nattractifs, les sites web changent fréquemment ce qui peut facilement\nrendre inopérant un robot.\nDe manière générale, l’un des principaux messages de ce\nchapitre, à retenir, est que le\nwebscraping est une solution de dernier ressort, pour des récupérations ponctuelles de données sans garantie de fonctionnement ultérieur. Il est préférable de privilégier les API lorsque celles-ci sont disponibles.\nCes dernières ressemblent à un contrat (formel ou non) entre un fournisseur de données\net un utilisateur où sont définis des besoins (les données) mais aussi des\nconditions d’accès (nombre de requêtes, volumétrie, authentification…) là\noù le webscraping est plus proche du comportement dans le Far West.\n\n\n13.1.3 Les bonnes pratiques\nLa possibilité de récupérer des données par l’intermédiaire\nd’un robot ne signifie pas qu’on peut se permettre de n’être\npas civilisé. En effet, lorsqu’il est non-maîtrisé, le\nwebscraping peut ressembler à une attaque informatique\nclassique pour faire sauter un site web: le déni de service.\nLe cours d’Antoine Palazzolo revient\nsur certaines bonnes pratiques qui ont émergé dans la communauté\ndes scrapeurs. Il est recommandé de lire cette ressource\npour en apprendre plus sur ce sujet. Y sont évoqués\nplusieurs conventions, parmi lesquelles :\n\nSe rendre, depuis la racine du site,\nsur le fichier robots.txt pour vérifier les consignes\nproposées par les développeurs du site web pour\ncadrer le comportement des robots ;\nEspacer chaque requêtes de plusieurs secondes, comme le ferait\nun humain, afin d’éviter de surcharger le site web et de le\nfaire sauter par déni de service ;\nFaire les requêtes dans les heures creuses de fréquentation du\nsite web s’il ne s’agit pas d’un site consulté internationalement.\nPar exemple, pour un site en Français, lancer le robot\npendant la nuit en France métropolitaine, est une bonne pratique.\nPour lancer un robot depuis Python a une heure programmée\nà l’avancer, il existe les cronjobs."
  },
  {
    "objectID": "content/course/manipulation/04a_webscraping_TP/index.html#un-détour-par-le-web-comment-fonctionne-un-site",
    "href": "content/course/manipulation/04a_webscraping_TP/index.html#un-détour-par-le-web-comment-fonctionne-un-site",
    "title": "13  Webscraping avec python",
    "section": "13.2 Un détour par le Web : comment fonctionne un site ?",
    "text": "13.2 Un détour par le Web : comment fonctionne un site ?\nMême si ce TP ne vise pas à faire un cours de web, il vous faut néanmoins certaines bases sur la manière dont un site internet fonctionne afin de comprendre comment sont structurées les informations sur une page.\nUn site Web est un ensemble de pages codées en HTML qui permet de décrire à la fois le contenu et la forme d’une page Web.\nPour voir cela, ouvrez n’importe quelle page web et faites un clic-droit dessus.\n- Sous Chrome  : Cliquez ensuite sur “Affichez le code source de la page” (CTRL+U) ;\n- Sous Firefox  : “Code source de la page” (CTRL+MAJ+K) ;\n- Sous Edge  : “Affichez la page source” (CTRL+U) ;\n- Sous Safari  : voir comment faire ici\n\n13.2.1 Les balises\nSur une page web, vous trouverez toujours à coup sûr des éléments comme &lt;head&gt;, &lt;title&gt;, etc. Il s’agit des codes qui vous permettent de structurer le contenu d’une page HTML et qui s’appellent des balises.\nCitons, par exemple, les balises &lt;p&gt;, &lt;h1&gt;, &lt;h2&gt;, &lt;h3&gt;, &lt;strong&gt; ou &lt;em&gt;.\nLe symbole &lt; &gt; est une balise : il sert à indiquer le début d’une partie. Le symbole &lt;/ &gt; indique la fin de cette partie. La plupart des balises vont par paires, avec une balise ouvrante et une balise fermante (par exemple &lt;p&gt; et &lt;/p&gt;).\n\n13.2.1.1 Exemple : les balise des tableaux\n\n\n\nBalise\nDescription\n\n\n\n\n&lt;table&gt;\nTableau\n\n\n&lt;caption&gt;\nTitre du tableau\n\n\n&lt;tr&gt;\nLigne de tableau\n\n\n&lt;th&gt;\nCellule d’en-tête\n\n\n&lt;td&gt;\nCellule\n\n\n&lt;thead&gt;\nSection de l’en-tête du tableau\n\n\n&lt;tbody&gt;\nSection du corps du tableau\n\n\n&lt;tfoot&gt;\nSection du pied du tableau\n\n\n\nApplication : un tableau en HTML\nLe code HTML du tableau suivant\n&lt;table&gt;\n&lt;caption&gt; Le Titre de mon tableau &lt;/caption&gt;\n\n   &lt;tr&gt;\n      &lt;th&gt;Prénom&lt;/th&gt;\n      &lt;th&gt;Nom&lt;/th&gt;\n      &lt;th&gt;Profession&lt;/th&gt;\n   &lt;/tr&gt;\n   &lt;tr&gt;\n      &lt;td&gt;Mike &lt;/td&gt;\n      &lt;td&gt;Stuntman&lt;/td&gt;\n      &lt;td&gt;Cascadeur&lt;/td&gt;\n   &lt;/tr&gt;\n   &lt;tr&gt;\n      &lt;td&gt;Mister&lt;/td&gt;\n      &lt;td&gt;Pink&lt;/td&gt;\n      &lt;td&gt;Gangster&lt;/td&gt;\n   &lt;/tr&gt;\n&lt;/table&gt;\nDonnera dans le navigateur :\n\n\nLe Titre de mon tableau\n\n\nPrénom\nNom\nProfession\n\n\nMike\nStuntman\nCascadeur\n\n\nMister\nPink\nGangster\n\n\n\n\n\n\n13.2.1.2 Parent et enfant\nDans le cadre du langage HTML, les termes de parent (parent) et enfant (child) servent à désigner des élements emboîtés les uns dans les autres. Dans la construction suivante, par exemple :\n&lt; div&gt; \n    &lt; p&gt;\n       bla,bla\n    &lt; /p&gt;\n&lt; /div&gt;\nSur la page web, cela apparaitra de la manière suivante :\n\n&lt;p&gt;\n   bla,bla\n&lt;/p&gt;\n\nOn dira que l’élément &lt;div&gt; est le parent de l’élément &lt;p&gt; tandis que l’élément &lt;p&gt; est l’enfant de l’élément &lt;div&gt;.\n\nMais pourquoi apprendre ça pour “scraper” ?\n\nParce que, pour bien récupérer les informations d’un site internet, il faut pouvoir comprendre sa structure et donc son code HTML. Les fonctions Python qui servent au scraping sont principalement construites pour vous permettre de naviguer entre les balises.\nAvec Python, vous allez en fait reproduire votre comportement manuel de recherche de manière\nà l’automatiser."
  },
  {
    "objectID": "content/course/manipulation/04a_webscraping_TP/index.html#scraper-avec-python-le-package-beautifulsoup",
    "href": "content/course/manipulation/04a_webscraping_TP/index.html#scraper-avec-python-le-package-beautifulsoup",
    "title": "13  Webscraping avec python",
    "section": "13.3 Scraper avec Python: le package BeautifulSoup",
    "text": "13.3 Scraper avec Python: le package BeautifulSoup\n\n13.3.1 Les packages disponibles\nDans la première partie de ce chapitre,\nnous allons essentiellement utiliser le package BeautifulSoup4,\nen conjonction avec urllib\nou requests. Ces deux derniers packages permettent de récupérer le texte\nbrut d’une page qui sera ensuite\ninspecté via BeautifulSoup4.\nBeautifulSoup sera suffisant quand vous voudrez travailler sur des pages HTML statiques. Dès que les informations que vous recherchez sont générées via l’exécution de scripts Javascript, il vous faudra passer par des outils comme Selenium.\nDe même, si vous ne connaissez pas l’URL, il faudra passer par un framework comme Scrapy, qui passe facilement d’une page à une autre. On appelle\ncette technique le “webcrawling”. Scrapy est plus complexe à manipuler que BeautifulSoup : si vous voulez plus de détails, rendez-vous sur la page du tutoriel Scrapy.\nLe webscraping est un domaine où la reproductibilité est compliquée à mettre en oeuvre.\nUne page web évolue\npotentiellement régulièrement et d’une page web à l’autre, la structure peut\nêtre très différente ce qui rend certains codes difficilement exportables.\nPar conséquent, la meilleure manière d’avoir un programme fonctionnel est\nde comprendre la structure d’une page web et dissocier les éléments exportables\nà d’autres cas d’usages des requêtes ad hoc.\n\nimport urllib\nimport bs4\nimport pandas\nfrom urllib import request\n\n\n\n13.3.2 Récupérer le contenu d’une page HTML\nOn va commencer doucement. Prenons une page wikipedia,\npar exemple celle de la Ligue 1 de football, millésime 2019-2020 : Championnat de France de football 2019-2020. On va souhaiter récupérer la liste des équipes, ainsi que les url des pages Wikipedia de ces équipes.\nEtape :one: : se connecter à la page wikipedia et obtenir le code source.\nPour cela, le plus simple est d’utiliser le package urllib ou, mieux, requests.\nNous allons ici utiliser la fonction request du package urllib:\n\nurl_ligue_1 = \"https://fr.wikipedia.org/wiki/Championnat_de_France_de_football_2019-2020\"\n    \nrequest_text = request.urlopen(url_ligue_1).read()\n# print(request_text[:1000])    \n\n\ntype(request_text)\n\nbytes\n\n\nEtape :two: : utiliser le package BeautifulSoup\nqui permet de rechercher efficacement\nles balises contenues dans la chaine de caractères\nrenvoyée par la fonction request:\n\npage = bs4.BeautifulSoup(request_text, \"lxml\")\n\nSi on print l’objet page créée avec BeautifulSoup,\non voit que ce n’est plus une chaine de caractères mais bien une page HTML avec des balises.\nOn peut à présent chercher des élements à l’intérieur de ces balises.\n\n\n13.3.3 La méthode find\nPar exemple, si on veut connaître le titre de la page, on utilise la méthode .find et on lui demande “title”\n\nprint(page.find(\"title\"))\n\n&lt;title&gt;Championnat de France de football 2019-2020 — Wikipédia&lt;/title&gt;\n\n\nLa methode .find ne renvoie que la première occurence de l’élément.\nPour vous en assurer vous pouvez :\n\ncopier le bout de code source obtenu,\nle coller dans une cellule de votre notebook\net passer la cellule en “Markdown”\n\nLa cellule avec le copier-coller du code source donne :\n\nprint(page.find(\"table\"))\n\n&lt;table&gt;&lt;caption style=\"background-color:#99cc99;color:#000000;\"&gt;Généralités&lt;/caption&gt;&lt;tbody&gt;&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Sport&lt;/th&gt;\n&lt;td&gt;\n&lt;a href=\"/wiki/Football\" title=\"Football\"&gt;Football&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Organisateur(s)&lt;/th&gt;\n&lt;td&gt;\n&lt;a href=\"/wiki/Ligue_de_football_professionnel_(France)\" title=\"Ligue de football professionnel (France)\"&gt;LFP&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Édition&lt;/th&gt;\n&lt;td&gt;\n&lt;abbr class=\"abbr\" title=\"Quatre-vingt-deuxième (huitante-deuxième / octante-deuxième)\"&gt;82&lt;sup&gt;e&lt;/sup&gt;&lt;/abbr&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Lieu(x)&lt;/th&gt;\n&lt;td&gt;\n&lt;span class=\"datasortkey\" data-sort-value=\"France\"&gt;&lt;span class=\"flagicon\"&gt;&lt;span class=\"mw-image-border noviewer\" typeof=\"mw:File\"&gt;&lt;a class=\"mw-file-description\" href=\"/wiki/Fichier:Flag_of_France.svg\" title=\"Drapeau de la France\"&gt;&lt;img alt=\"Drapeau de la France\" class=\"mw-file-element\" data-file-height=\"600\" data-file-width=\"900\" decoding=\"async\" height=\"13\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Flag_of_France.svg/20px-Flag_of_France.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Flag_of_France.svg/30px-Flag_of_France.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Flag_of_France.svg/40px-Flag_of_France.svg.png 2x\" width=\"20\"/&gt;&lt;/a&gt;&lt;/span&gt; &lt;/span&gt;&lt;a href=\"/wiki/France\" title=\"France\"&gt;France&lt;/a&gt;&lt;/span&gt; et &lt;br/&gt;&lt;span class=\"datasortkey\" data-sort-value=\"Monaco\"&gt;&lt;span class=\"flagicon\"&gt;&lt;span class=\"mw-image-border noviewer\" typeof=\"mw:File\"&gt;&lt;a class=\"mw-file-description\" href=\"/wiki/Fichier:Flag_of_Monaco.svg\" title=\"Drapeau de Monaco\"&gt;&lt;img alt=\"Drapeau de Monaco\" class=\"mw-file-element\" data-file-height=\"800\" data-file-width=\"1000\" decoding=\"async\" height=\"16\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Flag_of_Monaco.svg/20px-Flag_of_Monaco.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Flag_of_Monaco.svg/30px-Flag_of_Monaco.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Flag_of_Monaco.svg/40px-Flag_of_Monaco.svg.png 2x\" width=\"20\"/&gt;&lt;/a&gt;&lt;/span&gt; &lt;/span&gt;&lt;a href=\"/wiki/Monaco\" title=\"Monaco\"&gt;Monaco&lt;/a&gt;&lt;/span&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Date&lt;/th&gt;\n&lt;td&gt;\nDu &lt;time class=\"nowrap date-lien\" data-sort-value=\"2019-08-09\" datetime=\"2019-08-09\"&gt;&lt;a href=\"/wiki/9_ao%C3%BBt_en_sport\" title=\"9 août en sport\"&gt;9&lt;/a&gt; &lt;a class=\"mw-redirect\" href=\"/wiki/Ao%C3%BBt_2019_en_sport\" title=\"Août 2019 en sport\"&gt;août&lt;/a&gt; &lt;a href=\"/wiki/2019_en_football\" title=\"2019 en football\"&gt;2019&lt;/a&gt;&lt;/time&gt;&lt;br/&gt;au &lt;time class=\"nowrap date-lien\" data-sort-value=\"2020-03-08\" datetime=\"2020-03-08\"&gt;&lt;a href=\"/wiki/8_mars_en_sport\" title=\"8 mars en sport\"&gt;8 mars&lt;/a&gt; &lt;a href=\"/wiki/2020_en_football\" title=\"2020 en football\"&gt;2020&lt;/a&gt;&lt;/time&gt; &lt;small&gt;(arrêt définitif)&lt;/small&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Participants&lt;/th&gt;\n&lt;td&gt;\n20 équipes&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Matchs joués&lt;/th&gt;\n&lt;td&gt;\n279 (sur 380 prévus)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Site web officiel&lt;/th&gt;\n&lt;td&gt;\n&lt;a class=\"external text\" href=\"https://www.ligue1.fr/\" rel=\"nofollow\"&gt;Site officiel&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;\n\n\nce qui est le texte source permettant de générer le tableau suivant:\n\n\n\nGénéralités\n\n\n\n\nSport\n\n\nFootball\n\n\n\n\nOrganisateur(s)\n\n\nLFP\n\n\n\n\nÉdition\n\n\n82e\n\n\n\n\nLieu(x)\n\n\n France et  Monaco\n\n\n\n\nDate\n\n\nDu 9 août 2019au 8 mars 2020 (arrêt définitif)\n\n\n\n\nParticipants\n\n\n20 équipes\n\n\n\n\nMatchs joués\n\n\n279 (sur 380 prévus)\n\n\n\n\nSite web officiel\n\n\nSite officiel\n\n\n\n\n\n\n\n13.3.4 La méthode findAll\nPour trouver toutes les occurences, on utilise .findAll().\n\nprint(\"Il y a\", len(page.findAll(\"table\")), \"éléments dans la page qui sont des &lt;table&gt;\")\n\nIl y a 34 éléments dans la page qui sont des &lt;table&gt;"
  },
  {
    "objectID": "content/course/manipulation/04a_webscraping_TP/index.html#exercice-guidé-obtenir-la-liste-des-équipes-de-ligue-1",
    "href": "content/course/manipulation/04a_webscraping_TP/index.html#exercice-guidé-obtenir-la-liste-des-équipes-de-ligue-1",
    "title": "13  Webscraping avec python",
    "section": "13.4 Exercice guidé : obtenir la liste des équipes de Ligue 1",
    "text": "13.4 Exercice guidé : obtenir la liste des équipes de Ligue 1\nDans le premier paragraphe de la page “Participants”,\non a le tableau avec les résultats de l’année.\n\n\n Exercice 1 : Récupérer les participants de la Ligue 1\nPour cela, nous allons procéder en 6 étapes:\n\nTrouver le tableau\nRécupérer chaque ligne du table\nNettoyer les sorties en ne gardant que le texte sur une ligne\nGénéraliser sur toutes les lignes\nRécupérer les entêtes du tableau\nFinalisation du tableau\n\n\n\n1️ Trouver le tableau\n\n# on identifie le tableau en question : c'est le premier qui a cette classe \"wikitable sortable\"\ntableau_participants = page.find('table', {'class' : 'wikitable sortable'})\n\n\nprint(tableau_participants)\n\n\n\n\nClub\n\n\nDernièremontée\n\n\nBudget[3]en M€\n\n\nClassement2018-2019\n\n\nEntraîneur\n\n\nDepuis\n\n\nStade\n\n\nCapacitéen L1[4]\n\n\nNombrede saisonsen L1\n\n\n\n\nParis Saint-Germain\n\n\n1974\n\n\n637\n\n\n1er\n\n\n Thomas Tuchel\n\n\n2018\n\n\nParc des Princes\n\n\n47 929\n\n\n46\n\n\n\n\nLOSC Lille\n\n\n2000\n\n\n120\n\n\n2e\n\n\n Christophe Galtier\n\n\n2017\n\n\nStade Pierre-Mauroy\n\n\n49 712\n\n\n59\n\n\n\n\nOlympique lyonnais\n\n\n1989\n\n\n310\n\n\n3e\n\n\n Rudi Garcia\n\n\n2019\n\n\nGroupama Stadium\n\n\n57 206\n\n\n60\n\n\n\n\nAS Saint-Étienne\n\n\n2004\n\n\n100\n\n\n4e\n\n\n Claude Puel\n\n\n2019\n\n\nStade Geoffroy-Guichard\n\n\n41 965\n\n\n66\n\n\n\n\nOlympique de Marseille\n\n\n1996\n\n\n110\n\n\n5e\n\n\n André Villas-Boas\n\n\n2019\n\n\nOrange Vélodrome\n\n\n66 226\n\n\n69\n\n\n\n\nMontpellier HSC\n\n\n2009\n\n\n40\n\n\n6e\n\n\n Michel Der Zakarian\n\n\n2017\n\n\nStade de la Mosson\n\n\n22 000\n\n\n27\n\n\n\n\nOGC Nice\n\n\n2002\n\n\n50\n\n\n7e\n\n\n Patrick Vieira\n\n\n2018\n\n\nAllianz Riviera\n\n\n35 596\n\n\n60\n\n\n\n\nStade de Reims\n\n\n2018\n\n\n45\n\n\n8e\n\n\n David Guion\n\n\n2017\n\n\nStade Auguste-Delaune\n\n\n20 546\n\n\n35\n\n\n\n\nNîmes Olympique\n\n\n2018\n\n\n27\n\n\n9e\n\n\n Bernard Blaquart\n\n\n2015\n\n\nStade des Costières\n\n\n15 788\n\n\n35\n\n\n\n\nStade rennais FC\n\n\n1994\n\n\n65\n\n\n10e\n\n\n Julien Stéphan\n\n\n2018\n\n\nRoazhon Park\n\n\n29 194\n\n\n62\n\n\n\n\nRC Strasbourg Alsace\n\n\n2017\n\n\n43\n\n\n11e\n\n\n Thierry Laurey\n\n\n2016\n\n\nStade de la Meinau\n\n\n26 109\n\n\n58\n\n\n\n\nFC Nantes\n\n\n2013\n\n\n70\n\n\n12e\n\n\n Christian Gourcuff\n\n\n2019\n\n\nStade de la Beaujoire - Louis Fonteneau\n\n\n35 322\n\n\n51\n\n\n\n\nSCO d’Angers\n\n\n2015\n\n\n32\n\n\n13e\n\n\n Stéphane Moulin\n\n\n2011\n\n\nStade Raymond-Kopa\n\n\n14 582\n\n\n27\n\n\n\n\nGirondins de Bordeaux\n\n\n1992\n\n\n70\n\n\n14e\n\n\n Paulo Sousa\n\n\n2019\n\n\nMatmut Atlantique\n\n\n42 115\n\n\n66\n\n\n\n\nAmiens SC\n\n\n2017\n\n\n30\n\n\n15e\n\n\n Luka Elsner\n\n\n2019\n\n\nStade Crédit Agricole la Licorne\n\n\n12 999\n\n\n2\n\n\n\n\nToulouse FC\n\n\n2003\n\n\n35\n\n\n16e\n\n\n Denis Zanko\n\n\n2020\n\n\nStadium de Toulouse\n\n\n33 033\n\n\n32\n\n\n\n\nAS Monaco\n\n\n2013\n\n\n220\n\n\n17e\n\n\n Robert Moreno\n\n\n2019\n\n\nStade Louis-II\n\n\n16 500\n\n\n60\n\n\n\n\nDijon FCO\n\n\n2016\n\n\n38\n\n\n18e\n\n\n Stéphane Jobard\n\n\n2019\n\n\nParc des Sports Gaston-Gérard\n\n\n15 459\n\n\n4\n\n\n\n\nFC Metz\n\n\n2019\n\n\n40\n\n\n1er (Ligue 2)\n\n\n Vincent Hognon\n\n\n2019\n\n\nStade Saint-Symphorien\n\n\n25 865\n\n\n61\n\n\n\n\nStade brestois 29\n\n\n2019\n\n\n30\n\n\n2e (Ligue 2)\n\n\n Olivier Dall’Oglio\n\n\n2019\n\n\nStade Francis-Le Blé\n\n\n14 920\n\n\n13\n\n\n\n\n\n2️ Récupérer chaque ligne du tableau.\nOn recherche d’abord toutes les lignes du tableau avec la balise tr\n\ntable_body = tableau_participants.find('tbody')\nrows = table_body.find_all('tr')\n\nOn obtient une liste où chaque élément est une des lignes du tableau\nPour illustrer cela, on va d’abord afficher la première ligne.\nCelle-ci correspont aux entêtes de colonne:\n\nprint(rows[0])\n\n&lt;tr&gt;\n&lt;th scope=\"col\"&gt;Club\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Dernière&lt;br/&gt;montée\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Budget&lt;sup class=\"reference\" id=\"cite_ref-3\"&gt;&lt;a href=\"#cite_note-3\"&gt;&lt;span class=\"cite_crochet\"&gt;[&lt;/span&gt;3&lt;span class=\"cite_crochet\"&gt;]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;&lt;br/&gt;en M&lt;a href=\"/wiki/Euro\" title=\"Euro\"&gt;€&lt;/a&gt;\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Classement&lt;br/&gt;&lt;a href=\"/wiki/Championnat_de_France_de_football_2018-2019\" title=\"Championnat de France de football 2018-2019\"&gt;2018-2019&lt;/a&gt;\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Entraîneur\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Depuis\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Stade\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Capacité&lt;br/&gt;en L1&lt;sup class=\"reference\" id=\"cite_ref-4\"&gt;&lt;a href=\"#cite_note-4\"&gt;&lt;span class=\"cite_crochet\"&gt;[&lt;/span&gt;4&lt;span class=\"cite_crochet\"&gt;]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Nombre&lt;br/&gt;de saisons&lt;br/&gt;en L1\n&lt;/th&gt;&lt;/tr&gt;\n\n\nLa seconde ligne va correspondre à la ligne du premier club présent dans le tableau:\n\nprint(rows[1])\n\n&lt;tr bgcolor=\"#97DEFF\"&gt;\n&lt;td&gt;&lt;a href=\"/wiki/Paris_Saint-Germain_Football_Club\" title=\"Paris Saint-Germain Football Club\"&gt;Paris Saint-Germain&lt;/a&gt;\n&lt;/td&gt;\n&lt;td&gt;1974\n&lt;/td&gt;\n&lt;td&gt;637\n&lt;/td&gt;\n&lt;td&gt;&lt;span data-sort-value=\"101 !\"&gt;&lt;/span&gt;&lt;abbr class=\"abbr\" title=\"Premier\"&gt;1&lt;sup&gt;er&lt;/sup&gt;&lt;/abbr&gt;\n&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;span class=\"flagicon\"&gt;&lt;span class=\"mw-image-border noviewer\" typeof=\"mw:File\"&gt;&lt;a class=\"mw-file-description\" href=\"/wiki/Fichier:Flag_of_Germany.svg\" title=\"Drapeau : Allemagne\"&gt;&lt;img alt=\"\" class=\"mw-file-element\" data-file-height=\"600\" data-file-width=\"1000\" decoding=\"async\" height=\"12\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Flag_of_Germany.svg/20px-Flag_of_Germany.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Flag_of_Germany.svg/30px-Flag_of_Germany.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Flag_of_Germany.svg/40px-Flag_of_Germany.svg.png 2x\" width=\"20\"/&gt;&lt;/a&gt;&lt;/span&gt;&lt;/span&gt; &lt;a href=\"/wiki/Thomas_Tuchel\" title=\"Thomas Tuchel\"&gt;Thomas Tuchel&lt;/a&gt;\n&lt;/td&gt;\n&lt;td&gt;2018\n&lt;/td&gt;\n&lt;td&gt;&lt;a href=\"/wiki/Parc_des_Princes\" title=\"Parc des Princes\"&gt;Parc des Princes&lt;/a&gt;\n&lt;/td&gt;\n&lt;td&gt;47 929\n&lt;/td&gt;\n&lt;td&gt;46\n&lt;/td&gt;&lt;/tr&gt;\n\n\n3️ Nettoyer les sorties en ne gardant que le texte sur une ligne\nOn va utiliser l’attribut text afin de se débarasser de toute la couche de HTML qu’on obtient à l’étape 2.\nUn exemple sur la ligne du premier club :\n- on commence par prendre toutes les cellules de cette ligne, avec la balise td.\n- on fait ensuite une boucle sur chacune des cellules et on ne garde que le texte de la cellule avec l’attribut text.\n- enfin, on applique la méthode strip() pour que le texte soit bien mis en forme (sans espace inutile etc).\n\ncols = rows[1].find_all('td')\nprint(cols[0])\nprint(cols[0].text.strip())\n\n&lt;td&gt;&lt;a href=\"/wiki/Paris_Saint-Germain_Football_Club\" title=\"Paris Saint-Germain Football Club\"&gt;Paris Saint-Germain&lt;/a&gt;\n&lt;/td&gt;\nParis Saint-Germain\n\n\n\nfor ele in cols : \n    print(ele.text.strip())\n\nParis Saint-Germain\n1974\n637\n1er\nThomas Tuchel\n2018\nParc des Princes\n47 929\n46\n\n\n4️ Généraliser sur toutes les lignes :\n\nfor row in rows:\n    cols = row.find_all('td')\n    cols = [ele.text.strip() for ele in cols]\n    print(cols)\n\n[]\n['Paris Saint-Germain', '1974', '637', '1er', 'Thomas Tuchel', '2018', 'Parc des Princes', '47\\xa0929', '46']\n['LOSC Lille', '2000', '120', '2e', 'Christophe Galtier', '2017', 'Stade Pierre-Mauroy', '49\\xa0712', '59']\n['Olympique lyonnais', '1989', '310', '3e', 'Rudi Garcia', '2019', 'Groupama Stadium', '57\\xa0206', '60']\n['AS Saint-Étienne', '2004', '100', '4e', 'Claude Puel', '2019', 'Stade Geoffroy-Guichard', '41\\xa0965', '66']\n['Olympique de Marseille', '1996', '110', '5e', 'André Villas-Boas', '2019', 'Orange Vélodrome', '66\\xa0226', '69']\n['Montpellier HSC', '2009', '40', '6e', 'Michel Der Zakarian', '2017', 'Stade de la Mosson', '22\\xa0000', '27']\n['OGC Nice', '2002', '50', '7e', 'Patrick Vieira', '2018', 'Allianz Riviera', '35\\xa0596', '60']\n['Stade de Reims', '2018', '45', '8e', 'David Guion', '2017', 'Stade Auguste-Delaune', '20\\xa0546', '35']\n['Nîmes Olympique', '2018', '27', '9e', 'Bernard Blaquart', '2015', 'Stade des Costières', '15\\xa0788', '35']\n['Stade rennais FC', '1994', '65', '10e', 'Julien Stéphan', '2018', 'Roazhon Park', '29\\xa0194', '62']\n['RC Strasbourg Alsace', '2017', '43', '11e', 'Thierry Laurey', '2016', 'Stade de la Meinau', '26\\xa0109', '58']\n['FC Nantes', '2013', '70', '12e', 'Christian Gourcuff', '2019', 'Stade de la Beaujoire - Louis Fonteneau', '35\\xa0322', '51']\n['SCO d’Angers', '2015', '32', '13e', 'Stéphane Moulin', '2011', 'Stade Raymond-Kopa', '14\\xa0582', '27']\n['Girondins de Bordeaux', '1992', '70', '14e', 'Paulo Sousa', '2019', 'Matmut Atlantique', '42\\xa0115', '66']\n['Amiens SC', '2017', '30', '15e', 'Luka Elsner', '2019', 'Stade Crédit Agricole la Licorne', '12\\xa0999', '2']\n['Toulouse FC', '2003', '35', '16e', 'Denis Zanko', '2020', 'Stadium de Toulouse', '33\\xa0033', '32']\n['AS Monaco', '2013', '220', '17e', 'Robert Moreno', '2019', 'Stade Louis-II', '16\\xa0500', '60']\n['Dijon FCO', '2016', '38', '18e', 'Stéphane Jobard', '2019', 'Parc des Sports Gaston-Gérard', '15\\xa0459', '4']\n['FC Metz', '2019', '40', '1er (Ligue 2)', 'Vincent Hognon', '2019', 'Stade Saint-Symphorien', '25\\xa0865', '61']\n['Stade brestois 29', '2019', '30', '2e (Ligue 2)', \"Olivier Dall'Oglio\", '2019', 'Stade Francis-Le Blé', '14\\xa0920', '13']\n\n\nOn a bien réussi à avoir les informations contenues dans le tableau des participants du championnat.\nMais la première ligne est étrange : c’est une liste vide …\nIl s’agit des en-têtes : elles sont reconnues par la balise th et non td.\nOn va mettre tout le contenu dans un dictionnaire, pour le transformer ensuite en DataFrame pandas :\n\ndico_participants = dict()\nfor row in rows:\n    cols = row.find_all('td')\n    cols = [ele.text.strip() for ele in cols]\n    if len(cols) &gt; 0 : \n        dico_participants[cols[0]] = cols[1:]\ndico_participants\n\n{'Paris Saint-Germain': ['1974',\n  '637',\n  '1er',\n  'Thomas Tuchel',\n  '2018',\n  'Parc des Princes',\n  '47\\xa0929',\n  '46'],\n 'LOSC Lille': ['2000',\n  '120',\n  '2e',\n  'Christophe Galtier',\n  '2017',\n  'Stade Pierre-Mauroy',\n  '49\\xa0712',\n  '59'],\n 'Olympique lyonnais': ['1989',\n  '310',\n  '3e',\n  'Rudi Garcia',\n  '2019',\n  'Groupama Stadium',\n  '57\\xa0206',\n  '60'],\n 'AS Saint-Étienne': ['2004',\n  '100',\n  '4e',\n  'Claude Puel',\n  '2019',\n  'Stade Geoffroy-Guichard',\n  '41\\xa0965',\n  '66'],\n 'Olympique de Marseille': ['1996',\n  '110',\n  '5e',\n  'André Villas-Boas',\n  '2019',\n  'Orange Vélodrome',\n  '66\\xa0226',\n  '69'],\n 'Montpellier HSC': ['2009',\n  '40',\n  '6e',\n  'Michel Der Zakarian',\n  '2017',\n  'Stade de la Mosson',\n  '22\\xa0000',\n  '27'],\n 'OGC Nice': ['2002',\n  '50',\n  '7e',\n  'Patrick Vieira',\n  '2018',\n  'Allianz Riviera',\n  '35\\xa0596',\n  '60'],\n 'Stade de Reims': ['2018',\n  '45',\n  '8e',\n  'David Guion',\n  '2017',\n  'Stade Auguste-Delaune',\n  '20\\xa0546',\n  '35'],\n 'Nîmes Olympique': ['2018',\n  '27',\n  '9e',\n  'Bernard Blaquart',\n  '2015',\n  'Stade des Costières',\n  '15\\xa0788',\n  '35'],\n 'Stade rennais FC': ['1994',\n  '65',\n  '10e',\n  'Julien Stéphan',\n  '2018',\n  'Roazhon Park',\n  '29\\xa0194',\n  '62'],\n 'RC Strasbourg Alsace': ['2017',\n  '43',\n  '11e',\n  'Thierry Laurey',\n  '2016',\n  'Stade de la Meinau',\n  '26\\xa0109',\n  '58'],\n 'FC Nantes': ['2013',\n  '70',\n  '12e',\n  'Christian Gourcuff',\n  '2019',\n  'Stade de la Beaujoire - Louis Fonteneau',\n  '35\\xa0322',\n  '51'],\n 'SCO d’Angers': ['2015',\n  '32',\n  '13e',\n  'Stéphane Moulin',\n  '2011',\n  'Stade Raymond-Kopa',\n  '14\\xa0582',\n  '27'],\n 'Girondins de Bordeaux': ['1992',\n  '70',\n  '14e',\n  'Paulo Sousa',\n  '2019',\n  'Matmut Atlantique',\n  '42\\xa0115',\n  '66'],\n 'Amiens SC': ['2017',\n  '30',\n  '15e',\n  'Luka Elsner',\n  '2019',\n  'Stade Crédit Agricole la Licorne',\n  '12\\xa0999',\n  '2'],\n 'Toulouse FC': ['2003',\n  '35',\n  '16e',\n  'Denis Zanko',\n  '2020',\n  'Stadium de Toulouse',\n  '33\\xa0033',\n  '32'],\n 'AS Monaco': ['2013',\n  '220',\n  '17e',\n  'Robert Moreno',\n  '2019',\n  'Stade Louis-II',\n  '16\\xa0500',\n  '60'],\n 'Dijon FCO': ['2016',\n  '38',\n  '18e',\n  'Stéphane Jobard',\n  '2019',\n  'Parc des Sports Gaston-Gérard',\n  '15\\xa0459',\n  '4'],\n 'FC Metz': ['2019',\n  '40',\n  '1er (Ligue 2)',\n  'Vincent Hognon',\n  '2019',\n  'Stade Saint-Symphorien',\n  '25\\xa0865',\n  '61'],\n 'Stade brestois 29': ['2019',\n  '30',\n  '2e (Ligue 2)',\n  \"Olivier Dall'Oglio\",\n  '2019',\n  'Stade Francis-Le Blé',\n  '14\\xa0920',\n  '13']}\n\n\n\ndata_participants = pandas.DataFrame.from_dict(dico_participants,orient='index')\ndata_participants.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\nParis Saint-Germain\n1974\n637\n1er\nThomas Tuchel\n2018\nParc des Princes\n47 929\n46\n\n\nLOSC Lille\n2000\n120\n2e\nChristophe Galtier\n2017\nStade Pierre-Mauroy\n49 712\n59\n\n\nOlympique lyonnais\n1989\n310\n3e\nRudi Garcia\n2019\nGroupama Stadium\n57 206\n60\n\n\nAS Saint-Étienne\n2004\n100\n4e\nClaude Puel\n2019\nStade Geoffroy-Guichard\n41 965\n66\n\n\nOlympique de Marseille\n1996\n110\n5e\nAndré Villas-Boas\n2019\nOrange Vélodrome\n66 226\n69\n\n\n\n\n\n\n\n5️ Récupérer les en-têtes du tableau:\n\nfor row in rows:\n    cols = row.find_all('th')\n    print(cols)\n    if len(cols) &gt; 0 : \n        cols = [ele.get_text(separator=' ').strip().title() for ele in cols]\n        columns_participants = cols\n\n[&lt;th scope=\"col\"&gt;Club\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Dernière&lt;br/&gt;montée\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Budget&lt;sup class=\"reference\" id=\"cite_ref-3\"&gt;&lt;a href=\"#cite_note-3\"&gt;&lt;span class=\"cite_crochet\"&gt;[&lt;/span&gt;3&lt;span class=\"cite_crochet\"&gt;]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;&lt;br/&gt;en M&lt;a href=\"/wiki/Euro\" title=\"Euro\"&gt;€&lt;/a&gt;\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Classement&lt;br/&gt;&lt;a href=\"/wiki/Championnat_de_France_de_football_2018-2019\" title=\"Championnat de France de football 2018-2019\"&gt;2018-2019&lt;/a&gt;\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Entraîneur\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Depuis\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Stade\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Capacité&lt;br/&gt;en L1&lt;sup class=\"reference\" id=\"cite_ref-4\"&gt;&lt;a href=\"#cite_note-4\"&gt;&lt;span class=\"cite_crochet\"&gt;[&lt;/span&gt;4&lt;span class=\"cite_crochet\"&gt;]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Nombre&lt;br/&gt;de saisons&lt;br/&gt;en L1\n&lt;/th&gt;]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n\n\n\ncolumns_participants\n\n['Club',\n 'Dernière Montée',\n 'Budget [ 3 ] En M €',\n 'Classement 2018-2019',\n 'Entraîneur',\n 'Depuis',\n 'Stade',\n 'Capacité En L1 [ 4 ]',\n 'Nombre De Saisons En L1']\n\n\n6️ Finalisation du tableau\n\ndata_participants.columns = columns_participants[1:]\n\n\ndata_participants.head()\n\n\n\n\n\n\n\n\nDernière Montée\nBudget [ 3 ] En M €\nClassement 2018-2019\nEntraîneur\nDepuis\nStade\nCapacité En L1 [ 4 ]\nNombre De Saisons En L1\n\n\n\n\nParis Saint-Germain\n1974\n637\n1er\nThomas Tuchel\n2018\nParc des Princes\n47 929\n46\n\n\nLOSC Lille\n2000\n120\n2e\nChristophe Galtier\n2017\nStade Pierre-Mauroy\n49 712\n59\n\n\nOlympique lyonnais\n1989\n310\n3e\nRudi Garcia\n2019\nGroupama Stadium\n57 206\n60\n\n\nAS Saint-Étienne\n2004\n100\n4e\nClaude Puel\n2019\nStade Geoffroy-Guichard\n41 965\n66\n\n\nOlympique de Marseille\n1996\n110\n5e\nAndré Villas-Boas\n2019\nOrange Vélodrome\n66 226\n69"
  },
  {
    "objectID": "content/course/manipulation/04a_webscraping_TP/index.html#pour-aller-plus-loin",
    "href": "content/course/manipulation/04a_webscraping_TP/index.html#pour-aller-plus-loin",
    "title": "13  Webscraping avec python",
    "section": "13.5 Pour aller plus loin",
    "text": "13.5 Pour aller plus loin\n\n13.5.1 Récupération des localisations des stades\nEssayez de comprendre pas à pas ce qui est fait dans les étapes qui suivent (la récupération d’informations supplémentaires en naviguant dans les pages des différents clubs).\n\nimport urllib\nimport pandas as pd\nimport bs4 \n\ndivision=[]\nequipe=[]\nstade=[]\nlatitude_stade=[]        \nlongitude_stade=[]     \n\ndef dms2dd(degrees, minutes, seconds, direction):\n    dd = float(degrees) + float(minutes)/60 + float(seconds)/(60*60);\n    if direction == 'S' or direction == 'O':\n        dd *= -1\n    return dd;\n\nurl_list=[\"http://fr.wikipedia.org/wiki/Championnat_de_France_de_football_2019-2020\", \"http://fr.wikipedia.org/wiki/Championnat_de_France_de_football_de_Ligue_2_2019-2020\"]\n\nfor url_ligue in url_list :\n       \n    print(url_ligue)\n    sock = urllib.request.urlopen(url_ligue).read() \n    page=bs4.BeautifulSoup(sock)\n\n# Rechercher les liens des équipes dans la liste disponible sur wikipedia \n\n    for team in page.findAll('span' , {'class' : 'toponyme'}) :  \n        \n        # Indiquer si c'est de la ligue 1 ou de la ligue 2\n        \n        if url_ligue==url_list[0] :\n            division.append(\"L1\")\n        else :\n            division.append(\"L2\")\n\n       # Trouver le nom et le lien de l'équipe\n            \n        if team.find('a')!=None :\n            team_url=team.find('a').get('href')\n            name_team=team.find('a').get('title')\n            equipe.append(name_team)\n            url_get_info = \"http://fr.wikipedia.org\"+team_url\n            print(url_get_info)\n \n       # aller sur la page de l'équipe\n           \n            search = urllib.request.urlopen(url_get_info).read()\n            search_team=bs4.BeautifulSoup(search)\n\n       # trouver le stade             \n            compteur = 0\n            for stadium in search_team.findAll('tr'):\n                for x in stadium.findAll('th' , {'scope' : 'row'} ) :\n                    if x.contents[0].string==\"Stade\" and compteur == 0:\n                        compteur = 1\n                        # trouver le lien du stade et son nom\n                        url_stade=stadium.findAll('a')[1].get('href')\n                        name_stadium=stadium.findAll('a')[1].get('title')\n                        stade.append(name_stadium)\n                        url_get_stade = \"http://fr.wikipedia.org\"+url_stade\n                        print(url_get_stade)\n                        \n                        # Aller sur la page du stade et trouver ses coodronnées géographiques\n                        \n                        search_stade = urllib.request.urlopen(url_get_stade).read()\n                        soup_stade=bs4.BeautifulSoup(search_stade) \n                        kartographer = soup_stade.find('a',{'class': \"mw-kartographer-maplink\"})\n                        if kartographer == None :\n                          latitude_stade.append(None)\n                          longitude_stade.append(None) \n                        else :\n                            for coordinates in kartographer :\n                                print(coordinates)\n                                liste =   coordinates.split(\",\")          \n                                latitude_stade.append(str(liste[0]).replace(\" \", \"\") + \"'\")\n                                longitude_stade.append(str(liste[1]).replace(\" \", \"\") + \"'\")\n                            \n\ndict = {'division' : division , 'equipe': equipe, 'stade': stade, 'latitude': latitude_stade, 'longitude' : longitude_stade}\ndata = pd.DataFrame(dict)\ndata = data.dropna()\n\nhttp://fr.wikipedia.org/wiki/Championnat_de_France_de_football_2019-2020\nhttp://fr.wikipedia.org/wiki/Paris_Saint-Germain_Football_Club\nhttp://fr.wikipedia.org/wiki/Parc_des_Princes\n48° 50′ 29″ N, 2° 15′ 11″ E\nhttp://fr.wikipedia.org/wiki/LOSC_Lille\nhttp://fr.wikipedia.org/wiki/Stade_Pierre-Mauroy\n50° 36′ 43″ N, 3° 07′ 50″ E\nhttp://fr.wikipedia.org/wiki/Olympique_lyonnais\nhttp://fr.wikipedia.org/wiki/Parc_Olympique_lyonnais\n45° 45′ 55″ N, 4° 58′ 55″ E\nhttp://fr.wikipedia.org/wiki/Association_sportive_de_Saint-%C3%89tienne\nhttp://fr.wikipedia.org/wiki/Stade_Geoffroy-Guichard\n45° 27′ 39″ N, 4° 23′ 25″ E\nhttp://fr.wikipedia.org/wiki/Olympique_de_Marseille\nhttp://fr.wikipedia.org/wiki/Orange_V%C3%A9lodrome\n43° 16′ 11″ N, 5° 23′ 45″ E\nhttp://fr.wikipedia.org/wiki/Montpellier_H%C3%A9rault_Sport_Club\nhttp://fr.wikipedia.org/wiki/Stade_de_la_Mosson\n43° 37′ 19″ N, 3° 48′ 44″ E\nhttp://fr.wikipedia.org/wiki/Stade_de_Reims\nhttp://fr.wikipedia.org/wiki/Stade_Auguste-Delaune\n49° 14′ 48″ N, 4° 01′ 30″ E\nhttp://fr.wikipedia.org/wiki/Olympique_Gymnaste_Club_Nice\nhttp://fr.wikipedia.org/wiki/Allianz_Riviera\n43° 42′ 18″ N, 7° 11′ 33″ E\nhttp://fr.wikipedia.org/wiki/N%C3%AEmes_Olympique\nhttp://fr.wikipedia.org/wiki/Stade_des_Antonins\n43° 48′ 39″ N, 4° 21′ 23″ E\nhttp://fr.wikipedia.org/wiki/Racing_Club_de_Strasbourg_Alsace\nhttp://fr.wikipedia.org/wiki/Stade_de_la_Meinau\n48° 33′ 36″ N, 7° 45′ 18″ E\nhttp://fr.wikipedia.org/wiki/Stade_rennais_Football_Club\nhttp://fr.wikipedia.org/wiki/Roazhon_Park\n48° 06′ 27″ N, 1° 42′ 46″ O\nhttp://fr.wikipedia.org/wiki/Angers_sporting_club_de_l%27Ouest\nhttp://fr.wikipedia.org/wiki/Stade_Raymond-Kopa\n47° 27′ 38″ N, 0° 31′ 51″ O\nhttp://fr.wikipedia.org/wiki/Football_Club_de_Metz\nhttp://fr.wikipedia.org/wiki/Stade_Saint-Symphorien\n49° 06′ 35″ N, 6° 09′ 33″ E\nhttp://fr.wikipedia.org/wiki/Football_Club_de_Nantes\nhttp://fr.wikipedia.org/wiki/Stade_de_la_Beaujoire\n47° 15′ 20″ N, 1° 31′ 31″ O\nhttp://fr.wikipedia.org/wiki/Stade_brestois_29\nhttp://fr.wikipedia.org/wiki/Stade_Francis-Le_Bl%C3%A9\n48° 24′ 11″ N, 4° 27′ 42″ O\nhttp://fr.wikipedia.org/wiki/Football_Club_des_Girondins_de_Bordeaux\nhttp://fr.wikipedia.org/wiki/Matmut_Atlantique\n44° 53′ 50″ N, 0° 33′ 41″ O\nhttp://fr.wikipedia.org/wiki/Toulouse_Football_Club\nhttp://fr.wikipedia.org/wiki/Stadium_de_Toulouse\n43° 35′ 00″ N, 1° 26′ 03″ E\nhttp://fr.wikipedia.org/wiki/Amiens_Sporting_Club\nhttp://fr.wikipedia.org/wiki/Stade_de_la_Licorne\n49° 53′ 38″ N, 2° 15′ 49″ E\nhttp://fr.wikipedia.org/wiki/Association_sportive_de_Monaco_football_club\nhttp://fr.wikipedia.org/wiki/Stade_Louis-II\n43° 43′ 39″ N, 7° 24′ 56″ E\nhttp://fr.wikipedia.org/wiki/Dijon_Football_C%C3%B4te-d%27Or\nhttp://fr.wikipedia.org/wiki/Stade_Gaston-G%C3%A9rard\n47° 19′ 28″ N, 5° 04′ 06″ E\nhttp://fr.wikipedia.org/wiki/Championnat_de_France_de_football_de_Ligue_2_2019-2020\nhttp://fr.wikipedia.org/wiki/Athletic_Club_ajaccien\nhttp://fr.wikipedia.org/wiki/Stade_Fran%C3%A7ois_Coty\n41° 55′ 54″ N, 8° 46′ 44″ E\nhttp://fr.wikipedia.org/wiki/Association_sportive_Nancy-Lorraine\nhttp://fr.wikipedia.org/wiki/Stade_Marcel-Picot\n48° 41′ 43″ N, 6° 12′ 38″ E\nhttp://fr.wikipedia.org/wiki/Le_Havre_Athletic_Club_(football)\nhttp://fr.wikipedia.org/wiki/Stade_Oc%C3%A9ane\n49° 29′ 56″ N, 0° 10′ 11″ E\nhttp://fr.wikipedia.org/wiki/Stade_Malherbe_Caen_Calvados_Basse-Normandie\nhttp://fr.wikipedia.org/wiki/Stade_Michel-d%27Ornano\n49° 10′ 46″ N, 0° 23′ 48″ O\nhttp://fr.wikipedia.org/wiki/Clermont_Foot_63\nhttp://fr.wikipedia.org/wiki/Stade_Gabriel-Montpied\n45° 48′ 57″ N, 3° 07′ 18″ E\nhttp://fr.wikipedia.org/wiki/En_Avant_de_Guingamp\nhttp://fr.wikipedia.org/wiki/Stade_de_Roudourou\n48° 33′ 58″ N, 3° 09′ 52″ O\nhttp://fr.wikipedia.org/wiki/Football_Club_Lorient\nhttp://fr.wikipedia.org/wiki/Stade_du_Moustoir\n47° 44′ 56″ N, 3° 22′ 09″ O\nhttp://fr.wikipedia.org/wiki/Paris_Football_Club\nhttp://fr.wikipedia.org/wiki/Stade_Charl%C3%A9ty\n48° 49′ 07″ N, 2° 20′ 47″ E\nhttp://fr.wikipedia.org/wiki/La_Berrichonne_de_Ch%C3%A2teauroux\nhttp://fr.wikipedia.org/wiki/Stade_Gaston-Petit\n46° 48′ 07″ N, 1° 43′ 18″ E\nhttp://fr.wikipedia.org/wiki/Association_de_la_jeunesse_auxerroise\nhttp://fr.wikipedia.org/wiki/Stade_de_l%27Abb%C3%A9-Deschamps\n47° 47′ 12″ N, 3° 35′ 19″ E\nhttp://fr.wikipedia.org/wiki/Union_sportive_Orl%C3%A9ans_Loiret_football\nhttp://fr.wikipedia.org/wiki/Stade_de_la_Source\n47° 50′ 25″ N, 1° 56′ 28″ E\nhttp://fr.wikipedia.org/wiki/Valenciennes_Football_Club\nhttp://fr.wikipedia.org/wiki/Stade_du_Hainaut\n50° 20′ 55″ N, 3° 31′ 56″ E\nhttp://fr.wikipedia.org/wiki/Chamois_niortais_Football_Club\nhttp://fr.wikipedia.org/wiki/Stade_Ren%C3%A9-Gaillard\n46° 19′ 01″ N, 0° 29′ 21″ O\nhttp://fr.wikipedia.org/wiki/Grenoble_Foot_38\nhttp://fr.wikipedia.org/wiki/Stade_des_Alpes\n45° 11′ 15″ N, 5° 44′ 24″ E\nhttp://fr.wikipedia.org/wiki/Football_Club_Sochaux-Montb%C3%A9liard\nhttp://fr.wikipedia.org/wiki/Stade_Auguste-Bonal\n47° 30′ 44″ N, 6° 48′ 41″ E\nhttp://fr.wikipedia.org/wiki/Rodez_Aveyron_Football\nhttp://fr.wikipedia.org/wiki/Stade_Paul-Lignon\n44° 21′ 06″ N, 2° 33′ 49″ E\nhttp://fr.wikipedia.org/wiki/Football_Club_de_Chambly_Oise\nhttp://fr.wikipedia.org/wiki/Stade_Walter_Luzi\n49° 10′ 45″ N, 2° 14′ 01″ E\nhttp://fr.wikipedia.org/wiki/Esp%C3%A9rance_sportive_Troyes_Aube_Champagne\nhttp://fr.wikipedia.org/wiki/Stade_de_l%27Aube\n48° 18′ 27″ N, 4° 05′ 55″ E\nhttp://fr.wikipedia.org/wiki/Racing_Club_de_Lens\nhttp://fr.wikipedia.org/wiki/Stade_Bollaert-Delelis\n50° 25′ 58″ N, 2° 48′ 54″ E\nhttp://fr.wikipedia.org/wiki/Le_Mans_Football_Club\nhttp://fr.wikipedia.org/wiki/Stade_Marie-Marvingt\n47° 57′ 32″ N, 0° 13′ 29″ E\n\n\n\ndata.head(5)\n\n\n\n\n\n\n\n\ndivision\nequipe\nstade\nlatitude\nlongitude\n\n\n\n\n0\nL1\nParis Saint-Germain Football Club\nParc des Princes\n48° 50′ 29″ N'\n2° 15′ 11″ E'\n\n\n1\nL1\nLOSC Lille\nStade Pierre-Mauroy\n50° 36′ 43″ N'\n3° 07′ 50″ E'\n\n\n2\nL1\nOlympique lyonnais\nParc Olympique lyonnais\n45° 45′ 55″ N'\n4° 58′ 55″ E'\n\n\n3\nL1\nAssociation sportive de Saint-Étienne\nStade Geoffroy-Guichard\n45° 27′ 39″ N'\n4° 23′ 25″ E'\n\n\n4\nL1\nOlympique de Marseille\nOrange Vélodrome\n43° 16′ 11″ N'\n5° 23′ 45″ E'\n\n\n\n\n\n\n\nOn va transformer les coordonnées en degrés en coordonnées numériques\nafin d’être en mesure de faire une carte\n\nimport re\n\ndef dms2dd(degrees, minutes, seconds, direction):\n    dd = float(degrees) + float(minutes)/60 + float(seconds)/(60*60);\n    if direction in ('S', 'O'):\n        dd *= -1\n    return dd\n\ndef parse_dms(dms):\n    parts = re.split('[^\\d\\w]+', dms)\n    lat = dms2dd(parts[0], parts[1], parts[2], parts[3])\n    #lng = dms2dd(parts[4], parts[5], parts[6], parts[7])\n    return lat\n\n\ndata['latitude'] = data['latitude'].apply(parse_dms)\ndata['longitude'] = data['longitude'].apply(parse_dms)\n\nTous les éléments sont en place pour faire une belle carte à ce stade. On\nva utiliser folium pour celle-ci, qui est présenté dans la partie\nvisualisation.\n\n\n13.5.2 Carte des stades avec folium\n\n#!pip install geopandas\nimport geopandas as gpd\nfrom pathlib import Path\nimport folium\n\ngdf = gpd.GeoDataFrame(\n    data, geometry=gpd.points_from_xy(data.longitude, data.latitude))\n\nPath(\"leaflet\").mkdir(parents=True, exist_ok=True)\n\ncenter = gdf[['latitude', 'longitude']].mean().values.tolist()\nsw = gdf[['latitude', 'longitude']].min().values.tolist()\nne = gdf[['latitude', 'longitude']].max().values.tolist()\n\nm = folium.Map(location = center, tiles='Stamen Toner')\n\n# I can add marker one by one on the map\nfor i in range(0,len(gdf)):\n    folium.Marker([gdf.iloc[i]['latitude'], gdf.iloc[i]['longitude']], popup=gdf.iloc[i]['stade']).add_to(m) \n\nm.fit_bounds([sw, ne])\n\nLa carte obtenue doit ressembler à la suivante:\n\n# Afficher la carte\nm"
  },
  {
    "objectID": "content/course/manipulation/04a_webscraping_TP/index.html#récupérer-des-informations-sur-les-pokemons",
    "href": "content/course/manipulation/04a_webscraping_TP/index.html#récupérer-des-informations-sur-les-pokemons",
    "title": "13  Webscraping avec python",
    "section": "13.6 Récupérer des informations sur les pokemons",
    "text": "13.6 Récupérer des informations sur les pokemons\nLe prochain exercice pour mettre en pratique le webscraping\nconsiste à récupérer des informations sur les\npokemons à partir du\nsite internet pokemondb.net.\n\n13.6.1 Version non guidée\n\n\n Exercice 2 : Les pokemon (version non guidée)\nPour cet exercice, nous vous demandons d’obtenir différentes informations sur les pokémons :\n\nles informations personnelles des 893 pokemons sur le site internet pokemondb.net.\nLes informations que nous aimerions obtenir au final dans un DataFrame sont celles contenues dans 4 tableaux :\n\n\nPokédex data\nTraining\nBreeding\nBase stats\n\n\nNous aimerions que vous récupériez également les images de chacun des pokémons et que vous les enregistriez dans un dossier\n\n\nPetit indice : utilisez les modules request et shutil\nPour cette question, il faut que vous cherchiez de vous même certains éléments, tout n’est pas présent dans le TD.\n\n\n\nPour la question 1, l’objectif est d’obtenir le code source d’un tableau comme\ncelui qui suit\n(Pokemon Nincada.)\n\n\n\nPokédex data\n\n\n\n\n\nNational №\n\n\n290\n\n\n\n\nType\n\n\nBug Ground\n\n\n\n\nSpecies\n\n\nTrainee Pokémon\n\n\n\n\nHeight\n\n\n0.5 m (1′08″)\n\n\n\n\nWeight\n\n\n5.5 kg (12.1 lbs)\n\n\n\n\nAbilities\n\n\n1. Compound EyesRun Away (hidden ability)\n\n\n\n\nLocal №\n\n\n042 (Ruby/Sapphire/Emerald)111 (X/Y — Central Kalos)043 (Omega Ruby/Alpha Sapphire)104 (Sword/Shield)\n\n\n\n\n\n\n\n\n\nTraining\n\n\n\n\n\nEV yield\n\n\n1 Defense\n\n\n\n\nCatch rate\n\n\n255 (33.3% with PokéBall, full HP)\n\n\n\n\nBase Friendship\n\n\n70 (normal)\n\n\n\n\nBase Exp.\n\n\n53\n\n\n\n\nGrowth Rate\n\n\nErratic\n\n\n\n\n\n\n\nBreeding\n\n\n\n\n\nEgg Groups\n\n\nBug\n\n\n\n\nGender\n\n\n50% male, 50% female\n\n\n\n\nEgg cycles\n\n\n15 (3,599–3,855 steps)\n\n\n\n\n\n\n\n\n\n\n\n\nBase stats\n\n\n\n\n\n\nHP\n\n\n31\n\n\n\n\n\n\n\n172\n\n\n266\n\n\n\n\nAttack\n\n\n45\n\n\n\n\n\n\n\n85\n\n\n207\n\n\n\n\nDefense\n\n\n90\n\n\n\n\n\n\n\n166\n\n\n306\n\n\n\n\nSp. Atk\n\n\n30\n\n\n\n\n\n\n\n58\n\n\n174\n\n\n\n\nSp. Def\n\n\n30\n\n\n\n\n\n\n\n58\n\n\n174\n\n\n\n\nSpeed\n\n\n40\n\n\n\n\n\n\n\n76\n\n\n196\n\n\n\n\n\n\nTotal\n\n\n266\n\n\n\n\nMin\n\n\nMax\n\n\n\n\n\n\n\nPour la question 2, l’objectif est d’obtenir\nl’une des images suivantes:\n\n\n\n13.6.2 Version guidée\nLes prochaines parties permettront de faire l’exercice ci-dessus\nétape par étape,\nde manière guidée.\nNous souhaitons tout d’abord obtenir les\ninformations personnelles de tous\nles pokemons sur pokemondb.net.\nLes informations que nous aimerions obtenir au final pour les pokemons sont celles contenues dans 4 tableaux :\n\nPokédex data\nTraining\nBreeding\nBase stats\n\nNous proposons ensuite de récupérer et afficher les images.\n\n13.6.2.1 Etape 1: constituer un DataFrame de caractéristiques\n\n\n Exercice 2b : Les pokémons (version guidée)\nPour récupérer les informations, le code devra être divisé en plusieurs étapes :\n\nTrouvez la page principale du site et la transformer en un objet intelligible pour votre code.\nLes fonctions suivantes vous seront utiles :\n\n\nurllib.request.Request\nurllib.request.urlopen\nbs4.BeautifulSoup\n\n\nCréez une fonction qui permet de récupérer la page d’un pokémon à partir de son nom.\nA partir de la page de bulbasaur, obtenez les 4 tableaux qui nous intéressent :\n\n\non va chercher l’élément suivant : ('table', { 'class' : \"vitals-table\"})\npuis stocker ses éléments dans un dictionnaire\n\n\nRécupérez par ailleurs la liste de noms des pokémons qui nous permettra de faire une boucle par la suite. Combien trouvez-vous de pokémons ?\nEcrire une fonction qui récupère l’ensemble des informations sur les dix premiers pokémons de la liste et les intègre dans un DataFrame\n\n\n\nA l’issue de la question 3,\nvous devriez obtenir une liste de caractéristiques proche de celle-ci:\n\n\ndefaultdict(None,\n            {'National №': '0001',\n             'name': 'bulbasaur',\n             'Type': ' Grass Poison ',\n             'Species': 'Seed Pokémon',\n             'Height': '0.7\\xa0m (2′04″)',\n             'Weight': '6.9\\xa0kg (15.2\\xa0lbs)',\n             'Abilities': '1. OvergrowChlorophyll (hidden ability)',\n             'Local №': \"0001 (Red/Blue/Yellow)0226 (Gold/Silver/Crystal)0001 (FireRed/LeafGreen)0231 (HeartGold/SoulSilver)0080 (X/Y — Central Kalos)0001 (Let's Go Pikachu/Let's Go Eevee)0068 (The Isle of Armor)\",\n             'EV yield': ' 1 Sp. Atk ',\n             'Catch rate': ' 45 (5.9% with PokéBall, full HP) ',\n             'Base Friendship': ' 50 (normal) ',\n             'Base Exp.': '64',\n             'Growth Rate': 'Medium Slow',\n             'Egg Groups': 'Grass, Monster',\n             'Gender': '87.5% male, 12.5% female',\n             'Egg cycles': '20 (4,884–5,140 steps) ',\n             'HP': '45',\n             'Attack': '49',\n             'Defense': '49',\n             'Sp. Atk': '65',\n             'Sp. Def': '65',\n             'Speed': '45'})\n\n\nLa structure est ici en dictionnaire, ce qui est pratique.\nEnfin, vous les\ninformations sur les dix premiers pokémons de la liste intégrées dans un\nDataFrame prendront l’aspect suivant:\n\n\n\n\n\n\n\n\n\nNational №\nname\nType\nSpecies\nHeight\nWeight\nAbilities\nLocal №\nEV yield\nCatch rate\n...\nGrowth Rate\nEgg Groups\nGender\nEgg cycles\nHP\nAttack\nDefense\nSp. Atk\nSp. Def\nSpeed\n\n\n\n\n0\n0001\nbulbasaur\nGrass Poison\nSeed Pokémon\n0.7 m (2′04″)\n6.9 kg (15.2 lbs)\n1. OvergrowChlorophyll (hidden ability)\n0001 (Red/Blue/Yellow)0226 (Gold/Silver/Crysta...\n1 Sp. Atk\n45 (5.9% with PokéBall, full HP)\n...\nMedium Slow\nGrass, Monster\n87.5% male, 12.5% female\n20 (4,884–5,140 steps)\n45\n49\n49\n65\n65\n45\n\n\n1\n0002\nivysaur\nGrass Poison\nSeed Pokémon\n1.0 m (3′03″)\n13.0 kg (28.7 lbs)\n1. OvergrowChlorophyll (hidden ability)\n0002 (Red/Blue/Yellow)0227 (Gold/Silver/Crysta...\n1 Sp. Atk, 1 Sp. Def\n45 (5.9% with PokéBall, full HP)\n...\nMedium Slow\nGrass, Monster\n87.5% male, 12.5% female\n20 (4,884–5,140 steps)\n60\n62\n63\n80\n80\n60\n\n\n2\n0003\nvenusaur\nGrass Poison\nSeed Pokémon\n2.0 m (6′07″)\n100.0 kg (220.5 lbs)\n1. OvergrowChlorophyll (hidden ability)\n0003 (Red/Blue/Yellow)0228 (Gold/Silver/Crysta...\n2 Sp. Atk, 1 Sp. Def\n45 (5.9% with PokéBall, full HP)\n...\nMedium Slow\nGrass, Monster\n87.5% male, 12.5% female\n20 (4,884–5,140 steps)\n80\n82\n83\n100\n100\n80\n\n\n3\n0004\ncharmander\nFire\nLizard Pokémon\n0.6 m (2′00″)\n8.5 kg (18.7 lbs)\n1. BlazeSolar Power (hidden ability)\n0004 (Red/Blue/Yellow)0229 (Gold/Silver/Crysta...\n1 Speed\n45 (5.9% with PokéBall, full HP)\n...\nMedium Slow\nDragon, Monster\n87.5% male, 12.5% female\n20 (4,884–5,140 steps)\n39\n52\n43\n60\n50\n65\n\n\n4\n0005\ncharmeleon\nFire\nFlame Pokémon\n1.1 m (3′07″)\n19.0 kg (41.9 lbs)\n1. BlazeSolar Power (hidden ability)\n0005 (Red/Blue/Yellow)0230 (Gold/Silver/Crysta...\n1 Sp. Atk, 1 Speed\n45 (5.9% with PokéBall, full HP)\n...\nMedium Slow\nDragon, Monster\n87.5% male, 12.5% female\n20 (4,884–5,140 steps)\n58\n64\n58\n80\n65\n80\n\n\n\n\n5 rows × 22 columns\n\n\n\n\n\n13.6.2.2 Etape 2: récupérer et afficher des photos de Pokemon\nNous aimerions que vous récupériez également les images des 5 premiers pokémons\net que vous les enregistriez dans un dossier.\n\n\n Exercice 2b : Les pokémons (version guidée)\n\nLes URL des images des pokemon prennent la forme “https://img.pokemondb.net/artwork/{pokemon}.jpg”.\nUtiliser les modules requests et shutil pour télécharger\net enregistrer en local les images.\nImporter ces images stockées au format JPEG dans Python grâce à la fonction imread du package skimage.io"
  },
  {
    "objectID": "content/course/manipulation/04a_webscraping_TP/index.html#selenium-mimer-le-comportement-dun-utilisateur-internet",
    "href": "content/course/manipulation/04a_webscraping_TP/index.html#selenium-mimer-le-comportement-dun-utilisateur-internet",
    "title": "13  Webscraping avec python",
    "section": "13.7 Selenium : mimer le comportement d’un utilisateur internet",
    "text": "13.7 Selenium : mimer le comportement d’un utilisateur internet\nJusqu’à présent,\nnous avons raisonné comme si nous connaissions toujours l’url qui nous intéresse.\nDe plus, les pages que nous visitons sont “statiques”,\nelles ne dépendent pas d’une action ou d’une recherche de l’internaute.\nNous allons voir à présent comment nous en sortir pour remplir\ndes champs sur un site web et récupérer ce qui nous intéresse.\nLa réaction d’un site web à l’action d’un utilisateur passe régulièrement par\nl’usage de JavaScript dans le monde du développement web.\nLe package Selenium permet\nde reproduire, depuis un code automatisé, le comportement\nmanuel d’un utilisateur. Il permet ainsi\nd’obtenir des informations du site qui ne sont pas dans le\ncode HTML mais qui apparaissent uniquement à la suite de\nl’exécution de script JavaScript en arrière plan.\nSelenium se comporte comme un utilisateur lambda sur internet :\nil clique sur des liens, il remplit des formulaires, etc.\n\n13.7.1 Premier exemple en scrapant un moteur de recherche\nDans cet exemple, nous allons essayer d’aller sur le\nsite de Bing Actualités\net entrer dans la barre de recherche un sujet donné.\nPour tester, nous allons faire une recherche avec le mot-clé “Trump”.\nL’installation de Selenium nécessite d’avoir Chromium qui est un\nnavigateur Google Chrome minimaliste.\nLa version de chromedriver\ndoit être &gt;= 2.36 et dépend de la version de Chrome que vous avez sur votre environnement\nde travail. Pour installer cette version minimaliste de Chrome sur un environnement\nLinux, vous pouvez\nvous référer à l’encadré dédié\n\n\n `Installation de Selenium`\nD’abord, il convient d’installer les dépendances.\nSur Colab, vous pouvez utiliser les commandes suivantes:\n\n!sudo apt-get update\n!sudo apt install -y unzip xvfb libxi6 libgconf-2-4 -y\n!sudo apt install chromium-chromedriver -y\n!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n\nSi vous êtes sur le SSP-Cloud, vous pouvez\nexécuter les commandes suivantes:\n\n!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb -O /tmp/chrome.deb\n!sudo apt-get update\n!sudo -E apt-get install -y /tmp/chrome.deb\n!pip install chromedriver-autoinstaller selenium\n\nimport chromedriver_autoinstaller\nchromedriver_autoinstaller.install()\n\nVous pouvez ensuite installer Selenium.\nPar\nexemple, depuis une\ncellule de Notebook:\n\n!pip install selenium\n\n\n\nAprès avoir installé Chromium,\nil est nécessaire d’indiquer à Python où\nle trouver. Si vous êtes sur Linux et que vous\navez suivi les consignes précédentes, vous\npouvez faire:\n\nimport sys\nsys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\nimport selenium\npath_to_web_driver = \"chromedriver\"\n\nEn premier lieu, il convient d’initialiser le comportement\nde Selenium en répliquant les paramètres\ndu navigateur. Pour cela, on va d’abord initialiser\nnotre navigateur avec quelques options:\n\nimport time\n\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\n\nchrome_options = webdriver.ChromeOptions()\nchrome_options.add_argument('--headless')\nchrome_options.add_argument('--no-sandbox')\n#chrome_options.add_argument('--verbose') \n\nPuis on lance le navigateur:\n\nfrom selenium.webdriver.chrome.service import Service\nservice = Service(executable_path=path_to_web_driver)\n\nbrowser = webdriver.Chrome(service=service,\n                           options=chrome_options)\n\nOn va sur le site de Bing Actualités, et on lui indique le mot clé que nous souhaitons chercher.\nEn l’occurrence, on s’intéresse aux actualités de Donald Trump.\nAprès avoir inspecté la page depuis les outils de développement du navigateur,\non voit que la barre de recherche est un élement du code appelé q (comme query).\nOn va ainsi demander à selenium de chercher cet élément:\n\nbrowser.get('https://www.bing.com/news')\n\nsearch = browser.find_element(\"name\", \"q\")\nprint(search)\nprint([search.text, search.tag_name, search.id])\n\n# on envoie à cet endroit le mot qu'on aurait tapé dans la barre de recherche\nsearch.send_keys(\"Trump\")\n\nsearch_button = browser.find_element(\"xpath\", \"//input[@id='sb_form_go']\") \nsearch_button.click()\n\nselenium permet de capturer l’image qu’on verrait dans le navigateur\navec get_screenshot_as_png. Cela peut être utile pour vérifier qu’on\na fait la bonne action:\n\npng = browser.get_screenshot_as_png()\n\n\nfrom IPython.display import Image\nImage(png, width='500')\n\n\n\n\n\n\n\n\nEnfin, on peut extraire les résultats. Plusieurs\nméthodes sont disponibles. La méthode la plus\npratique, lorsqu’elle est disponible,\nest d’utiliser le XPath qui est un chemin\nnon ambigu pour accéder à un élement. En effet,\nplusieurs éléments peuvent partager la même classe ou\nle même attribut ce qui peut faire qu’une recherche\nde ce type peut renvoyer plusieurs échos.\nPour déterminer le XPath d’un objet, les outils\nde développeurs de votre site web sont pratiques.\nPar exemple, sous Firefox, une fois que vous\navez trouvé un élément dans l’inspecteur, vous\npouvez faire click droit &gt; Copier &gt; XPath.\n\nfrom selenium.common.exceptions import StaleElementReferenceException\nlinks = browser.find_elements(\"xpath\", \"//div/a[@class='title'][@href]\")\n\nresults = []\nfor link in links:\n    try:\n        url = link.get_attribute('href')\n    except StaleElementReferenceException as e:\n        print(\"Issue with '{0}' and '{1}'\".format(url, link))\n        print(\"It might be due to slow javascript which produces the HTML page.\")\n    results.append(url)\n\nEnfin, pour mettre fin à notre session, on demande\nà Python de quitter le navigateur\n\nbrowser.quit()\n\nOn a obtenu les résultats suivants:\n\nprint(results)\n\n['https://www.msn.com/en-us/news/other/donald-trump-faces-three-trials-in-one-month-as-legal-cases-collide/ar-AA1f99tY', 'https://news.yahoo.com/trump-lawyers-face-off-jack-120000397.html', 'https://www.msn.com/en-gb/news/world/trump-election-case-lawyers-in-court-to-push-back-on-prosecutor-s-bid-to-stop-ex-president-publicizing-evidence-live/ar-AA1f92eU', 'https://www.msn.com/en-us/news/other/trump-could-face-big-picture-rico-case-in-georgia-expert-says/ar-AA1f93IV']\n\n\nLes autres méthodes utiles de Selenium:\nfind_element(****).click() | Une fois qu’on a trouvé un élément réactif, notamment un bouton, on peut cliquer dessus pour activer une nouvelle page |\nfind_element(****).send_keys(\"toto\") | Une fois qu’on a trouvé un élément, notamment un champ où s’authentifier, on peut envoyer une valeur, ici “toto”.\n\n\n13.7.2 Utiliser selenium pour jouer à 2048\nDans cet exemple, on utilise le module pour que Python\nappuie lui même sur les touches du clavier afin de jouer à 2048.\nNote : ce bout de code ne donne pas une solution à 2048,\nil permet juste de voir ce qu’on peut faire avec Selenium\n\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.common.keys import Keys\n\n# on ouvre la page internet du jeu 2048\nservice = Service(executable_path=path_to_web_driver)\n\nbrowser = webdriver.Chrome(service=service)\nbrowser.get('https://play2048.co//')\n\n# Ce qu'on va faire : une boucle qui répète inlassablement la même chose : haut / droite / bas / gauche\n\n# on commence par cliquer sur la page pour que les touches sachent \nbrowser.find_element(\"class name\", 'grid-container').click()\ngrid = browser.find_element(\"tag name\", 'body')\n\n# pour savoir quels coups faire à quel moment, on crée un dictionnaire\ndirection = {0: Keys.UP, 1: Keys.RIGHT, 2: Keys.DOWN, 3: Keys.LEFT}\ncount = 0\n\nwhile True:\n    try: # on vérifie que le bouton \"Try again\" n'est pas là - sinon ça veut dire que le jeu est fini\n        retryButton = browser.find_element(\"link text\",'Try again')\n        scoreElem = browser.find_element(\"class name\", 'score-container')\n        break\n    except:\n        #Do nothing.  Game is not over yet\n        pass\n    # on continue le jeu - on appuie sur la touche suivante pour le coup d'après\n    count += 1\n    grid.send_keys(direction[count % 4]) \n    time.sleep(0.1)\n\nprint('Score final : {} en {} coups'.format(scoreElem.text, count))    \nbrowser.quit()"
  },
  {
    "objectID": "content/course/manipulation/04a_webscraping_TP/index.html#exercices-supplémentaires",
    "href": "content/course/manipulation/04a_webscraping_TP/index.html#exercices-supplémentaires",
    "title": "13  Webscraping avec python",
    "section": "13.8 Exercices supplémentaires",
    "text": "13.8 Exercices supplémentaires\n\n13.8.1 Récupérer les noms et âges des ministres français\nPour cet exercice, on propose de scraper la liste des ministres français depuis le site du gouvernement. L’objectif sera, in fine de faire un graphique qui représente la distribution de leurs âges.\nLa solution pour cet exercice a été proposée\npar Tien-Thinh\net Antoine Palazzolo.\nPour être en mesure de faire cet exercice, il est\nrecommandé d’installer le package dateparser\n\n!pip install dateparser\n#depuis un notebook. En ligne de commande, retirer le !\n\nPour cet exercice, nous proposons d’utiliser les packages\nsuivants:\n\nimport time\nfrom tqdm import tqdm\nimport urllib\nimport re, datetime\nfrom dateutil.parser import parse as parse_dt\nimport dateparser\n\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd\nimport bs4\n\nNous proposons également d’utiliser la fonction suivante\npour calculer l’âge à partir de la date de naissance.\n\ndef from_birth_to_age(birth):\n    today = datetime.date.today()\n    return today.year - birth.year - ((today.month, today.day) &lt; (birth.month, birth.day))\n\n\n\n Exercice : Les ministres français \n\nCréer des variables globales url_gouvernement et url_gouvernement qui représenteront,\nrespectivement, la racine de l’URL du site web et le chemin au sein de celui-ci ;\nUtiliser bs4 pour récupérer la composition du gouvernement, qui est contenue dans un &lt;div&gt;\nayant une classe ad hoc. Nommer cet objet compo\nUtiliser find_all pour récupérer la liste des ministres dans compo. Nommer\ncet objet ministres\nInspecter la structure des champs au sein de ministres. Répérer les id biography. Comme\nla structure est générique, on va écrire une fonction from_bio_to_age sur laquelle on va itérer\npour chaque élément de la liste ministres. Cette fonction effectuera les opérations suivantes:\n\nRemplacer les champs de dates de naissance non numériques (par exemple “1er”), en valeur numérique (par exemple 1).\nUtiliser la regex [0-3]?\\d \\S* \\d{4} avec le package re pour extraire les dates\nde naissance. Nommer l’objet str_date.\nAppliquer dateparser.parse pour convertir sous forme de date\nAppliquer from_birth_to_age pour transformer cette date de naissance en âge\n\nPour chaque élément de la liste ministres, faire une boucle (en introduisant un\ntime.sleep(0.25) entre chaque itération pour ne pas surcharger le site):\n\nRécupérer les noms et prénoms, fonctions pour chaque ministre\nRécupérer l’URL de la photo\nCréer un URL pour chaque ministre afin d’appliquer la fonction\nfrom_bio_to_age\n\nUtiliser matplotlib ou seaborn pour faire un histogramme d’âge\n\n\n\nA l’issue de la question 4, on devrait\nretrouver les informations suivantes:\n\nprint(f\"Nous retrouvons ainsi {len(ministres)} ministres.\")\n\nNous retrouvons ainsi 44 ministres.\n\n\n\ndef from_bio_to_age(url):\n    html = urllib.request.urlopen(url).read()\n    page = bs4.BeautifulSoup(html)\n    s = page.find(\"div\", {\"id\":\"biography\"}).text.replace(\"1er\", \"1\") # un peu ad hoc\n    expression = re.compile(\"[0-3]?\\d \\S* \\d{4}\") # renvoie parfois des dates autres que dates de naissance\n    str_date = expression.findall(s)[0]\n    date_de_naissance = dateparser.parse(str_date).date()\n    return from_birth_to_age(date_de_naissance)\n\nIn fine, on obtient une liste dont le premier élément\nprend la forme suivante:\n\nliste[0]\n\n{'Nom complet': 'Élisabeth Borne',\n 'Fonction': 'Première ministre',\n 'Photo': 'https://www.gouvernement.fr/sites/default/files/styles/composition_large/public/pm_elisabeth_borne_portrait_matignon_.jpg?itok=ay_WErm3',\n 'href': 'https://www.gouvernement.fr/ministre/elisabeth-borne',\n 'Age': nan}\n\n\nFinalement, le DataFrame pourra être\nstructuré sous la forme suivante. On va éliminer\nles âges égaux à 0 sont qui sont des erreurs\nde scraping:\nlorsque la date de naissance complète n’est pas disponible\nsur la biographie d’un ministre.\n\ndf = pd.DataFrame(liste)\ndf = df.loc[df['Age'] != 0]\ndf.head(3)\n\n\n\n\n\n\n\n\nNom complet\nFonction\nPhoto\nhref\nAge\n\n\n\n\n0\nÉlisabeth Borne\nPremière ministre\nhttps://www.gouvernement.fr/sites/default/file...\nhttps://www.gouvernement.fr/ministre/elisabeth...\nNaN\n\n\n1\nOlivier Véran\nMinistre délégué auprès de la Première ministr...\nhttps://www.gouvernement.fr/sites/default/file...\nhttps://www.gouvernement.fr/ministre/olivier-v...\nNaN\n\n\n2\nFranck Riester\nMinistre délégué auprès de la Première ministr...\nhttps://www.gouvernement.fr/sites/default/file...\nhttps://www.gouvernement.fr/ministre/franck-ri...\nNaN\n\n\n\n\n\n\n\nFinalement, l’histogramme aura l’aspect suivant:\n\nplt.hist(df.Age, bins=np.arange(25, 80, 4))\n\n/opt/mamba/lib/python3.9/site-packages/matplotlib/axes/_axes.py:6763: RuntimeWarning:\n\nAll-NaN slice encountered\n\n/opt/mamba/lib/python3.9/site-packages/matplotlib/axes/_axes.py:6764: RuntimeWarning:\n\nAll-NaN slice encountered\n\n\n\n(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n array([25., 29., 33., 37., 41., 45., 49., 53., 57., 61., 65., 69., 73.,\n        77.]),\n &lt;BarContainer object of 13 artists&gt;)"
  },
  {
    "objectID": "content/course/manipulation/04c_API_TP/index.html#introduction-quest-ce-quune-api",
    "href": "content/course/manipulation/04c_API_TP/index.html#introduction-quest-ce-quune-api",
    "title": "14  Récupérer des données avec des API depuis Python",
    "section": "14.1 Introduction : Qu’est-ce qu’une API ?",
    "text": "14.1 Introduction : Qu’est-ce qu’une API ?\n\n14.1.1 Définition\nPour expliquer le principe d’une API, je vais reprendre le début de\nla fiche dédiée dans la documentation collaborative\nutilitR que je recommande de lire :\n\nUne Application Programming Interface (ou API) est une interface de programmation qui permet d’utiliser une application existante pour restituer des données. Le terme d’API peut être paraître intimidant, mais il s’agit simplement d’une façon de mettre à disposition des données : plutôt que de laisser l’utilisateur consulter directement des bases de données (souvent volumineuses et complexes), l’API lui propose de formuler une requête qui est traitée par le serveur hébergeant la base de données, puis de recevoir des données en réponse à sa requête.\nD’un point de vue informatique, une API est une porte d’entrée clairement identifiée par laquelle un logiciel offre des services à d’autres logiciels (ou utilisateurs). L’objectif d’une API est de fournir un point d’accès à une fonctionnalité qui soit facile à utiliser et qui masque les détails de la mise en oeuvre. Par exemple, l’API Sirene permet de récupérer la raison sociale d’une entreprise à partir de son identifiant Siren en interrogeant le référentiel disponible sur Internet directement depuis un script R, sans avoir à connaître tous les détails du répertoire Sirene.\nÀ l’Insee comme ailleurs, la connexion entre les bases de données pour les nouveaux projets tend à se réaliser par des API. L’accès à des données par des API devient ainsi de plus en plus commun et est amené à devenir une compétence de base de tout utilisateur de données.\nutilitR\n\n\n\n14.1.2 Avantages des API\nA nouveau, citons la documentation utilitR\nLes API présentent de multiples avantages :\n\n\nLes API rendent les programmes plus reproductibles. En effet, grâce aux API, il est possible de mettre à jour facilement les données utilisées par un programme si celles-ci évoluent. Cette flexibilité accrue pour l’utilisateur évite au producteur de données d’avoir à réaliser de multiples extractions, et réduit le problème de la coexistence de versions différentes des données.\nGrâce aux API, l’utilisateur peut extraire facilement une petite partie d’une base de données plus conséquente.\nLes API permettent de mettre à disposition des données tout en limitant le nombre de personnes ayant accès aux bases de données elles-mêmes.\nGrâce aux API, il est possible de proposer des services sur mesure pour les utilisateurs (par exemple, un accès spécifique pour les gros utilisateurs).\n\nutilitR\n\nL’utilisation accrue d’API dans le cadre de stratégies open-data est l’un\ndes piliers des 15 feuilles de route ministérielles\nen matière d’ouverture, de circulation et de valorisation des données publiques.\n\n\n14.1.3 Utilisation des API\nCitons encore une fois\nla documentation utilitR\n\nUne API peut souvent être utilisée de deux façons : par une interface Web, et par l’intermédiaire d’un logiciel (R, Python…). Par ailleurs, les API peuvent être proposées avec un niveau de liberté variable pour l’utilisateur :\n\nsoit en libre accès (l’utilisation n’est pas contrôlée et l’utilisateur peut utiliser le service comme bon lui semble) ;\nsoit via la génération d’un compte et d’un jeton d’accès qui permettent de sécuriser l’utilisation de l’API et de limiter le nombre de requêtes.\n\nutilitR\n\nDe nombreuses API nécessitent une authentification, c’est-à-dire un\ncompte utilisateur afin de pouvoir accéder aux données.\nDans un premier temps,\nnous regarderons exclusivement les API ouvertes sans restriction d’accès.\nCertains exercices et exemples permettront néanmoins d’essayer des API\navec restrictions d’accès."
  },
  {
    "objectID": "content/course/manipulation/04c_API_TP/index.html#requêter-une-api",
    "href": "content/course/manipulation/04c_API_TP/index.html#requêter-une-api",
    "title": "14  Récupérer des données avec des API depuis Python",
    "section": "14.2 Requêter une API",
    "text": "14.2 Requêter une API\n\n14.2.1 Principe général\n\nL’utilisation de l’interface Web est utile dans une démarche exploratoire mais trouve rapidement ses limites, notamment lorsqu’on consulte régulièrement l’API. L’utilisateur va rapidement se rendre compte qu’il est beaucoup plus commode d’utiliser une API via un logiciel de traitement pour automatiser la consultation ou pour réaliser du téléchargement de masse. De plus, l’interface Web n’existe pas systématiquement pour toutes les API.\nLe mode principal de consultation d’une API consiste à adresser une requête à cette API via un logiciel adapté (R, Python, Java…). Comme pour l’utilisation d’une fonction, l’appel d’une API comprend des paramètres qui sont détaillées dans la documentation de l’API.\nutilitR\n\nVoici les éléments importants à avoir en tête sur les requêtes (j’emprunte encore\nà utilitR):\n\nLe point d’entrée d’un service offert par une API se présente sous la forme d’une URL (adresse web).\nChaque service proposé par une API a sa propre URL. Par exemple, dans le cas de l’OpenFood Facts,\nl’URL à utiliser pour obtenir des informations sur un produit particulier (l’identifiant 737628064502)\nest https://world.openfoodfacts.org/api/v0/product/737628064502.json\nCette URL doit être complétée avec différents paramètres qui précisent la requête (par exemple l’identifiant Siren). Ces paramètres viennent s’ajouter à l’URL, souvent à la suite de ?. Chaque service proposé par une API a ses propres paramètres, détaillés dans la documentation.\nLorsque l’utilisateur soumet sa requête, l’API lui renvoie une réponse structurée contenant l’ensemble des informations demandées. Le résultat envoyé par une API est majoritairement aux formats JSON ou XML (deux formats dans lesquels les informations sont hiérarchisées de manière emboitée). Plus rarement, certains services proposent une information sous forme plate (de type csv).\n\nDu fait de la dimension hiérarchique des formats JSON ou XML, le résultat n’est pas toujours facile à récupérer mais\npython propose d’excellents outils pour cela (meilleurs que ceux de R). Certains packages, notamment json, facilitent l’extraction de champs d’une sortie d’API. Dans certains cas, des packages spécifiques à une API ont été créés pour simplifier l’écriture d’une requête ou la récupération du résultat. Par exemple, le package\npynsee\npropose des options qui seront retranscrites automatiquement dans l’URL de\nrequête pour faciliter le travail sur les données Insee.\n\n\n14.2.2 Illustration avec une API de l’Ademe pour obtenir des diagnostics energétiques\nLe diagnostic de performance énergétique (DPE)\nrenseigne sur la performance énergétique d’un logement ou d’un bâtiment,\nen évaluant sa consommation d’énergie et son impact en terme d’émissions de gaz à effet de serre.\nLes données des performances énergétiques des bâtiments sont\nmises à disposition par l’Ademe.\nComme ces données sont relativement\nvolumineuses, une API peut être utile lorsqu’on ne s’intéresse\nqu’à un sous-champ des données.\nUne documentation et un espace de test de l’API sont disponibles\nsur le site API GOUV1.\nSupposons qu’on désire récupérer une centaine de valeurs pour la commune\nde Villieu-Loyes-Mollon dans l’Ain (code Insee 01450).\nL’API comporte plusieurs points d’entrée. Globalement, la racine\ncommune est:\n\nhttps://koumoul.com/data-fair/api/v1/datasets/dpe-france\n\nEnsuite, en fonction de l’API désirée, on va ajouter des éléments\nà cette racine. En l’occurrence, on va utiliser\nl’API field qui permet de récupérer des lignes en fonction d’un\nou plusieurs critères (pour nous, la localisation géographique):\nL’exemple donné dans la documentation technique est\n\nGET https://koumoul.com/data-fair/api/v1/datasets/dpe-france/values/{field}\n\nce qui en python se traduira par l’utilisation de la méthode get du\npackage request\nsur un url dont la structure est la suivante:\n\nil commencera par https://koumoul.com/data-fair/api/v1/datasets/dpe-france/values/ ;\nil sera ensuite suivi par des paramètres de recherche? Le champ {field}\ncommande ainsi généralement par un ? qui permet ensuite de spécifier des paramètres\nsous la forme nom_parameter=value\n\nA la lecture de la documentation, les premiers paramètres qu’on désire:\n\nLe nombre de pages, ce qui nous permet d’obtenir un certain nombre d’échos. On\nva seulement récupérer 10 pages ce qui correspond à une centaine d’échos. On va\nnéanmoins préciser qu’on veut 100 échos\nLe format de sortie. On va privilégier le JSON qui est un format standard dans le\nmonde des API. Python offre beaucoup de flexibilité grâce à l’un de\nses objets de base, à savoir le dictionnaire (type dict), pour manipuler de tels\nfichiers\nLe code commune des données qu’on désire obtenir. Comme on l’a évoqué,\non va récupérer les données dont le code commune est 01450. D’après la doc,\nil convient de passer le code commune sous le format:\ncode_insee_commune_actualise:{code_commune}. Pour éviter tout risque de\nmauvais formatage, on va utiliser %3A% pour signifier :\nD’autres paramètres annexes, suggérés par la documentation\n\nCela nous donne ainsi un URL dont la structure est la suivante:\n\ncode_commune=\"01450\"\nsize = 100\napi_root=\"https://koumoul.com/data-fair/api/v1/datasets/dpe-france/lines\"\nurl_api = f\"{api_root}?page=1&after=10&format=json&q_mode=simple&qs=code_insee_commune_actualise\" + \"%3A%22\" + f\"{code_commune}\" + \"%22\" + f\"&size={size}&select=\" + \"%2A&sampling=neighbors\"\n\nSi vous introduisez cet URL dans votre navigateur, vous devriez aboutir\nsur un JSON non formaté2. En Python,\non peut utiliser requests pour récupérer les données3:\n\nimport requests\nimport pandas as pd\n\nreq = requests.get(url_api)\nwb = req.json()\n\nPrenons par exemple les 1000 premiers caractères du résultat, pour se donner\nune idée du résultat et se convaincre que notre filtre au niveau\ncommunal est bien passé :\nprint(req.content[:1000])\nb’{“total”: 121,“next”: “https://koumoul.com/data-fair/api/v1/datasets/dpe-france/lines?after=102721&format=json&q_mode=simple&qs=code_insee_commune_actualise%3A%2201450%22&size=100&select=*&sampling=neighbors”,“results”: [\\n {“classe_consommation_energie”: “D”,“tr001_modele_dpe_type_libelle”: “Vente”,“annee_construction”: 1947,“_geopoint”: “45.925922,5.229964”,“latitude”: 45.925922,“surface_thermique_lot”: 117.16,“_i”: 487,“tr002_type_batiment_description”: “Maison Individuelle”,“geo_adresse”: “Rue de la Brugni8re 01800 Villieu-Loyes-Mollon”,“_rand”: 23215,“code_insee_commune_actualise”: “01450”,“estimation_ges”: 53,“geo_score”: 0.4,“classe_estimation_ges”: “E”,“nom_methode_dpe”: “M9thode Facture”,“tv016_departement_code”: “01”,“consommation_energie”: 178,“date_etablissement_dpe”: “2013-06-13”,“longitude”: 5.229964,“_score”: null,’\nIci, il n’est même pas nécessaire en première approche\nd’utiliser le package json, l’information\nétant déjà tabulée dans l’écho renvoyé (on a la même information pour tous les pays):\nOn peut donc se contenter de pandas pour transformer nos données en\nDataFrame et geopandas pour convertir en données\ngéographiques :\n\nimport pandas as pandas\nimport geopandas as gpd\n\ndef get_dpe_from_url(url):\n\n    req = requests.get(url)\n    wb = req.json()\n    df = pd.json_normalize(wb[\"results\"])\n\n    dpe = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude), crs = 4326)\n    dpe = dpe.dropna(subset = ['longitude', 'latitude'])\n\n    return dpe\n\ndpe = get_dpe_from_url(url_api)\ndpe.head(2)\n\n/opt/mamba/lib/python3.9/site-packages/geopandas/_compat.py:123: UserWarning:\n\nThe Shapely GEOS version (3.11.2-CAPI-1.17.2) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n\n/tmp/ipykernel_3229/2008334648.py:2: UserWarning:\n\nShapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n\nimport os\nos.environ['USE_PYGEOS'] = '0'\nimport geopandas\n\nIn a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n\n\n\n\n\n\n\n\n\n\nclasse_consommation_energie\ntr001_modele_dpe_type_libelle\nannee_construction\n_geopoint\nlatitude\nsurface_thermique_lot\n_i\ntr002_type_batiment_description\ngeo_adresse\n_rand\n...\nclasse_estimation_ges\nnom_methode_dpe\ntv016_departement_code\nconsommation_energie\ndate_etablissement_dpe\nlongitude\n_score\n_id\nversion_methode_dpe\ngeometry\n\n\n\n\n0\nD\nVente\n1947\n45.925922,5.229964\n45.925922\n117.16\n487\nMaison Individuelle\nRue de la Brugnière 01800 Villieu-Loyes-Mollon\n23215\n...\nE\nMéthode Facture\n01\n178.00\n2013-06-13\n5.229964\nNone\n04JZNel3WCJYcfsHpCcHv\nNaN\nPOINT (5.22996 45.92592)\n\n\n2\nD\nNeuf\n2006\n45.923421,5.223777\n45.923421\n90.53\n689\nMaison Individuelle\nChemin du Pont-vieux 01800 Villieu-Loyes-Mollon\n401672\n...\nC\nFACTURE - DPE\n01\n227.99\n2013-06-11\n5.223777\nNone\nrkdV2lJn2wxaidVBaHBFY\nV2012\nPOINT (5.22378 45.92342)\n\n\n\n\n2 rows × 23 columns\n\n\n\nEssayons de représenter sur une carte ces DPE avec les\nannées de construction des logements.\nAvec folium, on obtient la carte interactive suivante:\n\nimport seaborn as sns\nimport folium\n\npalette = sns.color_palette(\"coolwarm\", 8)\n\ndef interactive_map_dpe(dpe):\n\n    # convert in number\n    dpe['color'] = [ord(dpe.iloc[i]['classe_consommation_energie'].lower()) - 96 for i in range(len(dpe))]\n    dpe = dpe.loc[dpe['color']&lt;=7]\n    dpe['color'] = [palette.as_hex()[x] for x in dpe['color']]\n\n\n    center = dpe[['latitude', 'longitude']].mean().values.tolist()\n    sw = dpe[['latitude', 'longitude']].min().values.tolist()\n    ne = dpe[['latitude', 'longitude']].max().values.tolist()\n\n    m = folium.Map(location = center, tiles='Stamen Toner')\n\n    # I can add marker one by one on the map\n    for i in range(0,len(dpe)):\n        folium.Marker([dpe.iloc[i]['latitude'], dpe.iloc[i]['longitude']],\n                    popup=f\"Année de construction: {dpe.iloc[i]['annee_construction']}, &lt;br&gt;DPE: {dpe.iloc[i]['classe_consommation_energie']}\",\n                    icon=folium.Icon(color=\"black\", icon=\"home\", icon_color = dpe.iloc[i]['color'])).add_to(m)\n\n    m.fit_bounds([sw, ne])\n\n    return m\n\nm = interactive_map_dpe(dpe)\n\n/opt/mamba/lib/python3.9/site-packages/geopandas/geodataframe.py:1443: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n# Afficher la carte\nm\n\nOn remarque un problème dans les données: un logement qui n’a\nrien à voir avec les autres. Il faudrait donc idéalement\nnettoyer un peu le jeu de données pour filtrer en fonction de\nlimites géographiques.\nUn des paramètres qui peut permettre ceci est geo_distance.\nPour commencer, on va tricher un petit peu pour déterminer\nles longitudes et latitudes de départ. Idéalement, on\nrécupérerait le découpage de la commune et utiliserait, par\nexemple, le centroid de cette commune. Cela nécessite\nnéanmoins l’appel à une autre API que nous n’avons\npour le moment pas décrite. Nous allons donc\nnous contenter d’utiliser les longitudes et latitudes\ndu point médian et fixer un rayon de plusieurs kilomètres\npour exclure les points aberrants.\n\nx_median = dpe['longitude'].median()\ny_median = dpe['latitude'].median()\n\nLa documentation nous informe du format à utiliser:\n\nLe format est ‘lon,lat,distance’. La distance optionnelle (0 par défaut) et est exprimée en mètres.\n\n\nparam_distance = f'{x_median},{y_median},1000'\nprint(param_distance)\n\n5.223777,45.922457,1000\n\n\nNotre requête devient ainsi:\n\nurl_api = f\"{api_root}?page=1&after=10&format=json&q_mode=simple&qs=code_insee_commune_actualise\" + \"%3A%22\" + f\"{code_commune}\" + \"%22\" + f\"&size={size}&select=\" + \"%2A&sampling=neighbors\" + f\"&geodistance={param_distance}\"\nprint(url_api)\n\nhttps://koumoul.com/data-fair/api/v1/datasets/dpe-france/lines?page=1&after=10&format=json&q_mode=simple&qs=code_insee_commune_actualise%3A%2201450%22&size=100&select=%2A&sampling=neighbors&geodistance=5.223777,45.922457,1000\n\n\n\ndpe_geo_filter = get_dpe_from_url(url_api)\nm_geo_filter = interactive_map_dpe(dpe)\n\n/opt/mamba/lib/python3.9/site-packages/geopandas/geodataframe.py:1443: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n# Afficher la carte\nm_geo_filter\n\n\n\n14.2.3 Un catalogue incomplet d’API existantes\nDe plus en plus de sites mettent des API à disposition des développeurs et autres curieux.\nPour en citer quelques-unes très connues :\n\nTwitter  : https://dev.twitter.com/rest/public\nFacebook  : https://developers.facebook.com/\nInstagram  : https://www.instagram.com/developer/\nSpotify  : https://developer.spotify.com/web-api/\n\nCependant, il est intéressant de ne pas se restreindre à celles-ci dont les\ndonnées ne sont pas toujours les plus intéressantes. Beaucoup\nde producteurs de données, privés comme publics, mettent à disposition\nleurs données sous forme d’API\n\nAPI gouv: beaucoup d’API officielles de l’Etat français\net accès à de la documentation\nInsee: https://api.insee.fr/catalogue/ et pynsee\nPole Emploi : https://www.emploi-store-dev.fr/portail-developpeur-cms/home.html\nSNCF : https://data.sncf.com/api\nBanque Mondiale : https://datahelpdesk.worldbank.org/knowledgebase/topics/125589"
  },
  {
    "objectID": "content/course/manipulation/04c_API_TP/index.html#lapi-dvf-accéder-à-des-données-de-transactions-immobilières-simplement",
    "href": "content/course/manipulation/04c_API_TP/index.html#lapi-dvf-accéder-à-des-données-de-transactions-immobilières-simplement",
    "title": "14  Récupérer des données avec des API depuis Python",
    "section": "14.3 L’API DVF : accéder à des données de transactions immobilières simplement",
    "text": "14.3 L’API DVF : accéder à des données de transactions immobilières simplement\n⚠️ Cette partie nécessite une mise à jour pour privilégier l’API DVF du Cerema\nLe site DVF (demandes de valeurs foncières) permet de visualiser toutes les données relatives aux mutations à titre onéreux (ventes de maisons, appartements, garages…) réalisées durant les 5 dernières années.\nUn site de visualisation est disponible sur https://app.dvf.etalab.gouv.fr/.\nCe site est très complet quand il s’agit de connaître le prix moyen au mètre\ncarré d’un quartier ou de comparer des régions entre elles.\nL’API DVF permet d’aller plus loin afin de récupérer les résultats dans\nun logiciel de traitement de données. Elle a été réalisée par\nChristian Quest et le code\nsource est disponible sur Github .\nLes critères de recherche sont les suivants :\n- code_commune = code INSEE de la commune (ex: 94068)\n- section = section cadastrale (ex: 94068000CQ)\n- numero_plan = identifiant de la parcelle, (ex: 94068000CQ0110)\n- lat + lon + dist (optionnel): pour une recherche géographique, dist est par défaut un rayon de 500m\n- code_postal\nLes filtres de sélection complémentaires :\n- nature_mutation (Vente, etc)\n- type_local (Maison, Appartement, Local, Dépendance)\nLes requêtes sont de la forme : http://api.cquest.org/dvf?code_commune=29168.\n\n\n Exercice 1 : Exploiter l'API DVF\n\nRechercher toutes les transactions existantes dans DVF à Plogoff (code commune 29168, en Bretagne).\nAfficher les clés du JSON et en déduire le nombre de transactions répertoriées.\nN’afficher que les transactions portant sur des maisons.\nUtiliser l’API geo pour\nrécupérer le découpage communal de la ville de Plogoff\nReprésenter l’histogramme des prix de vente\n\nN’hésitez pas à aller plus loin en jouant sur des variables de\ngroupes par exemple\n\n\nLe résultat de la question 2 devrait\nressembler au DataFrame suivant:\nL’histogramme des prix de vente (question 4) aura l’aspect suivant:\nOn va faire une carte des ventes en affichant le prix de l’achat.\nLa cartographie réactive sera présentée dans les chapitres\nconsacrés à la visualisation de données.\nSupposons que le DataFrame des ventes s’appelle ventes. Il faut d’abord le\nconvertir\nen objet geopandas.\n\nventes = ventes.dropna(subset = ['lat','lon'])\nventes = gpd.GeoDataFrame(ventes, geometry=gpd.points_from_xy(ventes.lon, ventes.lat))\nventes\n\nAvant de faire une carte, on va convertir\nles limites de la commune de Plogoff en geoJSON pour faciliter\nsa représentation avec folium\n(voir la doc geopandas à ce propos):\n\ngeo_j = plgf.to_json()\n\nPour représenter graphiquement, on peut utiliser le code suivant (essayez de\nle comprendre et pas uniquement de l’exécuter).\n\nimport folium\nimport numpy as np\n\nventes['map_color'] = pd.qcut(ventes['valeur_fonciere'], [0,0.8,1], labels = ['lightblue','red'])\nventes['icon'] = np.where(ventes['type_local']== 'Maison', \"home\", \"\")\nventes['num_voie_clean'] = np.where(ventes['numero_voie'].isnull(), \"\", ventes['numero_voie'])\nventes['text'] = ventes.apply(lambda s: \"Adresse: {num} {voie} &lt;br&gt;Vente en {annee} &lt;br&gt;Prix {prix:.0f} €\".format(\n                        num = s['num_voie_clean'],\n                        voie = s[\"voie\"],\n                        annee = s['date_mutation'].split(\"-\")[0],\n                        prix = s[\"valeur_fonciere\"]),\n             axis=1)\n             \ncenter = ventes[['lat', 'lon']].mean().values.tolist()\nsw = ventes[['lat', 'lon']].min().values.tolist()\nne = ventes[['lat', 'lon']].max().values.tolist()\n\nm = folium.Map(location = center, tiles='Stamen Toner')\n\n# I can add marker one by one on the map\nfor i in range(0,len(ventes)):\n    folium.Marker([ventes.iloc[i]['lat'], ventes.iloc[i]['lon']],\n                  popup=ventes.iloc[i]['text'],\n                  icon=folium.Icon(color=ventes.iloc[i]['map_color'], icon=ventes.iloc[i]['icon'])).add_to(m)\n\nm.fit_bounds([sw, ne])\n\n\n# Afficher la carte\nm"
  },
  {
    "objectID": "content/course/manipulation/04c_API_TP/index.html#géocoder-des-données-grâce-aux-api-officielles",
    "href": "content/course/manipulation/04c_API_TP/index.html#géocoder-des-données-grâce-aux-api-officielles",
    "title": "14  Récupérer des données avec des API depuis Python",
    "section": "14.4 Géocoder des données grâce aux API officielles",
    "text": "14.4 Géocoder des données grâce aux API officielles\nJusqu’à présent, nous avons travaillés sur des données où la dimension\ngéographique était déjà présente ou relativement facile à intégrer.\nCe cas idéal ne se rencontre pas nécessairement dans la pratique.\nOn dispose parfois de localisations plus ou moins précises et plus ou\nmoins bien formattées pour déterminer la localisation de certains\nlieux.\nDepuis quelques années, un service officiel de géocodage a été mis en place.\nCelui-ci est gratuit et permet de manière efficace de coder des adresses\nà partir d’une API. Cette API, connue sous le nom de la Base d’Adresses Nationale\n(BAN) a bénéficié de la mise en commun de données de plusieurs\nacteurs (collectivités locales, Poste) et de compétences d’acteurs\ncomme Etalab. La documentation de celle-ci est disponible à l’adresse\nhttps://api.gouv.fr/les-api/base-adresse-nationale\nPour illustrer la manière de géocoder des données avec Python, nous\nallons partir de la base\ndes résultats des auto-écoles à l’examen du permis sur l’année 2018.\nCes données nécessitent un petit peu de travail pour être propres à une\nanalyse statistique.\nAprès avoir renommé les colonnes, nous n’allons conserver que\nles informations relatives au permis B (permis voiture classique) et\nles auto-écoles ayant présenté au moins 20 personnes à l’examen.\n\nimport pandas as pd\nimport xlrd\nimport geopandas as gpd\n\ndf = pd.read_excel(\"https://www.data.gouv.fr/fr/datasets/r/d4b6b072-8a7d-4e04-a029-8cdbdbaf36a5\", header = [0,1])\n\nindex_0 = [\"\" if df.columns[i][0].startswith(\"Unnamed\") else df.columns[i][0] for i in range(len(df.columns))]\nindex_1 = [df.columns[i][1] for i in range(len(df.columns))]\nkeep_index = [True if el in ('', \"B\") else False for el in index_0] \n\ncols = [index_0[i] + \" \" + index_1[i].replace(\"+\", \"_\") for i in range(len(df.columns))]\ndf.columns = cols\ndf = df.loc[:, keep_index]\ndf.columns = df.columns.str.replace(\"(^ |°)\", \"\", regex = True).str.replace(\" \", \"_\")\ndf = df.dropna(subset = ['B_NB'])\ndf = df.loc[~df[\"B_NB\"].astype(str).str.contains(\"(\\%|\\.)\"),:]\n\ndf['B_NB'] = df['B_NB'].astype(int)\ndf['B_TR'] = df['B_TR'].str.replace(\",\", \".\").str.replace(\"%\",\"\").astype(float)\n\ndf = df.loc[df[\"B_NB\"]&gt;20]\n\n/tmp/ipykernel_3229/1059845781.py:16: UserWarning:\n\nThis pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n\n\n\nSur cet échantillon, le taux de réussite moyen était, en 2018, de 58.02%\nNos informations géographiques prennent la forme suivante:\n\ndf.loc[:,['Adresse','CP','Ville']].head(5)\n\n\n\n\n\n\n\n\nAdresse\nCP\nVille\n\n\n\n\n0\n56 RUE CHARLES ROBIN\n01000\nBOURG EN BRESSE\n\n\n2\n7, avenue Revermont\n01250\nCeyzeriat\n\n\n3\n72 PLACE DE LA MAIRIE\n01000\nSAINT-DENIS LES BOURG\n\n\n4\n6 RUE DU LYCEE\n01000\nBOURG EN BRESSE\n\n\n5\n9 place Edgard Quinet\n01000\nBOURG EN BRESSE\n\n\n\n\n\n\n\nAutrement dit, nous disposons d’une adresse, d’un code postal et d’un nom\nde ville. Ces informations peuvent servir à faire une recherche\nsur la localisation d’une auto-école.\n\n14.4.1 Utiliser l’API BAN\nLa documentation officielle de l’API\npropose un certain nombre d’exemples de manière de géolocaliser des données.\nDans notre situation, deux points d’entrée paraissent intéressants:\n\nL’API /search/ qui représente un point d’entrée avec des URL de la forme\nhttps://api-adresse.data.gouv.fr/search/?q=&lt;adresse&gt;&postcode=&lt;codepostal&gt;&limit=1\nL’API /search/csv qui prend un CSV en entrée et retourne ce même CSV avec\nles observations géocodées. La requête prend la forme suivante, en apparence\nmoins simple à mettre en oeuvre :\ncurl -X POST -F data=@search.csv -F columns=adresse -F columns=postcode https://api-adresse.data.gouv.fr/search/csv/\n\nLa tentation serait forte d’utiliser la première méthode avec une boucle sur les\nlignes de notre DataFrame pour géocoder l’ensemble de notre jeu de données.\nCela serait néanmoins une mauvaise idée car les communications entre notre\nsession Python et les serveurs de l’API seraient beaucoup trop nombreuses\npour offrir des performances satisfaisantes.\nPour vous en convaincre, vous pouvez exécuter le code suivant sur un petit\néchantillon de données (par exemple 100 comme ici) et remarquer que le temps\nd’exécution est assez important\n\nimport time\n\ndfgeoloc = df.loc[:, ['Adresse','CP','Ville']].apply(lambda s: s.str.lower().str.replace(\",\",\" \"))\ndfgeoloc['url'] = (dfgeoloc['Adresse'] + \"+\" + dfgeoloc['Ville'].str.replace(\"-\",'+')).str.replace(\" \",\"+\")\ndfgeoloc['url'] = 'https://api-adresse.data.gouv.fr/search/?q=' + dfgeoloc['url'] + \"&postcode=\" + df['CP'] + \"&limit=1\"\ndfgeoloc = dfgeoloc.dropna()\n\nstart_time = time.time()\n\ndef get_geoloc(i):\n    print(i)\n    return gpd.GeoDataFrame.from_features(requests.get(dfgeoloc['url'].iloc[i]).json()['features'])\n\nlocal = [get_geoloc(i) for i in range(len(dfgeoloc.head(10)))]\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\nComme l’indique la documentation, si on désire industrialiser notre processus\nde géocodage, on va privilégier l’API CSV.\nPour obtenir une requête CURL cohérente avec le format désiré par l’API\non va à nouveau utiliser requests mais cette fois avec des paramètres\nsupplémentaires:\n\ndata va nous permettre de passer des paramètres à CURL (équivalents aux -F\nde la requête CURL):\n\ncolumns: Les colonnes utilisées pour localiser une donnée. En l’occurrence,\non utilise l’adresse et la ville (car les codes postaux n’étant pas uniques,\nun même nom de voirie peut se trouver dans plusieurs villes partageant le même\ncode postal)\npostcode: Le code postal de la ville. Idéalement nous aurions utilisé\nle code Insee mais nous ne l’avons pas dans nos données.\nresult_columns: on restreint les données échangées avec l’API aux\ncolonnes qui nous intéressent. Cela permet d’accélérer les processus (on\néchange moins de données) et de réduire l’impact carbone de notre activité\n(moins de transferts = moins d’énergie dépensée). En l’occurrence, on ne ressort\nque les données géolocalisées et un score de confiance en la géolocalisation.\n\nfiles: permet d’envoyer un fichier via CURL\n\nLes données sont récupérées avec request.post. Comme il s’agit d’une\nchaîne de caractère, nous pouvons directement la lire avec pandas en\nutilisant io.StringIO pour éviter d’écrire des données intermédiaires.\nLe nombre d’échos semblant être limité, je propose de procéder par morceaux\n(ici je découpe mon jeu de données en 5 morceaux).\n\nimport requests\nimport io   \nimport numpy as np\nimport time\n\nparams = {\n    'columns': ['Adresse', 'Ville'],\n    'postcode': 'CP',\n    'result_columns': ['result_score', 'latitude', 'longitude'],\n}\n\ndf[['Adresse','CP','Ville']] = df.loc[:, ['Adresse','CP','Ville']].apply(lambda s: s.str.lower().str.replace(\",\",\" \"))\n\ndef geoloc_chunk(x):\n    dfgeoloc = x.loc[:, ['Adresse','CP','Ville']]\n    dfgeoloc.to_csv(\"datageocodage.csv\", index=False)\n    response = requests.post('https://api-adresse.data.gouv.fr/search/csv/', data=params, files={'data': ('datageocodage.csv', open('datageocodage.csv', 'rb'))})\n    geoloc = pd.read_csv(io.StringIO(response.text), dtype = {'CP': 'str'})\n    return geoloc\n    \nstart_time = time.time()\ngeodata = [geoloc_chunk(dd) for dd in np.array_split(df, 10)]\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\n--- 26.41576337814331 seconds ---\n\n\nCette méthode est beaucoup plus rapide et permet ainsi, une fois retourné à nos\ndonnées initiales, d’avoir un jeu de données géolocalisé\n\ngeodata = pd.concat(geodata, ignore_index = True)\ndf_xy = df.merge(geodata, on = ['Adresse','CP','Ville'])\ndf_xy = df_xy.dropna(subset = ['latitude','longitude'])\ndf_xy['text'] = df_xy['Raison_Sociale'] + '&lt;br&gt;' + df_xy['Adresse'] + '&lt;br&gt;' + df_xy['Ville'] + '&lt;br&gt;Nombre de candidats:' + df_xy['B_NB'].astype(str)\n\ndf_xy.filter(['Raison_Sociale','Adresse','CP','Ville','latitude','longitude'], axis = \"columns\").sample(10)\n\n\n\n\n\n\n\n\nRaison_Sociale\nAdresse\nCP\nVille\nlatitude\nlongitude\n\n\n\n\n1996\nATOUT PERMIS\n5 impasse charles garnier\n26120\nchabeuil\n44.899297\n5.006765\n\n\n8332\nLA POSTE AUTO ECOLE\n100 rue saint denis\n77400\nlagny sur marne\n48.879119\n2.708853\n\n\n7397\nBOLLEE 99\n99 avenue bollée\n72000\nle mans\n48.001416\n0.210633\n\n\n331\nNANCY CONDUITE\n20 rue verdi\n06000\nnice\n43.699650\n7.260008\n\n\n4665\nGILLES\n48 rue du neufbourg\n50000\nsaint-lo\n49.115560\n-1.086690\n\n\n7917\n1 - FORUM CONDUITE\n7 boulevard de la marne\n76000\nrouen\n49.447181\n1.094138\n\n\n9571\nVANESSA\n5 allée du nord\n89120\ncharny\n47.897517\n3.091429\n\n\n7112\nL ETOILE\n42 rue de la poste\n69220\nbelleville sur saone\n46.110332\n4.750423\n\n\n3516\nMONTSPERMIS\n11 rue du val de l'indre\n37260\nmonts\n47.279926\n0.643142\n\n\n4675\nMAUPAS\n26 avenue henri poincarre\n50100\ncherbourg octeville\n49.630611\n-1.608324\n\n\n\n\n\n\n\nIl ne reste plus qu’à utiliser geopandas\net nous serons en mesure de faire une carte des localisations des auto-écoles :\n\nimport geopandas as gpd\ndfgeo = gpd.GeoDataFrame(df_xy, geometry=gpd.points_from_xy(df_xy.longitude, df_xy.latitude))\n\nNous allons représenter les stations dans l’Essonne avec un zoom initialement\nsur les villes de Massy et Palaiseau. Le code est le suivant:\n\nimport folium\n\n# Représenter toutes les autoécoles de l'Essonne\ndf_91 = df_xy.loc[df_xy[\"Dept\"] == \"091\"]\n\n# Centrer la vue initiale sur Massy-Palaiseau\ndf_pal = df_xy.loc[df_xy['Ville'].isin([\"massy\", \"palaiseau\"])]\ncenter = df_pal[['latitude', 'longitude']].mean().values.tolist()\nsw = df_pal[['latitude', 'longitude']].min().values.tolist()\nne = df_pal[['latitude', 'longitude']].max().values.tolist()\n\nm = folium.Map(location = center, tiles='Stamen Toner')\n\n# I can add marker one by one on the map\nfor i in range(0,len(df_91)):\n    folium.Marker([df_91.iloc[i]['latitude'], df_91.iloc[i]['longitude']],\n                  popup=df_91.iloc[i]['text'],\n                  icon=folium.Icon(icon='car', prefix='fa')).add_to(m)\n\nm.fit_bounds([sw, ne])\n\nCe qui permet d’obtenir la carte:\n\n# Afficher la carte\nm\n\nVous pouvez aller plus loin avec l’exercice suivant.\nCelui-ci nécessite une fonction pour créer un cercle autour d’un point (source ici),\nla voici:\n\nfrom functools import partial\nimport pyproj\nfrom shapely.ops import transform\nfrom shapely.geometry import Point\n\nproj_wgs84 = pyproj.Proj('+proj=longlat +datum=WGS84')\n\n\ndef geodesic_point_buffer(lat, lon, km):\n    # Azimuthal equidistant projection\n    aeqd_proj = '+proj=aeqd +lat_0={lat} +lon_0={lon} +x_0=0 +y_0=0'\n    project = partial(\n        pyproj.transform,\n        pyproj.Proj(aeqd_proj.format(lat=lat, lon=lon)),\n        proj_wgs84)\n    buf = Point(0, 0).buffer(km * 1000)  # distance in metres\n    return transform(project, buf).exterior.coords[:]\n\n\n\n Exercice 2 : Quelles sont les auto-écoles les plus proches de chez moi ?\nOn va supposer que vous cherchez, dans un rayon donné autour d’un centre ville,\nles auto-écoles disponibles.\n\nPour commencer, utiliser l’API geo\npour la ville de Palaiseau\nAppliquer la fonction geodesic_point_buffer au centre ville de Palaiseau\nNe conserver que les auto-écoles dans ce cercle et les ordonner\n\nSi vous avez la réponse à la question 3, n’hésitez pas à la soumettre sur Github afin que je complète la correction :wink: !\n\n\n\n\nERROR 1: PROJ: proj_create_from_database: Open of /opt/mamba/share/proj failed\n\n\n\n\n/opt/mamba/lib/python3.9/site-packages/shapely/ops.py:276: FutureWarning:\n\nThis function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n\n\n\nPour se convaincre, de notre cercle constitué lors de\nla question 2, on peut représenter une carte.\nOn a bien un cercle centré autour de Palaiseau:\n\nimport matplotlib.pyplot as plt\nimport contextily as ctx\n\nfig,ax = plt.subplots(figsize=(10, 10))\ncircle.to_crs(\"EPSG:3857\").plot(ax = ax, color = 'red')\npal.to_crs(\"EPSG:3857\").plot(ax = ax, color = 'green')\nctx.add_basemap(ax, source = ctx.providers.Stamen.Toner)\nax\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "content/course/manipulation/04c_API_TP/index.html#exercices-supplémentaires",
    "href": "content/course/manipulation/04c_API_TP/index.html#exercices-supplémentaires",
    "title": "14  Récupérer des données avec des API depuis Python",
    "section": "14.5 Exercices supplémentaires",
    "text": "14.5 Exercices supplémentaires\nPour vous aidez, vous pouvez regarder une exemple de structure du json ici : https://world.openfoodfacts.org/api/v0/product/3274080005003.json en particulier la catégorie nutriments.\n\n\n Exercice 3 : Retrouver des produits dans l'openfood facts :pizza:\nVoici une liste de code-barres:\n3274080005003,  5449000000996, 8002270014901, 3228857000906, 3017620421006, 8712100325953\nUtiliser l’API d’openfoodfacts\n(l’API, pas depuis le CSV !)\npour retrouver les produits correspondant\net leurs caractéristiques nutritionnelles.\nLe panier paraît-il équilibré ? :chocolate_bar:\nRécupérer l’URL d’une des images et l’afficher dans votre navigateur.\n\n\nVoici par exemple la photo du produit ayant le code-barre 5449000000996. Vous le reconnaissez ?\n\n\n\n\n\n\n14.5.1 Exemple avec l’API de la Banque Mondiale\nAvec l’API de la Banque mondiale, voici comme s’écrit une requête :\n\nhttp://api.worldbank.org/v2/countries?incomeLevel=LMC\n\n\nLe point d’entrée est l’URL http://api.worldbank.org/v24\nUn filtre est appliqué sur les pays (countries?) afin de ne conserver\nque celles telles que incomeLevel=LMC (“Lower middle income”)\n\nEn cliquant sur le lien, le site renvoie des données en XML,\nqui ressemblent pas mal à ce qu’on a vu plus tôt avec le scraping : une structure avec des balises qui s’ouvrent et qui se ferment.\nPour obtenir la même information en Python, il faut revenir aux fondamentaux : on va avoir besoin du module requests. Suivant les API, nous avons soit besoin de rien de plus si nous parvenons directement à obtenir un json, soit devoir utiliser un parser comme BeautifulSoup dans le cas contraire.\nAvec l’API de la banque mondiale, on va utiliser le module requests et sa méthode get : on lui donne l’url de l’API qui nous intéresse, on lui demande d’en faire un json et le tour est en apparence joué.\n\nimport requests\nreq = requests.get('http://api.worldbank.org/v2/countries?incomeLevel=LMC')\n\nQuand on regarde de plus près, on voit que les informations suivantes apparaissent :\n\nCode du pays\nNom du pays\nRégion\nClassification en termes de revenus\nLes types de prêt pour ces pays\nLa capitale\nLongitude\nLatitude\n\nLe format XML est fortement balisé, ce qui n’est pas très pratique.\nEn utilisant désormais un autre URL, on obtient un JSON, plus pratique pour travailler :\n\nhttp://api.worldbank.org/v2/countries?incomeLevel=LMC&format=json\n\n\nimport requests\nimport pandas as pd\n\nreq = requests.get('http://api.worldbank.org/v2/countries?incomeLevel=LMC&format=json')\nwb = req.json()\nwb = pd.json_normalize(wb[1])\nwb.head(5)\n\n\n\n\n\n\n\n\nid\niso2Code\nname\ncapitalCity\nlongitude\nlatitude\nregion.id\nregion.iso2code\nregion.value\nadminregion.id\nadminregion.iso2code\nadminregion.value\nincomeLevel.id\nincomeLevel.iso2code\nincomeLevel.value\nlendingType.id\nlendingType.iso2code\nlendingType.value\n\n\n\n\n0\nAGO\nAO\nAngola\nLuanda\n13.242\n-8.81155\nSSF\nZG\nSub-Saharan Africa\nSSA\nZF\nSub-Saharan Africa (excluding high income)\nLMC\nXN\nLower middle income\nIBD\nXF\nIBRD\n\n\n1\nBEN\nBJ\nBenin\nPorto-Novo\n2.6323\n6.4779\nSSF\nZG\nSub-Saharan Africa\nSSA\nZF\nSub-Saharan Africa (excluding high income)\nLMC\nXN\nLower middle income\nIDX\nXI\nIDA\n\n\n2\nBGD\nBD\nBangladesh\nDhaka\n90.4113\n23.7055\nSAS\n8S\nSouth Asia\nSAS\n8S\nSouth Asia\nLMC\nXN\nLower middle income\nIDX\nXI\nIDA\n\n\n3\nBOL\nBO\nBolivia\nLa Paz\n-66.1936\n-13.9908\nLCN\nZJ\nLatin America & Caribbean\nLAC\nXJ\nLatin America & Caribbean (excluding high income)\nLMC\nXN\nLower middle income\nIBD\nXF\nIBRD\n\n\n4\nBTN\nBT\nBhutan\nThimphu\n89.6177\n27.5768\nSAS\n8S\nSouth Asia\nSAS\n8S\nSouth Asia\nLMC\nXN\nLower middle income\nIDX\nXI\nIDA\n\n\n\n\n\n\n\nCependant, si on regarde la dimension de l’objet obtenu, on obtient un\nchiffre rond (50 lignes). Ceci est suspect et un petit tour dans la\ndocumentation de l’API nous apprendrait que c’est le nombre maximal de\nretour possible. Il faut donc faire attention à la documentation et\najouter un paramètre page=2 pour rattraper les derniers échos:\n\nwb2 = pd.json_normalize(\n    requests.get(\"http://api.worldbank.org/v2/countries?incomeLevel=LMC&format=json&page=2\").json()[1]\n    )\npd.concat([wb, wb2]).head(5)\n\n\n\n\n\n\n\n\nid\niso2Code\nname\ncapitalCity\nlongitude\nlatitude\nregion.id\nregion.iso2code\nregion.value\nadminregion.id\nadminregion.iso2code\nadminregion.value\nincomeLevel.id\nincomeLevel.iso2code\nincomeLevel.value\nlendingType.id\nlendingType.iso2code\nlendingType.value\n\n\n\n\n0\nAGO\nAO\nAngola\nLuanda\n13.242\n-8.81155\nSSF\nZG\nSub-Saharan Africa\nSSA\nZF\nSub-Saharan Africa (excluding high income)\nLMC\nXN\nLower middle income\nIBD\nXF\nIBRD\n\n\n1\nBEN\nBJ\nBenin\nPorto-Novo\n2.6323\n6.4779\nSSF\nZG\nSub-Saharan Africa\nSSA\nZF\nSub-Saharan Africa (excluding high income)\nLMC\nXN\nLower middle income\nIDX\nXI\nIDA\n\n\n2\nBGD\nBD\nBangladesh\nDhaka\n90.4113\n23.7055\nSAS\n8S\nSouth Asia\nSAS\n8S\nSouth Asia\nLMC\nXN\nLower middle income\nIDX\nXI\nIDA\n\n\n3\nBOL\nBO\nBolivia\nLa Paz\n-66.1936\n-13.9908\nLCN\nZJ\nLatin America & Caribbean\nLAC\nXJ\nLatin America & Caribbean (excluding high income)\nLMC\nXN\nLower middle income\nIBD\nXF\nIBRD\n\n\n4\nBTN\nBT\nBhutan\nThimphu\n89.6177\n27.5768\nSAS\n8S\nSouth Asia\nSAS\n8S\nSouth Asia\nLMC\nXN\nLower middle income\nIDX\nXI\nIDA\n\n\n\n\n\n\n\nSi on regarde l’information présente dans le DataFrame, on voit qu’elle se\nprésente sous forme lendingType.value. C’est parce que pandas a\nconcaténé les différents niveaux de notre dictionnaire. Si on désire\ns’en assurer, on peut regarder sur un exemple:\n\nreq.json()[1][0]['incomeLevel']['value'] == wb.loc[0, 'incomeLevel.value'] \n\nTrue"
  },
  {
    "objectID": "content/course/manipulation/04b_regex_TP/index.html#introduction",
    "href": "content/course/manipulation/04b_regex_TP/index.html#introduction",
    "title": "15  Maîtriser les expressions régulières",
    "section": "15.1 Introduction",
    "text": "15.1 Introduction\nPython offre énormément de fonctionalités très pratiques pour la manipulation de données\ntextuelles. C’est l’une des raisons de son\nsuccès dans la communauté du traitement automatisé du langage (NLP, voir partie dédiée).\nDans les chapitres précédents, nous avons parfois été amenés à chercher des éléments textuels basiques. Cela était possible avec la méthode str.find du package Pandas qui constitue une version vectorisée de la méthode find\nde base. Nous avons d’ailleurs\npu utiliser cette dernière directement, notamment lorsqu’on a fait du webscraping.\nCependant, cette fonction de recherche\ntrouve rapidement ses limites.\nPar exemple, si on désire trouver à la fois les occurrences d’un terme au singulier\net au pluriel, il sera nécessaire d’utiliser\nau moins deux fois la méthode find.\nPour des verbes conjugués, cela devient encore plus complexe, en particulier si ceux-ci changent de forme selon le sujet.\nPour des expressions compliquées, il est conseillé d’utiliser les expressions régulières,\nou “regex”. C’est une fonctionnalité qu’on retrouve dans beaucoup de langages. C’est une forme de grammaire qui permet de rechercher des expressions.\nUne partie du contenu de cette partie\nest une adaptation de la\ndocumentation collaborative sur R nommée utilitR à laquelle j’ai participé. Ce chapitre reprend aussi du contenu du\nlivre R for Data Science qui présente un chapitre\ntrès pédagogique sur les regex.\nNous allons utiliser le package re pour illustrer nos exemples d’expressions\nrégulières. Il s’agit du package de référence, qui est utilisé, en arrière-plan,\npar Pandas pour vectoriser les recherches textuelles.\n\nimport re\nimport pandas as pd\n\n\n\n Hint\nLes expressions régulières (regex) sont notoirement difficiles à maîtriser. Il existe des outils qui facilitent le travail avec les expressions régulières.\n\nL’outil de référence pour ceci est [https://regex101.com/] qui permet de tester des regex en Python\ntout en ayant une explication qui accompagne ce test\nDe même pour ce site qui comporte une cheat sheet en bas de la page.\nLes jeux de Regex Crossword permettent d’apprendre les expressions régulières en s’amusant\n\nIl peut être pratique de demander à des IA assistantes, comme Github Copilot ou ChatGPT, une\npremière version d’une regex en expliquant le contenu qu’on veut extraire.\nCela peut faire économiser pas mal de temps, sauf quand l’IA fait preuve d’une confiance excessive\net vous propose avec aplomb une regex totalement fausse…"
  },
  {
    "objectID": "content/course/manipulation/04b_regex_TP/index.html#principe",
    "href": "content/course/manipulation/04b_regex_TP/index.html#principe",
    "title": "15  Maîtriser les expressions régulières",
    "section": "15.2 Principe",
    "text": "15.2 Principe\nLes expressions régulières sont un outil permettant de décrire un ensemble de chaînes de caractères possibles selon une syntaxe précise, et donc de définir un motif (ou pattern). Les expressions régulières servent par exemple lorsqu’on veut extraire une partie d’une chaîne de caractères, ou remplacer une partie d’une chaîne de caractères. Une expression régulière prend la forme d’une chaîne de caractères, qui peut contenir à la fois des éléments littéraux et des caractères spéciaux qui ont un sens logique.\nPar exemple, \"ch.+n\" est une expression régulière qui décrit le motif suivant: la chaîne littérale ch, suivi de n’importe quelle chaîne d’au moins un caractère (.+), suivie de la lettre n. Dans la chaîne \"J'ai un chien.\", la sous-chaîne \"chien\" correspond à ce motif. De même pour \"chapeau ron\" dans \"J'ai un chapeau rond\". En revanche, dans la chaîne \"La soupe est chaude.\", aucune sous-chaîne ne correpsond à ce motif (car aucun n n’apparaît après le ch).\nPour s’en convaincre, nous pouvons déjà regarder\nles deux premiers cas:\n\npattern = \"ch.+n\"\nprint(re.search(pattern, \"J'ai un chien.\"))\nprint(re.search(pattern, \"J'ai un chapeau rond.\"))\n\n&lt;re.Match object; span=(8, 13), match='chien'&gt;\n&lt;re.Match object; span=(8, 19), match='chapeau ron'&gt;\n\n\nCependant, dans le dernier cas, nous ne trouvons pas\nle pattern recherché:\n\nprint(re.search(pattern, \"La soupe est chaude.\"))\n\nNone\n\n\nLa regex précédente comportait deux types de caractères:\n\nles caractères littéraux: lettres et nombres qui sont reconnus de manière littérale\nles méta-caractères: symboles qui ont un sens particulier dans les regex.\n\nLes principaux méta-caractères sont ., +, *, [, ], ^ et $ mais il\nen existe beaucoup d’autres.\nParmi cet ensemble, on utilise principalement les quantifieurs (., +, *…),\nles classes de caractères (ensemble qui sont délimités par [ et ])\nou les ancres (^, $…)\nDans l’exemple précédent,\nnous retrouvions deux quantifieurs accolés .+. Le premier (.) signifie n’importe quel caractère1. Le deuxième (+) signifie “répète le pattern précédent”.\nDans notre cas, la combinaison .+ permet ainsi de répéter n’importe quel caractère avant de trouver un n.\nLe nombre de fois est indeterminé: cela peut ne pas être pas nécessaire d’intercaler des caractères avant le n\nou cela peut être nécessaire d’en intercepter plusieurs:\n\nprint(re.search(pattern, \"J'ai un chino\"))\nprint(re.search(pattern, \"J'ai un chiot très mignon.\"))\n\n&lt;re.Match object; span=(8, 12), match='chin'&gt;\n&lt;re.Match object; span=(8, 25), match='chiot très mignon'&gt;\n\n\n\n15.2.1 Classes de caractères\nLors d’une recherche, on s’intéresse aux caractères et souvent aux classes de caractères : on cherche un chiffre, une lettre, un caractère dans un ensemble précis ou un caractère qui n’appartient pas à un ensemble précis. Certains ensembles sont prédéfinis, d’autres doivent être définis à l’aide de crochets.\nPour définir un ensemble de caractères, il faut écrire cet ensemble entre crochets. Par exemple, [0123456789] désigne un chiffre. Comme c’est une séquence de caractères consécutifs, on peut résumer cette écriture en [0-9].\nPar\nexemple, si on désire trouver tous les pattern qui commencent par un c suivi\nd’un h puis d’une voyelle (a, e, i, o, u), on peut essayer\ncette expression régulière.\n\nre.findall(\"[c][h][aeiou]\", \"chat, chien, veau, vache, chèvre\")\n\n['cha', 'chi', 'che']\n\n\nIl serait plus pratique d’utiliser Pandas dans ce cas pour isoler les\nlignes qui répondent à la condition logique (en ajoutant les accents\nqui ne sont pas compris sinon):\n\nimport pandas as pd\ntxt = pd.Series(\"chat, chien, veau, vache, chèvre\".split(\", \"))\ntxt.str.match(\"ch[aeéèiou]\")\n\n0     True\n1     True\n2    False\n3    False\n4     True\ndtype: bool\n\n\nCependant, l’usage ci-dessus des classes de caractères\nn’est pas le plus fréquent.\nOn privilégie celles-ci pour identifier des\npattern complexe plutôt qu’une suite de caractères littéraux.\nLes tableaux d’aide mémoire illustrent une partie des\nclasses de caractères les plus fréquentes\n([:digit:] ou \\d…)\n\n\n15.2.2 Quantifieurs\nNous avons rencontré les quantifieurs avec notre première expression\nrégulière. Ceux-ci contrôlent le nombre de fois\nqu’un pattern est rencontré.\nLes plus fréquents sont:\n\n? : 0 ou 1 match ;\n+ : 1 ou plus de matches ;\n* : 0 or more matches.\n\nPar exemple, colou?r permettra de matcher à la fois l’écriture américaine et anglaise\n\nre.findall(\"colou?r\", \"Did you write color or colour?\")\n\n['color', 'colour']\n\n\nCes quantifiers peuvent bien-sûr être associés à\nd’autres types de caractères, notamment les classes de caractères.\nCela peut être extrèmement pratique.\nPar exemple, \\d+ permettra de capturer un ou plusieurs chiffres, \\s?\npermettra d’ajouter en option un espace,\n[\\w]{6,8} un mot entre six et huit lettres qu’on écrira…\nIl est aussi possible de définir le nombre de répétitions\navec {}:\n\n{n} matche exactement n fois ;\n{n,} matche au moins n fois ;\n{n,m} matche entre n et m fois.\n\nCependant, la répétition des termes\nne s’applique par défaut qu’au dernier\ncaractère précédent le quantifier.\nOn peut s’en convaincre avec l’exemple ci-dessus:\n\nprint(re.match(\"toc{4}\",\"toctoctoctoc\"))\n\nNone\n\n\nPour pallier ce problème, il existe les parenthèses.\nLe principe est le même qu’avec les règles numériques:\nles parenthèses permettent d’introduire une hiérarchie.\nPour reprendre l’exemple précédent, on obtient\nbien le résultat attendu grâce aux parenthèses:\n\nprint(re.match(\"(toc){4}\",\"toctoctoctoc\"))\nprint(re.match(\"(toc){5}\",\"toctoctoctoc\"))\nprint(re.match(\"(toc){2,4}\",\"toctoctoctoc\"))\n\n&lt;re.Match object; span=(0, 12), match='toctoctoctoc'&gt;\nNone\n&lt;re.Match object; span=(0, 12), match='toctoctoctoc'&gt;\n\n\n\n\n Note\nL’algorithme des expressions régulières essaye toujours de faire correspondre le plus grand morceau à l’expression régulière.\nPar exemple, soit une chaine de caractère HTML:\n\ns = \"&lt;h1&gt;Super titre HTML&lt;/h1&gt;\"\n\nL’expression régulière re.findall(\"&lt;.*&gt;\", s) correspond, potentiellement,\nà trois morceaux :\n\n&lt;h1&gt;\n&lt;/h1&gt;\n&lt;h1&gt;Super titre HTML&lt;/h1&gt;\n\nC’est ce dernier qui sera choisi, car le plus grand. Pour\nsélectionner le plus petit,\nil faudra écrire les multiplicateurs comme ceci : *?, +?.\nEn voici quelques exemples:\n\ns = \"&lt;h1&gt;Super titre HTML&lt;/h1&gt;\\n&lt;p&gt;&lt;code&gt;Python&lt;/code&gt; est un langage très flexible&lt;/p&gt;\"\nprint(re.findall(\"&lt;.*&gt;\", s))\nprint(re.findall(\"&lt;p&gt;.*&lt;/p&gt;\", s))\nprint(re.findall(\"&lt;p&gt;.*?&lt;/p&gt;\", s))\nprint(re.compile(\"&lt;.*?&gt;\").findall(s))\n\n['&lt;h1&gt;Super titre HTML&lt;/h1&gt;', '&lt;p&gt;&lt;code&gt;Python&lt;/code&gt; est un langage très flexible&lt;/p&gt;']\n['&lt;p&gt;&lt;code&gt;Python&lt;/code&gt; est un langage très flexible&lt;/p&gt;']\n['&lt;p&gt;&lt;code&gt;Python&lt;/code&gt; est un langage très flexible&lt;/p&gt;']\n['&lt;h1&gt;', '&lt;/h1&gt;', '&lt;p&gt;', '&lt;code&gt;', '&lt;/code&gt;', '&lt;/p&gt;']\n\n\n\n\n\n\n15.2.3 Aide-mémoire\nLe tableau ci-dessous peut servir d’aide-mémoire\nsur les regex:\n\n\n\n\n\n\n\nExpression régulière\nSignification\n\n\n\n\n\"^\"\nDébut de la chaîne de caractères\n\n\n\"$\"\nFin de la chaîne de caractères\n\n\n\"\\\\.\"\nUn point\n\n\n\".\"\nN’importe quel caractère\n\n\n\".+\"\nN’importe quelle suite de caractères non vide\n\n\n\".*\"\nN’importe quelle suite de caractères, éventuellement vi\n\n\n\"[:alnum:]\"\nUn caractère alphanumérique\n\n\n\"[:alpha:]\"\nUne lettre\n\n\n\"[:digit:]\"\nUn chiffre\n\n\n\"[:lower:]\"\nUne lettre minuscule\n\n\n\"[:punct:]\"\nUn signe de ponctuation\n\n\n\"[:space:]\"\nun espace\n\n\n\"[:upper:]\"\nUne lettre majuscule\n\n\n\"[[:alnum:]]+\"\nUne suite d’au moins un caractère alphanumérique\n\n\n\"[[:alpha:]]+\"\nUne suite d’au moins une lettre\n\n\n\"[[:digit:]]+\"\nUne suite d’au moins un chiffre\n\n\n\"[[:lower:]]+\"\nUne suite d’au moins une lettre minuscule\n\n\n\"[[:punct:]]+\"\nUne suite d’au moins un signe de ponctuation\n\n\n\"[[:space:]]+\"\nUne suite d’au moins un espace\n\n\n\"[[:upper:]]+\"\nUne suite d’au moins une lettre majuscule\n\n\n\"[[:alnum:]]*\"\nUne suite de caractères alphanumériques, éventuellement vide\n\n\n\"[[:alpha:]]*\"\nUne suite de lettres, éventuellement vide\n\n\n\"[[:digit:]]*\"\nUne suite de chiffres, éventuellement vide\n\n\n\"[[:lower:]]*\"\nUne suite de lettres minuscules, éventuellement vide\n\n\n\"[[:upper:]]*\"\nUne suite de lettres majuscules, éventuellement vide\n\n\n\"[[:punct:]]*\"\nUne suite de signes de ponctuation, éventuellement vide\n\n\n\"[^[:alpha:]]+\"\nUne suite d’au moins un caractère autre qu’une lettre\n\n\n\"[^[:digit:]]+\"\nUne suite d’au moins un caractère autre qu’un chiffre\n\n\n\"\\|\"\nL’une des expressions x ou y est présente\n\n\n[abyz]\nUn seul des caractères spécifiés\n\n\n[abyz]+\nUn ou plusieurs des caractères spécifiés (éventuellement répétés)\n\n\n[^abyz]\nAucun des caractères spécifiés n’est présent\n\n\n\nCertaines classes de caractères bénéficient d’une syntaxe plus légère car\nelles sont très fréquentes. Parmi-celles:\n\n\n\n\n\n\n\nExpression régulière\nSignification\n\n\n\n\n\\d\nN’importe quel chiffre\n\n\n\\D\nN’importe quel caractère qui n’est pas un caractère\n\n\n\\s\nN’importe quel espace (espace, tabulation, retour à la ligne)\n\n\n\\S\nN’importe quel caractère qui n’est pas un espace\n\n\n\\w\nN’importe quel type de mot (lettres et nombres)\n\n\n\\W\nN’importe quel ensemble qui n’est pas un mot (lettres et nombres)\n\n\n\nDans l’exercice suivant, vous allez pouvoir mettre en pratique\nles exemples précédents sur une regex un peu plus complète.\nCet exercice ne nécessite pas la connaissance des subtilités\ndu package re, vous n’aurez besoin que de re.findall.\nCet exercice utilisera la chaine de caractère suivante:\n\ns = \"\"\"date 0 : 14/9/2000\ndate 1 : 20/04/1971     date 2 : 14/09/1913     date 3 : 2/3/1978\ndate 4 : 1/7/1986     date 5 : 7/3/47     date 6 : 15/10/1914\ndate 7 : 08/03/1941     date 8 : 8/1/1980     date 9 : 30/6/1976\"\"\"\ns\n\n'date 0 : 14/9/2000\\ndate 1 : 20/04/1971     date 2 : 14/09/1913     date 3 : 2/3/1978\\ndate 4 : 1/7/1986     date 5 : 7/3/47     date 6 : 15/10/1914\\ndate 7 : 08/03/1941     date 8 : 8/1/1980     date 9 : 30/6/1976'\n\n\n\n\n Exercice 1\n\nOn va d’abord s’occuper d’extraire le jour de naissance.\n\nLe premier chiffre du jour est 0, 1, 2 ou 3. Traduire cela sous la forme d’une séquence [X-X]\nLe deuxième chiffre du jour est lui entre 0 et 9. Traduire cela sous la séquence adéquate\nRemarquez que le premier jour est facultatif. Intercaler entre les deux classes de caractère adéquate\nle quantifieur qui convient\nAjouter le slash à la suite du motif\nTester avec re.findall. Vous devriez obtenir beaucoup plus d’échos que nécessaire.\nC’est normal, à ce stade la\nregex n’est pas encore finalisée\n\nSuivre la même logique pour les mois en notant que les mois du calendrier grégorien ne dépassent\njamais la première dizaine. Tester avec re.findall\nDe même pour les années de naissance en notant que jusqu’à preuve du contraire, pour des personnes vivantes\naujourd’hui, les millénaires concernés sont restreints. Tester avec re.findall\nCette regex n’est pas naturelle, on pourrait très bien se satisfaire de classes de\ncaractères génériques \\d même si elles pourraient, en pratique, nous sélectionner des\ndates de naissance non possibles (43/78/4528 par exemple). Cela permettrait\nd’alléger la regex afin de la rendre plus intelligible. Ne pas oublier l’utilité des quantifieurs.\nComment adapter la regex pour qu’elle soit toujours valide pour nos cas mais permette aussi de\ncapturer les dates de type YYYY/MM/DD ? Tester sur 1998/07/12\n\n\n\nA l’issue de la question 1, vous devriez avoir ce résultat :\n\n\n['14/',\n '9/',\n '20/',\n '04/',\n '14/',\n '09/',\n '2/',\n '3/',\n '1/',\n '7/',\n '7/',\n '3/',\n '15/',\n '10/',\n '08/',\n '03/',\n '8/',\n '1/',\n '30/',\n '6/']\n\n\nA l’issue de la question 2, vous devriez avoir ce résultat, qui\ncommence à prendre forme:\n\n\n['14/9',\n '20/04',\n '14/09',\n '2/3',\n '1/7',\n '7/3',\n '15/10',\n '08/03',\n '8/1',\n '30/6']\n\n\nA l’issue de la question 3, on parvient bien\nà extraire les dates :\n\n\n['14/9/2000',\n '20/04/1971',\n '14/09/1913',\n '2/3/1978',\n '1/7/1986',\n '7/3/47',\n '15/10/1914',\n '08/03/1941',\n '8/1/1980',\n '30/6/1976']\n\n\nSi tout va bien, à la question 5, votre regex devrait\nfonctionner:\n\n\n['14/9/2000',\n '20/04/1971',\n '14/09/1913',\n '2/3/1978',\n '1/7/1986',\n '7/3/47',\n '15/10/1914',\n '08/03/1941',\n '8/1/1980',\n '30/6/1976',\n '1998/07/12']"
  },
  {
    "objectID": "content/course/manipulation/04b_regex_TP/index.html#principales-fonctions-de-re",
    "href": "content/course/manipulation/04b_regex_TP/index.html#principales-fonctions-de-re",
    "title": "15  Maîtriser les expressions régulières",
    "section": "15.3 Principales fonctions de re",
    "text": "15.3 Principales fonctions de re\nVoici un tableau récapitulatif des principales\nfonctions du package re suivi d’exemples.\nNous avons principalement\nutilisé jusqu’à présent re.findall qui est\nl’une des fonctions les plus pratiques du package.\nre.sub et re.search sont également bien pratiques.\nLes autres sont moins vitales mais peuvent dans des\ncas précis être utiles.\n\n\n\n\n\n\n\nFonction\nObjectif\n\n\n\n\nre.match(&lt;regex&gt;, s)\nTrouver et renvoyer le premier match de l’expression régulière &lt;regex&gt; à partir du début du string s\n\n\nre.search(&lt;regex&gt;, s)\nTrouver et renvoyer le premier match de l’expression régulière &lt;regex&gt; quelle que soit sa position dans le string s\n\n\nre.finditer(&lt;regex&gt;, s)\nTrouver et renvoyer un itérateur stockant tous les matches de l’expression régulière &lt;regex&gt; quelle que soit leur(s) position(s) dans le string s. En général, on effectue ensuite une boucle sur cet itérateur\n\n\nre.findall(&lt;regex&gt;, s)\nTrouver et renvoyer tous les matches de l’expression régulière &lt;regex&gt; quelle que soit leur(s) position(s) dans le string s sous forme de liste\n\n\nre.sub(&lt;regex&gt;, new_text, s)\nTrouver et remplacer tous les matches de l’expression régulière &lt;regex&gt; quelle que soit leur(s) position(s) dans le string s\n\n\n\nPour illustrer ces fonctions, voici quelques exemples:\n\nExemple de re.match 👇\nre.match ne peut servir qu’à capturer un pattern en début\nde string. Son utilité est donc limitée.\nCapturons néanmoins toto :\n\nre.match(\"(to){2}\", \"toto à la plage\")\n\n&lt;re.Match object; span=(0, 4), match='toto'&gt;\n\n\n\n\n\nExemple de re.search 👇\nre.search est plus puissant que re.match, on peut\ncapturer des termes quelle que soit leur position\ndans un string. Par exemple, pour capturer age:\n\nre.search(\"age\", \"toto a l'age d'aller à la plage\")\n\n&lt;re.Match object; span=(9, 12), match='age'&gt;\n\n\nEt pour capturer exclusivement “age” en fin\nde string:\n\nre.search(\"age$\", \"toto a l'age d'aller à la plage\")\n\n&lt;re.Match object; span=(28, 31), match='age'&gt;\n\n\n\n\n\nExemple de re.finditer 👇\nre.finditer est, à mon avis,\nmoins pratique que re.findall. Son utilité\nprincipale par rapport à re.findall\nest de capturer la position dans un champ textuel:\n\ns = \"toto a l'age d'aller à la plage\"\nfor match in re.finditer(\"age\", s):\n    start = match.start()\n    end = match.end()\n    print(f'String match \"{s[start:end]}\" at {start}:{end}')\n\nString match \"age\" at 9:12\nString match \"age\" at 28:31\n\n\n\n\n\nExemple de re.sub 👇\nre.sub permet de capturer et remplacer des expressions.\nPar exemple, remplaçons “age” par “âge”. Mais attention,\nil ne faut pas le faire lorsque le motif est présent dans “plage”.\nOn va donc mettre une condition négative: capturer “age” seulement\ns’il n’est pas en fin de string (ce qui se traduit en regex par ?!$)\n\nre.sub(\"age(?!$)\", \"âge\", \"toto a l'age d'aller à la plage\")\n\n\"toto a l'âge d'aller à la plage\"\n\n\n\n\n\n\n Quand utiliser re.compile et les raw strings ?\nre.compile peut être intéressant lorsque\nvous utilisez une expression régulière plusieurs fois dans votre code.\nCela permet de compiler l’expression régulière en un objet reconnu par re,\nce qui peut être plus efficace en termes de performance lorsque l’expression régulière\nest utilisée à plusieurs reprises ou sur des données volumineuses.\nLes chaînes brutes (raw string) sont des chaînes de caractères spéciales en Python,\nqui commencent par r. Par exemple r\"toto à la plage\".\nElles peuvent être intéressantes\npour éviter que les caractères d’échappement ne soient interprétés par Python\nPar exemple, si vous voulez chercher une chaîne qui contient une barre oblique inverse \\ dans une chaîne, vous devez utiliser une chaîne brute pour éviter que la barre oblique inverse ne soit interprétée comme un caractère d’échappement (\\t, \\n, etc.).\nLe testeur https://regex101.com/ suppose d’ailleurs que\nvous utilisez des raw string, cela peut donc être utile de s’habituer à les utiliser."
  },
  {
    "objectID": "content/course/manipulation/04b_regex_TP/index.html#généralisation-avec-pandas",
    "href": "content/course/manipulation/04b_regex_TP/index.html#généralisation-avec-pandas",
    "title": "15  Maîtriser les expressions régulières",
    "section": "15.4 Généralisation avec Pandas",
    "text": "15.4 Généralisation avec Pandas\nLes méthodes de Pandas sont des extensions de celles de re\nqui évitent de faire une boucle pour regarder,\nligne à ligne, une regex. En pratique, lorsqu’on traite des\nDataFrames, on utilise plutôt l’API Pandas que re. Les\ncodes de la forme df.apply(lambda x: re.&lt;fonction&gt;(&lt;regex&gt;,x), axis = 1)\nsont à bannir car très peu efficaces.\nLes noms changent parfois légèrement par rapport à leur\néquivalent re.\n\n\n\n\n\n\n\nMéthode\nDescription\n\n\n\n\nstr.count()\nCompter le nombre d’occurrences du pattern dans chaque ligne\n\n\nstr.replace()\nRemplacer le pattern par une autre valeur. Version vectorisée de re.sub()\n\n\nstr.contains()\nTester si le pattern apparaît, ligne à ligne. Version vectorisée de re.search()\n\n\nstr.extract()\nExtraire les groupes qui répondent à un pattern et les renvoyer dans une colonne\n\n\nstr.findall()\nTrouver et renvoyer toutes les occurrences d’un pattern. Si une ligne comporte plusieurs échos, une liste est renvoyée. Version vectorisée de re.findall()\n\n\n\nA ces fonctions, s’ajoutent les méthodes str.split() et str.rsplit() qui sont bien pratiques.\n\nExemple de str.count 👇\nOn peut compter le nombre de fois qu’un pattern apparaît avec\nstr.count\n\ndf = pd.DataFrame({\"a\": [\"toto\", \"titi\"]})\ndf['a'].str.count(\"to\")\n\n0    2\n1    0\nName: a, dtype: int64\n\n\n\n\n\nExemple de str.replace 👇\nRemplaçons le motif “ti” en fin de phrase\n\ndf = pd.DataFrame({\"a\": [\"toto\", \"titi\"]})\ndf['a'].str.replace(\"ti$\", \" punch\")\n\n0    toto\n1    titi\nName: a, dtype: object\n\n\n\n\n\nExemple de str.contains 👇\nVérifions les cas où notre ligne termine par “ti”:\n\ndf = pd.DataFrame({\"a\": [\"toto\", \"titi\"]})\ndf['a'].str.contains(\"ti$\")\n\n0    False\n1     True\nName: a, dtype: bool\n\n\n\n\n\nExemple de str.findall 👇\n\ndf = pd.DataFrame({\"a\": [\"toto\", \"titi\"]})\ndf['a'].str.findall(\"to\")\n\n0    [to, to]\n1          []\nName: a, dtype: object\n\n\n\n\n\n\n Warning\nA l’heure actuelle, il n’est pas nécessaire d’ajouter l’argument regex = True mais cela\ndevrait être le cas dans une future version de Pandas.\nCela peut valoir le coup de s’habituer à l’ajouter."
  },
  {
    "objectID": "content/course/manipulation/04b_regex_TP/index.html#pour-en-savoir-plus",
    "href": "content/course/manipulation/04b_regex_TP/index.html#pour-en-savoir-plus",
    "title": "15  Maîtriser les expressions régulières",
    "section": "15.5 Pour en savoir plus",
    "text": "15.5 Pour en savoir plus\n\ndocumentation collaborative sur R nommée utilitR\nR for Data Science\nRegular Expression HOWTO dans la documentation officielle de Python\nL’outil de référence [https://regex101.com/] pour tester des expressions régulières\nCe site qui comporte une cheat sheet en bas de la page.\nLes jeux de Regex Crossword permettent d’apprendre les expressions régulières en s’amusant"
  },
  {
    "objectID": "content/course/manipulation/04b_regex_TP/index.html#exercices-supplémentaires",
    "href": "content/course/manipulation/04b_regex_TP/index.html#exercices-supplémentaires",
    "title": "15  Maîtriser les expressions régulières",
    "section": "15.6 Exercices supplémentaires",
    "text": "15.6 Exercices supplémentaires\n\n15.6.1 Extraction d’adresses email\nIl s’agit d’un usage classique des regex\n\ntext_emails = 'Hello from toto@gmail.com to titi.grominet@yahoo.com about the meeting @2PM'\n\n\n\n Exercice : extraction d'adresses email\nUtiliser la structure d’une adresse mail [XXXX]@[XXXX] pour récupérer\nce contenu\n\n\n\n\n['toto@gmail.com', 'titi.grominet@yahoo.com']\n\n\n\n\n15.6.2 Extraire des années depuis un DataFrame Pandas\nL’objectif général de l’exercice est de nettoyer des colonnes d’un DataFrame en utilisant des expressions régulières.\n\n\n Exercice\nLa base en question contient des livres de la British Library et quelques informations les concernant. Le jeu de données est disponible ici : https://raw.githubusercontent.com/realpython/python-data-cleaning/master/Datasets/BL-Flickr-Images-Book.csv\nLa colonne “Date de Publication” n’est pas toujours une année, il y a parfois d’autres informations. Le but de l’exercice est d’avoir une date de publication du livre propre et de regarder la distribution des années de publications.\nPour ce faire, vous pouvez :\n\nSoit choisir de réaliser l’exercice sans aide. Votre lecture de l’énoncé s’arrête donc ici. Vous devez alors faire attention à bien regarder vous-même la base de données et la transformer avec attention.\nSoit suivre les différentes étapes qui suivent pas à pas.\n\nVersion guidée 👇\n\nLire les données depuis l’url https://raw.githubusercontent.com/realpython/python-data-cleaning/master/Datasets/BL-Flickr-Images-Book.csv. Attention au séparateur\nNe garder que les colonnes ['Identifier', 'Place of Publication', 'Date of Publication', 'Publisher', 'Title', 'Author']\nObserver la colonne ‘Date of Publication’ et remarquer le problème sur certaines lignes (par exemple la ligne 13)\nCommencez par regarder le nombre d’informations manquantes. On ne pourra pas avoir mieux après la regex, et normalement on ne devrait pas avoir moins…\nDéterminer la forme de la regex pour une date de publication. A priori, il y a 4 chiffres qui forment une année.\nUtiliser la méthode str.extract() avec l’argument expand = False (pour ne conserver que la première date concordant avec notre pattern)?\nOn a 2 NaN qui n’étaient pas présents au début de l’exercice. Quels sont-ils et pourquoi ?\nQuelle est la répartition des dates de publications dans le jeu de données ? Vous pouvez par exemple afficher un histogramme grâce à la méthode plot avec l’argument kind =\"hist\".\n\n\n\n\nVoici par exemple le problème qu’on demande de détecter à la question 3 :\n\n\n\n\n\n\n\n\n\nDate of Publication\nTitle\n\n\n\n\n13\n1839, 38-54\nDe Aardbol. Magazijn van hedendaagsche land- e...\n\n\n14\n1897\nCronache Savonesi dal 1500 al 1570 ... Accresc...\n\n\n15\n1865\nSee-Saw; a novel ... Edited [or rather, writte...\n\n\n16\n1860-63\nGéodésie d'une partie de la Haute Éthiopie,...\n\n\n17\n1873\n[With eleven maps.]\n\n\n18\n1866\n[Historia geográfica, civil y politica de la ...\n\n\n19\n1899\nThe Crisis of the Revolution, being the story ...\n\n\n\n\n\n\n\n\n\n181\n\n\nGrâce à notre regex (question 5), on obtient ainsi un DataFrame plus conforme à nos attentes\n\n\n\n\n\n\n\n\n\nDate of Publication\nyear\n\n\n\n\n0\n1879 [1878]\n1879\n\n\n7\nNaN\nNaN\n\n\n13\n1839, 38-54\n1839\n\n\n16\n1860-63\n1860\n\n\n23\n1847, 48 [1846-48]\n1847\n\n\n...\n...\n...\n\n\n8278\n1883, [1884]\n1883\n\n\n8279\n1898-1912\n1898\n\n\n8283\n1831, 32\n1831\n\n\n8284\n[1806]-22\n1806\n\n\n8286\n1834-43\n1834\n\n\n\n\n1759 rows × 2 columns\n\n\n\nQuant aux nouveaux NaN,\nil s’agit de lignes qui ne contenaient pas de chaînes de caractères qui ressemblaient à des années:\n\n\n\n\n\n\n\n\n\nDate of Publication\nyear\n\n\n\n\n1081\n112. G. & W. B. Whittaker\nNaN\n\n\n7391\n17 vols. University Press\nNaN\n\n\n\n\n\n\n\nEnfin, on obtient l’histogramme suivant des dates de publications:\n\n\n&lt;Axes: ylabel='Frequency'&gt;"
  },
  {
    "objectID": "content/course/manipulation/07_dask/index.html#pourquoi-utiliser-dask",
    "href": "content/course/manipulation/07_dask/index.html#pourquoi-utiliser-dask",
    "title": "16  Introduction à dask grâce aux données DVF",
    "section": "16.1 Pourquoi utiliser Dask ?",
    "text": "16.1 Pourquoi utiliser Dask ?\nOn peut se référer à la page https://docs.dask.org/en/stable/why.html\nPlusieurs points sont mis en avant dans la documentation officielle et sont résumés ci-dessous:\n- Dask ressemble fortement en termes de syntaxe à pandas et numpy ;\n- Dask peut être utilisé sur un ordinateur seul ou sur un cloud cluster. Avec Dask, on peut traiter des bases de 100GB sur un ordinateur portable, voire même 1TB sans même avoir besoin d’un cluster big data ;\n- Dask requiert peu de temps d’installation puisqu’il peut être installé avec le gestionnaire de packages conda (il est même livré dans la distribution par défaut d’Anaconda)\n\n16.1.1 Comment Dask se compare à Spark ?\nDans le monde du big-data, un écosystème concurrent existe: Spark. Globalement, lorsqu’on a compris la logique\nde l’un, il est très facile de faire la transition vers l’autre si besoin1. Pour ma part, j’ai principalement fait du Spark sur\ndes données de téléphonie de plusieurs TB. En fait, la logique sera la même que celle de Dask sur données moins volumineuses.\n\nSpark est écrit en Scala à l’origine. Le package pyspark permet d’écrire en Python et s’assure de la traduction en Python afin d’interagir avec les machines virtuelles Java (JVM) nécessaires pour la parallélisation des opérations Spark. Dask est quant à lui écrit en Python, ce qui est un écosystème plus léger. Pour gagner en performance, il permet d’interagir avec du code C/C++ entre autres ;\nL’installation de Spark est plus lourde que celle de Dask\nSpark est un projet Apache en lui-même alors que Dask intervient comme une composante de l’univers Python;\nSpark est un peu plus vieux (2010 versus 2014 pour Dask) ;\nSpark permet de très bien faire des opérations classiques SQL et des ETLs, et proposer ses propres librairies de parallélisation de modèles de machine learning. Pour faire du machine learning avec Spark il faut aller piocher dans Spark MLLib. Dask permet quant à lui de bien interagir avec scikit-learn et de faire de la modélisation.\n\nGlobalement, il faut retenir que Dask comme Spark ne sont intéressants que pour des données dont le traitement engendre des problèmes de RAM. Autrement, il\nvaut mieux se contenter de pandas."
  },
  {
    "objectID": "content/course/manipulation/07_dask/index.html#démonstration-de-quelques-features-de-dask",
    "href": "content/course/manipulation/07_dask/index.html#démonstration-de-quelques-features-de-dask",
    "title": "16  Introduction à dask grâce aux données DVF",
    "section": "16.2 Démonstration de quelques features de Dask",
    "text": "16.2 Démonstration de quelques features de Dask\n\n16.2.1 Présentation du Dask.DataFrame\nNous allons utiliser les données immobilières DVF pour montrer quelques éléments clefs de Dask.\n\n# Import dvf files \nimport pandas as pd\nimport dask.dataframe as dd\n\nd_urls = {\n    \"2019\" : 'https://www.data.gouv.fr/fr/datasets/r/3004168d-bec4-44d9-a781-ef16f41856a2',\n    \"2020\" : \"https://www.data.gouv.fr/fr/datasets/r/90a98de0-f562-4328-aa16-fe0dd1dca60f\",\n    \"2021\": \"https://www.data.gouv.fr/fr/datasets/r/817204ac-2202-4b4a-98e7-4184d154d98c\"\n}\n\n\ndef import_dvf_one_year(year, dict_url = d_urls):\n    df = pd.read_csv(dict_url[year], sep = \"|\", decimal=\",\")\n    df[\"year\"] = year\n    return df\n\ndef import_dvf_all_years(dict_url = d_urls):\n    dfs = [import_dvf_one_year(y, dict_url) for y in dict_url.keys()]\n    df = pd.concat(dfs).reset_index()\n    df = df.drop([\"level_0\", \"level_1\"], axis=1)\n    return df\n\nDans un premier temps, on va utiliser pandas pour\nimporter une année de données (millésime 2019), ces dernières tenant en mémoire\nsur un ordinateur normalement doté en RAM2:\n\ndvf = import_dvf_one_year(\"2019\")\ndvf.shape\ndvf.head()\n\n/tmp/ipykernel_3317/455432909.py:13: DtypeWarning:\n\nColumns (18,23,24,26,28,41) have mixed types. Specify dtype option on import or set low_memory=False.\n\n\n\n\n\n\n\n\n\n\nIdentifiant de document\nReference document\n1 Articles CGI\n2 Articles CGI\n3 Articles CGI\n4 Articles CGI\n5 Articles CGI\nNo disposition\nDate mutation\nNature mutation\n...\nNombre de lots\nCode type local\nType local\nIdentifiant local\nSurface reelle bati\nNombre pieces principales\nNature culture\nNature culture speciale\nSurface terrain\nyear\n\n\n\n\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n04/01/2019\nVente\n...\n1\n2.0\nAppartement\nNaN\n20.0\n1.0\nNaN\nNaN\nNaN\n2019\n\n\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n04/01/2019\nVente\n...\n2\n2.0\nAppartement\nNaN\n62.0\n3.0\nNaN\nNaN\nNaN\n2019\n\n\n2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n04/01/2019\nVente\n...\n1\n3.0\nDépendance\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\n2019\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n08/01/2019\nVente\n...\n0\n1.0\nMaison\nNaN\n90.0\n4.0\nS\nNaN\n940.0\n2019\n\n\n4\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n07/01/2019\nVente\n...\n0\n1.0\nMaison\nNaN\n101.0\n5.0\nS\nNaN\n490.0\n2019\n\n\n\n\n5 rows × 44 columns\n\n\n\nIci on travaille sur un DataFrame d’environ 3.5 millions de lignes et 44 variables.\nL’objet dvf est un pandas.DataFrame\nqui tient en mémoire sur le SSP-Cloud ou sur les serveurs utilisés\npour construire ce site web.\n\n\n Exercice 1\nOn aurait pu lire directement les csv dans un dask.DataFrame avec le read_csv de dask. Comme exercice, vous pouvez essayer de le faire\npour une année (analogue de la fonction import_dvf_one_year) puis sur toutes les données (analogue de la fonction import_dvf_all_years).\n\n\nOn peut créer une structure Dask directement à partir\nd’un DataFrame pandas avec la méthode from_pandas.\n\ndvf_dd = dd.from_pandas(dvf, npartitions=10) \ndvf_dd\n\nDask DataFrame Structure:\n\n\n\n\n\n\n\nIdentifiant de document\nReference document\n1 Articles CGI\n2 Articles CGI\n3 Articles CGI\n4 Articles CGI\n5 Articles CGI\nNo disposition\nDate mutation\nNature mutation\nValeur fonciere\nNo voie\nB/T/Q\nType de voie\nCode voie\nVoie\nCode postal\nCommune\nCode departement\nCode commune\nPrefixe de section\nSection\nNo plan\nNo Volume\n1er lot\nSurface Carrez du 1er lot\n2eme lot\nSurface Carrez du 2eme lot\n3eme lot\nSurface Carrez du 3eme lot\n4eme lot\nSurface Carrez du 4eme lot\n5eme lot\nSurface Carrez du 5eme lot\nNombre de lots\nCode type local\nType local\nIdentifiant local\nSurface reelle bati\nNombre pieces principales\nNature culture\nNature culture speciale\nSurface terrain\nyear\n\n\nnpartitions=10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nint64\nstring\nstring\nfloat64\nfloat64\nstring\nstring\nstring\nstring\nfloat64\nstring\nstring\nint64\nfloat64\nstring\nint64\nstring\nstring\nfloat64\nstring\nfloat64\nstring\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nint64\nfloat64\nstring\nfloat64\nfloat64\nfloat64\nstring\nstring\nfloat64\nstring\n\n\n362591\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3263313\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3625902\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n\n\n\nDask Name: to_pyarrow_string, 2 graph layers\n\n\nPour souligner la différence avec un pandas.DataFrame,\nl’affichage diffère. Seule la structure du dask.DataFrame\nest affichée et non son contenu car les données\ndask ne sont pas chargées en mémoire.\n\n\n Warning\nAttention, Dask ne peut créer un Dask.DataFrame à partir d’un pandas.DataFrame multi-indexé.\nDans ce cas il a fallu faire un reset_index() pour avoir un unique index.\n\n\nOn a ainsi la structure de notre dask.DataFrame, soit environ 3.5 millions de lignes, avec 44 colonnes en 10 partitions, soit environ 350 000 observations par partition.\nIl faut savoir que Dask produit des Array, Bag et DataFrames, qui fonctionnent comme dans Numpy et Pandas (il est possible de créer d’autres structures ad hoc, cf plus loin).\nDask, comme Spark et en fait comme la plupart des frameworks permettant de\ntraiter des données plus volumineuses que la RAM disponible,\nrepose sur le principe du partitionnement et de la parallélisation\ndes opérations. Les données ne sont jamais importées dans leur\nensemble mais par bloc. Un plan des opérations à effectuer est\nensuite appliquer sur chaque bloc (nous reviendrons\nsur ce principe), indépendamment. La particularité de Dask,\npar rapport à Spark,\nest que chaque bloc est un pandas.DataFrame, ce qui\nrend très facile l’application de manipulations de données\ntraditionnelles sur des sources volumineuses:\n\n\n\n\n\nLe site de Dask cite une règle qui est la suivante :\n\n“Have 5 to 10 times as much RAM as the size of your dataset”,\n@mckinney2017apache, 10 things I hate about pandas\n\nSur disque, en sauvegardant en CSV, on\nobtient une base de 1.4GB. Si l’on suit la règle du pouce donnée plus haut, on va avoir besoin d’une RAM entre 7-14GB pour traiter la donnée, en fonction de nos traitements qui seront plus ou moins intensifs. Autrement dit, si on a moins de 8GB de RAM, il devient intéressant de faire appel à dask, sinon il vaut mieux privilégier pandas (sauf si on fait des\ntraitements très intensifs en calculs).\nIl existe un autre objet dask, les Array pour reprendre la logique de numpy. De la même manière qu’un dask.DataFrame est en quelque sorte un ensemble de pandas.DataFrame, un dask.Array est un ensemble de numpy.Array qui sont plus importants en taille que la RAM. On pourra utiliser les opérations courantes numpy avec dask de la même manière que le dask DataFrame réplique la logique du pandas DataFrame.\n\n\n Hint\nLe choix du nombre de partition (10) est arbitraire ici. Bien qu’on puisse\ntrouver des règles du pouce pour fixer un nombre optimal de\npartitions, cela dépend de beaucoup de facteurs et, en pratique,\nrien ne remplace l’essai-erreur. Par exemple, la documentation Dask recommande des blocs d’environ\n100MB\nce qui peut convenir pour des ordinateurs à la RAM limitée mais n’a pas\nforcément de sens pour des machines ayant 16GB de RAM.\nUn nombre important de partition va permettre de faire des opérations\nsur des petits blocs de données, ce qui permettra de gagner en vitesse\nd’exécution. Le prix à payer est beaucoup d’input/output car\nDask va passer du temps à lire beaucoup de blocs de données et écrire\ndes bases intermédiaires.\n\n\nOn peut accéder aux index que couvrent les partitions de la manière suivante:\n\ndvf_dd.divisions\n\n(0,\n 362591,\n 725182,\n 1087773,\n 1450363,\n 1812953,\n 2175543,\n 2538133,\n 2900723,\n 3263313,\n 3625902)\n\n\nAutrement dit, la première partition couvrira les lignes 0 à 362591. La deuxième les lignes 362592 à 725182, etc.\nEt on peut directement accéder à une partition grâce aux crochets []:\n\ndvf_dd.partitions[0]\n\nDask DataFrame Structure:\n\n\n\n\n\n\n\nIdentifiant de document\nReference document\n1 Articles CGI\n2 Articles CGI\n3 Articles CGI\n4 Articles CGI\n5 Articles CGI\nNo disposition\nDate mutation\nNature mutation\nValeur fonciere\nNo voie\nB/T/Q\nType de voie\nCode voie\nVoie\nCode postal\nCommune\nCode departement\nCode commune\nPrefixe de section\nSection\nNo plan\nNo Volume\n1er lot\nSurface Carrez du 1er lot\n2eme lot\nSurface Carrez du 2eme lot\n3eme lot\nSurface Carrez du 3eme lot\n4eme lot\nSurface Carrez du 4eme lot\n5eme lot\nSurface Carrez du 5eme lot\nNombre de lots\nCode type local\nType local\nIdentifiant local\nSurface reelle bati\nNombre pieces principales\nNature culture\nNature culture speciale\nSurface terrain\nyear\n\n\nnpartitions=1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nint64\nstring\nstring\nfloat64\nfloat64\nstring\nstring\nstring\nstring\nfloat64\nstring\nstring\nint64\nfloat64\nstring\nint64\nstring\nstring\nfloat64\nstring\nfloat64\nstring\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nint64\nfloat64\nstring\nfloat64\nfloat64\nfloat64\nstring\nstring\nfloat64\nstring\n\n\n362591\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n\n\n\nDask Name: blocks, 3 graph layers\n\n\n\n\n16.2.2 La “lazy evaluation”\nDask fait de la “lazy evaluation”. Cela signifie que le résultat n’est calculé que si on le demande explicitement. Dans le cas, contraire, ce que l’on appelle un dask task graph est produit (on verra plus bas comment voir ce graph).\nPour demander explicitement le résultat d’un calcul, il faut utiliser la\nméthode compute.\nA noter que certaines méthodes vont déclencher un compute directement, comme par exemple len ou head.\nPar exemple, pour afficher le contenu des 100 premières lignes :\n\ndvf_dd.loc[0:100,:].compute()\n\n\n\n\n\n\n\n\nIdentifiant de document\nReference document\n1 Articles CGI\n2 Articles CGI\n3 Articles CGI\n4 Articles CGI\n5 Articles CGI\nNo disposition\nDate mutation\nNature mutation\n...\nNombre de lots\nCode type local\nType local\nIdentifiant local\nSurface reelle bati\nNombre pieces principales\nNature culture\nNature culture speciale\nSurface terrain\nyear\n\n\n\n\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n04/01/2019\nVente\n...\n1\n2.0\nAppartement\nNaN\n20.0\n1.0\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n2019\n\n\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n04/01/2019\nVente\n...\n2\n2.0\nAppartement\nNaN\n62.0\n3.0\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n2019\n\n\n2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n04/01/2019\nVente\n...\n1\n3.0\nDépendance\nNaN\n0.0\n0.0\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n2019\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n08/01/2019\nVente\n...\n0\n1.0\nMaison\nNaN\n90.0\n4.0\nS\n&lt;NA&gt;\n940.0\n2019\n\n\n4\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n07/01/2019\nVente\n...\n0\n1.0\nMaison\nNaN\n101.0\n5.0\nS\n&lt;NA&gt;\n490.0\n2019\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n96\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n04/01/2019\nVente\n...\n0\n1.0\nMaison\nNaN\n48.0\n2.0\nS\n&lt;NA&gt;\n935.0\n2019\n\n\n97\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n07/01/2019\nVente\n...\n0\nNaN\n&lt;NA&gt;\nNaN\nNaN\nNaN\nP\n&lt;NA&gt;\n3264.0\n2019\n\n\n98\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n18/01/2019\nVente\n...\n0\nNaN\n&lt;NA&gt;\nNaN\nNaN\nNaN\nP\n&lt;NA&gt;\n2870.0\n2019\n\n\n99\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n07/01/2019\nVente\n...\n0\nNaN\n&lt;NA&gt;\nNaN\nNaN\nNaN\nT\n&lt;NA&gt;\n1423.0\n2019\n\n\n100\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n03/01/2019\nVente\n...\n0\nNaN\n&lt;NA&gt;\nNaN\nNaN\nNaN\nP\n&lt;NA&gt;\n93.0\n2019\n\n\n\n\n101 rows × 44 columns\n\n\n\nCe qui est pratique avec dask.dataframe c’est que de nombreuses méthodes sont semblables à celles de pandas. Par exemple, si l’on souhaite connaitre les types de locaux présents dans la base en 2019:\n\ndvf_dd.loc[:,\"Type local\"].value_counts().compute()\n\nType local\nMaison                                      712874\nAppartement                                 656261\nDépendance                                  495774\nLocal industriel. commercial ou assimilé    143194\nName: count, dtype: int64[pyarrow]\n\n\nA titre de comparaison, comparons les temps de calculs entre pandas et dask ici:\n\nimport time\nstart_time = time.time()\ndvf_dd.loc[:,\"Type local\"].value_counts().compute()\nprint(f\"{time.time() - start_time} seconds\")\n\n5.368297100067139 seconds\n\n\n\nstart_time = time.time()\ndvf.loc[:,\"Type local\"].value_counts()\nprint(f\"{time.time() - start_time} seconds\")\n\n0.17524218559265137 seconds\n\n\nOn se rend compte que le pandas.DataFrame a un temps de calcul plus court, mais c’est parce que dask va nous servir avant tout à lire des bases dont le traitement excède notre RAM. Donc, cette comparaison n’existera tout simplement pas car le pandas.DataFrame n’aura pas été chargé en RAM. On voit dans cet exemple que lorsque le traitement du DataFrame tient en RAM, l’utilisation de Dask est inutile.\nLes méthodes dans Dask peuvent être chainées, comme dans pandas, par exemple, on pourra écrire:\n\nmean_by_year = dvf_dd.loc[~dvf_dd[\"Surface terrain\"].isna(),[\"Surface terrain\", \"year\"]].groupby(\"year\").mean()\n\n\nmean_by_year.compute()\n\n\n\n\n\n\n\n\nSurface terrain\n\n\nyear\n\n\n\n\n\n2019\n3064.674673\n\n\n\n\n\n\n\nLe principe de la lazy evaluation est donc d’annoncer à Dask\nqu’on va effectuer une série d’opération qui ne vont se réaliser\nque lorsqu’on fera un appel à compute. Dask, quant à lui,\nse chargera d’optimiser les traitements.\nComme le plan d’action peut devenir difficile à suivre si on\ndésire effectuer beaucoup d’opérations enchaînées, on peut\nvouloir visualiser le graph de computation de dask.\nAvec celui-ci, on voit toutes les étapes que jusqu’ici dask n’a pas executé\net qu’il va devoir exécuter pour calculer le résultat (compute()).\n\nmean_by_year.dask\n\nEn l’occurence on voit l’enchaînement des étapes\nfrom_pandas(), getitem, isna, inv et loc-series qui résultent de nos filtres sur le DataFrame. Ensuite,\non voit les étapes de groupby et, enfin, pour calculer la moyenne il convient de faire la somme et la division. Toutes ces étapes vont être effectuées quand on appelle compute() et pas avant (lazy evaluation).\nAfin de voir la structure du dask.DataFrame on peut utiliser la méthode visualize()\n\ndvf_dd.visualize() # attention graphviz est requis\n\n\n\n\n\n\n\n\n\n\n Note\ngraphviz est requis pour ce graphique. S’il n’est pas installé dans votre environnement, faire :\n!pip install graphviz\n\n\nPour construire de véritables pipelines de données,\nles principes du pipe de pandas évoqué dans cette partie du cours et celui des pipelines scikit, évoqué dans un chapitre dédié\nont été importés dans dask.\n\n\n16.2.3 Problèmes de lecture dus à des types problématiques\nLa méthode read_csv de dask va inférer les types du DataFrame à partir d’échantillon, et va les implémenter sur tout le DataFrame seulement au moment d’une étape compute.\nIl peut donc y avoir des erreurs de types dûs à un échantillon ne prenant pas en compte certains cas particuliers, causant des erreurs dans la lecture du fichier.\nDans ce cas, et comme de manière générale avec pandas, il peut être recommandé de faire appel au paramètre dtype de read_csv - qui est un dict - (la doc de dask nous dit aussi que l’on peut augmenter la taille de l’échantllon sample)."
  },
  {
    "objectID": "content/course/manipulation/07_dask/index.html#utiliser-dask-avec-le-format-parquet",
    "href": "content/course/manipulation/07_dask/index.html#utiliser-dask-avec-le-format-parquet",
    "title": "16  Introduction à dask grâce aux données DVF",
    "section": "16.3 Utiliser Dask avec le format parquet",
    "text": "16.3 Utiliser Dask avec le format parquet\nLe format parquet tend à devenir le format\nde référence dans le monde de la data-science.\nUne présentation extensive de celui-ci est disponible\ndans le chapitre dédié.\ndask permet de lire le format parquet, et plus précisément d’utiliser des fonctionnalités spécifiques à ce format. La lecture et l’écriture en parquet reposent par défaut sur pyarrow. On peut aussi utiliser fastparquet et préciser dans la lecture/écriture ce que l’on souhaite des deux.\n\ndvf_net = dvf.loc[:,[ 'Date mutation', 'Nature mutation', 'Valeur fonciere', 'Commune', \n       'Code commune', 'Type local', 'Identifiant local', 'Surface reelle bati',\n       'Nombre pieces principales', 'Nature culture',\n       'Nature culture speciale', 'Surface terrain', 'year']]\n\nOn va utiliser l’engine par défaut pour\nl’écriture de parquet qui est pyarrow (faire pip install pyarrow si vous ne l’avez pas déjà installé). to_parquet qui est une méthode pandas a été également étendue aux objets dask:\n\ndvf_net.to_parquet(\"dvf/\", partition_cols=\"year\")\n\nLorsqu’il est partitionné, le format parquet amène à une structure\nde fichiers similaire à celle-ci:\npath\n└── to\n    └── table\n        ├── gender=male\n        │   ├── ...\n        │   │\n        │   ├── country=US\n        │   │   └── data.parquet\n        │   ├── country=CN\n        │   │   └── data.parquet\n        │   └── ...\n        └── gender=female\n            ├── ...\n            │\n            ├── country=US\n            │   └── data.parquet\n            ├── country=CN\n            │   └── data.parquet\n            └── ...\nOn peut alors facilement traiter un sous-échantillon des données,\npar exemple l’année 2019:\n\ndvf_2019 = dd.read_parquet(\"dvf/year=2019/\", columns=[\"Date mutation\", \"Valeur fonciere\"]) # On peut sélectionner directement les deux colonnes\n\nLorsqu’il faudra passer à l’échelle, on changera le chemin en \"dvf/\npour utiliser l’ensemble des données."
  },
  {
    "objectID": "content/course/manipulation/07_dask/index.html#a-quoi-sert-persist",
    "href": "content/course/manipulation/07_dask/index.html#a-quoi-sert-persist",
    "title": "16  Introduction à dask grâce aux données DVF",
    "section": "16.4 A quoi sert persist ?",
    "text": "16.4 A quoi sert persist ?\nPar défaut, compute exécute l’ensemble du plan et ne conserve\nen mémoire que le résultat de celui-ci. Les données intermédiaires\nne sont pas conservées. Si on désire réutiliser une partie de celui-ci,\npar exemple les premières étapes, on devra donc ré-effectuer\nles calculs.\nIl est possible de garder une partie des données en mémoire avec persist(). Les données sont sauvegardées dans des objets appelés Futures. Cela peut être intéressant si un bloc particulier de données est utilisé dans plusieurs compute ou si l’on a besoin de voir ce qu’il y a à l’intérieur souvent.\n\ndvf_dd_mem = dvf_dd.persist()\n\n\nstart_time = time.time()\ndvf_dd_mem.head()\nprint(f\"{time.time() - start_time} seconds\")\n\n\nstart_time = time.time()\ndvf_dd.head()\nprint(f\"{time.time() - start_time} seconds\")\n\n0.5852344036102295 seconds\n\n\nOn a bien un temps plus important avec le dask.DataFrame initial, comparé avec celui sur lequel on a utilisé persist. L’opération qu’on réalise ici étant peu complexe, la différence n’est pas substantielle. Elle serait beaucoup plus marquée avec un jeu de données plus volumineux ou des étapes intensives en calcul."
  },
  {
    "objectID": "content/course/manipulation/07_dask/index.html#aller-plus-loin-utiliser-le-decorator-dask.delayed-pour-paralléliser-du-code",
    "href": "content/course/manipulation/07_dask/index.html#aller-plus-loin-utiliser-le-decorator-dask.delayed-pour-paralléliser-du-code",
    "title": "16  Introduction à dask grâce aux données DVF",
    "section": "16.5 Aller plus loin: Utiliser le decorator dask.delayed pour paralléliser du code",
    "text": "16.5 Aller plus loin: Utiliser le decorator dask.delayed pour paralléliser du code\nIl est possible de paralléliser des fonctions par exemple en utilisant le decorator dask.delayed. Cela permet de rendre les fonctions lazy. Cela signifie que lorsqu’on appelle la fonction, un delayed object est construit. Pour avoir le résultat, il faut faire un compute. Pour aller plus loin: https://tutorial.dask.org/03_dask.delayed.html.\nPrenons par exemple des fonctions permettant de calculer\ndes aires et des périmètres. Comme il s’agit d’une opération\ntrès peu complexe, on ajoute un délai de calcul avec time.sleep\npour que le timer ne nous suggère pas que l’opération est\ninstantanée.\n\ndef aire_carre(longueur):\n    time.sleep(1)\n    return longueur**2\n\ndef perimetre_carre(longueur):\n    time.sleep(1)\n    return 4*longueur\n\ndef ajout_aire_perim(a, b):\n    return a + b\n\nSans timer, c’est-à-dire de manière classique,\non ferait nos appels de fonctions de la\nmanière suivante:\n\nstart_time = time.time()\ncar1 = aire_carre(7)\ncar2 = perimetre_carre(9)\ncar3 = ajout_aire_perim(car1, car2)\ncar3\nprint(time.time() - start_time)\n\n2.002650260925293\n\n\nAvec le décorateur dask.delayed, on définit\nnos fonctions de la manière suivante:\n\nimport dask\n\n@dask.delayed\ndef aire_carre(longueur):\n    time.sleep(1)\n    return longueur**2\n\n@dask.delayed\ndef perimetre_carre(longueur):\n    time.sleep(1)\n    return 4*longueur\n\n@dask.delayed\ndef ajout_aire_perim(a, b):\n    return a + b\n\nL’appel de fonctions est identique\n\ncar1 = aire_carre(7)\ncar2 = perimetre_carre(9)\ncar3 = ajout_aire_perim(car1, car2)\n\nCependant, en fait rien n’a été calculé, si l’on souhaite le résultat, il faut appeler compute:\n\nstart_time = time.time()\ncar3.compute()\nprint(time.time() - start_time)\n\n1.002744436264038\n\n\nIci l’intérêt est assez limité, mais on voit que l’on réduit quand même de 2 à 1 seconde le temps de calcul. Mais l’idée derrière est que l’on a transformé car3 en un objet Delayed. Cela a généré un task graph permettant de paralléliser certaines opérations.\nIci il est important de noter que les fonctions que l’on parallélise doivent mettre un certain temps, sinon il n’y aura pas de gain de performance (si on retire le time.sleep il n’y a pas de gain de performance car le fait de paralléliser rajoute en fait du temps vu que chaque fonction a un temps de calcul trop faible pour que la parallélisation soit intéressante).\n\ncar3.visualize() # on peut visualiser le task graph et voir ce qui est fait en parallèle \n\n\n\n\n\n\n\n\nIl y a des exercices intéressants dans la doc de Dask sur les objets Delayed, notamment sur la parallélisation de séquence de traitement de données. Ils donnent l’exemple d’un ensemble de csv ayant le même format dont on veut résumer un indicateur final. On peut appliquer le decorator à une fonction permettant de lire le csv, puis utiliser une boucle for pour lire chaque fichier et appliquer les traitements. Ensuite, il faudra appeler compute sur l’objet final que l’on souhaite.\nPour aller plus loin sur l’utilisation de Dask sur un cluster voir https://tutorial.dask.org/04_distributed.html."
  },
  {
    "objectID": "content/course/manipulation/07_dask/index.html#remerciements",
    "href": "content/course/manipulation/07_dask/index.html#remerciements",
    "title": "16  Introduction à dask grâce aux données DVF",
    "section": "16.6 Remerciements",
    "text": "16.6 Remerciements\nCe chapitre a été rédigé avec Raphaële Adjerad."
  },
  {
    "objectID": "content/course/manipulation/06a_exo_supp_webscraping/index.html#construction-automatisée-dune-liste-de-courses-via-webscraping-spaghetti-pizza-strawberry",
    "href": "content/course/manipulation/06a_exo_supp_webscraping/index.html#construction-automatisée-dune-liste-de-courses-via-webscraping-spaghetti-pizza-strawberry",
    "title": "17  Exercices supplémentaires de webscraping",
    "section": "17.1 Construction automatisée d’une liste de courses via webscraping :spaghetti: :pizza: :strawberry:",
    "text": "17.1 Construction automatisée d’une liste de courses via webscraping :spaghetti: :pizza: :strawberry:\nLes comptes sont dans le rouge, le banquier appelle tous les jours.\nPlus le choix : fini les commandes de plats tout faits via des plateformes bien connues,\nil va falloir se faire des bons petits plats soi-même.\nMais la cuisine à l’ancienne, c’est long : il faut trouver le bon livre de cuisine,\nla bonne recette, faire des règles de trois pour calculer les bonnes proportions, etc.\nEt après ça, faire une liste de courses…\nHeureusement, Marmiton est là pour nous.\nDans ce TP, on va construire un outil Python qui permet d’exporter directement une liste de courses,\nen fonctions des plats que l’on a envie de manger cette semaine. Et tout ça en webscrapant les données de Marmiton. Plus d’excuse !\nPour cet exercice, on va utiliser principalement trois librairies très utilisées en webscraping :\n\nrequests & BeautifulSoup pour scraper des pages statiques ;\nselenium lorsque l’on aura besoin d’interagir avec les éléments scriptés des pages web.\n\nPour pouvoir utiliser selenium, il est nécessaire d’avoir installé le chromedriver (instructions),\nou bien le driver adapté si vous utilisez un autre navigateur que Google Chrome.\n\nAnalyser comment fonctionne la recherche d’une recette sur Marmiton (structure de l’URL) et coder un outil\npermettant de récupérer (à l’aide de requests) le code html des résultats de la recherche pour une recette donnée.\nFormatter ce code en un arbre lxml à l’aide de BeautifulSoup.\n\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n\nPLAT = \"pates carbonara\"\nBASE_URL = \"https://www.marmiton.org\"\nURL_SEARCH = BASE_URL + \"/recettes/recherche.aspx?aqt=\" + PLAT\n\nVous pouvez vérifier que python récupère bien un résultat à l’URL voulu en tapant\n\nrequests.get(URL_SEARCH).status_code\n\nSi le code retour est 200, il y a bien du contenu accessible sur la page.\n{{% box status=“note” title=“Note” icon=“fa fa-comment” %}}\nL’utilisation de l’option “lxml” avec BeautifulSoup nécessite d’avoir\ninstallé avant cela la librairie lxml.\n{{% /box %}}\n\nresponse = requests.get(URL_SEARCH).text\nsoup = BeautifulSoup(response, \"lxml\")\n\n\nAfficher le code source html de la page des résultats de la recherche à l’aide de votre navigateur (click droit sur la page =&gt; Inspecter),\nanalyser la structure de l’arbre,\net récupérer le code html de chacune des recettes.\nA l’aide d’une boucle, récupérer pour chaque recette sa note moyenne et le nombre de fois où elle a été notée.\n\n\nimport re\nimport numpy as np\n\n\nfound_recipes = soup.find_all(name=\"a\", class_=re.compile(\"^SearchResultsstyle__SearchCardResult\"))\n\nall_ratings = []\nall_nb_ratings = []\nfor recipe in found_recipes:\n    try:\n        ratings_info = recipe.find(name=\"div\", class_=re.compile(\"^RecipeCardResultstyle__RatingLayout\")).text\n    except AttributeError:\n        continue\n    matches = re.search(r\"([\\d\\.]+)/5\\(([\\d]+) avis\\)\", ratings_info, re.IGNORECASE)\n    rating = matches.group(1)\n    nb_ratings = matches.group(2)\n    all_ratings.append(float(rating))\n    all_nb_ratings.append(int(nb_ratings))\n\n\nSur Marmiton, on peut tomber sur de mauvaises surprises.\nPour éviter ça, restreindre les recettes à celles qui ont une note moyenne &gt;= 4 et un nombre de notes &gt;= 50.\nChoisir la recette la mieux notée au sein de cette liste de candidates, et récupérer son URL.\n\n\nMIN_RATING = 4\nMIN_NB_RATINGS = 50\n\nidxs_eligible = [i for i, x in enumerate(found_recipes)\n                 if all_ratings[i] &gt; MIN_RATING and all_nb_ratings[i] &gt;= MIN_NB_RATINGS]\nidx_chosen = np.argmax(np.array(all_ratings)[idxs_eligible])\nhref_chosen = found_recipes[idx_chosen].get(\"href\")\n\nif href_chosen is not None:\n    url_chosen_recipe = BASE_URL + href_chosen\nelse:\n    raise ValueError(\"Aucune recette n'a été trouvée pour les critères demandés.\")\n\n\nRécupérer une photo de la recette et l’afficher dans le Notebook.\n\n\nlist_imgs = found_recipes[idx_chosen].find(name=\"source\", type=\"image/jpeg\").get(\"srcset\")\nurl_img_big = re.split(\"\\s\\d+w,?\\s?\", list_imgs)[-2]\n\n\n# Dans un notebook\nfrom IPython.display import Image\nImage(url_img_big, width=400, height=400)\n\nNous avons choisi cette recette, un classique ! :spaghetti:\nConvaincu ?\nSinon, ne pas hésiter à changer de recette au début,\non ne va quand même pas faire tout ça pour rien.\nLa recette est choisie, pour nous c’est pates carbo.\nNouvel objectif : faire la liste de courses ! :purse:\nMais les choses se compliquent : pour quantifier les ingrédients selon le nombre de convives\net afficher la liste au format courses sur Marmiton, on va devoir cliquer sur des boutons qui exécutent du JavaScript.\nLes librairies requests et BeautifulSoup atteignent là leurs limites, mais pas de panique : Selenium est fait pour ça.\nIl va nous permettre d’ouvrir un navigateur “fantôme”, contrôlé via Python, avec lequel on va pouvoir effectuer des actions sur la page\n(comme le ferait une personne naviguant sur la page web).\nAutre subtilité : jusqu’à maintenant, on a repéré les éléments html par type et nom de classe.\nCette méthode fonctionne, mais elle pose également des problèmes :\nparfois les noms de classes changent sans raison (c’est d’ailleurs pour ça qu’on a utilisé des regex précédemment, pour faire du matching partiel),\net il est moins pratique d’interagir avec les éléments d’une page de cette manière. Parfois, il est\npertinent d’utiliser les sélécteurs XPath,\nqui permettent de sélectionner les éléments selon leur position dans l’arborescence html de la page.\nOn utilisera une combinaison des deux méthodes dans cette partie selon les cas.\nOn le voit, le webscraping reste une pratique assez instable,\ndans la mesure où les sites web évoluent en permanence.\nIl y a ainsi toutes les chances qu’au moment où vous effectuerez ce TP, le code proposé en solution ne fonctionne plus, car les balises auront changé.\nIl vous faudra alors revenir à l’exploration du code source html de la page, repérer les balises permanentes, et les substituer dans le code de solution.\n\nOuvrir la page de la recette choisie à l’aide d’un navigateur fantôme.\nProblème : la classique fenêtre de politique des cookies :cookie: s’ouvre, nous empêchant de naviguer sur la page.\nUtiliser Selenium pour cliquer sur le bouton permettant d’accepter tous les cookies.\n\n\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.common.exceptions import NoSuchElementException\n\n\ndriver = webdriver.Chrome()\ndriver.get(url_chosen_recipe)\n\n\nWebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, \"didomi-notice-agree-button\"))).click()\n\n\nChoisir pour combien de personnes on va cuisiner.\nComparer ce nombre au nombre utilisé par défaut sur Marmiton,\net construire une boucle qui va clicker automatiquement le bon nombre de fois,\nsur + ou - selon que le nombre de convives choisi est supérieur ou inférieur au nombre par défaut.\n\n\ntry:\n    counter = driver.find_element_by_class_name(\"quantity-counter\")\nexcept NoSuchElementException:\n    raise Exception(\"La structure de cette page est particulière, il va falloir trouver les bonnes balises à la main.\")\nelse:\n    xpath_current_count = './/input'\n    current_count = int(counter.find_element_by_xpath('.//div[2]/input').get_attribute(\"value\"))\n    xpath_minus = './/div[@class=\"quantity-counter__action minus\"]'\n    xpath_plus = './/div[@class=\"quantity-counter__action plus\"]'\n\n\nNB_PERSONNES = 8\n\nnb_clicks = NB_PERSONNES - current_count\nif nb_clicks &gt; 0:\n    xpath_button_nb_persons = xpath_plus\nelif nb_clicks &lt; 0:\n    xpath_button_nb_persons = xpath_minus\nfor i in range(abs(nb_clicks)):\n    driver.find_element_by_xpath(xpath_button_nb_persons).click()\n\n\nMarmiton a un mode liste de courses qui va nous être bien pratique pour récupérer les ingrédients au bon format.\nA l’aide de Selenium, cliquer sur le bouton “liste” (à droite de l’outil pour ajuster le nombre de personnes).\n\n\ndisplay_options = driver.find_element_by_class_name(\"ingredient-list__display-options\")\ndisplay_options.find_element_by_xpath(\".//i[2]\").click()\n\n\nSelon les cas, il peut être nécessaire de cliquer ensuite sur un autre bouton permettant de développer la liste.\nEffectuer cette action (si nécessaire !).\nCela permettra d’être sûr que l’on récupère bien tous les ingrédients pour construire notre liste de courses.\n\n\n# xpath_expand_list = \"/html/body/div[2]/div[3]/main/div/div/div[1]/div[1]/div[7]/div[2]/div[3]\"\n# try:\n#     driver.find_element_by_xpath(xpath_expand_list).click()\n# except ElementClickInterceptedException:\n#     pass\n\n# driver.implicitly_wait(2)  # Make sure that the elements are displayed after pressing button\n\n\nRécupérer la liste des ingrédients ainsi que des quantités nécessaires. Stocker les éléments dans une liste.\n\n\nlist_ings_div = driver.find_element_by_class_name(\"ingredient-list__ingredient-group\")\nlist_ings = [x.text for x in list_ings_div.find_elements_by_tag_name(\"li\")]\nprint(list_ings)\n\n['500 g de lardons', 'poivre', '2 pincées de sel', '1 kg de pâtes', '1 l de crème fraîche', \"6 jaunes d'oeuf\", '2 oignons']\n\nExporter la liste dans un fichier texte sur votre ordinateur.\n\n\nwith open(\"shopping_list.txt\", \"w\") as f:\n    for ing in list_ings:\n        f.write(ing + \"\\n\")\n\n\nLa liste est prête, mais il va aussi nous falloir la recette. Récupérer la recette, et l’exporter dans un fichier texte séparé, qui porte le nom du plat choisi.\n\n\nrecipe = driver.find_element_by_class_name(\"recipe-step-list\").text\n\nwith open(f\"recipe_{PLAT}.txt\", \"w\") as f:\n    f.write(recipe)\n\n\nL’outil fonctionne… pour un plat donné. Adapter le code précédent pour prendre en entrée une liste de plats, et retourner en sortie la liste de courses complète (en un seul fichier) pour pouvoir réaliser ces différents plats. Hint: il sera sûrement utile de faire une fonction qui prend en input un plat et exporte la liste de courses pour ce plat, et ensuite d’appeler cette fonction pour chaque plat dans le cadre d’une boucle. Attention de ne pas écraser la liste de courses précédentes à chaque fois !"
  },
  {
    "objectID": "content/course/visualisation/index.html#lécosystème-python",
    "href": "content/course/visualisation/index.html#lécosystème-python",
    "title": "Partie 2: visualiser les données",
    "section": "L’écosystème Python",
    "text": "L’écosystème Python\nL’écosystème Python pour la valorisation de données est très riche et\ntrès éclaté.\nIl est\npossible de consacrer des livres entiers à celui-ci (Dale 2022).\nPython propose\nde nombreuses librairies pour produire de manière rapide et relativement\nsimple des visualisations de données1.\nLes librairies graphiques se distinguent principalement en deux familles:\n\nLes librairies de représentations figées. Celles-ci ont plutôt vocation à être intégrées\ndans des publications figées type PDF ou documents texte. Nous présenterons\nprincipalement Matplotlib et Seaborn mais il en existe d’autres,\ncomme Plotnine.\nLes librairies de représentations dynamiques. Celles-ci sont adaptées à des représentations\nweb et offrent la possibilité aux lecteurs d’agir sur la représentation graphique affichée.\nLes librairies qui proposent ces fonctionnalités reposent généralement sur JavaScript, l’écosystème\ndu développement web, pour lequel elles offrent un point d’entrée via Python.\nNous évoquerons principalement Plotly et Folium dans cette famille mais il existe de nombreux\nautres frameworks dans ce domaine2.\n\nDans le domaine de la visualisation, ce cours adopte le parti pris\nd’explorer quelques\nlibrairies centrales à partir d’un nombre restreint d’exemples en\nrépliquant des graphiques qu’on peut trouver sur le site d’open data de la\nmairie de Paris.\nLa meilleure école pour la visualisation est la pratique sur des jeux de données.\n\nLes applications de visualisation\nCette partie du cours se focalise sur des représentations synthétiques simples.\nElle n’évoque pas (encore ?) la construction d’applications de visualisation\nde données où un ensemble de graphiques se mettent à jour de manière synchrone\nen fonction d’actions d’utilisateurs.\nCeci dépasse en effet le cadre d’un cours d’introduction car cela implique\nde maîtriser des concepts plus complexes comme l’interaction entre une page\nweb et un serveur (local). Néanmoins, j’ai déjà construit\navec Romain Avouac\nun tutoriel 101 très détaillé sur Streamlit\n(permettant de créer une application type Yuka)\npour une formation à l’Insee."
  },
  {
    "objectID": "content/course/visualisation/index.html#résumé-de-cette-partie",
    "href": "content/course/visualisation/index.html#résumé-de-cette-partie",
    "title": "Partie 2: visualiser les données",
    "section": "Résumé de cette partie",
    "text": "Résumé de cette partie\nCette partie est divisée en deux et chaque chapitre est lui-même\ndual, selon qu’on s’intéresse aux représentations figées\nou dynamiques :\n\nDans un premier temps, nous évoquerons des\nreprésentations graphiques standards (histogrammes, diagrammes\nen barre…) pour synthétiser certaines informations quantitatives ;\n\nLes représentations fixes reposeront sur Pandas, Matplotlib et Seaborn\nLes graphiques réactifs s’appuieront sur Plotly\n\nDans un deuxième temps, nous présenterons les représentations\ncartographiques:\n\nLes cartes fixes avec Geopandas ou Geoplot\nLes cartes réactives avec Folium (adaptation Python de la librairie Leaflet.js)"
  },
  {
    "objectID": "content/course/visualisation/index.html#références-utiles",
    "href": "content/course/visualisation/index.html#références-utiles",
    "title": "Partie 2: visualiser les données",
    "section": "Références utiles",
    "text": "Références utiles\nLa visualisation de données est un art qui s’apprend, au début, principalement\npar la pratique. Néanmoins, il n’est pas évident de produire\ndes visualisations lisibles et ergonomiques\net il est utile de s’inspirer d’exemples de\nspécialistes (les grands titres de presse disposent d’excellentes visualisations).\nVoici quelques ressources utiles sur ces sujets:\n\nDatawrapper propose un excellent blog sur les\nbonnes pratiques de visualisation, notamment\navec les articles de Lisa Charlotte Muth. Je recommande notamment cet article sur\nles couleurs ou\ncelui-ci sur les textes ;\nLe blog d’Eric Mauvière ;\n“La Sémiologie graphique de Jacques Bertin a cinquante ans” ;\nLes visualisations trending sur Observable ;\nLe New York Times (les rois de la dataviz) revient tous les ans sur les meilleures visualisations\nde l’année dans la veine du data scrollytelling. Voir par exemple la rétrospective de l’année 2022.\n\nEt quelques références supplémentaires, citées dans cette introduction:\n\n\nBertin, Jacques. 1967. Sémiologie Graphique. Paris: Mouton/Gauthier-Villars.\n\n\nDale, Kyran. 2022. Data Visualization with Python and JavaScript. \" O’Reilly Media, Inc.\".\n\n\nInsee. 2018. “Guide de Sémiologie Cartographique.”\n\n\nPalsky, Gilles. 2017. “La sémiologie Graphique de Jacques Bertin a Cinquante Ans.” Visions Carto (En Ligne).\n\n\nWilke, Claus O. 2019. Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures. O’Reilly Media."
  },
  {
    "objectID": "content/course/visualisation/matplotlib/index.html#premier-graphique-avec-lapi-matplotlib-de-pandas",
    "href": "content/course/visualisation/matplotlib/index.html#premier-graphique-avec-lapi-matplotlib-de-pandas",
    "title": "18  De beaux graphiques avec python: mise en pratique",
    "section": "18.1 Premier graphique avec l’API matplotlib de pandas",
    "text": "18.1 Premier graphique avec l’API matplotlib de pandas\n{{% box status=“exercise” title=“Exercice” icon=“fas fa-pencil-alt” %}}\nExercice 1 : Importer les données et produire un premier graphique\n\nImporter les données de compteurs de vélos. Vous pouvez utiliser l’url https://minio.lab.sspcloud.fr/projet-formation/diffusion/python-datascientist/bike.csv. ⚠️ Il s’agit de données\ncompressées au format gzip, il faut donc utiliser l’option compression = 'gzip'\nEn premier lieu, sans se préoccuper des éléments de style ni des labels des\ngraphiques, reproduire les deux premiers graphiques de la\npage d’analyse des données:\nLes 10 compteurs avec la moyenne horaire la plus élevée et Les 10 compteurs ayant comptabilisés le plus de vélos. Les valeurs chiffrées des graphiques seront différentes de celles de la page en ligne, c’est normal, nous travaillons sur des données plus anciennes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{{% /box %}}\n{{% box status=“note” title=“Conseil” icon=“fa fa-comment” %}}\nPour obtenir un graphique ordonné du plus grand au plus petit, il faut avoir les données ordonnées du plus petit au\nplus grand. C’est bizarre mais c’est comme ça…\n{{% /box %}}\nOn peut remarquer plusieurs éléments problématiques (par exemple les labels) mais\naussi des éléments ne correspondant pas (les titres des axes, etc.) ou\nmanquants (le nom du graphique…)\nComme les graphiques produits par pandas suivent la logique très flexible\nde matplotlib, il est possible de les customiser. Cependant, c’est\nsouvent beaucoup de travail et il peut être préférable de directement\nutiliser seaborn, qui offre quelques arguments prêts à l’emploi."
  },
  {
    "objectID": "content/course/visualisation/matplotlib/index.html#utiliser-directement-seaborn",
    "href": "content/course/visualisation/matplotlib/index.html#utiliser-directement-seaborn",
    "title": "18  De beaux graphiques avec python: mise en pratique",
    "section": "18.2 Utiliser directement seaborn",
    "text": "18.2 Utiliser directement seaborn\nVous pouvez repartir des deux dataframes précédents. On va suppose qu’ils se\nnomment df1 et df2.\n{{% box status=“exercise” title=“Exercice” icon=“fas fa-pencil-alt” %}}\nExercice 2 : Un peu de style !\nIl y a plusieurs manières de faire un bar plot en seaborn. La plus flexible,\nc’est-à-dire celle qui permet le mieux d’interagir avec matplotlib est\ncatplot\n\nRéinitialiser l’index des df pour avoir une colonne ‘Nom du compteur’\nRefaire le graphique précédent avec la fonction catplot de seaborn. Pour\ncontrôler la taille du graphique vous pouvez utiliser les arguments height et\naspect.\nAjouter les titres des axes et le titre du graphique pour le premier graphique\n\n\n\n\n\n\n\n\n\n\n\nRefaites l’exercice avec la fonction sns.barplot.\n\n\n\n\n\n\n\n\n\n\n\ng.figure.get_figure().savefig('featured.png')\n\n\nEssayez de colorer en rouge l’axe des x. Vous pouvez pré-définir un\nstyle avec sns.set_style(\"ticks\", {\"xtick.color\": \"red\"})\n\n\n\n\n\n\n\n\n\n\n{{% /box %}}\n{{% box status=“exercise” title=“Exercice” icon=“fas fa-pencil-alt” %}}\nExercice 3 : Refaire les graphiques\n\nRefaire le graphique Les 10 compteurs ayant comptabilisé le plus de vélos\n\n\n\n\n\n\n\n\n\n\n\nLes graphiques qui suivent vont nécessiter un peu d’agilité dans la gestion des dates. Il faut en effet commencer par créer une variable temporelle (vous pouvez la nommer\ntimestamp) et la transformer en variable mensuelle (grâce à\ndt.to_period('M')) et l’appeler month. Vous pouvez essayer de le faire vous même ou cliquer\nci-dessous pour la solution.\n\n\n\nSolution\n\n\ndf['timestamp'] = pd.to_datetime(df['Date et heure de comptage'], format='%Y-%m-%dT%H:%M:%SZ', errors='coerce')\ndf['month'] = df['timestamp'].dt.to_period('M')\n\n\n\nRefaire le graphique Moyenne mensuelle des comptages vélos.\n\n\n\n\n\n\n\n\n\n\n\nRefaire le graphique Moyenne journalière des comptages vélos (créer d’abord une variable de jour avec .dt.day)\n\n\n\n\n\n\n\n\n\n\n\nRefaire le graphique Comptages vélo au cours des 7 derniers jours (de l’échantillon)\n\n\n\n\n\n\n\n\n\n\n{{% /box %}}"
  },
  {
    "objectID": "content/course/visualisation/matplotlib/index.html#des-graphiques-dynamiques-avec-plotly",
    "href": "content/course/visualisation/matplotlib/index.html#des-graphiques-dynamiques-avec-plotly",
    "title": "18  De beaux graphiques avec python: mise en pratique",
    "section": "18.3 Des graphiques dynamiques avec Plotly",
    "text": "18.3 Des graphiques dynamiques avec Plotly\nLe package Plotly est une surcouche à la librairie Javascript\nPlotly.js qui permet de créer et manipuler des objets graphiques de manière\ntrès flexible afin de produire des objets réactifs sans avoir à recourir\nà Javascript.\nLe point d’entrée recommandé est le module plotly.express\n(documentation ici) qui offre une arborescence\nriche mais néanmoins intuitive pour construire des graphiques\n(objets plotly.graph_objects.Figure) pouvant être modifiés a posteriori\nsi besoin (par exemple pour customiser les axes).\n\n18.3.1 Comment visualiser un graphique plotly ?\nDans un notebook Jupyter classique, les lignes suivantes de code permettent\nd’afficher le résultat d’une commande Plotly sous un bloc de code:\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected = True)\nPour JupyterLab, l’extension jupyterlab-plotly s’avère nécessaire:\njupyter labextension install jupyterlab-plotly\nPour les utilisateurs de python via l’excellent package R reticulate, il\nest possible d’écrire le résultats dans un fichier .html et d’utiliser\nhtmltools::includeHTML pour l’afficher via R Markdown (les utilisateurs\nde R trouveront bien-sûr une technique bien plus simple: utiliser\ndirectement le package R plotly…)\n\n\n18.3.2 Réplication de l’exemple précédent avec plotly\nLes modules suivants seront nécessaires pour construire des graphiques\navec plotly:\n\nimport plotly\nimport plotly.express as px\nfrom IPython.display import HTML #pour afficher les graphs\n# dans une cellule de notebook\n\n{{% box status=“exercise” title=“Exercice” icon=“fas fa-pencil-alt” %}}\nExercice 4 : Premier graphique avec plotly\nL’objectif est de reconstuire le premier diagramme en barre rouge avec plotly.\n\nRéalisez le graphique en utilisant la fonction adéquate avec plotly.express et…\n\n\nNe pas prendre le\nthème par défaut mais un à fond blanc, pour avoir un résultat ressemblant\nà celui proposé sur le site de l’open-data.\nPour la couleur rouge,\nvous pouvez utiliser l’argument color_discrete_sequence.\nNe pas oublier de nommer les axes\nPensez à la couleur du texte de l’axe inférieur\n\n\nTester un autre thème, à fond sombre. Pour les couleurs, faire un\ngroupe stockant les trois plus fortes valeurs puis les autres.\n\n{{% /box %}}\nLa première question permet de construire le graphique suivant:\nAlors qu’avec le thème sombre (question 2), on obtient :"
  },
  {
    "objectID": "content/course/visualisation/matplotlib/index.html#exercices-supplémentaires",
    "href": "content/course/visualisation/matplotlib/index.html#exercices-supplémentaires",
    "title": "18  De beaux graphiques avec python: mise en pratique",
    "section": "18.4 Exercices supplémentaires",
    "text": "18.4 Exercices supplémentaires\nPour ces exercices, il est recommandé de s’inspirer\ndes modèles présents dans la librairie\nde graphiques Python présentée\ndans https://www.python-graph-gallery.com/\n\n18.4.1 Les lollipop chart\nCet exercice permet de s’entraîner\nsur le fichier des naissances et des\ndécès de l’Insee. Il s’inspire d’une\nexcellente visualisation faite\npar Jean Dupin\nsur Twitter mettant en avant l’évolution,\nannée par année, des décomptes des\npersonnes nommées “Jean” parmi les\npersonnes nées ou décédées:\nL’animation de Jean Dupin\nest beaucoup plus raffinée que\ncelle que nous allons mettre en\noeuvre.\n\n\n18.4.2 Récupération des données\nLa récupération des données étant un peu complexe,\nle code est donné pour vous permettre de vous\nconcentrer sur l’essentiel (si vous\nvoulez vous exercer avec le package requests,\nessayez de le faire vous-même).\nLes données des décès sont disponibles de manière\nhistorique dans des zip pour chaque année.\n\nimport shutil\nimport requests\nimport zipfile\nimport os\nimport glob\nimport pandas as pd\n\ndef import_by_decade(decennie = 1970):\n\n    url = f\"https://www.insee.fr/fr/statistiques/fichier/4769950/deces-{decennie}-{decennie+9}-csv.zip\"\n\n    req = requests.get(url)\n\n    with open(f\"deces_{decennie}.zip\",'wb') as f:\n        f.write(req.content)\n\n    with zipfile.ZipFile(f\"deces_{decennie}.zip\", 'r') as zip_ref:\n        zip_ref.extractall(f\"deces_{decennie}\")\n\n    csv_files = glob.glob(os.path.join(f\"deces_{decennie}\", \"*.csv\"))\n\n    df = [pd.read_csv(f, sep = \";\", encoding=\"utf-8\").assign(annee = f) for f in csv_files]\n    df = pd.concat(df)\n    df[['nom','prenom']] = df['nomprenom'].str.split(\"*\", expand=True)\n    df['prenom'] = df['prenom'].str.replace(\"/\",\"\")\n    df['annee'] = df['annee'].str.rsplit(\"/\").str[-1].str.replace(\"(Deces_|.csv|deces-)\",\"\").astype(int)\n\n    shutil.rmtree(f\"deces_{decennie}\")    \n    os.remove(f\"deces_{decennie}.zip\")\n\n    return df\n\n\ndfs = [import_by_decade(d) for d in [1970, 1980, 1990, 2000, 2010]]\ndeces = pd.concat(dfs)\n\nLe fichier des naissances est plus simple à récupérer.\nVoici le code pour l’obtenir:\n\nyear = 2021\nurl_naissance = f\"https://www.insee.fr/fr/statistiques/fichier/2540004/nat{year}_csv.zip\"\n\nreq = requests.get(url_naissance)\n\nwith open(f\"naissance_{year}.zip\",'wb') as f:\n    f.write(req.content)\n\nwith zipfile.ZipFile(f\"naissance_{year}.zip\", 'r') as zip_ref:\n    zip_ref.extractall(f\"naissance_{year}\")\n\nnaissance = pd.read_csv(f\"naissance_{year}/nat{year}.csv\", sep = \";\")\nnaissance = naissance.dropna(subset = ['preusuel'] )\n\nOn peut enfin restructurer les DataFrames pour obtenir un\nseul jeu de données, en se restreignant aux “JEAN”:\n\njean_naiss = naissance.loc[naissance['preusuel'] == \"JEAN\"].loc[:, ['annais', 'nombre']]\njean_naiss = jean_naiss.rename({\"annais\": \"annee\"}, axis = \"columns\")\njean_naiss = jean_naiss.groupby('annee').sum().reset_index()\njean_deces = deces.loc[deces[\"prenom\"] == \"JEAN\"]\njean_deces = jean_deces.groupby('annee').size().reset_index()\njean_deces.columns = ['annee', \"nombre\"]\njean_naiss.columns = ['annee', \"nombre\"]\ndf = pd.concat(\n    [\n        jean_deces.assign(source = \"deces\"),\n        jean_naiss.assign(source = \"naissance\")\n    ])\ndf = df.loc[df['annee'] != \"XXXX\"]\ndf['annee']=df['annee'].astype(int)\ndf = df.loc[df['annee'] &gt; 1971]\n\ndf.head(3)\n\n\n\n\n\n\n\n\n\n\nannee\nnombre\nsource\n\n\n\n\n0\n1972\n3017\ndeces\n\n\n1\n1973\n3116\ndeces\n\n\n2\n1974\n3298\ndeces\n\n\n\n\n\n\n\n\n\n18.4.3 Représentation graphique\nVous pouvez vous aider du modèle présent\ndans https://www.python-graph-gallery.com\n{{% box status=“exercise” title=“Exercice”\nicon=“fas fa-pencil-alt” %}}\nPour commencer, on va se concentrer sur la\nproduction d’un seul graphique\n(décès ou naissance, vous choisissez)\n\nCréer un objet df_plot qui se restreint à une\nsource\nFixer une année sous le nom max_year (par exemple\nvotre année de naissance). Elle servira ensuite de paramètre\nà une fonction\nRestreindre df_plot aux années antérieures à max_year\nCréer une variable my_range fixant la séquence des années\nentre la plus petite année du dataset et max_year (inclus)\nCréer un array numpy qui vaut orange lorsque l’observation\nen question est max_year et skyblue sinon\nUtiliser les fonctions adéquates de matplotlib pour créer\nle lollipop chart\n{{% /box %}}\n\nA ce stade, vous devriez avoir une version fonctionnelle\nqui peut servir de\nbase à la généralisation.\n{{% box status=“exercise” title=“Exercice”\nicon=“fas fa-pencil-alt” %}}\n\nA partir du code précédent, généraliser en utilisant\nune boucle for à partir du résultat de\nenumerate(df.source.value_counts().index.values) pour\ncréer un graphique pour une année donnée de maxyear.\nAvant cette boucle, ne pas oublier de créer un objet\nmatplotlib vide à remplir dans la boucle\n\nfig, axes = plt.subplots(1, 2, sharey = True)\n\nEncapsuler ce code dans une fonction qui\nprend en argument un DataFrame et une\nannée max_year\n\nVoici un exemple d’output pour max_year = 2010:\n\n\n\n\n\n\n\n\nPour créer une animation, on propose\nd’utiliser la solution présentée\ndans https://www.python-graph-gallery.com/animation/.\net qui nécessite le logiciel imagemagick.\nSauvegarder chaque itération dans un fichier\ndont le nom a la structure figure_{year}.png.\n\nEnfin, pour animer les images, on peut utiliser\nla librairie imageio:\n\nimport glob\nimport imageio.v2 as imageio\n#os.system(\"convert -delay 15 figure_*.png animation.gif\")\n\nfilenames=glob.glob(\"figure_*.png\")\nfilenames.sort()\n\nwith imageio.get_writer('animation.gif', mode='I') as writer:\n    for filename in filenames:\n        image = imageio.imread(filename)\n        writer.append_data(image)\n\nL’animation obtenue est la suivante:\n\n\n\nAnimation\n\n\n{{% /box %}}"
  },
  {
    "objectID": "content/course/visualisation/matplotlib/index.html#ressources-supplémentaires",
    "href": "content/course/visualisation/matplotlib/index.html#ressources-supplémentaires",
    "title": "18  De beaux graphiques avec python: mise en pratique",
    "section": "18.5 Ressources supplémentaires",
    "text": "18.5 Ressources supplémentaires\n\nUn post de datawrapper sur les textes dans les visualisations de données"
  },
  {
    "objectID": "content/course/visualisation/maps/index.html#première-carte-avec-lapi-matplotlib-de-geopandas",
    "href": "content/course/visualisation/maps/index.html#première-carte-avec-lapi-matplotlib-de-geopandas",
    "title": "19  De belles cartes avec python: mise en pratique",
    "section": "19.1 Première carte avec l’API matplotlib de geopandas",
    "text": "19.1 Première carte avec l’API matplotlib de geopandas\n\n\n Exercice 1: Importer les données\nImporter les données de compteurs de vélos en deux temps.\n\nD’abord, les comptages peuvent être trouvés à l’adresse https://minio.lab.sspcloud.fr/projet-formation/diffusion/python-datascientist/bike.csv. ⚠️ Il s’agit de données\ncompressées au format gzip, il faut donc utiliser l’option compression. Nommer cet objet comptages.\nImporter les données de localisation des compteurs à partir de l’url https://parisdata.opendatasoft.com/explore/dataset/comptage-velo-compteurs/download/?format=geojson&timezone=Europe/Berlin&lang=fr. Nommer cet objet compteurs.\nFaire attention à deux valeurs aberrantes. Utiliser\nla fonctionalité str.contains pour exclure les\nobservations contenant “Bike IN” ou “Bike OUT”\ndans la variable\nnom_compteur\nOn va également utiliser les données d’arrondissements de la ville de Paris. Importer ces données depuis https://opendata.paris.fr/explore/dataset/arrondissements/download/?format=geojson&timezone=Europe/Berlin&lang=fr. Nommer cet objet arrondissements.\nUtiliser la méthode plot pour représenter les localisations des compteurs dans l’espace. C’est, on peut l’avouer, peu informatif sans apport extérieur. Il va donc falloir travailler un peu l’esthétique\n\n\n\n\ncompteurs = compteurs.loc[~compteurs[\"nom_compteur\"].str.contains(r\"(Bike IN|Bike OUT)\")]\n\n/tmp/ipykernel_3454/3602001210.py:1: UserWarning:\n\nThis pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n\n\n\n\n\n Warning\nOn serait tenté de faire un merge de la base compteurs et comptages.\nEn l’occurrence, il s’agirait d’un produit cartésien puisqu’il s’agit de faire exploser la base spatiale.\nAvec des données spatiales, c’est souvent une très mauvaise idée. Cela duplique les points, créant des difficultés à représenter les données mais aussi ralentit les calculs.\nSauf à utiliser la méthode dissolve (qui va agréger k fois la même géométrie…), les géométries sont perdues lorsqu’on effectue des groupby.\n\n\nMaintenant, tout est prêt pour une première carte. matplotlib fonctionne selon\nle principe des couches. On va de la couche la plus lointaine à celle le plus\nen surface. L’exception est lorsqu’on ajoute un fond de carte contextily via\nctx.add_basemap: on met cet appel en dernier.\n\n\n Exercice 2: Première carte\nReprésenter une carte des compteurs avec le fonds de carte des arrondissements\n\nFaire attention à avoir des arrondissements dont l’intérieur est transparent (argument à utiliser: facecolor).\nFaire des bordures d’arrondissements noires et affichez les compteurs en rouge.\nPour obtenir un graphique plus grand, vous pouvez utiliser l’argument figsize = (10,10).\nPour les localisations, les points doivent être rouges en étant plus transparent au centre (argument à utiliser: alpha)\n\n\n\nVous devriez obtenir cette carte:\n\n\n\n\n\n\n\n\n\n\n\n Exercice 3 : Ajouter un fonds de carte avec contextily\nRepartir de la carte précédente.\n\nUtiliser ctx.add_basemap pour ajouter un fonds de carte. Pour ne pas afficher les axes, vous pouvez utiliser ax.set_axis_off().\n\n:warning: Par défaut, contextily désire un système de projection (crs) qui est le Web Mercator (epsg: 3857). Il faut changer la valeur de l’argument crs.\n:warning: Avec les versions anciennes des packages, il faut utiliser .to_string sur un objet CRS pour qu’il soit reconnu par contextily. Sur des versions récentes, la valeur numérique du code EPSG est suffisante.\n\nTrouver un fonds de carte plus esthétique, qui permette de visualiser les grands axes, parmi ceux possibles. Pour tester l’esthétique, vous pouvez utiliser cet url. La documentation de référence sur les tuiles disponibles est ici\n\n\n\n\nax.get_figure()\n\n\n\n\n\n\n\n\n\nax.get_figure()\n\n\n\n\n\n\n\n\n\nax.get_figure().savefig(\"featured.png\")\n\nLe principe de la heatmap est de construire, à partir d’un nuage de point bidimensionnel, une distribution 2D lissée. La méthode repose sur les estimateurs à noyaux qui sont des méthodes de lissage local.\n\n\n Exercice 3 : Ajouter un fonds de carte avec contextily\nPour le moment, la fonction geoplot.kdeplot n’incorpore pas toutes les fonctionalités de seaborn.kdeplot. Pour être en mesure de construire une heatmap avec des données pondérées (cf. cette issue dans le dépôt seaborn), il y a une astuce. Il faut simuler k points de valeur 1 autour de la localisation observée. La fonction ci-dessous, qui m’a été bien utile, est pratique\n\nimport numpy as np\ndef expand_points(shapefile,\n                  index_var = \"grid_id\",\n                  weight_var = 'prop',\n                  radius_sd = 100,\n                  crs = 2154):\n    \"\"\"\n    Multiply number of points to be able to have a weighted heatmap\n    :param shapefile: Shapefile to consider\n    :param index_var: Variable name to set index\n    :param weight_var: Variable that should be used\n    :param radius_sd: Standard deviation for the radius of the jitter\n    :param crs: Projection system that should be used. Recommended option\n      is Lambert 93 because points will be jitterized using meters\n    :return:\n      A geopandas point object with as many points by index as weight\n    \"\"\"\n\n    shpcopy = shapefile\n    shpcopy = shpcopy.set_index(index_var)\n    shpcopy['npoints'] = np.ceil(shpcopy[weight_var])\n    shpcopy['geometry'] = shpcopy['geometry'].centroid\n    shpcopy['x'] = shpcopy.geometry.x\n    shpcopy['y'] = shpcopy.geometry.y\n    shpcopy = shpcopy.to_crs(crs)\n    shpcopy = shpcopy.loc[np.repeat(shpcopy.index.values, shpcopy.npoints)]\n    shpcopy['x'] = shpcopy['x'] + np.random.normal(0, radius_sd, shpcopy.shape[0])\n    shpcopy['y'] = shpcopy['y'] + np.random.normal(0, radius_sd, shpcopy.shape[0])\n\n    gdf = gpd.GeoDataFrame(\n        shpcopy,\n        geometry = gpd.points_from_xy(shpcopy.x, shpcopy.y),\n        crs = crs)\n\n    return gdf\n\n\n\n\n\n Exercice 4 : Data cleaning avant de pouvoir faire une heatmap\n\nCalculer le trafic moyen, pour chaque station, entre 7 heures et 10 heures (bornes incluses) et nommer cet objet df1. Faire la même chose, en nommant df2, pour le trafic entre 17 et 20 heures (bornes incluses)\nNous allons désormais préparer les données de manière à faire une heatmap. Après avoir compris ce que permet de faire la fonction expand_points ci-dessus, créer une fonction explode_data qui suive les étapes suivantes.\n\n\nConvertir un DataFrame dans le système de projection Lambert 93 (epsg: 2154)\nAppliquer expand_points aux noms de variable adéquats. Vous pouvez fixer la valeur de radius_sd à 100.\nReconvertir l’output au format WGS84 (epsg: 4326)\n\n\nAppliquer cette fonction à df1 et df2\n\n\n\n\n\n Exercice 5 : Heatmap, enfin !\nReprésenter, pour ces deux moments de la journée, la heatmap du trafic de vélo avec geoplot.kdeplot. Pour cela :\n\nAppliquer la fonction geoplot.kdeplot avec comme consignes :\n\nd’utiliser les arguments shade=True et shade_lowest=True pour colorer l’intérieur des courbes de niveaux obtenues ;\nd’utiliser une palette de couleur rouge avec une transparence modérée (alpha = 0.6)\nd’utiliser l’argument clip pour ne pas déborder hors de Paris (en cas de doute, se référer à l’aide de geoplot.kdeplot)\nL’argument bw (pour bandwidth) détermine le plus ou moins fort lissage spatial. Vous pouvez partir d’un bandwidth égal à 0.01 et le faire varier pour voir l’effet sur le résultat\n\nNe pas oublier d’ajouter les arrondissements. Avec geoplot, il faut utiliser geoplot.polyplot.\n\n\n\n\nax.get_figure()"
  },
  {
    "objectID": "content/course/visualisation/maps/index.html#des-cartes-réactives-grâce-à-folium",
    "href": "content/course/visualisation/maps/index.html#des-cartes-réactives-grâce-à-folium",
    "title": "19  De belles cartes avec python: mise en pratique",
    "section": "19.2 Des cartes réactives grâce à folium",
    "text": "19.2 Des cartes réactives grâce à folium\nDe plus en plus de données de visualisation reposent sur la cartographie réactive. Que ce soit dans l’exploration des données ou dans la représentation finale de résultats, la cartographie réactive est très appréciable.\nfolium offre une interface très flexible et très facile à prendre à main. Les cartes sont construites grâce à la librairie JavaScript Leaflet.js mais, sauf si on désire aller loin dans la customisation du résultat, il n’est pas nécessaire d’avoir des notions dans le domaine.\nUn objet folium se construit par couche. La première est l’initialisation de la carte. Les couches suivantes sont les éléments à mettre en valeur. L’initialisation de la carte nécessite la définition d’un point central (paramètre location) et d’un zoom de départ (zoom_start). Plutôt que de fournir manuellement le point central et le zoom on peut :\n\nDéterminer le point central en construisant des colonnes longitudes et latitudes et en prenant la moyenne de celles-ci ;\nUtiliser la méthode fit_bounds qui cale la carte sur les coins sud-ouest et nord-est. En supposant que la carte s’appelle m, on fera m.fit_bounds([sw, ne])\n\nLe bout de code suivant permet de calculer le centre de la carte\n\ncompteurs['lon'] = compteurs.geometry.x\ncompteurs['lat'] = compteurs.geometry.y\ncenter = compteurs[['lat', 'lon']].mean().values.tolist()\nprint(center)\n\n[48.8546401015625, 2.349262265625]\n\n\nAlors que le code suivant permet de calculer les coins:\n\nsw = compteurs[['lat', 'lon']].min().values.tolist()\nne = compteurs[['lat', 'lon']].max().values.tolist()\nprint(sw, ne)\n\n[48.81964, 2.26526] [48.898946, 2.41143]\n\n\n\n\n Hint\nSi un fond gris s’affiche, c’est qu’il y a un problème de localisation ou d’accès à internet. Pour le premier cas, cela provient généralement d’un problème de projection ou d’une inversion des longitudes et latitudes.\nLes longitudes représentent les x (axe ouest-est) et les latitudes y (axe sud-nord). De manière contrintuitive, folium attend qu’on lui fournisse les données sous la forme [latitude, longitude] donc [y,x]\n\n\n\n\n Exercice 6 : Visualiser la localisation des stations\n\nCalculer le centre centerde la carte des données compteurs. Il s’obtient en agrègeant l’ensemble des géométries, calculant le centroid et récupèrant la valeur sous forme de liste. Avec une logique similaire, calculez les bornes du sud-ouest sw et du nord-est ne de la carte.\nReprésenter la localisation des stations en utilisant un zoom optimal.\n\n\n\n\n# Afficher la carte\nm\n\n\n\n Exercice 7: Représenter les stations\nFaire la même carte, avec des ronds proportionnels au nombre de comptages :\n\nPour le rayon de chaque cercle, vous pouvez appliquer la règle 500*x/max(x) (règle au doigt mouillé)\nVous pouvez réduire la taille des bordures de cercle avec l’option weight = 1 et fixer la couleur avec color = 'grey'\n(Optionnel) Colorer en rouge les 10 plus grosses stations. L’opacité étant, par défaut, un peu faible, le paramètre fill_opacity = 0.4 améliore le rendu.\n(Optionnel) Afficher, en supplément du nom du compteur lorsqu’on clique, la valeur du comptage en revenant à la ligne\n\n\n\nLa carte obtenue doit ressembler à la suivante:\n\n# Afficher la carte\nm"
  },
  {
    "objectID": "content/course/visualisation/maps/index.html#exercices-supplémentaires",
    "href": "content/course/visualisation/maps/index.html#exercices-supplémentaires",
    "title": "19  De belles cartes avec python: mise en pratique",
    "section": "19.3 Exercices supplémentaires",
    "text": "19.3 Exercices supplémentaires\n\n19.3.1 Densité de population dans la petite couronne parisienne\nPour cet exercice, le package cartiflette\nva être pratique pour récupérer un fonds de carte mélangeant arrondissements\nparisiens et communes dans les autres villes.\nNous allons privilégier une carte à ronds proportionnels (bubble map)\naux cartes chorolèpthes qui trompent\nl’oeil. Les instructions d’installation du package topojson\nsont disponibles dans la partie manipulation\n\n\n Exercice: bubble map de densité des populations\n\nRécupérer le fond de carte des départements 75, 92, 93 et 94\navec cartiflette. Pour cela, utiliser download_vectorfile_url_all\ndepuis cartiflette.s3 en fixant l’option level à COMMUNE_ARRONDISSEMENT.\nNommer cet objet df.\nAfin que les calculs ultérieurs de surface ne soient pas faussés,\nassurez-vous que les données sont en Lambert 93 en reprojetant\nnos contours (code EPSG: 2154).\nCréer un objet departements avec dissolve pour également disposer\nd’un fond de carte des départements\nCréer une variable surface et utilisant la méthode area. L’unité\ndoit être le km², il faut donc diviser par \\(10^6\\)\nCréer une variable densite\nUtiliser pd.cut avec les seuils 5000, 15000 et 30000 personnes\npar km². Vous pouvez utiliser l’option label pour dénommer les tranches\nCréer un GeoDataFrame de points en utilisant la méthode centroid. Celui-ci\nnous servira à localiser le centre de nos ronds.\nReprésenter la densité communale sous forme de carte avec ronds proportionnels.\nVous pouvez utiliser la variable créée à la question 5 pour les couleurs.\n\n\n\nLa carte obtenue devrait ressembler à celle-ci:\n\n\nText(0.3, 0.15, 'Source: IGN - AdminExpress')"
  },
  {
    "objectID": "content/course/modelisation/index.html#la-modélisation-une-approche-au-coeur-de-la-statistique",
    "href": "content/course/modelisation/index.html#la-modélisation-une-approche-au-coeur-de-la-statistique",
    "title": "Partie 3: modéliser",
    "section": "La modélisation, une approche au coeur de la statistique",
    "text": "La modélisation, une approche au coeur de la statistique\nUn modèle statistique\nest une représentation simplifiée et structurée d’un phénomène réel,\nconstruite à partir d’observations regroupées dans un ensemble partiel de données.\nUn modèle vise à capturer les relations et les schémas sous-jacents au sein de ces données, permettant ainsi de formuler des hypothèses, d’effectuer des prédictions et d’extrapoler des conclusions au-delà\nde l’ensemble de données mesurées.\nLes modèles statistiques fournissent ainsi un cadre analytique pour explorer, comprendre et interpréter les informations contenues dans les données.\nDans le domaine de la recherche économique, ils peuvent servir à\nassocier certains paramètres structurants des modèles de comportement\néconomique à des valeurs quantitatives.\nLes modèles statistiques, comme les modèles économiques\nprésentent néanmoins toujours une part d’irréalisme (Friedman 1953; Salmon 2010)\net accepter de manière trop litérale les implications d’un modèle, même s’il\na de bonnes performances prédictives, peut être dangereux et relever d’un biais\nscientiste. On sélectionne plutôt le moins mauvais modèle\nque le vrai processus générateur des données.\nReprésenter la réalité sous la forme d’un modèle est un principe à la\nbase de la statistique comme discipline scientifique et ayant des\napplications dans de nombreux champs disciplinaires: économie,\nsociologie, géographique, biologie, physique, etc.\nSelon les disciplines, le nom donné peut varier mais on retrouve\nrégulièrement la même approche scientifique: le modélisateur\nconstruit des relations entre plusieurs variables théoriques\nayant des contreparties empiriques afin d’expliquer tel ou tel\nprocessus.\nDans l’enseignement de l’ENSAE ce type d’approche empirique se retrouve\nprincipalement dans deux types d’approches: le machine learning et\nl’économétrie. La différence est certes\nsémantique - la régression linéaire peut être considérée comme une\ntechnique de machine learning ou d’économétrie - mais elle est\négalement conceptuelle:\n\nDans le domaine du machine learning,\nla structure imposée par le modélisateur est minimale et ce sont plutôt\nles algorithmes qui, sur des critères de performance statistique, vont\namener à choisir une loi mathématique qui correspond au mieux aux données ;\nEn économétrie,\nles hypothèses de structure des lois sont plus fortes (même dans un cadre semi ou non-paramétrique) et sont plus souvent imposées\npar le modélisateur.\n\nDans cette partie du cours, nous allons principalement\nparler de machine learning car il s’agit d’une perspective\nplus opérationnel que l’économétrie qui est plus directement associée\nà des concepts statistiques complexes comme la théorie asymptotique.\nL’adoption du machine learning dans la littérature économique a été longue\ncar la structuration des données est souvent le\npendant empirique d’hypothèses théoriques sur le comportement des acteurs ou des marchés (Athey and Imbens 2019; Charpentier, Flachaire, and Ly 2018).\nPour caricaturer, l’économétrie s’attacherait à comprendre la causalité de certaines variables sur une autre.\nCela implique que ce qui intéresse l’économètre\nest principalement de l’estimation des paramètres (et l’incertitude\nsur l’estimation de ceux-ci) qui permettent de quantifier l’effet d’une\nvariation d’une variable sur une autre.\nToujours pour caricaturer,\nle machine learning se focaliserait\nsur un simple objectif prédictif en exploitant les relations de corrélations entre les variables.\nDans cette perspective, l’important n’est pas la causalité mais le fait qu’une variation\nde \\(x\\)% d’une variable permette d’anticiper un changement de \\(\\beta x\\) de la variable\nd’intérêt ; peu importe la raison.\nMullainathan and Spiess (2017) ont ainsi, pour simplifier, proposé la différence fondamentale qui\nsuit: l’économétrie se préoccupe de \\(\\widehat{\\beta}\\) là où le machine learning\nse focalise sur \\(\\widehat{y}\\). Les deux sont bien-sûr reliés dans un cadre\nlinéaire mais cette différence d’approche a des implications importantes\nsur la structure des modèles étudiés, notamment leur parcimonie3."
  },
  {
    "objectID": "content/course/modelisation/index.html#quelques-définitions",
    "href": "content/course/modelisation/index.html#quelques-définitions",
    "title": "Partie 3: modéliser",
    "section": "Quelques définitions",
    "text": "Quelques définitions\nDans cette partie du cours nous allons employer un certain nombre\nde termes devenus familiers aux praticiens du machine learning\nmais qui méritent d’être explicités.\n\nMachine learning et deep learning\nJusqu’à présent nous avons beaucoup utilisé, sans le définir, le\nconcept de machine learning, dont la traduction française est\napprentissage automatique mais le terme anglo-saxon est suffisamment\nutilisé pour être considéré comme standard.\nLe machine learning est un ensemble de techniques algorithmiques\nqui permettent aux ordinateurs d’apprendre, à partir d’exemples, à ajuster un modèle\nsans avoir explicitement défini celui-ci. A partir d’algorithmes itératifs et d’une\nmétrique de performance, des règles de classification ou de prédiction vont permettre\nde mettre en relation des caractéristiques (features) avec une variable d’intérêt (label)4.\nDe nombreux algorithmes existent et se distinguent sur la manière d’introduire une structure plus ou\nmoins formelle dans la relation entre les variables observées. Nous n’allons voir que quelques\nuns de ces algorithmes: support vector machine (SVM), régression logistique, arbres de décision, forêts\naléatoires, etc. Simples à mettre en oeuvre grâce à la librairie Scikit-Learn, ils permettront\ndéjà de comprendre la démarche originale du machine learning que vous pourrez approfondir\nultérieurement.\nAu sein de la grande famille des algorithmes de machine learning, tendent de plus à plus à devenir\nautonomes les techniques de réseaux de neurone. Les techniques qui s’appuient sur les réseaux de neurone sont regroupés\ndans une famille qu’on\nappelle deep learning (apprentissage profond en Français).\nCes réseaux sont inspirés du fonctionnement du cerveau humain et sont composés de nombreuses couches de neurones interconnectés.\nLa structure canonique bien connue est illustrée dans la Figure 1.\nLe deep learning est intéressant pour créer des modèles capables d’apprendre de représentations\nde données complexes et abstraites à partir de données brutes,\nce qui évite parfois la complexe tâche de définir manuellement des caractéristiques spécifiques à cibler.\nLes champs de l’analyse d’image (computer vision) ou du traitement du langage naturel sont les principaux\ncas d’application de ces méthodes.\n\n\n\n\n\n\n\n\n\nFigure 1: Exemple de structure d’un réseau de neurone (source: lebigdata.fr)\n\n\nNous n’allons pas vraiment parler dans ce cours de deep learning car ces modèles, pour être pertinents, nécessitent\nsoit des données structurées d’un volume important (ce qui est rarement disponible\nen open data) soit des cas d’usage spécifiques, plus avancés que ne le permet\nun cours d’introduction. L’organisation HuggingFace, créatrice de la\nplateforme du même nom facilitant la réutilisation de modèles de deep learning\npropose d’excellents cours sur le sujet, notamment sur\nle traitement du langage naturel (NLP).\nNous ferons traitement du langage naturel dans la prochaine partie de ce cours mais\nde manière plus modeste en revenant sur les concepts nécessaires avant de mettre en oeuvre\nune modélisation sophistiquée du langage.\n\n\nApprentissage supervisé ou non supervisé\nUne ligne de clivage importante entre les méthodes à mettre en oeuvre est le fait d’observer ou non\nle label (la variable \\(y\\)) qu’on désire modéliser.\nPrenons par exemple un site de commerce qui dispose\nd’informations sur ses clients comme l’âge, le sexe, le lieu de résidence.\nCe site peut désirer\nexploiter cette information de différentes manières pour modéliser le comportement d’achat.\nEn premier lieu, ce site peut désirer\nanticiper le volume d’achat d’un nouveau client ayant certaines caractéristiques.\nDans ce cas, il est possible d’utiliser les montants dépensés par d’autres clients en fonction de leurs\ncaractéristiques. L’information pour notre nouveau client n’est pas mesurée mais elle peut s’appuyer\nsur un ensemble d’observations de la même variable.\nMais il est tout à fait possible d’entraîner un modèle sur un label qu’on ne mesure pas, en supposant\nqu’il fasse sens. Par exemple notre site de commerce peut désirer déterminer, en fonction des\ncaractéristiques de notre nouveau client et de sa clientèle existante, s’il appartient à tel ou\ntel groupe de consommateurs: les dépensiers, les économes… Bien sûr on ne sait jamais a priori\nà quel groupe appartient un consommateur mais le rapprochement entre consommateurs ayant un comportement\nsimilaire permettra de donner du sens à cette catégorie. Dans ce cas, l’algorithme apprendra à reconnaître\nquelles caractéristiques sont structurantes dans la constitution de groupes au comportement similaire et\npermettra d’associer tout nouveau consommateur à un groupe.\nCes deux exemples illustrent l’approche différente selon qu’on essaie de construire des modèles\nsur un label observé ou non. Cela constitue même l’une des dualités fondamentale dans les\ntechniques de machine learning:\n\nApprentissage supervisé : la valeur cible est connue et peut-être utilisée pour évaluer la qualité d’un modèle ;\nApprentissage non supervisé : la valeur cible est inconnue et ce sont des critères statistiques qui vont amener\nà sélectionner la structure de données la plus plausible.\n\nCette partie du cours illustrera ces deux approches de manière différente à partir du même\njeu de données, les résultats des élections américaines.\nDans le cas de l’apprentissage supervisé, nous chercherons à modéliser directement\nle résultat des candidats aux élections (soit le score soit le gagnant). Dans\nle cas de l’apprentissage non supervisé, nous essaierons de regrouper les\nterritoires au comportement de vote similaire en fonction de facteurs\nsocio-démographiques.\n\n\nClassification et régression\nUne deuxième dualité fondamentale qui est déterminante dans le choix de la méthode de machine learning\nà mettre en oeuvre est la nature du label. S’agit-il d’une variable continue ou d’une variable\ndiscrète c’est-à-dire prenant un nombre limité de modalités ?\nCette différence de nature entre les données amène à distinguer deux types d’approche:\n\nDans les problématiques de classification, où notre label \\(y\\) a un nombre fini de valeurs5,\non cherche à prédire dans quelle classe ou à quel groupe il est possible de rattacher nos données.\nPar exemple, si vous prenez du café le matin, faites-vous parti du groupe des personnes ronchon au lever ?\nLes métriques de performance utilisent généralement la proportion de bonnes ou mauvaises classification\npour estimer la qualité d’un modèle.\nDans les problématiques de régression, où notre label est une grandeur numérique, on\ncherche à prédire directement la valeur de notre variable dans le modèle. Par exemple, si vous\navez tel ou tel âge, quel est votre dépense quotidienne en fast food. Les métriques\nde performance sont généralement des moyennes plus ou moins sophistiquées d’écarts entre\nla prédiction et la valeur observée.\n\nEn résumé, l’aide-mémoire suivante, issue de l’aide de Scikit-Learn, peut déjà donner de premiers enseignements sur les différentes familles de modèles:\n\n\n\n\n\n\n\n\n\nFigure 2: Une cheatsheet des algorithmes disponibles dans Scikit-Learn"
  },
  {
    "objectID": "content/course/modelisation/index.html#données",
    "href": "content/course/modelisation/index.html#données",
    "title": "Partie 3: modéliser",
    "section": "Données",
    "text": "Données\nLa plupart des exemples de cette partie s’appuient sur les résultats des\nélections US 2020 au niveau comtés. Plusieurs bases sont utilisées pour\ncela:\n\nLes données électorales sont une reconstruction à partir des données du MIT election lab\nproposées sur Github par tonmcg\nou directement disponibles sur le site du MIT Election Lab\nLes données socioéconomiques (population, données de revenu et de pauvreté,\ntaux de chômage, variables d’éducation) proviennent de l’USDA (source)\nLe shapefile vient des données du Census Bureau. Le fichier peut\nêtre téléchargé directement depuis cet url:\nhttps://www2.census.gov/geo/tiger/GENZ2019/shp/cb_2019_us_county_20m.zip\n\nLe code pour construire une base unique à partir de ces sources diverses\nest disponible ci-dessous :\n\n\nimport urllib\nimport urllib.request\nimport os\nimport zipfile\nfrom urllib.request import Request, urlopen\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\n\ndef download_url(url, save_path):\n    with urllib.request.urlopen(url) as dl_file:\n        with open(save_path, 'wb') as out_file:\n            out_file.write(dl_file.read())\n\n\ndef create_votes_dataframes():\n    \n  Path(\"data\").mkdir(parents=True, exist_ok=True)\n  \n  \n  download_url(\"https://www2.census.gov/geo/tiger/GENZ2019/shp/cb_2019_us_county_20m.zip\", \"data/shapefile\")\n  with zipfile.ZipFile(\"data/shapefile\", 'r') as zip_ref:\n      zip_ref.extractall(\"data/counties\")\n  \n  shp = gpd.read_file(\"data/counties/cb_2019_us_county_20m.shp\")\n  shp = shp[~shp[\"STATEFP\"].isin([\"02\", \"69\", \"66\", \"78\", \"60\", \"72\", \"15\"])]\n  shp\n  \n  df_election = pd.read_csv(\"https://raw.githubusercontent.com/tonmcg/US_County_Level_Election_Results_08-20/master/2020_US_County_Level_Presidential_Results.csv\")\n  df_election.head(2)\n  population = pd.read_excel(\"https://www.ers.usda.gov/webdocs/DataFiles/48747/PopulationEstimates.xls?v=290.4\", header = 2).rename(columns = {\"FIPStxt\": \"FIPS\"})\n  education = pd.read_excel(\"https://www.ers.usda.gov/webdocs/DataFiles/48747/Education.xls?v=290.4\", header = 4).rename(columns = {\"FIPS Code\": \"FIPS\", \"Area name\": \"Area_Name\"})\n  unemployment = pd.read_excel(\"https://www.ers.usda.gov/webdocs/DataFiles/48747/Unemployment.xls?v=290.4\", header = 4).rename(columns = {\"fips_txt\": \"FIPS\", \"area_name\": \"Area_Name\", \"Stabr\": \"State\"})\n  income = pd.read_excel(\"https://www.ers.usda.gov/webdocs/DataFiles/48747/PovertyEstimates.xls?v=290.4\", header = 4).rename(columns = {\"FIPStxt\": \"FIPS\", \"Stabr\": \"State\", \"Area_name\": \"Area_Name\"})\n  \n  \n  dfs = [df.set_index(['FIPS', 'State']) for df in [population, education, unemployment, income]]\n  data_county = pd.concat(dfs, axis=1)\n  df_election = df_election.merge(data_county.reset_index(), left_on = \"county_fips\", right_on = \"FIPS\")\n  df_election['county_fips'] = df_election['county_fips'].astype(str).str.lstrip('0')\n  shp['FIPS'] = shp['GEOID'].astype(str).str.lstrip('0')\n  votes = shp.merge(df_election, left_on = \"FIPS\", right_on = \"county_fips\")\n  \n  req = Request('https://dataverse.harvard.edu/api/access/datafile/3641280?gbrecs=false')\n  req.add_header('User-Agent', 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:77.0) Gecko/20100101 Firefox/77.0')\n  content = urlopen(req)\n  df_historical = pd.read_csv(content, sep = \"\\t\")\n  #df_historical = pd.read_csv('https://dataverse.harvard.edu/api/access/datafile/3641280?gbrecs=false', sep = \"\\t\")\n  \n  df_historical = df_historical.dropna(subset = [\"FIPS\"])\n  df_historical[\"FIPS\"] = df_historical[\"FIPS\"].astype(int)\n  df_historical['share'] = df_historical['candidatevotes']/df_historical['totalvotes']\n  df_historical = df_historical[[\"year\", \"FIPS\", \"party\", \"candidatevotes\", \"share\"]]\n  df_historical['party'] = df_historical['party'].fillna(\"other\")\n  \n  df_historical_wide = df_historical.pivot_table(index = \"FIPS\", values=['candidatevotes',\"share\"], columns = [\"year\",\"party\"])\n  df_historical_wide.columns = [\"_\".join(map(str, s)) for s in df_historical_wide.columns.values]\n  df_historical_wide = df_historical_wide.reset_index()\n  df_historical_wide['FIPS'] = df_historical_wide['FIPS'].astype(str).str.lstrip('0')\n  votes['FIPS'] = votes['GEOID'].astype(str).str.lstrip('0')\n  votes = votes.merge(df_historical_wide, on = \"FIPS\")\n  votes[\"winner\"] =  np.where(votes['votes_gop'] &gt; votes['votes_dem'], 'republican', 'democrats') \n\n  return votes\n\n\nCette partie n’est absolument pas exhaustive. Elle constitue un point\nd’entrée dans le sujet à partir d’une série d’exemples sur un fil rouge.\nDe nombreux modèles plus appronfondis, que ce soit en économétrie ou en machine learning\nmériteraient d’être évoqués. Pour les personnes désirant en savoir plus sur les\nmodèles économétriques, qui seront moins évoqués que ceux de machine learning\nje recommande la lecture de Turrell and contributors (2021)."
  },
  {
    "objectID": "content/course/modelisation/index.html#références",
    "href": "content/course/modelisation/index.html#références",
    "title": "Partie 3: modéliser",
    "section": "Références",
    "text": "Références\n\n\nArcep. 2019. “L’empreinte Carbone Du Numérique.” Rapport de l’Arcep.\n\n\nAthey, Susan, and Guido W Imbens. 2019. “Machine Learning Methods That Economists Should Know About.” Annual Review of Economics 11: 685–725.\n\n\nCharpentier, Arthur, Emmanuel Flachaire, and Antoine Ly. 2018. “Econometrics and Machine Learning.” Economie Et Statistique 505 (1): 147–69.\n\n\nFriedman, Milton. 1953. “The Methodology of Positive Economics.” In Essays in Positive Economics. Chicago: The University of Chicago Press.\n\n\nIzsak, Peter, Moshe Berchansky, and Omer Levy. 2021. “How to Train BERT with an Academic Budget.” https://arxiv.org/abs/2104.07705.\n\n\nMullainathan, Sendhil, and Jann Spiess. 2017. “Machine Learning: An Applied Econometric Approach.” Journal of Economic Perspectives 31 (2): 87–106. https://doi.org/10.1257/jep.31.2.87.\n\n\nSalmon, Pierre. 2010. “Le Problème Du réalisme Des Hypothèses En économie Politique.”\n\n\nStrubell, Emma, Ananya Ganesh, and Andrew McCallum. 2019. “Energy and Policy Considerations for Deep Learning in NLP.” https://arxiv.org/abs/1906.02243.\n\n\nTurrell, Arthur, and contributors. 2021. Coding for Economists. Online. https://aeturrell.github.io/coding-for-economists."
  },
  {
    "objectID": "content/course/modelisation/0_preprocessing/index.html#construction-de-la-base-de-données",
    "href": "content/course/modelisation/0_preprocessing/index.html#construction-de-la-base-de-données",
    "title": "20  Préparation des données pour construire un modèle",
    "section": "20.1 Construction de la base de données",
    "text": "20.1 Construction de la base de données\nLes sources de données étant diverses, le code qui construit la base finale est directement fourni. Le travail de construction d’une base unique\nest un peu fastidieux mais il s’agit d’un bon exercice, que vous pouvez tenter,\npour réviser pandas :\n\n\n Exercice 1 : Importer les données des élections US\nCet exercice est OPTIONNEL\n\nTélécharger et importer le shapefile depuis ce lien\nExclure les Etats suivants: “02”, “69”, “66”, “78”, “60”, “72”, “15”\nImporter les résultats des élections depuis ce lien\nImporter les bases disponibles sur le site de l’USDA en faisant attention à renommer les variables de code FIPS de manière identique\ndans les 4 bases\nMerger ces 4 bases dans une base unique de caractéristiques socioéconomiques\nMerger aux données électorales à partir du code FIPS\nMerger au shapefile à partir du code FIPS. Faire attention aux 0 à gauche dans certains codes. Il est\nrecommandé d’utiliser la méthode str.lstrip pour les retirer\nImporter les données des élections 2000 à 2016 à partir du MIT Election Lab?\nLes données peuvent être directement requêtées depuis l’url\nhttps://dataverse.harvard.edu/api/access/datafile/3641280?gbrecs=false\nCréer une variable share comptabilisant la part des votes pour chaque candidat.\nNe garder que les colonnes \"year\", \"FIPS\", \"party\", \"candidatevotes\", \"share\"\nFaire une conversion long to wide avec la méthode pivot_table pour garder une ligne\npar comté x année avec en colonnes les résultats de chaque candidat dans cet état.\nMerger à partir du code FIPS au reste de la base.\n\n\n\nSi vous ne faites pas l’exercice 1, pensez à charger les données en executant la fonction get_data.py :\n\n#!pip install --upgrade xlrd #colab bug verson xlrd\n#!pip install geopandas\n\nimport requests\n\nurl = 'https://raw.githubusercontent.com/linogaliana/python-datascientist/master/content/course/modelisation/get_data.py'\nr = requests.get(url, allow_redirects=True)\nopen('getdata.py', 'wb').write(r.content)\n\nimport getdata\nvotes = getdata.create_votes_dataframes()\n\nCe code introduit une base nommée votes dans l’environnement. Il s’agit d’une\nbase rassemblant les différentes sources. Elle a l’aspect\nsuivant:\n\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nCOUNTYNS\nAFFGEOID\nGEOID\nNAME\nLSAD\nALAND\nAWATER\ngeometry\n...\nshare_2008_democrat\nshare_2008_other\nshare_2008_republican\nshare_2012_democrat\nshare_2012_other\nshare_2012_republican\nshare_2016_democrat\nshare_2016_other\nshare_2016_republican\nwinner\n\n\n\n\n0\n29\n227\n00758566\n0500000US29227\n29227\nWorth\n06\n690564983\n493903\nPOLYGON ((-94.63203 40.57176, -94.53388 40.570...\n...\n0.363714\n0.034072\n0.602215\n0.325382\n0.041031\n0.633588\n0.186424\n0.041109\n0.772467\nrepublican\n\n\n1\n31\n061\n00835852\n0500000US31061\n31061\nFranklin\n06\n1491355860\n487899\nPOLYGON ((-99.17940 40.35068, -98.72683 40.350...\n...\n0.284794\n0.019974\n0.695232\n0.250000\n0.026042\n0.723958\n0.149432\n0.045427\n0.805140\nrepublican\n\n\n2\n36\n013\n00974105\n0500000US36013\n36013\nChautauqua\n06\n2746047476\n1139407865\nPOLYGON ((-79.76195 42.26986, -79.62748 42.324...\n...\n0.495627\n0.018104\n0.486269\n0.425017\n0.115852\n0.459131\n0.352012\n0.065439\n0.582550\nrepublican\n\n\n\n\n3 rows × 383 columns\n\n\n\nLa carte choroplèthe suivante permet de visualiser rapidement les résultats\n(l’Alaska et Hawaï ont été exclus).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# republican : red, democrat : blue\ncolor_dict = {'republican': '#FF0000', 'democrats': '#0000FF'}\n\nfig, ax = plt.subplots(figsize = (12,12))\ngrouped = votes.groupby('winner')\nfor key, group in grouped:\n    group.plot(ax=ax, column='winner', label=key, color=color_dict[key])\nplt.axis('off')\n\n(-127.6146362, -64.0610978, 23.253819649999997, 50.628669349999996)\n\n\n\n\n\n\n\n\n\nLes cartes choroplèthes peuvent donner une impression fallacieuse\nce qui exiplique que\nce type de carte a servi\nde justification pour contester les résultats du vote.\nEn effet, un biais\nconnu des représentations choroplèthes est qu’elles donnent une importance\nvisuelle excessive aux grands espaces. Or, ceux-ci sont souvent des espaces\npeu denses et influencent donc moins la variable d’intérêt (en l’occurrence\nle taux de vote en faveur des républicains/démocrates). Une représentation à\nprivilégier pour ce type de phénomènes est les\nronds proportionnels (voir Insee (2018), “Le piège territorial en cartographie”).\nLe GIF “Land does not vote, people do”\nqui avait eu un certain succès en 2020 propose un autre mode de visualisation.\nLa carte originale a probablement été construite avec JavaScript. Cependant,\non dispose avec Python de plusieurs outils\npour répliquer, à faible coût, cette carte\ngrâce à\nl’une des surcouches à JavaScript vue dans la partie visualisation.\nEn l’occurrence, on peut utiliser plotly pour tenir compte de la population:\nLa Figure a été obtenue avec le code suivant:\n\nimport plotly\nimport plotly.graph_objects as go\nimport pandas as pd\nimport geopandas as gpd\n\n\ncentroids = votes.copy()\ncentroids.geometry = centroids.centroid\ncentroids['size'] = centroids['CENSUS_2010_POP'] / 10000  # to get reasonable plotable number\n\ncolor_dict = {\"republican\": '#FF0000', 'democrats': '#0000FF'}\ncentroids[\"winner\"] =  np.where(centroids['votes_gop'] &gt; centroids['votes_dem'], 'republican', 'democrats') \n\n\ncentroids['lon'] = centroids['geometry'].x\ncentroids['lat'] = centroids['geometry'].y\ncentroids = pd.DataFrame(centroids[[\"county_name\",'lon','lat','winner', 'CENSUS_2010_POP',\"state_name\"]])\ngroups = centroids.groupby('winner')\n\ndf = centroids.copy()\n\ndf['color'] = df['winner'].replace(color_dict)\ndf['size'] = df['CENSUS_2010_POP']/6000\ndf['text'] = df['CENSUS_2010_POP'].astype(int).apply(lambda x: '&lt;br&gt;Population: {:,} people'.format(x))\ndf['hover'] = df['county_name'].astype(str) +  df['state_name'].apply(lambda x: ' ({}) '.format(x)) + df['text']\n\nfig_plotly = go.Figure(data=go.Scattergeo(\n    locationmode = 'USA-states',\n    lon=df[\"lon\"], lat=df[\"lat\"],\n    text = df[\"hover\"],\n    mode = 'markers',\n    marker_color = df[\"color\"],\n    marker_size = df['size'],\n    hoverinfo=\"text\"\n    ))\n\nfig_plotly.update_traces(\n  marker = {'opacity': 0.5, 'line_color': 'rgb(40,40,40)', 'line_width': 0.5, 'sizemode': 'area'}\n)\n\nfig_plotly.update_layout(\n        title_text = \"Reproduction of the \\\"Acres don't vote, people do\\\" map &lt;br&gt;(Click legend to toggle traces)\",\n        showlegend = True,\n        geo = {\"scope\": 'usa', \"landcolor\": 'rgb(217, 217, 217)'}\n    )\n\nLes cercles proportionnels permettent ainsi à l’oeil de se concentrer sur les\nzones les plus denses et non sur les grands espaces."
  },
  {
    "objectID": "content/course/modelisation/0_preprocessing/index.html#explorer-la-structure-des-données",
    "href": "content/course/modelisation/0_preprocessing/index.html#explorer-la-structure-des-données",
    "title": "20  Préparation des données pour construire un modèle",
    "section": "20.2 Explorer la structure des données",
    "text": "20.2 Explorer la structure des données\nLa première étape nécessaire à suivre avant de se lancer dans la modélisation\nest de déterminer les variables à inclure dans le modèle.\nLes fonctionnalités de pandas sont, à ce niveau, suffisantes pour explorer des structures simples.\nNéanmoins, lorsqu’on est face à un jeu de données présentant de\nnombreuses variables explicatives (features en machine learning, covariates en économétrie),\nil est souvent judicieux d’avoir une première étape de sélection de variables,\nce que nous verrons par la suite dans la partie dédiée.\nAvant d’être en mesure de sélectionner le meilleur ensemble de variables explicatives,\nnous allons en prendre un nombre restreint et arbitraire.\nLa première tâche est de représenter les relations entre les données,\nnotamment la relation des variables explicatives\nà la variable dépendante (le score du parti républicain)\nainsi que les relations entre les variables explicatives.\n\n\n Exercice 2 : Regarder les corrélations entre les variables\n\nCréer un DataFrame df2 plus petit avec les variables winner, votes_gop, Unemployment_rate_2019,\nMedian_Household_Income_2019,\nPercent of adults with less than a high school diploma, 2015-19,\nPercent of adults with a bachelor's degree or higher, 2015-19\nReprésenter grâce à un graphique la matrice de corrélation avec heatmap de seaborn.\nReprésenter une matrice de nuages de points des variables de la base df2 avec pd.plotting.scatter_matrix\n(optionnel) Refaire ces figures avec plotly qui offre également la possibilité de faire une matrice de corrélation.\n\n\n\n\n\n\n\n\n\n\n\n\nLa matrice construite avec seaborn (question 2) aura l’aspect suivant:\n\n\n\n\n\n\n\n\n\nAlors que celle construite directement avec corr de pandas\nressemblera plutôt à ce tableau :\n\n\n\n\n\n\n\n \nvotes_gop\nUnemployment_rate_2019\nMedian_Household_Income_2019\nPercent of adults with less than a high school diploma, 2015-19\nPercent of adults with a bachelor's degree or higher, 2015-19\n\n\n\n\nvotes_gop\n1.00\n-0.08\n0.35\n-0.11\n0.37\n\n\nUnemployment_rate_2019\n-0.08\n1.00\n-0.43\n0.36\n-0.36\n\n\nMedian_Household_Income_2019\n0.35\n-0.43\n1.00\n-0.51\n0.71\n\n\nPercent of adults with less than a high school diploma, 2015-19\n-0.11\n0.36\n-0.51\n1.00\n-0.59\n\n\nPercent of adults with a bachelor's degree or higher, 2015-19\n0.37\n-0.36\n0.71\n-0.59\n1.00\n\n\n\n\n\nLe nuage de point obtenu à l’issue de la question 3 ressemblera à :\n\n\n\n\n\n\n\n\n\n\n\narray([[&lt;Axes: xlabel='votes_gop', ylabel='votes_gop'&gt;,\n        &lt;Axes: xlabel='Unemployment_rate_2019', ylabel='votes_gop'&gt;,\n        &lt;Axes: xlabel='Median_Household_Income_2019', ylabel='votes_gop'&gt;,\n        &lt;Axes: xlabel='Percent of adults with less than a high school diploma, 2015-19', ylabel='votes_gop'&gt;,\n        &lt;Axes: xlabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\", ylabel='votes_gop'&gt;],\n       [&lt;Axes: xlabel='votes_gop', ylabel='Unemployment_rate_2019'&gt;,\n        &lt;Axes: xlabel='Unemployment_rate_2019', ylabel='Unemployment_rate_2019'&gt;,\n        &lt;Axes: xlabel='Median_Household_Income_2019', ylabel='Unemployment_rate_2019'&gt;,\n        &lt;Axes: xlabel='Percent of adults with less than a high school diploma, 2015-19', ylabel='Unemployment_rate_2019'&gt;,\n        &lt;Axes: xlabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\", ylabel='Unemployment_rate_2019'&gt;],\n       [&lt;Axes: xlabel='votes_gop', ylabel='Median_Household_Income_2019'&gt;,\n        &lt;Axes: xlabel='Unemployment_rate_2019', ylabel='Median_Household_Income_2019'&gt;,\n        &lt;Axes: xlabel='Median_Household_Income_2019', ylabel='Median_Household_Income_2019'&gt;,\n        &lt;Axes: xlabel='Percent of adults with less than a high school diploma, 2015-19', ylabel='Median_Household_Income_2019'&gt;,\n        &lt;Axes: xlabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\", ylabel='Median_Household_Income_2019'&gt;],\n       [&lt;Axes: xlabel='votes_gop', ylabel='Percent of adults with less than a high school diploma, 2015-19'&gt;,\n        &lt;Axes: xlabel='Unemployment_rate_2019', ylabel='Percent of adults with less than a high school diploma, 2015-19'&gt;,\n        &lt;Axes: xlabel='Median_Household_Income_2019', ylabel='Percent of adults with less than a high school diploma, 2015-19'&gt;,\n        &lt;Axes: xlabel='Percent of adults with less than a high school diploma, 2015-19', ylabel='Percent of adults with less than a high school diploma, 2015-19'&gt;,\n        &lt;Axes: xlabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\", ylabel='Percent of adults with less than a high school diploma, 2015-19'&gt;],\n       [&lt;Axes: xlabel='votes_gop', ylabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\"&gt;,\n        &lt;Axes: xlabel='Unemployment_rate_2019', ylabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\"&gt;,\n        &lt;Axes: xlabel='Median_Household_Income_2019', ylabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\"&gt;,\n        &lt;Axes: xlabel='Percent of adults with less than a high school diploma, 2015-19', ylabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\"&gt;,\n        &lt;Axes: xlabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\", ylabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\"&gt;]],\n      dtype=object)\n\n\nLe résultat de la question 4 devrait, quant à lui,\nressembler au graphique suivant :\n\n# Pour inclusion dans le site web\nhtmlsnip2.write_json(\"scatter.json\")"
  },
  {
    "objectID": "content/course/modelisation/0_preprocessing/index.html#transformer-les-données",
    "href": "content/course/modelisation/0_preprocessing/index.html#transformer-les-données",
    "title": "20  Préparation des données pour construire un modèle",
    "section": "20.3 Transformer les données",
    "text": "20.3 Transformer les données\nLes différences d’échelle ou de distribution entre les variables peuvent\ndiverger des hypothèses sous-jacentes dans les modèles.\nPar exemple, dans le cadre\nde la régression linéaire, les variables catégorielles ne sont pas traitées à la même\nenseigne que les variables ayant valeur dans \\(\\mathbb{R}\\). Une variable\ndiscrète (prenant un nombre fini de valeurs) devra être transformées en suite de\nvariables 0/1 par rapport à une modalité de référence pour être en adéquation\navec les hypothèses de la régression linéaire.\nOn appelle ce type de transformation\none-hot encoding, sur lequel nous reviendrons. Il s’agit d’une transformation,\nparmi d’autres, disponibles dans scikit pour mettre en adéquation un jeu de\ndonnées et des hypothèses mathématiques.\nL’ensemble de ces tâches s’appelle le preprocessing. L’un des intérêts\nd’utiliser scikit est qu’on peut considérer qu’une tâche de preprocessing\nest une tâche d’apprentissage (on apprend des paramètres d’une structure\nde données) qui est réutilisable pour un jeu de données à la structure\nsimilaire:\n\n\n\n\n\nNous allons voir deux processus très classiques de preprocessing :\n\nLa standardisation transforme des données pour que la distribution empirique suive une loi \\(\\mathcal{N}(0,1)\\).\nLa normalisation transforme les données de manière à obtenir une norme (\\(\\mathcal{l}_1\\) ou \\(\\mathcal{l}_2\\)) unitaire. Autrement dit, avec la norme adéquate, la somme des éléments est égale à 1.\n\n\n\n Warning\nPour un statisticien,\nle terme normalization dans le vocable scikit peut avoir un sens contre-intuitif.\nOn s’attendrait à ce que la normalisation consiste à transformer une variable de manière à ce que \\(X \\sim \\mathcal{N}(0,1)\\).\nC’est, en fait, la standardisation en scikit qui fait cela.\n\n\n\n20.3.1 Standardisation\nLa standardisation consiste à transformer des données pour que la distribution empirique suive une loi \\(\\mathcal{N}(0,1)\\). Pour être performants, la plupart des modèles de machine learning nécessitent souvent d’avoir des données dans cette distribution.\n\n\n Exercice 3: Standardisation\n\nStandardiser la variable Median_Household_Income_2019 (ne pas écraser les valeurs !) et regarder l’histogramme avant/après normalisation.\n\nNote : On obtient bien une distribution centrée à zéro et on pourrait vérifier que la variance empirique soit bien égale à 1. On pourrait aussi vérifier que ceci est vrai également quand on transforme plusieurs colonnes à la fois.\n\nCréer scaler, un Transformer que vous construisez sur les 1000 premières lignes de votre DataFrame df2 à l’exception de la variable à expliquer winner. Vérifier la moyenne et l’écart-type de chaque colonne sur ces mêmes observations.\n\nNote : Les paramètres qui seront utilisés pour une standardisation ultérieure sont stockés dans les attributs .mean_ et .scale_\nOn peut voir ces attributs comme des paramètres entraînés sur un certain jeu de\ndonnées et qu’on peut réutiliser sur un autre, à condition que les\ndimensions coïncident.\n\nAppliquer scaler sur les autres lignes du DataFrame et comparer les distributions obtenues de la variable Median_Household_Income_2019.\n\nNote : Une fois appliqués à un autre DataFrame, on peut remarquer que la distribution n’est pas exactement centrée-réduite dans le DataFrame sur lequel les paramètres n’ont pas été estimés. C’est normal, l’échantillon initial n’était pas aléatoire, les moyennes et variances de cet échantillon n’ont pas de raison de coïncider avec les moments de l’échantillon complet.\n\n\n\n\n20.3.2 Normalisation\nLa normalisation est l’action de transformer les données de manière\nà obtenir une norme (\\(\\mathcal{l}_1\\) ou \\(\\mathcal{l}_2\\)) unitaire.\nAutrement dit, avec la norme adéquate, la somme des éléments est égale à 1.\nPar défaut, la norme est dans \\(\\mathcal{l}_2\\).\nCette transformation est particulièrement utilisée en classification de texte ou pour effectuer du clustering.\n\n\n Exercice 4 : Normalisation\n\nNormaliser la variable Median_Household_Income_2019 (ne pas écraser les valeurs !) et regarder l’histogramme avant/après normalisation.\nVérifier que la norme \\(\\mathcal{l}_2\\) est bien égale à 1.\n\n\n\n\n\n Warning\npreprocessing.Normalizer n’accepte pas les valeurs manquantes, alors que preprocessing.StandardScaler() s’en accomode (dans la version 0.22 de scikit). Pour pouvoir aisément appliquer le normalizer, il faut\n\nretirer les valeurs manquantes du DataFrame avec la méthode dropna: df.dropna(how = \"any\");\nou les imputer avec un modèle adéquat. scikit permet de le faire.\n\n\n\n\n\n20.3.3 Encodage des valeurs catégorielles\nLes données catégorielles doivent être recodées\nsous forme de valeurs numériques pour être intégrés aux modèles de machine learning.\nCela peut être fait de plusieurs manières :\n\nLabelEncoder: transforme un vecteur [\"a\",\"b\",\"c\"] en vecteur numérique [0,1,2].\nCette approche a l’inconvénient d’introduire un ordre dans les modalités, ce qui n’est pas toujours souhaitable\nOrdinalEncoder: une version généralisée du LabelEncoder qui a vocation à s’appliquer sur des matrices (\\(X\\)),\nalors que LabelEncoder s’applique plutôt à un vecteur (\\(y\\))\npandas.get_dummies effectue une opération de dummy expansion.\nUn vecteur de taille n avec K catégories sera transformé en matrice de taille \\(n \\times K\\)\npour lequel chaque colonne sera une variable dummy pour la modalité k.\nIl y a ici \\(K\\) modalités et il y a donc multicolinéarité.\nAvec une régression linéaire avec constante,\nil convient de retirer une modalité avant l’estimation.\nOneHotEncoder est une version généralisée (et optimisée) de la dummy expansion.\nIl a plutôt vocation à s’appliquer sur les features (\\(X\\)) du modèle\n\n\n\n Exercice 5 : Encoder des variables catégorielles\n\nCréer df qui conserve uniquement les variables state_name et county_name dans votes.\nAppliquer à state_name un LabelEncoder\nNote : Le résultat du label encoding est relativement intuitif, notamment quand on le met en relation avec le vecteur initial.\nRegarder la dummy expansion de state_name\nAppliquer un OrdinalEncoder à df[['state_name', 'county_name']]\nNote : Le résultat du ordinal encoding est cohérent avec celui du label encoding\nAppliquer un OneHotEncoder à df[['state_name', 'county_name']]\n\nNote : scikit optimise l’objet nécessaire pour stocker le résultat d’un modèle de transformation. Par exemple, le résultat de l’encoding One Hot est un objet très volumineux. Dans ce cas, scikit utilise une matrice Sparse."
  },
  {
    "objectID": "content/course/modelisation/0_preprocessing/index.html#références",
    "href": "content/course/modelisation/0_preprocessing/index.html#références",
    "title": "20  Préparation des données pour construire un modèle",
    "section": "20.4 Références",
    "text": "20.4 Références\n\n\nInsee. 2018. “Guide de Sémiologie Cartographique.”"
  },
  {
    "objectID": "content/course/modelisation/1_modelevaluation/index.html#découper-léchantillon",
    "href": "content/course/modelisation/1_modelevaluation/index.html#découper-léchantillon",
    "title": "21  Evaluer la qualité d’un modèle",
    "section": "21.1 Découper l’échantillon",
    "text": "21.1 Découper l’échantillon\nLe chapitre précédent présentait le pipeline simple ci-dessous\npour introduire à la notion d’entraînement d’un modèle:\n\n\n\n\n\nCe pipeline fait abstraction d’hypothèses exogènes à l’estimation\nmais qui sont à faire sur des paramètres\ncar elles affectent la performance de la prédiction.\nPar exemple, de nombreux modèles proposent une pénalisation des modèles\nnon parcimonieux pour éviter le sur-apprentissage. Le choix de la pénalisation\nidéale dépend de la structure des données et n’est jamais connue, ex-ante\npar le modélisateur. Faut-il pénaliser fortement ou non le modèle ? En l’absence\nd’argument théorique, on aura tendance à tester plusieurs paramètres de\npénalisation et choisir celui qui permet la meilleure prédiction.\nLa notion de validation croisée permettra de généraliser cette approche. Ces paramètres\nqui affectent la prédiction seront pas la suite appelés des\nhyperparamètres. Comme nous allons le voir, nous allons aboutir à un\nraffinement de l’approche pour obtenir un pipeline ayant plutôt cet aspect:"
  },
  {
    "objectID": "content/course/modelisation/1_modelevaluation/index.html#le-problème-du-sur-apprentissage",
    "href": "content/course/modelisation/1_modelevaluation/index.html#le-problème-du-sur-apprentissage",
    "title": "21  Evaluer la qualité d’un modèle",
    "section": "21.2 Le problème du sur-apprentissage",
    "text": "21.2 Le problème du sur-apprentissage\nLe but du Machine Learning est de calibrer l’algorithme sur des exemples\nconnus (données labellisées) afin de généraliser à des\nexemples nouveaux (éventuellement non labellisés).\nOn vise donc de bonnes qualités\nprédictives et non un ajustement parfait\naux données historiques.\nIl existe un arbitrage biais-variance dans la qualité d’estimation[^1]. Soit \\(h(X,\\theta)\\) un modèle statistique. On\npeut décomposer l’erreur d’estimation en deux parties :\n\\[\n\\mathbb{E}\\bigg[(y - h(\\theta,X))^2 \\bigg] = \\underbrace{ \\bigg( y - \\mathbb{E}(h_\\theta(X)) \\bigg)^2}_{\\text{biais}^2} + \\underbrace{\\mathbb{V}\\big(h(\\theta,X)\\big)}_{\\text{variance}}\n\\]\nIl y a ainsi un compromis à faire entre biais et variance. Un modèle peu parcimonieux, c’est-à-dire proposant un grand nombre de paramètres, va, en général, avoir un faible biais mais une grande variance. En effet, le modèle va tendre à se souvenir d’une combinaison de paramètres à partir d’un grand nombre d’exemples sans être capable d’apprendre la règle qui permette de structurer les données.\n[^1]! Cette formule permet de bien comprendre la théorie statistique asymptotique, notamment le théorème de Cramer-Rao. Dans la classe des estimateurs sans biais, c’est-à-dire dont le premier terme est nul, trouver l’estimateur à variance minimale revient à trouver l’estimateur qui minimise \\(\\mathbb{E}\\bigg[(y - h_\\theta(X))^2 \\bigg]\\). C’est la définition même de la régression, ce qui, quand on fait des hypothèses supplémentaires sur le modèle statistique, explique le théorème de Cramer-Rao.\nPar exemple, la ligne verte ci-dessous est trop dépendante des données et risque de produire une erreur plus importante que la ligne noire (qui moyennise plus) sur de nouvelles données.\n\n\n\n\n\nPour renforcer la validité externe d’un modèle, il est ainsi commun, en Machine Learning:\n\nd’estimer un modèle sur un jeu de données (jeu d’apprentissage ou training set) mais d’évaluer la performance, et donc la pertinence du modèle, sur d’autres données, qui n’ont pas été mobilisées lors de la phase d’estimation (jeu de validation, de test ou testing set) ;\nd’avoir des mesures de performances qui pénalisent fortement les modèles peu parcimonieux (BIC) ou conduire une première phase de sélection de variable (par des méthodes de LASSO…)\n\nPour décomposer un modèle en jeu d’estimation et de test,\nla meilleure méthode est d’utiliser les fonctionnalités de scikit de la manière suivante :\n\nfrom sklearn.model_selection import train_test_split\nxTrain, xTest, yTrain, yTest = train_test_split(\n  x, y,\n  test_size = 0.2,\n  random_state = 0\n  )\n\nLa proportion d’observations dans le jeu de test est contrôlée par l’argument test_size.\nLa proportion optimale n’existe pas.\nLa règle du pouce habituelle est d’assigner aléatoirement 20 % des observations\ndans l’échantillon de test pour garder suffisamment d’observations\ndans l’échantillon d’estimation.\n\n\n Hint \nLorsqu’on travaille avec des séries temporelles, l’échantillonnage aléatoire des observations n’a pas vraiment de sens. Il vaut mieux tester la qualité de l’observation sur des périodes distinguées.\n\n\n\n\n Note\nAvec des données multi-niveaux,\ncomme c’est le cas de données géographiques ou de données individuelles avec des variables de classe,\nil peut être intéressant d’utiliser un échantillonnage stratifié.\nCela permet de garder une proportion équivalente de chaque groupe dans les deux jeux de données de test ou d’apprentissage.\nCe type d’échantillonnage stratifié est également possible avec scikit.\n\n\nL’exercice sur les SVM illustre cette construction et la manière\ndont elle facilite l’évaluation de la qualité d’un modèle."
  },
  {
    "objectID": "content/course/modelisation/1_modelevaluation/index.html#validation-croisée",
    "href": "content/course/modelisation/1_modelevaluation/index.html#validation-croisée",
    "title": "21  Evaluer la qualité d’un modèle",
    "section": "21.3 Validation croisée",
    "text": "21.3 Validation croisée\nCertains algorithmes font intervenir des hyperparamètres,\nc’est-à-dire des paramètres exogènes qui déterminent la prédiction mais ne sont pas estimés.\nLa validation croisée est une méthode permettant de choisir la valeur du paramètre\nqui optimise la qualité de la prédiction en agrégeant\ndes scores de performance sur des découpages différents de l’échantillon d’apprentissage.\nLa validation croisée permet d’évaluer les performances de modèles différents (SVM, random forest, etc.) ou, couplé à une stratégie de grid search de trouver les valeurs des hyperparamètres qui aboutissent à la meilleure prédiction.\n\n\n Note\nL’étape de découpage de l’échantillon de validation croisée est à distinguer de l’étape split_sample_test. A ce stade, on a déjà partitionné les données en échantillon d’apprentissage et test. C’est l’échantillon d’apprentissage qu’on découpe en sous-morceaux.\n\n\nLa méthode la plus commune est la validation croisée k-fold.\nOn partitionne les données en \\(K\\) morceaux et on considère chaque pli, tour à tour, comme un échantillon\nde test en apprenant sur les \\(K-1\\) échantillons restants. Les \\(K\\) indicateurs ainsi calculés sur les \\(K\\) échantillons de test peuvent être moyennés et\ncomparés pour plusieurs valeurs des hyperparamètres.\n\n\n\n\n\nIl existe d’autres types de validation croisée, notamment la leave one out qui consiste à considérer une fois\nexactement chaque observation comme l’échantillon de test (une n-fold cross validation)."
  },
  {
    "objectID": "content/course/modelisation/1_modelevaluation/index.html#mesurer-la-performance",
    "href": "content/course/modelisation/1_modelevaluation/index.html#mesurer-la-performance",
    "title": "21  Evaluer la qualité d’un modèle",
    "section": "21.4 Mesurer la performance",
    "text": "21.4 Mesurer la performance\nJusqu’à présent, nous avons passé sous silence la question du support de \\(y\\), c’est-à-dire\nde l’étendue des valeurs de notre variable d’intérêt.\nEn pratique, la distribution des \\(y\\)\nva néanmoins déterminer deux questions cruciales : la méthode et l’indicateur de performance.\nEn apprentissage supervisé, on distingue en général les problèmes de:\n\nClassification : la variable \\(y\\) est discrète\nRégression : la variable \\(y\\) est continue\n\nLes deux approches ne sont pas sans lien. On peut par exemple voir le modèle économétrique de choix d’offre de travail comme un problème de classification (participation ou non au marché du travail) ou de régression (régression sur un modèle à variable latente)\n\n21.4.1 Classification\nLa plupart des critères de performance sont construits à partir de la matrice de confusion:\n\n\n\nImage empruntée à https://www.lebigdata.fr/confusion-matrix-definition\n\n\nA partir des 4 coins de cette matrice, il existe plusieurs mesure de performance\n\n\n\n\n\n\n\n\nCritère\nMesure\nCalcul\n\n\n\n\nAccuracy\nTaux de classification correcte\nDiagonale du tableau: \\(\\frac{TP+TN}{TP+FP+FN+FP}\\)\n\n\nPrecision\nTaux de vrais positifs\nLigne des prédictions positives: \\(\\frac{TP}{TP+FP}\\)\n\n\nRecall (rappel)\nCapacité à identifier les labels positifs\nColonne des prédictions positives: \\(\\frac{TP}{TP+FN}\\)\n\n\nF1 Score\nMesure synthétique (moyenne harmonique) de la précision et du rappel\n\\(2 \\frac{precision \\times recall}{precision + recall}\\)\n\n\n\nEn présence de classes désequilibrées, la\nF-mesure est plus pertinente pour évaluer les\nperformances mais l’apprentissage restera\nmauvais si l’algorithme est sensible à ce\nproblème. Notamment, si on désire avoir une performance équivalente sur les classes minoritaires, il faut généralement les sur-pondérer (ou faire un échantillonnage stratifié) lors de la constitution de l’échantillon d’observation.\nIl est possible de construire des modèles à partir des probabilités prédites d’appartenir à la classe d’intérêt. Pour cela, on fixe un seuil \\(c\\) tel que\n\\[\n\\mathbb{P}(y_i=1|X_i) &gt; c \\Rightarrow \\widehat{y}_i = 1\n\\]\nPlus on augmente \\(c\\), plus on est sélectif sur le critère d’appartenance à la classe.\nLe rappel, i.e. le taux de faux négatifs, diminue. Mais on augmente le nombre de positifs manqués. Pour chaque valeur de \\(c\\) correspond une matrice de confusion et donc des mesures de performances.\nLa courbe ROC est un outil classique pour représenter en un graphique l’ensemble de ces\ninformations en faisant varier \\(c\\) de 0 à 1:\n\n\n\n\n\nL’aire sous la courbe (AUC) permet d’évaluer quantitativement le meilleur modèle au\nsens de ce critère. L’AUC représente la probabilité que le modèle soit capable de distinguer entre la classe positive et négative.\n\n\n21.4.2 Régression\nEn Machine Learning, les indicateurs de performance en régression sont les suivants:\n\n\n\n\n\n\n\nNom\nFormule\n\n\n\n\nMean squared error\n\\(MSE = \\mathbb{E}\\left[(y - h_\\theta(X))^2\\right]\\)\n\n\nRoot Mean squared error\n\\(RMSE = \\sqrt{\\mathbb{E}\\left[(y - h_\\theta(X))^2\\right]}\\)\n\n\nMean Absolute Error\n\\(MAE = \\mathbb{E} \\bigg[ \\lvert y - h_\\theta(X) \\rvert \\bigg]\\)\n\n\nMean Absolute Percentage Error\n\\(MAE = \\mathbb{E}\\left[ \\left\\lvert \\frac{y - h_\\theta(X)}{y} \\right\\rvert \\right]\\)\n\n\n\nL’économètre se focalise moins sur la qualité de la prédiction et utilisera\nd’autres critères pour évaluer la qualité d’un modèle (certains, comme le BIC, sont\nà regarder aussi dans une optique Machine Learning): \\(R^2\\), \\(BIC\\),\n\\(AIC\\), log-likelihood, etc."
  },
  {
    "objectID": "content/course/modelisation/2_SVM/index.html#la-méthode-des-svm-support-vector-machines",
    "href": "content/course/modelisation/2_SVM/index.html#la-méthode-des-svm-support-vector-machines",
    "title": "22  Classification: premier modèle avec les SVM",
    "section": "22.1 La méthode des SVM (Support Vector Machines)",
    "text": "22.1 La méthode des SVM (Support Vector Machines)\nL’une des méthodes de Machine Learning les plus utilisées en classification est les SVM.\nIl s’agit de trouver, dans un système de projection adéquat (noyau ou kernel),\nles paramètres de l’hyperplan (en fait d’un hyperplan à marges maximales)\nséparant les classes de données:\n\n\n\n\n\n\n\n Formalisation mathématique\nOn peut, sans perdre de généralité, supposer que le problème consiste à supposer l’existence d’une loi de probabilité \\(\\mathbb{P}(x,y)\\) (\\(\\mathbb{P} \\to \\{-1,1\\}\\)) qui est inconnue. Le problème de discrimination\nvise à construire un estimateur de la fonction de décision idéale qui minimise la probabilité d’erreur, autrement dit \\(\\theta = \\arg\\min_\\Theta \\mathbb{P}(h_\\theta(X) \\neq y |x)\\)\nLes SVM les plus simples sont les SVM linéaires. Dans ce cas, on suppose qu’il existe un séparateur linéaire qui permet d’associer chaque classe à son signe:\n\\[\nh_\\theta(x) = \\text{signe}(f_\\theta(x)) ; \\text{ avec } f_\\theta(x) = \\theta^T x + b\n\\]\navec \\(\\theta \\in \\mathbb{R}^p\\) et \\(w \\in \\mathbb{R}\\).\n\n\n\n\n\nLorsque des observations sont linéairement séparables, il existe une infinité de frontières de décision linéaire séparant les deux classes. Le “meilleur” choix est de prendre la marge maximale permettant de séparer les données. La distance entre les deux marges est \\(\\frac{2}{||\\theta||}\\). Donc maximiser cette distance entre deux hyperplans revient à minimiser \\(||\\theta||^2\\) sous la contrainte \\(y_i(\\theta^Tx_i + b) \\geq 1\\).\nDans le cas non linéairement séparable, la hinge loss \\(\\max\\big(0,y_i(\\theta^Tx_i + b)\\big)\\) permet de linéariser la fonction de perte:\n\n\n\n\n\nce qui donne le programme d’optimisation suivant:\n\\[\n\\frac{1}{n} \\sum_{i=1}^n \\max\\big(0,y_i(\\theta^Tx_i + b)\\big) + \\lambda ||\\theta||^2\n\\]\nLa généralisation au cas non linéaire implique d’introduire des noyaux transformant l’espace de coordonnées des observations."
  },
  {
    "objectID": "content/course/modelisation/2_SVM/index.html#exercice",
    "href": "content/course/modelisation/2_SVM/index.html#exercice",
    "title": "22  Classification: premier modèle avec les SVM",
    "section": "22.2 Exercice",
    "text": "22.2 Exercice\n\n# packages utiles\nfrom sklearn import svm\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\n\n\n Exercice 1 : Premier algorithme de classification\n\nCréer une variable dummy appelée y dont la valeur vaut 1 quand les républicains l’emportent.\nEn utilisant la fonction prête à l’emploi nommée train_test_split de la librairie sklearn.model_selection,\ncréer des échantillons de test (20 % des observations) et d’estimation (80 %) avec comme features: 'Unemployment_rate_2019', 'Median_Household_Income_2019', 'Percent of adults with less than a high school diploma, 2015-19', \"Percent of adults with a bachelor's degree or higher, 2015-19\" et comme label la variable y.\n\n\nA column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel()\n\nNote : Pour éviter ce warning à chaque fois que vous estimez votre modèle, vous pouvez utiliser DataFrame[['y']].values.ravel() plutôt que DataFrame[['y']] lorsque vous constituez vos échantillons.\n\nEntraîner un classifieur SVM avec comme paramètre de régularisation C = 1. Regarder les mesures de performance suivante : accuracy, f1, recall et precision.\nVérifier la matrice de confusion : vous devriez voir que malgré des scores en apparence pas si mauvais, il y a un problème notable.\nRefaire les questions précédentes avec des variables normalisées. Le résultat est-il différent ?\nChanger de variables x. Utiliser uniquement le résultat passé du vote démocrate et le revenu (votes_gop et Median_Household_Income_2019). Regarder les résultats, notamment la matrice de confusion.\n[OPTIONNEL] Faire une 5-fold validation croisée pour déterminer le paramètre C idéal.\n\n\n\nA l’issue de la question 3,\nle classifieur avec C = 1\ndevrait avoir les performances suivantes:\n\n\n\n\nScore\n\n\n\n\nAccuracy\n0.868167\n\n\nRecall\n0.878307\n\n\nPrecision\n0.97456\n\n\nF1\n0.923933\n\n\n\nLa matrice de confusion associée\nprend cette forme:\n\n\n\n\n\n\n\n\n\n\nA l’issue de la question 6,\nle nouveau classifieur avec devrait avoir les performances suivantes :\n\n\n\n\nScore\n\n\n\n\nAccuracy\n0.868167\n\n\nRecall\n0.878307\n\n\nPrecision\n0.97456\n\n\nF1\n0.923933\n\n\n\nEt la matrice de confusion associée:"
  },
  {
    "objectID": "content/course/modelisation/3_regression/index.html#principe-général",
    "href": "content/course/modelisation/3_regression/index.html#principe-général",
    "title": "23  Régression : une introduction",
    "section": "23.1 Principe général",
    "text": "23.1 Principe général\nLe principe général de la régression consiste à trouver une loi \\(h_\\theta(X)\\)\ntelle que\n\\[\nh_\\theta(X) = \\mathbb{E}_\\theta(Y|X)\n\\]\nCette formalisation est extrêmement généraliste et ne se restreint d’ailleurs\npar à la régression linéaire.\nEn économétrie, la régression offre une alternative aux méthodes de maximum\nde vraisemblance et aux méthodes des moments. La régression est un ensemble\ntrès vaste de méthodes, selon la famille de modèles\n(paramétriques, non paramétriques, etc.) et la structure de modèles.\n\n23.1.1 La régression linéaire\nC’est la manière la plus simple de représenter la loi \\(h_\\theta(X)\\) comme\ncombinaison linéaire de variables \\(X\\) et de paramètres \\(\\theta\\). Dans ce\ncas,\n\\[\n\\mathbb{E}_\\theta(Y|X) = X\\beta\n\\]\nCette relation est encore, sous cette formulation, théorique. Il convient\nde l’estimer à partir des données observées \\(y\\). La méthode des moindres\ncarrés consiste à minimiser l’erreur quadratique entre la prédiction et\nles données observées (ce qui explique qu’on puisse voir la régression comme\nun problème de Machine Learning). En toute généralité, la méthode des\nmoindres carrés consiste à trouver l’ensemble de paramètres \\(\\theta\\)\ntel que\n\\[\n\\theta = \\arg \\min_{\\theta \\in \\Theta} \\mathbb{E}\\bigg[ \\left( y - h_\\theta(X) \\right)^2 \\bigg]\n\\]\nCe qui, dans le cadre de la régression linéaire, s’exprime de la manière suivante:\n\\[\n\\beta = \\arg\\min \\mathbb{E}\\bigg[ \\left( y - X\\beta \\right)^2 \\bigg]\n\\]\nLorsqu’on amène le modèle théorique (\\(\\mathbb{E}_\\theta(Y|X) = X\\beta\\)) aux données,\non formalise le modèle de la manière suivante:\n\\[\nY = X\\beta + \\epsilon\n\\]\nAvec une certaine distribution du bruit \\(\\epsilon\\) qui dépend\ndes hypothèses faites. Par exemple, avec des\n\\(\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)\\) i.i.d., l’estimateur \\(\\beta\\) obtenu\nest équivalent à celui du Maximum de Vraisemblance dont la théorie asymptotique\nnous assure l’absence de biais, la variance minimale (borne de Cramer-Rao).\n\n# packages utiles\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n\n\n Exercice 1a : Régression linéaire avec scikit\nCet exercice vise à illustrer la manière d’effectuer une régression linéaire avec scikit.\nDans ce domaine,\nstatsmodels est nettement plus complet, ce que montrera l’exercice suivant.\nL’intérêt principal de faire\ndes régressions avec scikit est de pouvoir comparer les résultats d’une régression linéaire\navec d’autres modèles de régression. Cependant, le chapitre sur les\npipelines montrera qu’on peut très bien insérer, avec quelques efforts\nde programmation orientée objet, une régression statsmodels dans\nun pipeline scikit.\nL’objectif est d’expliquer le score des Républicains en fonction de quelques\nvariables. Contrairement au chapitre précédent, où on se focalisait sur\nun résultat binaire (victoire/défaite des Républicains), cette\nfois on va chercher à modéliser directement le score des Républicains.\n\nA partir de quelques variables, par exemple, ‘Unemployment_rate_2019’, ‘Median_Household_Income_2019’, ‘Percent of adults with less than a high school diploma, 2015-19’, “Percent of adults with a bachelor’s degree or higher, 2015-19”, expliquer la variable per_gop à l’aide d’un échantillon d’entraînement X_train constitué au préalable.\n\n:warning: utiliser la variable Median_Household_Income_2019\nen log sinon son échelle risque d’écraser tout effet.\n\nAfficher les valeurs des coefficients, constante comprise\nEvaluer la pertinence du modèle avec le \\(R^2\\) et la qualité du fit avec le MSE.\nReprésenter un nuage de points des valeurs observées\net des erreurs de prédiction. Observez-vous\nun problème de spécification ?\n\n\n\n\n# packages utiles\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n\n\n Exercice 1b : Régression linéaire avec statsmodels\nCet exercice vise à illustrer la manière d’effectuer une régression linéaire avec statsmodels qui offre des fonctionnalités plus proches de celles de R, et moins orientées Machine Learning.\nL’objectif est toujours d’expliquer le score des Républicains en fonction de quelques\nvariables.\n\nA partir de quelques variables, par exemple, ‘Unemployment_rate_2019’, ‘Median_Household_Income_2019’, ‘Percent of adults with less than a high school diploma, 2015-19’, “Percent of adults with a bachelor’s degree or higher, 2015-19”, expliquer la variable per_gop. :warning: utiliser la variable Median_Household_Income_2019\nen log sinon son échelle risque d’écraser tout effet.\nAfficher un tableau de régression.\nEvaluer la pertinence du modèle avec le R^2.\nUtiliser l’API formula pour régresser le score des républicains en fonction de la variable Unemployment_rate_2019, de Unemployment_rate_2019 au carré et du log de\nMedian_Household_Income_2019.\n\n\n\n\n\n Hint\nPour sortir une belle table pour un rapport sous \\(\\LaTeX\\), il est possible d’utiliser\nla méthode Summary.as_latex. Pour un rapport HTML, on utilisera Summary.as_html\n\n\n\n\n Note\nLes utilisateurs de R retrouveront des éléments très familiers avec statsmodels,\nnotamment la possibilité d’utiliser une formule pour définir une régression.\nLa philosophie de statsmodels est similaire à celle qui a influé sur la construction\ndes packages stats et MASS de R: offrir une librairie généraliste, proposant\nune large gamme de modèles. Néanmoins, statsmodels bénéficie de sa jeunesse\npar rapport aux packages R. Depuis les années 1990, les packages R visant\nà proposer des fonctionalités manquantes dans stats et MASS se sont\nmultipliés alors que statsmodels, enfant des années 2010, n’a eu qu’à\nproposer un cadre général (les generalized estimating equations) pour\nenglober ces modèles.\n\n\n\n\n23.1.2 La régression logistique\nCe modèle s’applique à une distribution binaire.\nDans ce cas, \\(\\mathbb{E}\\_{\\theta}(Y|X) = \\mathbb{P}\\_{\\theta}(Y = 1|X)\\).\nLa régression logistique peut être vue comme un modèle linéaire en probabilité:\n\\[\n\\text{logit}\\bigg(\\mathbb{E}\\_{\\theta}(Y|X)\\bigg) = \\text{logit}\\bigg(\\mathbb{P}\\_{\\theta}(Y = 1|X)\\bigg) = X\\beta\n\\]\nLa fonction \\(\\text{logit}\\) est \\(]0,1[ \\to \\mathbb{R}: p \\mapsto \\log(\\frac{p}{1-p})\\).\nElle permet ainsi de transformer une probabilité dans \\(\\mathbb{R}\\).\nSa fonction réciproque est la sigmoïde (\\(\\frac{1}{1 + e^{-x}}\\)),\nobjet central du Deep Learning.\nIl convient de noter que les probabilités ne sont pas observées, c’est l’outcome\nbinaire (0/1) qui l’est. Cela amène à voir la régression logistique de deux\nmanières différentes :\n\nEn économétrie, on s’intéresse au modèle latent qui détermine le choix de\nl’outcome. Par exemple, si on observe les choix de participer ou non au marché\ndu travail, on va modéliser les facteurs déterminant ce choix ;\nEn Machine Learning, le modèle latent n’est nécessaire que pour classifier\ndans la bonne catégorie les observations\n\nL’estimation des paramètres \\(\\beta\\) peut se faire par maximum de vraisemblance\nou par régression, les deux solutions sont équivalentes sous certaines\nhypothèses.\n\n\n Note\nPar défaut, scikit applique une régularisation pour pénaliser les modèles\npeu parcimonieux (comportement différent\nde celui de statsmodels). Ce comportement par défaut est à garder à l’esprit\nsi l’objectif n’est pas de faire de la prédiction.\n\n\n\n# packages utiles\nfrom sklearn.linear_model import LogisticRegression\nimport sklearn.metrics\n\n\n\n Exercice 2a : Régression logistique avec scikit\nAvec scikit, en utilisant échantillons d’apprentissage et d’estimation :\n\nEvaluer l’effet des variables déjà utilisées sur la probabilité des Républicains\nde gagner. Affichez la valeur des coefficients.\nDéduire une matrice de confusion et\nune mesure de qualité du modèle.\nSupprimer la régularisation grâce au paramètre penalty. Quel effet sur les paramètres estimés ?\n\n\n\n\n# packages utiles\nfrom scipy import stats\n\n\n\n Exercice 2b : Régression logistique avec statmodels\nEn utilisant échantillons d’apprentissage et d’estimation :\n\nEvaluer l’effet des variables déjà utilisées sur la probabilité des Républicains\nde gagner.\nFaire un test de ratio de vraisemblance concernant l’inclusion de la variable de (log)-revenu.\n\n\n\n\n\n Hint\nLa statistique du test est:\n\\[\nLR = -2\\log\\bigg(\\frac{\\mathcal{L}_{\\theta}}{\\mathcal{L}_{\\theta_0}}\\bigg) = -2(\\mathcal{l}_{\\theta} - \\mathcal{l}_{\\theta_0})\n\\]"
  },
  {
    "objectID": "content/course/modelisation/4_featureselection/index.html#principe-du-lasso",
    "href": "content/course/modelisation/4_featureselection/index.html#principe-du-lasso",
    "title": "24  Sélection de variables : une introduction",
    "section": "24.1 Principe du LASSO",
    "text": "24.1 Principe du LASSO\n\n24.1.1 Principe général\nLa classe des modèles de feature selection est ainsi très vaste et regroupe\nun ensemble très diverse de modèles. Nous allons nous focaliser sur le LASSO\n(Least Absolute Shrinkage and Selection Operator)\nqui est une extension de la régression linéaire qui vise à sélectionner des\nmodèles sparses. Ce type de modèle est central dans le champ du\nCompressed sensing (où on emploie plutôt le terme\nde L1-regularization que de LASSO). Le LASSO est un cas particulier des\nrégressions elastic-net dont un autre cas fameux est la régression ridge.\nContrairement à la régression linéaire classique, elles fonctionnent également\ndans un cadre où \\(p&gt;N\\), c’est à dire où le nombre de régresseurs est très grand puisque supérieur\nau nombre d’observations.\n\n\n24.1.2 Pénalisation\nEn adoptant le principe d’une fonction objectif pénalisée,\nle LASSO permet de fixer un certain nombre de coefficients à 0.\nLes variables dont la norme est non nulle passent ainsi le test de sélection.\n\n\n Hint\nLe LASSO est un programme d’optimisation sous contrainte. On cherche à trouver l’estimateur \\(\\beta\\) qui minimise l’erreur quadratique (régression linéaire) sous une contrainte additionnelle régularisant les paramètres:\n\\[\n\\min_{\\beta} \\frac{1}{2}\\mathbb{E}\\bigg( \\big( X\\beta - y  \\big)^2 \\bigg) \\\\\n\\text{s.t. } \\sum_{j=1}^p |\\beta_j| \\leq t\n\\]\nCe programme se reformule grâce au Lagrangien est permet ainsi d’obtenir un programme de minimisation plus maniable :\n\\[\n\\beta^{\\text{LASSO}} = \\arg \\min_{\\beta} \\frac{1}{2}\\mathbb{E}\\bigg( \\big( X\\beta - y  \\big)^2 \\bigg) + \\alpha \\sum_{j=1}^p |\\beta_j| = \\arg \\min_{\\beta} ||y-X\\beta||_{2}^{2} + \\alpha ||\\beta||_1\n\\]\noù \\(\\lambda\\) est une réécriture de la régularisation précédente.\n\n\n\n\n24.1.3 Première régression LASSO\nAvant de se lancer dans les exercices, on va éliminer quelques colonnes redondantes,\ncelles qui concernent les votes des partis concurrents (forcément très\ncorrélés au vote Républicain…) :\n\ndf2 = votes.loc[:,~votes.columns.str.endswith(\n  ('_democrat','_green','_other', 'per_point_diff', 'per_dem')\n  )]\n\nNous allons utiliser par la suite les fonctions ou\npackages suivants:\n\n# packages utiles\nimport numpy as np\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Lasso\nimport sklearn.metrics\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import lasso_path\nimport seaborn as sns\n\n\n\n Exercice 1 : Premier LASSO\nOn cherche toujours à prédire la variable per_gop.\n\nPréparez les variables à utiliser.\n\n\nNe garder que les colonnes numériques (idéalement on transformerait\nles variables non numériques en numériques)\nRemplacer les valeurs infinies par des NaN et les valeurs manquantes par 0.\nStandardiser les features (c’est-à-dire les variables autres que la variable per_gop) avec StandardScaler\n\n\nOn cherche toujours à prédire la variable per_gop. Créez un échantillon d’entraînement et un échantillon test.\nEstimer un modèle LASSO pénalisé avec \\(alpha = 0.1\\). Afficher les valeurs des coefficients. Quelles variables ont une valeur non nulle ?\nMontrer que les variables sélectionnées sont parfois très corrélées.\nComparer la performance de ce modèle parcimonieux avec celle d’un modèle avec plus de variables\nUtiliser la fonction lasso_path pour évaluer le nombre de paramètres sélectionnés par LASSO lorsque \\(\\alpha\\)\nvarie (parcourir \\(\\alpha \\in [0.001,0.01,0.02,0.025,0.05,0.1,0.25,0.5,0.8,1.0]\\) ).\n\n\n\nA l’issue de la question 3,\nles variables sélectionnées sont :\n\n\n['ALAND',\n 'FIPS_y',\n 'INTERNATIONAL_MIG_2017',\n 'DOMESTIC_MIG_2014',\n 'DOMESTIC_MIG_2017',\n 'RESIDUAL_2010',\n 'RESIDUAL_2019',\n 'R_death_2012',\n 'R_death_2019',\n 'R_NATURAL_INC_2019',\n 'R_INTERNATIONAL_MIG_2011',\n 'R_DOMESTIC_MIG_2012',\n \"Percent of adults with a bachelor's degree or higher, 1990\",\n 'Percent of adults with a high school diploma only, 2000',\n \"Percent of adults with a bachelor's degree or higher, 2000\",\n \"Percent of adults with a bachelor's degree or higher, 2015-19\",\n 'Rural_urban_continuum_code_2013',\n 'Metro_2013',\n 'Unemployment_rate_2002',\n 'Unemployment_rate_2003',\n 'Unemployment_rate_2012',\n 'Rural-urban_Continuum_Code_2003',\n 'Rural-urban_Continuum_Code_2013',\n 'CI90LB517P_2019',\n 'candidatevotes_2016_republican',\n 'share_2012_republican',\n 'share_2016_republican']\n\n\nCertaines variables font sens, comme les variables d’éducation par exemple. Notamment, un des meilleurs prédicteurs pour le score des Républicains en 2020 est… le score des Républicains (et mécaniquement des démocrates) en 2016.\nPar ailleurs, on sélectionne des variables redondantes. Une phase plus approfondie de nettoyage des données serait en réalité nécessaire.\n\n\n\n\n\n\n\n\n\nOn voit que plus \\(\\alpha\\) est élevé, moins le modèle sélectionne de variables."
  },
  {
    "objectID": "content/course/modelisation/4_featureselection/index.html#validation-croisée-pour-sélectionner-le-modèle",
    "href": "content/course/modelisation/4_featureselection/index.html#validation-croisée-pour-sélectionner-le-modèle",
    "title": "24  Sélection de variables : une introduction",
    "section": "24.2 Validation croisée pour sélectionner le modèle",
    "text": "24.2 Validation croisée pour sélectionner le modèle\nQuel \\(\\alpha\\) faut-il privilégier ? Pour cela,\nil convient d’effectuer une validation croisée afin de choisir le modèle pour\nlequel les variables qui passent la phase de sélection permettent de mieux\nprédire le résultat Républicain :\n\nfrom sklearn.linear_model import LassoCV\n\ndf3 = df2.select_dtypes(include=np.number)\ndf3.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf3 = df3.fillna(0)\nscaler = StandardScaler()\nyindex = df3.columns.get_loc(\"per_gop\")\ndf3_scale = scaler.fit(df3).transform(df3)\n# X_train, X_test , y_train, y_test = train_test_split(np.delete(data, yindex, axis = 1),data[:,yindex], test_size=0.2, random_state=0)\n\nlcv = LassoCV(alphas=my_alphas, fit_intercept=False,random_state=0,cv=5).fit(np.delete(df3_scale, yindex, axis = 1), df3_scale[:,yindex])\n\n\nprint(\"alpha optimal :\", lcv.alpha_)\n\nalpha optimal : 0.001\n\n\n\nlasso2 = Lasso(fit_intercept=True, alpha = lcv.alpha_).fit(X_train,y_train)\nfeatures_selec2 = df2.select_dtypes(include=np.number).drop(\"per_gop\", axis = 1).columns[np.abs(lasso2.coef_)&gt;0].tolist()\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning:\n\nObjective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.486e+03, tolerance: 6.352e+01\n\n\n\nLes variables sélectionnées sont :\n\nprint(features_selec2)\n\n['ALAND', 'AWATER', 'votes_gop', 'diff', 'Rural-urban_Continuum Code_2003', 'Rural-urban_Continuum Code_2013', 'Urban_Influence_Code_2013', 'Economic_typology_2015', 'CENSUS_2010_POP', 'N_POP_CHG_2013', 'N_POP_CHG_2016', 'N_POP_CHG_2017', 'N_POP_CHG_2018', 'N_POP_CHG_2019', 'Births_2011', 'Births_2015', 'Deaths_2015', 'Deaths_2017', 'Deaths_2018', 'NATURAL_INC_2012', 'NATURAL_INC_2013', 'NATURAL_INC_2014', 'NATURAL_INC_2016', 'NATURAL_INC_2018', 'INTERNATIONAL_MIG_2010', 'INTERNATIONAL_MIG_2011', 'INTERNATIONAL_MIG_2012', 'INTERNATIONAL_MIG_2013', 'INTERNATIONAL_MIG_2014', 'INTERNATIONAL_MIG_2015', 'INTERNATIONAL_MIG_2016', 'INTERNATIONAL_MIG_2017', 'INTERNATIONAL_MIG_2018', 'INTERNATIONAL_MIG_2019', 'DOMESTIC_MIG_2010', 'DOMESTIC_MIG_2012', 'DOMESTIC_MIG_2013', 'DOMESTIC_MIG_2015', 'DOMESTIC_MIG_2016', 'DOMESTIC_MIG_2018', 'NET_MIG_2011', 'NET_MIG_2014', 'NET_MIG_2018', 'NET_MIG_2019', 'RESIDUAL_2010', 'RESIDUAL_2011', 'RESIDUAL_2012', 'RESIDUAL_2013', 'RESIDUAL_2014', 'RESIDUAL_2015', 'RESIDUAL_2016', 'RESIDUAL_2017', 'RESIDUAL_2018', 'RESIDUAL_2019', 'GQ_ESTIMATES_BASE_2010', 'GQ_ESTIMATES_2013', 'GQ_ESTIMATES_2015', 'GQ_ESTIMATES_2017', 'R_birth_2011', 'R_birth_2013', 'R_birth_2014', 'R_birth_2016', 'R_birth_2017', 'R_birth_2019', 'R_death_2011', 'R_death_2012', 'R_death_2013', 'R_death_2014', 'R_death_2015', 'R_death_2016', 'R_death_2017', 'R_death_2018', 'R_death_2019', 'R_NATURAL_INC_2012', 'R_NATURAL_INC_2015', 'R_NATURAL_INC_2017', 'R_NATURAL_INC_2018', 'R_NATURAL_INC_2019', 'R_INTERNATIONAL_MIG_2011', 'R_INTERNATIONAL_MIG_2012', 'R_INTERNATIONAL_MIG_2013', 'R_INTERNATIONAL_MIG_2014', 'R_INTERNATIONAL_MIG_2015', 'R_INTERNATIONAL_MIG_2016', 'R_INTERNATIONAL_MIG_2017', 'R_INTERNATIONAL_MIG_2018', 'R_INTERNATIONAL_MIG_2019', 'R_DOMESTIC_MIG_2011', 'R_DOMESTIC_MIG_2012', 'R_DOMESTIC_MIG_2013', 'R_DOMESTIC_MIG_2015', 'R_DOMESTIC_MIG_2016', 'R_DOMESTIC_MIG_2018', 'R_NET_MIG_2014', 'R_NET_MIG_2017', 'R_NET_MIG_2019', '2003 Rural-urban Continuum Code', 'Less than a high school diploma, 1970', 'High school diploma only, 1970', 'Some college (1-3 years), 1970', 'Four years of college or higher, 1970', 'Percent of adults with less than a high school diploma, 1970', 'Percent of adults with a high school diploma only, 1970', 'Percent of adults completing some college (1-3 years), 1970', 'Percent of adults completing four years of college or higher, 1970', 'Less than a high school diploma, 1980', 'High school diploma only, 1980', 'Some college (1-3 years), 1980', 'Four years of college or higher, 1980', 'Percent of adults with less than a high school diploma, 1980', 'Percent of adults with a high school diploma only, 1980', 'Percent of adults completing some college (1-3 years), 1980', 'Percent of adults completing four years of college or higher, 1980', 'Less than a high school diploma, 1990', 'Percent of adults with less than a high school diploma, 1990', 'Percent of adults with a high school diploma only, 1990', 'Less than a high school diploma, 2000', 'High school diploma only, 2000', \"Some college or associate's degree, 2000\", \"Bachelor's degree or higher, 2000\", 'Percent of adults with less than a high school diploma, 2000', 'Percent of adults with a high school diploma only, 2000', \"Percent of adults completing some college or associate's degree, 2000\", \"Percent of adults with a bachelor's degree or higher, 2000\", 'Less than a high school diploma, 2015-19', 'High school diploma only, 2015-19', \"Some college or associate's degree, 2015-19\", \"Bachelor's degree or higher, 2015-19\", 'Percent of adults with less than a high school diploma, 2015-19', 'Percent of adults with a high school diploma only, 2015-19', \"Percent of adults completing some college or associate's degree, 2015-19\", \"Percent of adults with a bachelor's degree or higher, 2015-19\", 'Metro_2013', 'Unemployed_2000', 'Unemployment_rate_2000', 'Unemployment_rate_2001', 'Unemployed_2002', 'Unemployment_rate_2002', 'Unemployed_2003', 'Unemployment_rate_2003', 'Civilian_labor_force_2004', 'Employed_2004', 'Unemployment_rate_2004', 'Civilian_labor_force_2005', 'Unemployed_2005', 'Unemployment_rate_2005', 'Civilian_labor_force_2006', 'Unemployed_2006', 'Unemployment_rate_2006', 'Unemployed_2007', 'Unemployment_rate_2007', 'Unemployed_2008', 'Unemployment_rate_2008', 'Employed_2009', 'Unemployment_rate_2009', 'Employed_2010', 'Unemployment_rate_2010', 'Civilian_labor_force_2011', 'Employed_2011', 'Unemployed_2011', 'Civilian_labor_force_2012', 'Employed_2012', 'Unemployed_2012', 'Unemployment_rate_2012', 'Unemployed_2013', 'Unemployment_rate_2013', 'Unemployed_2014', 'Unemployment_rate_2014', 'Civilian_labor_force_2015', 'Employed_2015', 'Unemployment_rate_2015', 'Unemployed_2016', 'Unemployment_rate_2016', 'Unemployed_2017', 'Unemployment_rate_2017', 'Unemployed_2018', 'Unemployment_rate_2018', 'Unemployment_rate_2019', 'Med_HH_Income_Percent_of_State_Total_2019', 'Rural-urban_Continuum_Code_2003', 'Urban_Influence_Code_2003', 'Rural-urban_Continuum_Code_2013', 'POVALL_2019', 'CI90LBALL_2019', 'CI90UBALL_2019', 'CI90LBALLP_2019', 'CI90UBALLP_2019', 'POV017_2019', 'CI90LB017_2019', 'CI90UB017_2019', 'CI90LB017P_2019', 'CI90LB517_2019', 'CI90UB517_2019', 'PCTPOV517_2019', 'CI90LB517P_2019', 'CI90LBINC_2019', 'CI90UBINC_2019', 'candidatevotes_2000_republican', 'candidatevotes_2004_republican', 'candidatevotes_2008_republican', 'candidatevotes_2012_republican', 'candidatevotes_2016_republican', 'share_2000_republican', 'share_2008_republican', 'share_2012_republican', 'share_2016_republican']\n\n\n\ndf2.select_dtypes(include=np.number).drop(\"per_gop\", axis = 1).columns[np.abs(lasso2.coef_)&gt;0]\nnlasso = sum(np.abs(lasso2.coef_)&gt;0)\n\nCela correspond à un modèle avec 206 variables sélectionnées.\n\n\n Hint\nDans le cas où le modèle paraîtrait trop peu parcimonieux, il faudrait revoir la phase de définition des variables pertinentes pour comprendre si des échelles différentes de certaines variables ne seraient pas plus appropriées (par exemple du log)."
  },
  {
    "objectID": "content/course/modelisation/5_clustering/index.html#introduction-sur-le-clustering",
    "href": "content/course/modelisation/5_clustering/index.html#introduction-sur-le-clustering",
    "title": "25  Clustering",
    "section": "25.1 Introduction sur le clustering",
    "text": "25.1 Introduction sur le clustering\nJusqu’à présent, nous avons fait de l’apprentissage supervisé puisque nous\nconnaissions la vraie valeur de la variable à expliquer/prédire (y). Ce n’est plus le cas avec\nl’apprentissage non supervisé.\nLe clustering est un champ d’application de l’apprentissage non-supervisé.\nIl s’agit d’exploiter l’information disponible pour regrouper des observations\nqui se ressemblent.\nL’objectif est de créer des clusters d’observations pour lesquels :\n\nau sein de chaque cluster, les observations sont homogènes (variance intra-cluster minimale)\nles clusters ont des profils hétérogènes, c’est-à-dire qu’ils se distinguent les uns des autres (variance inter-cluster maximale)\n\nEn Machine Learning, les méthodes de clustering sont très utilisées pour\nfaire de la recommandation. En faisant, par exemple, des classes homogènes de\nconsommateurs, il est plus facile d’identifier et cibler des comportements\npropres à chaque classe de consommateurs.\nCes méthodes ont également un intérêt en économie et sciences sociales parce qu’elles permettent\nde regrouper des observations sans a priori et ainsi interpréter une variable\nd’intérêt à l’aune de ces résultats. Cette publication sur la ségrégation spatiale utilisant des données de téléphonie mobile\nutilise par exemple cette approche.\nLes méthodes de clustering peuvent aussi intervenir en amont d’un problème de classification (dans des\nproblèmes d’apprentissage semi-supervisé).\nLe manuel Hands-on machine learning with scikit-learn, Keras et TensorFlow présente dans le\nchapitre dédié à l’apprentissage non supervisé quelques exemples.\nDans certaines bases de données, on peut se retrouver avec quelques exemples labellisés mais la plupart sont\nnon labellisés. Les labels ont par exemple été faits manuellement par des experts.\nPar exemple, supposons que dans la base MNIST des chiffres manuscrits, les chiffres ne soient pas labellisés\net que l’on se demande quelle est la meilleure stratégie pour labelliser cette base.\nOn pourrait regarder des images de chiffres manuscrits au hasard de la base et les labelliser.\nLes auteurs du livre montrent qu’il existe toutefois une meilleure stratégie.\nIl vaut mieux appliquer un algorithme de clustering en amont pour regrouper les images ensemble et avoir une\nimage représentative par groupe, et labelliser ces images représentatives au lieu de labelliser au hasard.\nLes méthodes de clustering sont nombreuses.\nNous allons nous pencher sur la plus intuitive : les k-means."
  },
  {
    "objectID": "content/course/modelisation/5_clustering/index.html#les-k-means",
    "href": "content/course/modelisation/5_clustering/index.html#les-k-means",
    "title": "25  Clustering",
    "section": "25.2 Les k-means",
    "text": "25.2 Les k-means\n\n25.2.1 Principe\nL’objectif des k-means est de partitionner l’espace des observations en trouvant des points (centroids) jouant le rôle de centres de gravité pour lesquels les observations proches peuvent être regroupées dans une classe homogène.\nL’algorithme k-means fonctionne par itération, en initialisant les centroïdes puis en les mettant à jour à chaque\nitération, jusqu’à ce que les centroïdes se stabilisent. Quelques exemples de clusters issus de la méthode k-means :\n\n\n\n\n\n\n\n Hint\nL’objectif des k-means est de trouver une partition des données \\(S=\\{S_1,...,S_K\\}\\) telle que\n\\[\n\\arg\\min_{S} \\sum_{i=1}^K \\sum_{x \\in S_i} ||x - \\mu_i||^2\n\\]\navec \\(\\mu_i\\) la moyenne des \\(x_i\\) dans l’ensemble de points \\(S_i\\)\n\n\n\n# packages utiles\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nfrom sklearn.cluster import KMeans #pour kmeans\nimport seaborn as sns #pour scatterplots\n\n\n\n Exercice 1 : Principe des k-means\n\nImporter les données et se restreindre aux variables 'Unemployment_rate_2019', 'Median_Household_Income_2019', 'Percent of adults with less than a high school diploma, 2015-19', \"Percent of adults with a bachelor's degree or higher, 2015-19\" et bien-sûr 'per_gop'. Appelez cette base restreinte df2 et enlevez les valeurs manquantes.\nFaire un k-means avec \\(k=4\\).\nCréer une variable label dans votes stockant le résultat de la typologie\nAfficher cette typologie sur une carte.\nChoisir les variables Median_Household_Incomme_2019 et Unemployment_rate_2019 et représenter le nuage de points en colorant différemment\nen fonction du label obtenu.\nReprésenter la distribution du vote pour chaque cluster\n\n\n\nLa carte obtenue à la question 4, qui permet de\nreprésenter spatialement nos groupes, est\nla suivante:\n\n\n\n\n\n\n\n\n\nLe nuage de point de la question 5, permettant de représenter\nla relation entre Median_Household_Incomme_2019\net Unemployment_rate_2019, aura l’aspect suivant:\n\n\n\n\n\n\n\n\n\nEnfin, l’histogramme des votes pour chaque cluster est :\n\n\n\n\n\n\n\n\n\n\n\n Hint\nIl faut noter plusieurs points sur l’algorithme implémenté par défaut par scikit-learn, que l’on peut lire dans\nla documentation :\n- l’algorithme implémenté par défaut est kmeans ++ (cf. paramètre init). Cela signifie que\nl’initialisation des centroïdes est faite de manière intelligente pour que les centroïdes initiaux soient choisis\nafin de ne pas être trop proches.\n- l’algorithme va être démarré avec n_init centroïdes différents et le modèle va choisir la meilleure initialisation\nen fonction de l’inertia du modèle, par défaut égale à 10.\nLe modèle renvoie les cluster_centers_, les labels labels_, l’inertia inertia_ et le nombre d’itérations\nn_iter_.\n\n\n\n\n25.2.2 Choisir le nombre de clusters\nLe nombre de clusters est fixé par le modélisateur.\nIl existe plusieurs façons de fixer ce nombre :\n\nconnaissance a priori du problème ;\nanalyse d’une métrique spécifique pour définir le nombre de clusters à choisir ;\netc.\n\nIl y a un arbitrage à faire\nentre biais et variance :\nun trop grand nombre de clusters implique une variance\nintra-cluster très faible (sur-apprentissage, même s’il n’est jamais possible de déterminer\nle vrai type d’une observation puisqu’on est en apprentissage non supervisé).\nSans connaissance a priori du nombre de clusters, on peut recourir à deux familles de méthodes :\n\nLa méthode du coude (elbow method): On prend le point d’inflexion de la courbe\nde performance du modèle. Cela représente le moment où ajouter un cluster\n(complexité croissante du modèle) n’apporte que des gains modérés dans la\nmodélisation des données.\nLe score de silhouette : On mesure la similarité entre un point et les autres points\ndu cluster par rapport aux autres clusters. Plus spécifiquement :\n\n\nSilhouette value is a measure of how similar an object is to its own cluster\n(cohesion) compared to other clusters (separation). The silhouette ranges\nfrom −1 to +1, where a high value indicates that the object is\nwell matched to its own cluster and poorly matched to neighboring\nclusters. If most objects have a high value, then the clustering\nconfiguration is appropriate. If many points have a low or negative\nvalue, then the clustering configuration may have too many or too few clusters\nSource: Wikipedia\n\nLe score de silhouette d’une observation est donc égal à\n(m_nearest_cluster - m_intra_cluster)/max( m_nearest_cluster,m_intra_cluster)\noù m_intra_cluster est la moyenne des distances de l’observation aux observations du même cluster\net m_nearest_cluster est la moyenne des distances de l’observation aux observations du cluster le plus proche.\nLe package yellowbrick fournit une extension utile à scikit pour représenter\nfacilement la performance en clustering.\n\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\n\nvisualizer = KElbowVisualizer(model, k=(2,12))\nvisualizer.fit(df2[xvars])        # Fit the data to the visualizer\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\nKElbowVisualizer(ax=&lt;Axes: &gt;, estimator=KMeans(n_clusters=11), k=(2, 12))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KElbowVisualizerKElbowVisualizer(ax=&lt;Axes: &gt;, estimator=KMeans(n_clusters=11), k=(2, 12))estimator: KMeansKMeans(n_clusters=11)KMeansKMeans(n_clusters=11)\n\n\n\n\n\n\n\n\n\n\n\n&lt;Axes: title={'center': 'Distortion Score Elbow for KMeans Clustering'}, xlabel='k', ylabel='distortion score'&gt;\n\n\n&lt;Figure size 768x528 with 0 Axes&gt;\n\n\n\n\n\n\n\nPour la méthode du coude, la courbe\nde performance du modèle marque un coude léger à \\(k=4\\). Le modèle initial\nsemblait donc approprié.\nyellowbrick permet également de représenter des silhouettes mais\nl’interprétation est moins aisée et le coût computationnel plus élevé :\n\nfrom yellowbrick.cluster import SilhouetteVisualizer\n\nfig, ax = plt.subplots(2, 2, figsize=(15,8))\nj=0\nfor i in [3, 4, 6, 10]:\n    j += 1\n    '''\n    Create KMeans instance for different number of clusters\n    '''\n    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)\n    q, mod = divmod(j, 2)\n    '''\n    Create SilhouetteVisualizer instance with KMeans instance\n    Fit the visualizer\n    '''\n    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])\n    ax[q-1][mod].set_title(\"k = \" + str(i))\n    visualizer.fit(df2[xvars])\n\n\n\n\n\n\nLe score de silhouette offre une représentation plus riche que la courbe coudée.\nSur ce graphique, les barres verticales en rouge et en pointillé représentent le score de silhouette\nglobal pour chaque k choisi. On voit par exemple que pour tous les k représentés ici, le\nscore de silhouette se situe entre 0.5 et 0.6 et varie peu.\nEnsuite, pour un k donné, on va avoir la représentation des scores de silhouette de chaque\nobservation, regroupées par cluster.\nPar exemple, pour k = 4, ici, on voit bien quatre couleurs différentes qui sont les 4 clusters modélisés.\nLes ordonnées sont toutes les observations clusterisées et en abscisses on a le score de silhouette de\nchaque observation. Si au sein d’un cluster, les observations ont un score de silhouette plus faible que le\nscore de silhouette global (ligne verticale en rouge), cela signifie que les observations du clusters sont\ntrop proches des autres clusters.\nGrâce à cette représentation, on peut aussi se rendre compte de la taille relative des clusters. Par exemple,\navec k = 3, on voit qu’on a deux clusters conséquents et un plus “petit” cluster relativement aux deux autres.\nCela peut nous permettre de choisir des clusters de tailles homogènes ou non.\nEnfin, quand le score de silhouette est négatif, cela signifie que la moyenne des distances de l’observation\naux observations du cluster le plus proche est inférieure à la moyenne des distances de l’observation aux\nobservations de son cluster. Cela signifie que l’observation est mal classée.\n\n\n25.2.3 Autres méthodes de clustering\nIl existe de nombreuses autres méthodes de clustering. Parmi les plus connues, on peut citer deux exemples en particulier :\n\nDBSCAN\nles mélanges de Gaussiennes\n\n\n25.2.3.1 DBSCAN\nL’algorithme DBSCAN est implémenté dans sklearn.cluster.\nIl peut être utilisé pour faire de la détection d’anomalies\nnotamment.\nEn effet, cette méthode repose sur le clustering en régions où la densité\ndes observations est continue, grâce à la notion de voisinage selon une certaine distance epsilon.\nPour chaque observation, on va regarder si dans son voisinage selon une distance epsilon, il y a des voisins. S’il y a au\nmoins min_samples voisins, alors l’observation sera une core instance.\nLes observations qui ne sont pas des core instances et qui n’en ont pas dans leur voisinage selon une distance espilon\nvont être détectées comme des anomalies.\n\n\n25.2.3.2 Les mélanges de gaussiennes\nEn ce qui concerne la théorie, voir le cours Probabilités numériques et statistiques computationnelles, M1 Jussieu, V.Lemaire et T.Rebafka\nSe référer notamment aux notebooks pour l’algorithme EM pour mélange gaussien.\nDans sklearn, les mélanges gaussiens sont implémentés dans sklearn.mixture comme GaussianMixture.\nLes paramètres importants sont alors le nombre de gaussiennes n_components et le nombre d’initiatisations n_init.\nIl est possible de faire de la détection d’anomalie savec les mélanges de gaussiennes.\n\n\n Pour aller plus loin\nIl existe de nombreuses autres méthodes de clustering :\n\nLocal outlier factor ;\nbayesian gaussian mixture models ;\ndifférentes méthodes de clustering hiérarchique ;\netc."
  },
  {
    "objectID": "content/course/modelisation/6_pipeline/index.html#pourquoi-utiliser-les-pipelines",
    "href": "content/course/modelisation/6_pipeline/index.html#pourquoi-utiliser-les-pipelines",
    "title": "26  Premier pas vers l’industrialisation avec les pipelines scikit",
    "section": "26.1 Pourquoi utiliser les pipelines ?",
    "text": "26.1 Pourquoi utiliser les pipelines ?\nLes chapitres précédents ont permis de montrer des bouts de code\népars pour entraîner des modèles ou faire du preprocessing.\nCette démarche est intéressante pour tâtonner mais risque d’être coûteuse\nultérieurement s’il est nécessaire d’ajouter une étape de preprocessing\nou de changer d’algorithme.\nHeureusement, scikit propose un excellent outil pour proposer un cadre\ngénéral pour créer une chaîne de production machine learning. Il\ns’agit des\npipelines.\nIls présentent de nombreux intérêts, parmi lesquels:\n\nIls sont très pratiques et lisibles. On rentre des données en entrée, on n’appelle qu’une seule fois les méthodes fit et predict ce qui permet de s’assurer une gestion cohérente des transformations de variables, par exemple après l’appel d’un StandardScaler\nLa modularité rend aisée la mise à jour d’un pipeline et renforce la capacité à le réutiliser\nIls permettent de facilement chercher les hyperparamètres d’un modèle. Sans pipeline, écrire un code qui fait du tuning d’hyperparamètres peut être pénible. Avec les pipelines, c’est une ligne de code.\nLa sécurité d’être certain que les étapes de preprocessing sont bien appliquées aux jeux de données désirés avant l’estimation.\n\n{{% box status=“hint” title=“Hint” icon=“fa fa-lightbulb” %}}\nUn des intérêts des pipelines scikit est qu’ils fonctionnent aussi avec\ndes méthodes qui ne sont pas issues de scikit.\nIl est très\nfacile d’introduire un modèle de réseau de neurone Keras dans\nun pipeline scikit.\nPour introduire un modèle économétrique statsmodels\nc’est un peu plus coûteux mais nous allons proposer des exemples\nqui peuvent servir de modèle et qui montrent que c’est faisable\nsans trop de difficulté.\n{{% /box %}}\n{{% box status=“warning” title=“Warning” icon=“fa fa-exclamation-triangle” %}}\nLes éléments présents dans ce chapitre nécessitent une version assez récente\nde scikit (au moins la version 1.0). Pour vérifier la version, faire:\n\nimport sklearn\nprint(sklearn.__version__)\n\nIl faut également une version récente de yellowbrick pour éviter l’erreur\nsuivante quand on utilise une version récente de scikit (ce que\nje recommande):\n\nImportError: cannot import name 'safe_indexing' from 'sklearn.utils'\n\n{{% /box %}}"
  },
  {
    "objectID": "content/course/modelisation/6_pipeline/index.html#comment-créer-un-pipeline",
    "href": "content/course/modelisation/6_pipeline/index.html#comment-créer-un-pipeline",
    "title": "26  Premier pas vers l’industrialisation avec les pipelines scikit",
    "section": "26.2 Comment créer un pipeline",
    "text": "26.2 Comment créer un pipeline\nUn pipeline est un enchaînement d’opérations qu’on code en enchainant\ndes pairs (clé, valeur):\n\nla clé est le nom du pipeline, cela peut être utile lorsqu’on va\nreprésenter le pipeline sous forme de diagramme acyclique (visualisation DAG)\nou qu’on veut afficher des informations sur une étape\nla valeur représente la transformation à mettre en oeuvre dans le pipeline\n(c’est-à-dire, à l’exception de la dernière étape,\nmettre en oeuvre une méthode transform et éventuellement une\ntransformation inverse).\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\n\nestimators = [('reduce_dim', PCA()), ('clf', SVC())]\npipe = Pipeline(estimators)\n\n{{% box status=“hint” title=“Hint” icon=“fa fa-lightbulb” %}}\nIl est pratique de visualiser un pipeline sous forme de DAG.\nPour cela, dans un notebook, on utilise la configuration\nsuivante:\n#| eval: false\nfrom sklearn import set_config\nset_config(display='diagram') \n{{% /box %}}\npipe\nAu sein d’une étape de pipeline, les paramètres d’un estimateur\nsont accessibles avec la notation &lt;estimator&gt;__&lt;parameter&gt;.\nCela permet de fixer des valeurs pour les arguments des fonctions scikit\nqui sont appelées au sein d’un pipeline.\nC’est cela qui rendra l’approche des pipelines particulièrement utile\npour la grid search:\n\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {\"reduce_dim__n_components\":[2, 5, 10], \"clf__C\":[0.1, 10, 100]}\ngrid_search = GridSearchCV(pipe, param_grid=param_grid)\n\n\n26.2.1 Données utilisées\nNous allons utiliser les données de transactions immobilières DVF pour chercher\nla meilleure manière de prédire, sachant les caractéristiques d’un bien, son\nprix.\nCes données peuvent être importées directement depuis data.gouv:\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nmutations = pd.read_csv('https://www.data.gouv.fr/fr/datasets/r/3004168d-bec4-44d9-a781-ef16f41856a2', sep = \"|\", decimal=\",\")\n\nOn propose d’enrichir la base de quelques variables qui pourraient servir\nultérieurement:\n\nmutations['Date mutation'] = pd.to_datetime(mutations['Date mutation'], format = \"%d/%m/%Y\")\nmutations['year'] = mutations['Date mutation'].dt.year\nmutations['month'] = mutations['Date mutation'].dt.month\nmutations['dep'] = mutations['Code postal'].astype(str).str[:2]\nmutations['lprix'] = np.log(mutations[\"Valeur fonciere\"])\n\nSi vous travaillez avec les données de 2020, n’oubliez pas\nd’intégrer l’effet\nconfinement strict dans vos modèles. Pour cela, vous pouvez créer une variable\nindicatrice entre les dates en question:\n\nmutations['confinement'] = mutations['Date mutation'].between(pd.to_datetime(\"2020-03-17\"), pd.to_datetime(\"2020-05-03\")).astype(int)\n\nLes données DVF proposent une observation par transaction. Ces transactions\npeuvent concerner plusieurs lots.\nPour simplifier,\non va créer une variable de surface qui agrège les différentes informations\nde surface disponibles dans le jeu de données. En effet, les variables\nen question sont très corrélées les unes entre elles :\n\ng.figure.get_figure()\n\nLes agréger revient à supposer que le modèle de fixation des prix est le même\nentre chaque lot. C’est une hypothèse simplificatrice qu’une personne plus\nexperte du marché immobilier, ou qu’une approche propre de sélection\nde variable pourrait amener à nier\n\nmutations['surface'] = mutations.loc[:, mutations.columns[mutations.columns.str.startswith('Surface Carrez')].tolist()].sum(axis = 1)"
  },
  {
    "objectID": "content/course/modelisation/6_pipeline/index.html#un-premier-pipeline-random-forest-sur-des-variables-standardisées",
    "href": "content/course/modelisation/6_pipeline/index.html#un-premier-pipeline-random-forest-sur-des-variables-standardisées",
    "title": "26  Premier pas vers l’industrialisation avec les pipelines scikit",
    "section": "26.3 Un premier pipeline: random forest sur des variables standardisées",
    "text": "26.3 Un premier pipeline: random forest sur des variables standardisées\nNotre premier pipeline va nous permettre d’intégrer ensemble:\n\nUne étape de preprocessing avec la standardisation de variables\nUne étape d’estimation du prix en utilisant un modèle de random forest\n\nPour le moment, on va prendre comme acquis un certain nombre de variables\nexplicatives (les features) et les hyperparamètres du modèle\n\n26.3.1 Définition des ensembles train/test\nNous allons donc nous restreindre à un sous-ensemble de colonnes dans un\npremier temps :\n\nxvars = ['dep', 'Nombre de lots', 'Code type local', 'surface', 'Nombre pieces principales']\nxvars2 = pd.Series(xvars).str.replace(\" \",\"_\").tolist()\n\nmutations2 = mutations.loc[:, xvars + [\"Valeur fonciere\"]]\n\nNous allons également ne conserver que les transactions inférieures à 5 millions\nd’euros (on anticipe que celles ayant un montant supérieur sont des transactions\nexceptionnelles dont le mécanisme de fixation du prix diffère)\n\nmutations2  = mutations2.dropna()\nmutations2 = mutations2.loc[mutations2['Valeur fonciere'] &lt; 5e6] #keep only values below 10 millions\n\nmutations2.columns = mutations2.columns.str.replace(\" \", \"_\")\nnumeric_features = mutations2.columns[~mutations2.columns.isin(['dep','Code_type_local', 'confinement'])].tolist()\ncategorical_features = ['dep','Code_type_local']\n\nAu passage, nous avons abandonné la variable de code postal pour privilégier\nla commune afin de réduire la dimension de notre jeu de données. Si on voulait\nvraiment avoir un bon modèle, il faudrait faire autrement car le code postal\nest probablement un très bon prédicteur du prix d’un bien, une fois que\nles caractéristiques du bien sont contrôlées.\nNous allons stratifier notre échantillonage de train/test par département\nafin de tenir compte, de manière minimale, de la géographie.\nPour accélérer les calculs pour ce tutoriel, nous n’allons considérer que\n20% des transactions observées sur chaque département.\n\nfrom sklearn.model_selection import train_test_split\n\nmutations2 = mutations2.groupby('dep').sample(frac = 0.2, random_state = 123)\n\nX_train, X_test, y_train, y_test = train_test_split(mutations2[xvars2], mutations2[[\"Valeur_fonciere\"]].values.ravel(), test_size = 0.2, random_state = 0, stratify=mutations2[['dep']])\n\n\n\n26.3.2 Définition du premier pipeline\nNous allons donc partir d’un random forest avec des valeurs d’hyperparamètres\ndonnées.\nLes random forest sont une méthode d’aggrégation1 d’arbres de décision.\nOn calcule \\(K\\) arbres de décision et en tire, par une méthode d’agrégation,\nune règle de décision moyenne qu’on va appliquer pour tirer une\nprédiction de nos données.\n\n\n\n\n\nC’est un article de Léo Breiman (2001)2, statisticien à Berkeley, qui\nest à l’origine du succès des random forests. L’un des intérêts\ndes random forest est qu’il existe des méthodes pour déterminer\nl’importance relative de chaque variable dans la prédiction.\nPour commencer, nous allons fixer la taille des arbres de décision avec\nl’hyperparamètre max_depth = 2.\n\nfrom sklearn.ensemble import RandomForestRegressor\nregr = RandomForestRegressor(max_depth=2, random_state=123)\n\nNotre pipeline va intégrer les étapes suivantes:\n\nPreprocessing:\n\nLes variables numériques vont être standardisées avec un StandardScaler.\nPour cela, nous allons utiliser la liste numeric_features définie précédemment.\nLes variables catégorielles vont être explosées avec un one hot encoding\n(méthode OneHotEncoder de scikit)\nPour cela, nous allons utiliser la liste categorical_features\n\nRandom forest: nous allons appliquer l’estimateur regr défini plus haut\n\nJ’ajoute en commentaire un exemple de comment s’introduirait une imputation\nde valeurs manquantes. La version 1.0 de scikit facilite l’intégration\nd’étapes complexes dans les pipelines3. Si vous utilisez une\nversion antérieure à la 1.0 de scikit, vous pouvez vous rendre dans\nla section Annexe pour avoir des exemples de définition alternative\n(attention cependant, vous ne pourrez récupérer le nom des features\ntransformées comme ici, ce qui peut pénaliser l’analyse d’importance\nde variables)\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import make_column_transformer\n\nnumeric_pipeline = make_pipeline(\n  #SimpleImputer(),\n  StandardScaler()\n)\ntransformer = make_column_transformer(\n    (numeric_pipeline, numeric_features[:-1]),\n    (OneHotEncoder(sparse = False, handle_unknown = \"ignore\"), categorical_features))\npipe = Pipeline(steps=[('preprocessor', transformer),\n                      ('randomforest', regr)])\n\nNous avons construit ce pipeline sous forme de couches successives. La couche\nrandomforest prendra automatiquement le résultat de la couche preprocessor\nen input. La couche features permet d’introduire de manière relativement\nsimple (quand on a les bonnes méthodes) la complexité du preprocessing\nsur données réelles dont les types divergent.\nOn peut visualiser le graphe et ainsi se représenter la manière dont\nce pipeline opère:\npipe\nMaintenant, il ne reste plus qu’à estimer notre modèle sur l’ensemble\nd’entraînement. C’est très simple avec un pipeline : il suffit d’utiliser\nde mettre à jour le pipeline avec la méthode fit\nOn peut utiliser le nom du pipeline en conjonction de certaines méthodes\npour appliquer cette étape sur un jeu de données pour visualiser\nl’effet de la transformation.\nPar exemple, pour visualiser le jeu de données transformé avant l’étape\nd’estimation, on peut\nfaire\n\npipe[:-1].fit_transform(X_train)\n\nDe même, si on veut récupérer le nom des features en sortie du preprocessing,\non utilisera la méthode .get_feature_names_out qui est bien pratique\n(c’est cette méthode qui est plus complexe à appeler dans les versions scikit\nancienne qui nous a fait privilégier le pipeline ci-dessous)\n\nfeatures_names=pipe['preprocessor'].get_feature_names_out()\nfeatures_names\n\n\n\n26.3.3 Variable importance\nOn ne va représenter, parmi notre ensemble important de colonnes, que celles\nqui ont une importance non nulle. Grâce à notre vecteur features_names,\non va pouvoir facilement afficher le nom des colonnes en question (et donc\ncomprendre les features déterminantes)\nL’importance va être définie à partir\nde la mesure d’impureté4\nOn voit donc que deux variables déterminantes sont des effets fixes\ngéographiques (qui servent à ajuster de la différence de prix entre\nParis et les Hauts de Seine et le reste de la France), une autre variable\nest un effet fixe type de bien. Les deux variables qui pourraient introduire\nde la variabilité, à savoir la surface et, dans une moindre mesure, le\nnombre de lots, ont une importance moindre.\n{{% box status=“note” title=“Note” icon=“fa fa-comment” %}}\nIdéalement, on utiliserait yellowbrick pour représenter l’importance des variables\nMais en l’état actuel du pipeline on a beaucoup de variables dont le poids\nest nul qui viennent polluer la visualisation. Vous pouvez\nconsulter la\ndocumentation de yellowbrick sur ce sujet\n{{% /box %}}\n\n\n26.3.4 Prédiction\nL’analyse de l’importance de variables permet de mieux comprendre\nle fonctionnement interne des random forests.\nOn obtient un modèle dont les performances sont les suivantes:\n\nfrom sklearn.metrics import mean_squared_error\n\n\ncompar = pd.DataFrame([y_test, pipe.predict(X_test)]).T\ncompar.columns = ['obs','pred']\ncompar['diff'] = compar.obs - compar.pred\n\nprint(\"Le RMSE sur le jeu de test est {:,}\".format(\n   int(np.sqrt(mean_squared_error(y_test, pipe.predict(X_test))))\n))\n\nLe RMSE n’est pas très bon. Pour comprendre pourquoi, représentons\nnotre nuage de point des valeurs observées et prédites:\nC’est très décevant. La prédiction a trop peu de variabilité pour capturer\nla variance des prix observée. Cela vient du fait que les variables\nayant de l’importance dans la prédiction sont principalement des effets fixes,\nqui ne permettent donc qu’une variabilité limitée.\n\ng.figure.get_figure()"
  },
  {
    "objectID": "content/course/modelisation/6_pipeline/index.html#recherche-des-hyperparamètres-optimaux-avec-une-validation-croisée",
    "href": "content/course/modelisation/6_pipeline/index.html#recherche-des-hyperparamètres-optimaux-avec-une-validation-croisée",
    "title": "26  Premier pas vers l’industrialisation avec les pipelines scikit",
    "section": "26.4 Recherche des hyperparamètres optimaux avec une validation croisée",
    "text": "26.4 Recherche des hyperparamètres optimaux avec une validation croisée\nOn détecte que le premier modèle n’est pas très bon et ne nous aidera\npas vraiment à évaluer de manière fiable la maison de nos rêves.\nOn va essayer de voir si notre modèle ne serait pas meilleur avec des\nhyperparamètres plus adaptés. Après tout, nous avons choisi par défaut\nla profondeur de l’arbre mais c’était un choix au doigt mouillé.\nQuels sont les hyperparamètres qu’on peut essayer d’optimiser ?\n\npipe['randomforest'].get_params()\n\nUn détour par la documentation\nnous aide à comprendre ceux sur lesquels on va jouer. Par exemple, il serait\nabsurde de jouer sur le paramètre random_state qui est la racine du générateur\npseudo-aléatoire.\nComme l’objectif est de se concentrer sur la démarche plus qu’essayer de\ntrouver un bon modèle,\nnous allons également réduire la taille des données pour accélérer\nles calculs\n\nmutations2 = mutations2.groupby('dep').sample(frac = 0.5, random_state = 123)\n\nX_train, X_test, y_train, y_test = train_test_split(mutations2[xvars2], mutations2[[\"Valeur_fonciere\"]].values.ravel(), test_size = 0.2, random_state = 0, stratify=mutations2[['dep']])\n\nX = pd.concat((X_train, X_test), axis=0)\nY = np.concatenate([y_train,y_test])\n\nNous allons nous contenter de jouer sur les paramètres:\n\nn_estimators: Le nombre d’arbres de décision que notre forêt contient\nmax_depth: La profondeur de chaque arbre\n\nIl existe plusieurs manières de faire de la validation croisée. Nous allons ici\nutiliser la grid search qui consiste à estimer et tester le modèle sur chaque\ncombinaison d’une grille de paramètres et sélectionner le couple de valeurs\ndes hyperparamètres amenant à la meilleure prédiction. Par défaut, scikit\neffectue une 5-fold cross validation. Nous n’allons pas changer\nce comportement.\nComme expliqué précédemment, les paramètres s’appelent sous la forme\n&lt;step&gt;__&lt;parameter_name&gt;\nLa validation croisée pouvant être très consommatrice de temps, nous\nn’allons l’effectuer que sur un nombre réduit de valeurs de notre grille.\nIl est possible de passer la liste des valeurs à passer au crible sous\nforme de liste (comme pour l’argument max_depth ci-dessous) ou\nsous forme d’array (comme pour l’argument n_estimators) ce qui est\nsouvent pratique pour générer un criblage d’un intervalle avec np.linspace.\nOn peut récupérer les paramètres optimaux avec la méthode best_params_:\n\ngrid_search.best_params_\n\nOn pourra aussi ré-utiliser le modèle optimal de la manière suivante:\ngrid_search.best_estimator_\nToutes les performances sur les ensembles d’échantillons et de test sur la grille\nd’hyperparamètres sont disponibles dans l’attribut:\n\nperf_random_forest = pd.DataFrame(grid_search.cv_results_)\n\nRegardons les résultats moyens pour chaque valeur des hyperparamètres:\nGlobalement, à profondeur d’arbre donnée, le nombre d’arbres change\nmarginalement la performance (cela détériore\nla performance quand la profondeur est de 4, cela améliore quand\non fixe la profondeur de 2).\nEn revanche, changer la profondeur de l’arbre améliore la\nperformance de manière plus marquée.\nMaintenant, il nous reste à re-entraîner le modèle avec ces nouveaux\nparamètres sur l’ensemble du jeu de train et l’évaluer sur l’ensemble\ndu jeu de test:\nOn obtient le RMSE suivant:\n\nprint(\"Le RMSE sur le jeu de test est {:,}\".format(\n   int(np.sqrt(mean_squared_error(y_test, pipe_optimal.predict(X_test))))\n))\n\nEt si on regarde la qualité en prédiction:\n\ng.figure.get_figure()\n\nOn obtient plus de variance dans la prédiction, c’est déjà un peu mieux.\nCependant, cela reste décevant pour plusieurs raisons:\n\nnous n’avons pas fait d’étape de sélection de variable\nnous n’avons pas chercher à déterminer si la variable à prédire la plus\npertinente était le prix ou une transformation de celle-ci\n(par exemple le prix au \\(m^2\\))\n\n\nfeatures_names=pipe_optimal['preprocessor'].get_feature_names_out()\nimportances = pipe_optimal['randomforest'].feature_importances_\nstd = np.std([tree.feature_importances_ for tree in pipe_optimal['randomforest'].estimators_], axis=0)\n\nforest_importances = pd.Series(importances[importances&gt;0], index=features_names[importances&gt;0])\n\nfig, ax = plt.subplots()\nforest_importances.plot.bar(yerr=std[importances&gt;0], ax=ax)\nax.set_title(\"Feature importances using MDI\")\nax.set_ylabel(\"Mean decrease in impurity\")\nfig.tight_layout()\n\n\n26.4.1 Remarque sur la performance\nLes estimations sont, par défaut, menées de manière séquentielle (l’une après\nl’autre). Nous sommes cependant face à un problème\nembarassingly parallel.\nPour gagner en performance, il est recommandé d’utiliser l’argument\nn_jobs=-1.\n\n\n26.4.2 Remarque sur l’échantillonnage\nEn l’état actuel de l’échantillonnage entre train et test au sein de la\ngrid search,\non est face à un problème de data leaking car l’échantillon\nn’est pas balancé entre nos classes (les départements).\nCertaines classes se\nretrouvent hors de l’échantillon d’estimation mais dans l’échantillon de prédiction.\nAutrement dit, notre pipeline de preprocessing se retrouve à devoir\nnettoyer des valeurs qu’il ne connaît pas.\nNous avons choisi une option, dans notre pipeline pour se faciliter la vie\nà ce propos. Nous ne rencontrons pas d’erreur car nous avons utilisé l’option\nhandle_unknown = \"ignore\" plutôt que\nhandle_unknown = \"error\" (défaut) dans le one hot encoding.\nCette option est dangereuse et n’est pas recommandée pour un vrai pipeline.\nDe manière générale, il vaut mieux adopter une approche de\nprogrammation défensive en n’hésitant pas à renvoyer une erreur si la\nstructure du DataFrame de prédiction diffère vraiment de celle du DataFrame\nd’entraînement.\nPour éviter cette erreur, il serait mieux de définir explicitement le schéma de\nvalidation croisée à mettre en oeuvre.\nPrécédemment, nous avions utilisé un échantillonnage stratifié.\nCela pourrait être fait ici avec\nla méthode StratifiedGroupKFold (plus d’éléments à venir)\nfrom sklearn.model_selection import StratifiedGroupKFold\ncv = StratifiedGroupKFold(n_splits=5)\n#grid_search.fit(pd.concat((X_train, X_test), axis=0), np.concatenate([y_train,y_test]), cv = cv, groups = pd.concat((X_train, X_test), axis=0)['dep'])"
  },
  {
    "objectID": "content/course/modelisation/6_pipeline/index.html#eléments-supplémentaires-à-venir",
    "href": "content/course/modelisation/6_pipeline/index.html#eléments-supplémentaires-à-venir",
    "title": "26  Premier pas vers l’industrialisation avec les pipelines scikit",
    "section": "26.5 Eléments supplémentaires à venir",
    "text": "26.5 Eléments supplémentaires à venir\nCe chapitre est amené à être enrichi des éléments suivants\n(cf. #207)\n\nComparaison performance entre modèles grâce aux pipelines\nIntégration d’une étape de sélection de variable dans un pipeline\nstatsmodels dans un pipeline\nKeras dans un pipeline"
  },
  {
    "objectID": "content/course/modelisation/6_pipeline/index.html#annexes-pipelines-alternatifs",
    "href": "content/course/modelisation/6_pipeline/index.html#annexes-pipelines-alternatifs",
    "title": "26  Premier pas vers l’industrialisation avec les pipelines scikit",
    "section": "26.6 Annexes: pipelines alternatifs",
    "text": "26.6 Annexes: pipelines alternatifs\n\n26.6.1 Préalable : quelques méthodes pour gagner en flexibilité dans le preprocessing\nNotre DataFrame comporte des types hétérogènes de variables:\n\nDes variables numériques dont les variances sont très hétérogènes\nDes variables textuelles qui mériteraient un recodage sous forme numérique\nDes variables discrètes dont les modalités devraient être éclatées (one hot encoding)\n\nPour gagner en flexibilité, nous allons proposer certaines méthodes qui permettent\nd’appliquer les étapes de preprocessing adéquates à un sous-ensemble de\nvariables5. Ces méthodes ne sont plus nécessaires dans les versions\nrécentes de scikit.\nPour cela, il convient d’adopter l’approche de la programmation orientée objet.\nOn va créer des classes avec des méthodes transform et fit_transform\nqui pourront ainsi être intégrées directement dans les pipelines, comme s’il\ns’agissait de méthodes issues de scikit.\nLa première généralise LabelEncoder à un sous-ensemble de colonnes:\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nclass MultiColumnLabelEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)\n\nLa seconde généralise cette fois le one hot encoding à un sous ensemble de\nfonctions\n\nclass MultiColumnOneHotEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                output[col] = OneHotEncoder(sparse=False).fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = OneHotEncoder(sparse=False).fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)\n\nLes méthodes suivantes vont nous permettre de passer en arguments les noms\nde colonnes pour intégrer la récupération des bonnes colonnes de nos\ndataframes dans le pipeline:\n\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n\nclass Columns(BaseEstimator, TransformerMixin):\n    def __init__(self, names=None):\n        self.names = names\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X):\n        return X[self.names]\n\nclass Normalize(BaseEstimator, TransformerMixin):\n    def __init__(self, func=None, func_param={}):\n        self.func = func\n        self.func_param = func_param\n\n    def transform(self, X):\n        if self.func != None:\n            return self.func(X, **self.func_param)\n        else:\n            return X\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\nEnfin, on va créer une méthode intermédiaire sous forme de hack\n(elle prend une matrice en entrée et renvoie la même matrice)\npour\npouvoir facilement récupérer notre matrice de feature afin de vérifier\nses caractéristiques (notamment le nombre de colonnes disponibles):\n\nclass Collect(BaseEstimator, TransformerMixin):\n\n    def transform(self, X):\n        #print(X.shape)\n        #self.shape = shape\n        # what other output you want\n        return X\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n\nfrom sklearn.pipeline import make_pipeline, FeatureUnion\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\n\npipe2 = Pipeline([\n    (\"features\", FeatureUnion([\n        ('numeric', make_pipeline(Columns(names=numeric_features[:-1]),StandardScaler())),\n        ('categorical', make_pipeline(Columns(names=categorical_features),OneHotEncoder(sparse=False)))\n    ])),\n    ('identity', Collect()),\n    ('randomforest', regr)\n])\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('numeric', StandardScaler(), numeric_features[:-1]),\n        ('categorical', OneHotEncoder(sparse=False, handle_unknown = \"ignore\"), categorical_features)])\n\npipe3 = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('randomforest', regr)])"
  },
  {
    "objectID": "content/course/modelisation/6_pipeline/index.html#références",
    "href": "content/course/modelisation/6_pipeline/index.html#références",
    "title": "26  Premier pas vers l’industrialisation avec les pipelines scikit",
    "section": "26.7 Références",
    "text": "26.7 Références\n\nBreiman L (2001). “Random Forests”. Machine Learning. 45 (1): 5–32."
  },
  {
    "objectID": "content/course/NLP/index.html#résumé-de-la-partie",
    "href": "content/course/NLP/index.html#résumé-de-la-partie",
    "title": "Partie 4 : Natural Language Processing (NLP)",
    "section": "Résumé de la partie",
    "text": "Résumé de la partie\nPython est un excellent outil pour l’analyse de données textuelles.\nLes méthodes de base ou les librairies spécialisées\ncomme NLTK et SpaCy permettent d’effectuer ces tâches de manière\ntrès efficace. Les ressources en ligne sur le sujet sont très\nnombreuses. Python est bien mieux outillé que R pour l’analyse de\ndonnées textuelles.\nDans un premier temps, cette partie propose\nde revenir sur la manière de structurer et nettoyer un corpus\ntextuel au travers de l’approche bag of words (sac de mots).\nElle vise à montrer comment transformer un corpus en outil propre à une\nanalyse statistique :\n\nElle propose d’abord une introduction aux enjeux du nettoyage des données\ntextuelles à travers l’analyse du Comte de Monte Cristo d’Alexandre Dumas\nici qui permet de synthétiser rapidement l’information disponible\ndans un large volume de données (à l’image de la Figure 27.1)\nElle propose ensuite une série d’exercices sur le nettoyage de textes à partir des\noeuvres d’Edgar Allan Poe, Mary Shelley et H.P. Lovecraft visant à distinguer la\nspécificité du vocabulaire employé par chaque auteurs (par exemple Figure 28.2). Ces exercices sont\ndisponibles dans le deuxième chapitre de la partie.\n\nEnsuite, nous proposerons d’explorer une approche alternative, prenant en compte\nle contexte d’apparition d’un mot. L’introduction à la\nLatent Dirichlet Allocation (LDA) sera l’occasion de présenter la modélisation\nde documents sous la forme de topics.\nEnfin, nous introduirons aux enjeux de la transformation de champs textuels\nsous forme de vecteurs numériques. Pour cela, nous présenterons le principe\nde Word2Vec qui permet ainsi, par exemple,\nmalgré une distance syntaxique importante,\nde dire que sémantiquement Homme et Femme sont proches.\nCe chapitre est une passerelle vers le concept d’embedding, véritable\nrévolution récente du NLP, et qui permet de rapprocher des corpus\nnon seulement sur leur proximité syntaxique (partagent-ils par exemple des mots\ncommuns ?) mais aussi sur leur proximité sémantique (partagent-ils un thème ou un sens commun ?).4"
  },
  {
    "objectID": "content/course/NLP/index.html#pour-aller-plus-loin",
    "href": "content/course/NLP/index.html#pour-aller-plus-loin",
    "title": "Partie 4 : Natural Language Processing (NLP)",
    "section": "Pour aller plus loin",
    "text": "Pour aller plus loin\nLa recherche dans le domaine du NLP est très active. Il est donc recommandé\nde faire preuve de curiosité pour en apprendre plus car une ressource\nunique ne compilera pas l’ensemble des connaissances, a fortiori dans\nun champ de recherche aussi dynamique que le NLP.\nPour approfondir les compétences évoquées dans ce cours, je recommande vivement\nce cours d’HuggingFace."
  },
  {
    "objectID": "content/course/NLP/01_intro/index.html#base-dexemple",
    "href": "content/course/NLP/01_intro/index.html#base-dexemple",
    "title": "27  Quelques éléments pour comprendre les enjeux du NLP",
    "section": "27.1 Base d’exemple",
    "text": "27.1 Base d’exemple\nLa base d’exemple est le Comte de Monte Cristo d’Alexandre Dumas.\nIl est disponible\ngratuitement sur le site\nProject Gutemberg comme des milliers\nd’autres livres du domaine public. La manière la plus simple de le récupérer\nest de télécharger avec le package request le fichier texte et le retravailler\nlégèrement pour ne conserver que le corpus du livre :\n\nfrom urllib import request\n\nurl = \"https://www.gutenberg.org/files/17989/17989-0.txt\"\nresponse = request.urlopen(url)\nraw = response.read().decode('utf8')\n\ndumas = raw.split(\"*** START OF THE PROJECT GUTENBERG EBOOK LE COMTE DE MONTE-CRISTO, TOME I ***\")[1].split(\"*** END OF THE PROJECT GUTENBERG EBOOK LE COMTE DE MONTE-CRISTO, TOME I ***\")[0]\n\nimport re\n\ndef clean_text(text):\n    text = text.lower() # mettre les mots en minuscule\n    text = \" \".join(text.split())\n    return text\n\ndumas = clean_text(dumas)\n\ndumas[10000:10500]\n\n\" mes yeux. --vous avez donc vu l'empereur aussi? --il est entré chez le maréchal pendant que j'y étais. --et vous lui avez parlé? --c'est-à-dire que c'est lui qui m'a parlé, monsieur, dit dantès en souriant. --et que vous a-t-il dit? --il m'a fait des questions sur le bâtiment, sur l'époque de son départ pour marseille, sur la route qu'il avait suivie et sur la cargaison qu'il portait. je crois que s'il eût été vide, et que j'en eusse été le maître, son intention eût été de l'acheter; mais je lu\""
  },
  {
    "objectID": "content/course/NLP/01_intro/index.html#la-particularité-des-données-textuelles",
    "href": "content/course/NLP/01_intro/index.html#la-particularité-des-données-textuelles",
    "title": "27  Quelques éléments pour comprendre les enjeux du NLP",
    "section": "27.2 La particularité des données textuelles",
    "text": "27.2 La particularité des données textuelles\n\n27.2.1 Objectif\nLe natural language processing (NLP) ou\ntraitement automatisé de la langue (TAL) en Français,\nvise à extraire de l’information de textes à partir d’une analyse statistique du contenu.\nCette définition permet d’inclure de nombreux champs d’applications au sein\ndu NLP (traduction, analyse de sentiment, recommandation, surveillance, etc. ) ainsi que de méthodes.\nCette approche implique de transformer un texte, qui est une information compréhensible par un humain, en un nombre, information appropriée pour un ordinateur et une approche statistique ou algorithmique.\nTransformer une information textuelle en valeurs numériques propres à une analyse statistique n’est pas une tâche évidente. Les données textuelles sont non structurées puisque l’information cherchée, qui est propre à chaque analyse, est perdue au milieu d’une grande masse d’informations qui doit, de plus, être interprétée dans un certain contexte (un même mot ou une phrase n’ayant pas la même signification selon le contexte).\nSi cette tâche n’était pas assez difficile comme ça, on peut ajouter d’autres difficultés propres à l’analyse textuelle car ces données sont :\n\nbruitées : ortographe, fautes de frappe…\nchangeantes : la langue évolue avec de nouveaux mots, sens…\ncomplexes : structures variables, accords…\nambigues : synonymie, polysémie, sens caché…\npropres à chaque langue : il n’existe pas de règle de passage unique entre deux langues\ngrande dimension : des combinaisons infinies de séquences de mots\n\n\n\n27.2.2 Méthode\nL’unité textuelle peut être le mot ou encore une séquence de n\nmots (un n-gramme) ou encore une chaîne de caractères (e.g. la\nponctuation peut être signifiante). On parle de token. L’analyse textuelle vise à transformer le texte en données\nnumériques manipulables.\nOn peut ensuite utiliser diverses techniques (clustering,\nclassification supervisée) suivant l’objectif poursuivi pour exploiter\nl’information transformée. Mais les étapes de nettoyage de texte sont indispensables car sinon un algorithme sera incapable de détecter une information pertinente dans l’infini des possibles."
  },
  {
    "objectID": "content/course/NLP/01_intro/index.html#nettoyer-un-texte",
    "href": "content/course/NLP/01_intro/index.html#nettoyer-un-texte",
    "title": "27  Quelques éléments pour comprendre les enjeux du NLP",
    "section": "27.3 Nettoyer un texte",
    "text": "27.3 Nettoyer un texte\nLes wordclouds sont des représentations graphiques assez pratiques pour visualiser\nles mots les plus fréquents. Elles sont très simples à implémenter en Python\navec le module wordcloud qui permet même d’ajuster la forme du nuage à\nune image :\n\n\nimport wordcloud\nimport numpy as np\nimport io\nimport requests\nimport PIL\nimport matplotlib.pyplot as plt\n\nimg = \"https://raw.githubusercontent.com/linogaliana/python-datascientist/master/content/course/NLP/book.png\"\nbook_mask = np.array(PIL.Image.open(io.BytesIO(requests.get(img).content)))\n\nfig = plt.figure()\n\ndef make_wordcloud(corpus):\n    wc = wordcloud.WordCloud(background_color=\"white\", max_words=2000, mask=book_mask, contour_width=3, contour_color='steelblue')\n    wc.generate(corpus)\n    return wc\n\nplt.imshow(make_wordcloud(dumas), interpolation='bilinear')\nplt.axis(\"off\")\n#plt.show()\n#plt.savefig('word.png', bbox_inches='tight')\n\n\n(-0.5, 1429.5, 783.5, -0.5)\n(a) Nuage de mot produit à partir du Comte de Monte Cristo\n\n\n\n\n\n\n(b)\n\n\n\nFigure 27.1: ?(caption)\n\n\nCela montre clairement qu’il est nécessaire de nettoyer notre texte. Le nom\ndu personnage principal, Dantès, est ainsi masqué par un certain nombre\nd’articles ou mots de liaison qui perturbent l’analyse. Ces mots sont des\nstop-words. La librairie NLTK (Natural Language ToolKit), librairie\nde référence dans le domaine du NLP, permet de facilement retirer ces\nstopwords (cela pourrait également être fait avec\nla librairie plus récente, spaCy). Avant cela, il est nécessaire\nde transformer notre texte en le découpant par unités fondamentales (les tokens).\nLes exemples suivants, extraits de Galiana and Castillo (2022), montrent l’intérêt du\nnettoyage de textes lorsqu’on désire comparer des corpus\nentre eux. En l’occurrence, il s’agit de comparer un corpus de\nnoms de produits dans des collectes automatisées de produits\nde supermarché (scanner-data) avec des noms de produits\ndans les données de l’OpenFoodFacts, une base de données\ncontributive. Sans nettoyage, le bruit l’emporte sur le signal\net il est impossible de déceler des similarités entre les jeux\nde données. Le nettoyage permet d’harmoniser\nun peu ces jeux de données pour avoir une chance d’être en\nmesure de les comparer.\n\n\n\n\n\nOpenFoodFacts avant nettoyage\n\n\n\n\n\nScanner-data avant nettoyage\n\n\n\n\n\n\n\n\n\nOpenFoodFacts après nettoyage\n\n\n\n\n\nScanner-data après nettoyage\n\n\n\n\n\n27.3.1 Tokenisation\n\n\n Hint\nLors de la première utilisation de NLTK, il est nécessaire de télécharger\nquelques éléments nécessaires à la tokenisation, notamment la ponctuation.\nPour cela, il est recommandé d’utiliser la commande suivante:\nimport nltk\nnltk.download('punkt')\n\n\n\nimport nltk\nnltk.download('punkt')\n\n[nltk_data] Downloading package punkt to /github/home/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n\n\nTrue\n\n\nLa tokenisation consiste à découper un texte en morceaux. Ces morceaux\npourraient être des phrases, des chapitres, des n-grammes ou des mots. C’est\ncette dernière option que l’on va choisir, plus simple pour retirer les\nstopwords :\n\nimport nltk\n\nwords = nltk.word_tokenize(dumas, language='french')\nwords[1030:1050]\n\n['que',\n 'voulez-vous',\n ',',\n 'monsieur',\n 'edmond',\n ',',\n 'reprit',\n \"l'armateur\",\n 'qui',\n 'paraissait',\n 'se',\n 'consoler',\n 'de',\n 'plus',\n 'en',\n 'plus',\n ',',\n 'nous',\n 'sommes',\n 'tous']\n\n\nOn remarque que les mots avec apostrophes sont liés en un seul, ce qui est\npeut-être faux sur le plan de la grammaire mais peu avoir un sens pour une\nanalyse statistique. Il reste des signes de ponctuation qu’on peut éliminer\navec la méthode isalpha:\n\nwords = [word for word in words if word.isalpha()]\nwords[1030:1050]\n\n['assez',\n 'sombre',\n 'obséquieux',\n 'envers',\n 'ses',\n 'supérieurs',\n 'insolent',\n 'envers',\n 'ses',\n 'subordonnés',\n 'aussi',\n 'outre',\n 'son',\n 'titre',\n 'comptable',\n 'qui',\n 'est',\n 'toujours',\n 'un',\n 'motif']\n\n\nComme indiqué ci-dessus, pour télécharger\nle corpus de ponctuation, il est\nnécessaire d’exécuter la ligne de\ncommande suivante :\n\n\n27.3.2 Retirer les stop-words\nLe jeu de données est maintenant propre. On peut désormais retirer les\nmots qui n’apportent pas de sens et servent seulement à faire le\nlien entre deux prépositions. On appelle ces mots des\nstop words dans le domaine du NLP.\n\n\n Hint\nLors de la première utilisation de NLTK, il est nécessaire de télécharger\nles stopwords.\nimport nltk\nnltk.download('stopwords')\n\n\nComme indiqué ci-dessus, pour télécharger\nle corpus de stopwords1, il est\nnécessaire d’exécuter la ligne de\ncommande suivante :\n\nimport nltk\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to /github/home/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\n\nfrom nltk.corpus import stopwords\nprint(stopwords.words(\"french\"))\n\nstop_words = set(stopwords.words('french'))\n\n\nwords = [w for w in words if not w in stop_words]\nprint(words[1030:1050])\n\n['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n['celui', 'dantès', 'a', 'déposé', 'passant', 'comment', 'paquet', 'déposer', 'danglars', 'rougit', 'passais', 'devant', 'porte', 'capitaine', 'entrouverte', 'vu', 'remettre', 'paquet', 'cette', 'lettre']\n\n\nCes retraitements commencent à porter leurs fruits puisque des mots ayant plus\nde sens commencent à se dégager, notamment les noms des personnages\n(Fernand, Mercédès, Villefort, etc.)\n\nwc = make_wordcloud(' '.join(words))\n\nfig = plt.figure()\n\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\n\n(-0.5, 1429.5, 783.5, -0.5)\n\n\n\n\n\n\n\n\n\n\n\n27.3.3 Stemming\nPour réduire la complexité d’un texte, on peut tirer partie de\n“classes d’équivalence” : on peut\nconsidérer que différentes formes d’un même mot (pluriel,\nsingulier, conjugaison) sont équivalentes et les remplacer par une\nmême forme dite canonique. Il existe deux approches dans le domaine :\n\nla lemmatisation qui requiert la connaissance des statuts\ngrammaticaux (exemple : chevaux devient cheval)\nla racinisation (stemming) plus fruste mais plus rapide, notamment\nen présence de fautes d’orthographes. Dans ce cas, chevaux peut devenir chev\nmais être ainsi confondu avec chevet ou cheveux\n\nLa racinisation est plus simple à mettre en oeuvre car elle peut s’appuyer sur\ndes règles simples pour extraire la racine d’un mot.\nPour réduire un mot dans sa forme “racine”, c’est-à-dire en s’abstrayant des\nconjugaisons ou variations comme les pluriels, on applique une méthode de\nstemming. Le but du stemming est de regrouper de\nnombreuses variantes d’un mot comme un seul et même mot.\nPar exemple, une fois que l’on applique un stemming, “chats” et “chat”\ndeviennent un même mot.\nCette approche a l’avantage de réduire la taille du vocabulaire à maîtriser\npour l’ordinateur et le modélisateur. Il existe plusieurs algorithmes de\nstemming, notamment le Porter Stemming Algorithm ou le\nSnowball Stemming Algorithm. Nous pouvons utiliser ce dernier en Français :\n\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer(language='french')\n\nstemmed = [stemmer.stem(word) for word in words]\nprint(stemmed[1030:1050])\n\n['celui', 'dantes', 'a', 'dépos', 'pass', 'comment', 'paquet', 'dépos', 'danglar', 'roug', 'pass', 'dev', 'port', 'capitain', 'entrouvert', 'vu', 'remettr', 'paquet', 'cet', 'lettr']\n\n\nA ce niveau, les mots commencent à être moins intelligibles par un humain.\nLa machine prendra le relais, on lui a préparé le travail\n\n\n Note\nIl existe aussi le stemmer suivant :\nfrom nltk.stem.snowball import FrenchStemmer\nstemmer = FrenchStemmer()"
  },
  {
    "objectID": "content/course/NLP/01_intro/index.html#reconnaissance-des-entités-nommées",
    "href": "content/course/NLP/01_intro/index.html#reconnaissance-des-entités-nommées",
    "title": "27  Quelques éléments pour comprendre les enjeux du NLP",
    "section": "27.4 Reconnaissance des entités nommées",
    "text": "27.4 Reconnaissance des entités nommées\nCette étape n’est pas une étape de préparation mais illustre la capacité\ndes librairies Python a extraire du sens d’un texte. La librairie\nspaCy permet de faire de la reconnaissance d’entités nommées, ce qui peut\nêtre pratique pour extraire rapidement certains personnages de notre oeuvre.\n\n\nLa librairie spaCy\nNTLK est la librairie historique d’analyse textuelle en Python. Elle existe\ndepuis les années 1990. L’utilisation industrielle du NLP dans le monde\nde la data-science est néanmoins plus récente et doit beaucoup à la collecte\naccrue de données non structurées par les réseaux sociaux. Cela a amené à\nun renouvelement du champ du NLP, tant dans le monde de la recherche que dans\nsa mise en application dans l’industrie de la donnée.\nLe package spaCy est l’un des packages qui a permis\ncette industrialisation des méthodes de NLP. Conçu autour du concept\nde pipelines de données, il est beaucoup plus pratique à mettre en oeuvre\npour une chaîne de traitement de données textuelles mettant en oeuvre\nplusieurs étapes de transformation des données.\n\n\n#!pip install deplacy\n#!python -m spacy download fr_core_news_sm\nimport spacy\n\nnlp=spacy.load(\"fr_core_news_sm\")\ndoc = nlp(dumas)\nimport spacy\nfrom spacy import displacy\ndisplacy.render(doc, style=\"ent\", jupyter=True)"
  },
  {
    "objectID": "content/course/NLP/01_intro/index.html#représentation-dun-texte-sous-forme-vectorielle",
    "href": "content/course/NLP/01_intro/index.html#représentation-dun-texte-sous-forme-vectorielle",
    "title": "27  Quelques éléments pour comprendre les enjeux du NLP",
    "section": "27.5 Représentation d’un texte sous forme vectorielle",
    "text": "27.5 Représentation d’un texte sous forme vectorielle\nUne fois nettoyé, le texte est plus propice à une représentation vectorielle.\nEn fait, implicitement, on a depuis le début adopté une démarche bag of words.\nIl s’agit d’une représentation, sans souci de contexte (ordre des mots, contexte d’utilisation),\noù chaque token représente un élément dans un vocabulaire de taille \\(|V|\\).\nOn peut ainsi avoir une représentation matricielle les occurrences de\nchaque token dans plusieurs documents (par exemple plusieurs livres,\nchapitres, etc.) pour, par exemple, en déduire une forme de similarité.\nAfin de réduire la dimension de la matrice bag of words,\non peut s’appuyer sur des pondérations.\nOn élimine ainsi certains mots très fréquents ou au contraire très rares.\nLa pondération la plus simple est basée sur la fréquence des mots dans le document.\nC’est l’objet de la métrique tf-idf (term frequency - inverse document frequency)\nabordée dans un prochain chapitre."
  },
  {
    "objectID": "content/course/NLP/01_intro/index.html#références",
    "href": "content/course/NLP/01_intro/index.html#références",
    "title": "27  Quelques éléments pour comprendre les enjeux du NLP",
    "section": "27.6 Références",
    "text": "27.6 Références\n\n\n\n\nGaliana, Lino, and Milena Suarez Castillo. 2022. “Fuzzy Matching on Big-Data an Illustration with Scanner Data and Crowd-Sourced Nutritional Data.”"
  },
  {
    "objectID": "content/course/NLP/02_exoclean/index.html#librairies-nécessaires",
    "href": "content/course/NLP/02_exoclean/index.html#librairies-nécessaires",
    "title": "28  Nettoyer un texte: des exercices pour découvrir l’approche bag-of-words",
    "section": "28.1 Librairies nécessaires",
    "text": "28.1 Librairies nécessaires\nCette page évoquera les principales librairies pour faire du NLP, notamment :\n\nWordCloud\nnltk\nSpaCy\nKeras\nTensorFlow\n\nIl faudra également installer les librairies gensim et pywaffle\n\n\n Hint\nComme dans la partie précédente, il faut télécharger quelques éléments pour que NTLK puisse fonctionner correctement. Pour cela, faire :\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('genesis')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\n\nLa liste des modules à importer est assez longue, la voici :\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport base64\nimport string\nimport re\nimport nltk\n\nfrom collections import Counter\nfrom time import time\n# from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords\nfrom sklearn.metrics import log_loss\nimport matplotlib.pyplot as plt\n#!pip install pywaffle\nfrom pywaffle import Waffle\n\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\n\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('genesis')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\n[nltk_data] Downloading package stopwords to /github/home/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /github/home/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package genesis to /github/home/nltk_data...\n[nltk_data]   Unzipping corpora/genesis.zip.\n[nltk_data] Downloading package wordnet to /github/home/nltk_data...\n[nltk_data] Downloading package omw-1.4 to /github/home/nltk_data...\n\n\nTrue"
  },
  {
    "objectID": "content/course/NLP/02_exoclean/index.html#données-utilisées",
    "href": "content/course/NLP/02_exoclean/index.html#données-utilisées",
    "title": "28  Nettoyer un texte: des exercices pour découvrir l’approche bag-of-words",
    "section": "28.2 Données utilisées",
    "text": "28.2 Données utilisées\n\n\n Exercice 1 : Importer les données spooky\nPour ceux qui ont envie de tester leurs connaissances en pandas\n\nImporter le jeu de données spooky à partir de l’URL https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv sous le nom train. L’encoding est latin-1\nMettre des majuscules au nom des colonnes.\nRetirer le prefix id de la colonne Id et appeler la nouvelle colonne ID.\nMettre l’ancienne colonne Id en index.\n\n\n\nSi vous ne faites pas l’exercice 1, pensez à charger les données en executant la fonction get_data.py :\n\nimport requests\n\nurl = 'https://raw.githubusercontent.com/linogaliana/python-datascientist/master/content/course/NLP/get_data.py'\nr = requests.get(url, allow_redirects=True)\nopen('getdata.py', 'wb').write(r.content)\n\nimport getdata\ntrain = getdata.create_train_dataframes()\n\nCe code introduit une base nommée train dans l’environnement.\nLe jeu de données met ainsi en regard un auteur avec une phrase qu’il a écrite :\n\ntrain.head()\n\n\n\n\n\n\n\n\nText\nAuthor\nID\n\n\nId\n\n\n\n\n\n\n\nid26305\nThis process, however, afforded me no means of...\nEAP\n26305\n\n\nid17569\nIt never once occurred to me that the fumbling...\nHPL\n17569\n\n\nid11008\nIn his left hand was a gold snuff box, from wh...\nEAP\n11008\n\n\nid27763\nHow lovely is spring As we looked from Windsor...\nMWS\n27763\n\n\nid12958\nFinding nothing else, not even gold, the Super...\nHPL\n12958\n\n\n\n\n\n\n\nOn peut se rendre compte que les extraits des 3 auteurs ne sont\npas forcément équilibrés dans le jeu de données.\nIl faudra en tenir compte dans la prédiction.\n\nfig = plt.figure()\ng = sns.barplot(x=['Edgar Allen Poe', 'Mary W. Shelley', 'H.P. Lovecraft'], y=train['Author'].value_counts())\n\n\n\n\n\n\n\n\n\n\n Note\nL’approche bag of words est présentée de\nmanière plus extensive dans le chapitre précédent.\nL’idée est d’étudier la fréquence des mots d’un document et la\nsurreprésentation des mots par rapport à un document de\nréférence (appelé corpus).\nCette approche un peu simpliste mais très\nefficace : on peut calculer des scores permettant par exemple de faire\nde classification automatique de document par thème, de comparer la\nsimilarité de deux documents. Elle est souvent utilisée en première analyse,\net elle reste la référence pour l’analyse de textes mal\nstructurés (tweets, dialogue tchat, etc.).\nLes analyses tf-idf (term frequency-inverse document frequency) ou les\nconstructions d’indices de similarité cosinus reposent sur ce type d’approche.\n\n\n\n28.2.1 Fréquence d’un mot\nAvant de s’adonner à une analyse systématique du champ lexical de chaque\nauteur, on se focaliser dans un premier temps sur un unique mot, le mot fear.\n\n\n Note\nL’exercice ci-dessous présente une représentation graphique nommée\nwaffle chart. Il s’agit d’une approche préférable aux\ncamemberts qui sont des graphiques manipulables car l’oeil humain se laisse\nfacilement berner par cette représentation graphique qui ne respecte pas\nles proportions.\n\n\n\n\n Exercice 2 : Fréquence d'un mot\n\nCompter le nombre de phrases, pour chaque auteur, où apparaît le mot fear.\nUtiliser pywaffle pour obtenir les graphiques ci-dessous qui résument\nde manière synthétique le nombre d’occurrences du mot “fear” par auteur.\nRefaire l’analyse avec le mot “horror”.\n\n\n\nA l’issue de la question 1, vous devriez obtenir le tableau\nde fréquence suivant:\n\n\n\n\n\n\n\n\n\nText\nID\nwordtoplot\n\n\nAuthor\n\n\n\n\n\n\n\nEAP\nThis process, however, afforded me no means of...\n2630511008096741351519322166071718908441148621...\n70\n\n\nHPL\nIt never once occurred to me that the fumbling...\n1756912958197641888620836080752790708121117330...\n160\n\n\nMWS\nHow lovely is spring As we looked from Windsor...\n2776322965009121673712799131170076400683052582...\n211\n\n\n\n\n\n\n\nCeci permet d’obtenir le waffle chart suivant:\n\n\n\n\n\nFigure 28.2: Répartition du terme fear dans le corpus de nos trois auteurs\n\n\n\n\nOn remarque ainsi de manière très intuitive\nle déséquilibre de notre jeu de données\nlorsqu’on se focalise sur le terme “peur”\noù Mary Shelley représente près de 50%\ndes observations.\nSi on reproduit cette analyse avec le terme “horror”, on peut\nen conclure que la peur est plus évoquée par Mary Shelley\n(sentiment assez naturel face à la créature du docteur Frankenstein) alors\nque Lovecraft n’a pas volé sa réputation d’écrivain de l’horreur !\n\n\n\n\n\n\n\n\n\n\n\n28.2.2 Premier wordcloud\nPour aller plus loin dans l’analyse du champ lexical de chaque auteur,\non peut représenter un wordcloud qui permet d’afficher chaque mot avec une\ntaille proportionnelle au nombre d’occurrence de celui-ci.\n\n\n Exercice 3 : Wordcloud\n\nEn utilisant la fonction wordCloud, faire trois nuages de mot pour représenter les mots les plus utilisés par chaque auteur.\nCalculer les 25 mots plus communs pour chaque auteur et représenter les trois histogrammes des décomptes.\n\n\n\nLe wordcloud pour nos différents auteurs est le suivant:\n\n\n\n\n\n\n\n\n\nEnfin, si on fait un histogramme des fréquences,\ncela donnera :\n\n\n\n\n\n\n\n\n\nOn voit ici que ce sont des mots communs, comme “the”, “of”, etc. sont très\nprésents. Mais ils sont peu porteurs d’information, on peut donc les éliminer\navant de faire une analyse syntaxique poussée.\nCeci est une démonstration par l’exemple qu’il vaut mieux nettoyer le texte avant de\nl’analyser (sauf si on est intéressé\npar la loi de Zipf, cf. exercice suivant).\n\n\n28.2.3 Aparté: la loi de Zipf\n\n\n La loi de Zipf\nDans son sens strict, la loi de Zipf prévoit que\ndans un texte donné, la fréquence d’occurrence \\(f(n_i)\\) d’un mot est\nliée à son rang \\(n_i\\) dans l’ordre des fréquences par une loi de la forme\n\\(f(n_i) = c/n_i\\) où \\(c\\) est une constante. Zipf, dans les années 1930, se basait sur l’oeuvre\nde Joyce, Ulysse pour cette affirmation.\nPlus généralement, on peut dériver la loi de Zipf d’une distribution exponentielle des fréquences: \\(f(n_i) = cn_{i}^{-k}\\). Cela permet d’utiliser la famille des modèles linéaires généralisés, notamment les régressions poissonniennes, pour mesurer les paramètres de la loi. Les modèles linéaire traditionnels en log souffrent en effet, dans ce contexte, de biais (la loi de Zipf est un cas particulier d’un modèle gravitaire, où appliquer des OLS est une mauvaise idée, cf. Galiana et al. (2020) pour les limites).\n\n\nUn modèle exponentiel peut se représenter par un modèle de Poisson ou, si\nles données sont très dispersées, par un modèle binomial négatif. Pour\nplus d’informations, consulter l’annexe de Galiana et al. (2020).\nLa technique économétrique associée pour l’estimation est\nles modèles linéaires généralisés (GLM) qu’on peut\nutiliser en Python via le\npackage statsmodels2:\n\\[\n\\mathbb{E}\\bigg( f(n_i)|n_i \\bigg) = \\exp(\\beta_0 + \\beta_1 \\log(n_i))\n\\]\nPrenons les résultats de l’exercice précédent et enrichissons les du rang et de la fréquence d’occurrence d’un mot :\n\ncount_words = pd.DataFrame({'counter' : train\n    .groupby('Author')\n    .apply(lambda s: ' '.join(s['Text']).split())\n    .apply(lambda s: Counter(s))\n    .apply(lambda s: s.most_common())\n    .explode()}\n)\ncount_words[['word','count']] = pd.DataFrame(count_words['counter'].tolist(), index=count_words.index)\ncount_words = count_words.reset_index()\n\ncount_words = count_words.assign(\n    tot_mots_auteur = lambda x: (x.groupby(\"Author\")['count'].transform('sum')),\n    freq = lambda x: x['count'] /  x['tot_mots_auteur'],\n    rank = lambda x: x.groupby(\"Author\")['count'].transform('rank', ascending = False)\n)\n\nCommençons par représenter la relation entre la fréquence et le rang:\nNous avons bien, graphiquement, une relation log-linéaire entre les deux:\n\ng.figure.get_figure()\n\n\n\n\n\n\n\n\nAvec statsmodels, vérifions plus formellement cette relation:\n\nimport statsmodels.api as sm\n\nexog = sm.add_constant(np.log(count_words['rank'].astype(float)))\n\nmodel = sm.GLM(count_words['freq'].astype(float), exog, family = sm.families.Poisson()).fit()\n\n# Afficher les résultats du modèle\nprint(model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                   freq   No. Observations:                69301\nModel:                            GLM   Df Residuals:                    69299\nModel Family:                 Poisson   Df Model:                            1\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -23.011\nDate:                Fri, 11 Aug 2023   Deviance:                     0.065676\nTime:                        15:50:40   Pearson chi2:                   0.0656\nNo. Iterations:                     5   Pseudo R-squ. (CS):          0.0002431\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.4388      1.089     -2.239      0.025      -4.574      -0.303\nrank          -0.9831      0.189     -5.196      0.000      -1.354      -0.612\n==============================================================================\n\n\nLe coefficient de la régression est presque 1 ce qui suggère bien une relation\nquasiment log-linéaire entre le rang et la fréquence d’occurrence d’un mot.\nDit autrement, le mot le plus utilisé l’est deux fois plus que le deuxième\nmois le plus fréquent qui l’est trois plus que le troisième, etc."
  },
  {
    "objectID": "content/course/NLP/02_exoclean/index.html#nettoyage-dun-texte",
    "href": "content/course/NLP/02_exoclean/index.html#nettoyage-dun-texte",
    "title": "28  Nettoyer un texte: des exercices pour découvrir l’approche bag-of-words",
    "section": "28.3 Nettoyage d’un texte",
    "text": "28.3 Nettoyage d’un texte\nLes premières étapes dans le nettoyage d’un texte, qu’on a\ndéveloppé au cours du chapitre précédent, sont :\n\nsuppression de la ponctuation\nsuppression des stopwords\n\nCela passe par la tokenisation d’un texte, c’est-à-dire la décomposition\nde celui-ci en unités lexicales (les tokens).\nCes unités lexicales peuvent être de différentes natures,\nselon l’analyse que l’on désire mener.\nIci, on va définir les tokens comme étant les mots utilisés.\nPlutôt que de faire soi-même ce travail de nettoyage,\navec des fonctions mal optimisées,\non peut utiliser la librairie nltk comme détaillé précédemment.\n\n\n Exercice 4 : Nettoyage du texte\nRepartir de train, notre jeu de données d’entraînement. Pour rappel, train a la structure suivante:\n\nTokeniser chaque phrase avec nltk.\nRetirer les stopwords avec nltk.\n\n\n\nPour rappel, au début de l’exercice, le DataFrame présente l’aspect suivant:\n\n\n\n\n\n\n\n\n\nText\nAuthor\nID\nwordtoplot\n\n\nId\n\n\n\n\n\n\n\n\nid26305\nThis process, however, afforded me no means of...\nEAP\n26305\n0\n\n\nid17569\nIt never once occurred to me that the fumbling...\nHPL\n17569\n0\n\n\n\n\n\n\n\nAprès tokenisation, il devrait avoir cet aspect :\n\n\nID     Author\n00001  MWS       [Idris, was, well, content, with, this, resolv...\n00002  HPL       [I, was, faint, even, fainter, than, the, hate...\ndtype: object\n\n\nAprès le retrait des stopwords, cela donnera:\n\n\n Hint\nLa méthode apply est très pratique ici car nous avons une phrase par ligne. Plutôt que de faire un DataFrame par auteur, ce qui n’est pas une approche très flexible, on peut directement appliquer la tokenisation\nsur notre DataFrame grâce à apply, sans le diviser.\n\n\nCe petit nettoyage permet d’arriver à un texte plus intéressant en termes d’analyse lexicale. Par exemple, si on reproduit l’analyse précédente… :\n\n\n\n\n\n\n\n\n\nPour aller plus loin dans l’harmonisation d’un texte, il est possible de\nmettre en place les classes d’équivalence développées dans la\npartie précédente afin de remplacer différentes variations d’un même\nmot par une forme canonique :\n\nla racinisation (stemming) assez fruste mais rapide, notamment\nen présence de fautes d’orthographe. Dans ce cas, chevaux peut devenir chev\nmais être ainsi confondu avec chevet ou cheveux.\nCette méthode est généralement plus simple à mettre en oeuvre, quoique\nplus fruste.\nla lemmatisation qui requiert la connaissance des statuts\ngrammaticaux (exemple : chevaux devient cheval).\nElle est mise en oeuvre, comme toujours avec nltk, à travers un\nmodèle. En l’occurrence, un WordNetLemmatizer (WordNet est une base\nlexicographique ouverte). Par exemple, les mots “women”, “daughters”\net “leaves” seront ainsi lemmatisés de la manière suivante :\n\n\nfrom nltk.stem import WordNetLemmatizer\nlemm = WordNetLemmatizer()\n\nfor word in [\"women\",\"daughters\", \"leaves\"]:\n    print(\"The lemmatized form of %s is: {}\".format(lemm.lemmatize(word)) % word)\n\nThe lemmatized form of women is: woman\nThe lemmatized form of daughters is: daughter\nThe lemmatized form of leaves is: leaf\n\n\n\n\n Note\nPour disposer du corpus nécessaire à la lemmatisation, il faut, la première fois,\ntélécharger celui-ci grâce aux commandes suivantes:\nimport nltk\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\n\nOn va se restreindre au corpus d’Edgar Allan Poe et repartir de la base de données\nbrute:\n\neap_clean = train[train[\"Author\"] == \"EAP\"]\neap_clean = ' '.join(eap_clean['Text'])\n#Tokenisation naïve sur les espaces entre les mots =&gt; on obtient une liste de mots\n#tokens = eap_clean.split()\nword_list = nltk.word_tokenize(eap_clean)\n\n\n\n Exercice 5 : Lemmatisation avec nltk\nUtiliser un WordNetLemmatizer et observer le résultat.\nOptionnel: Effectuer la même tâche avec spaCy\n\n\nLe WordNetLemmatizer donnera le résultat suivant:"
  },
  {
    "objectID": "content/course/NLP/02_exoclean/index.html#tf-idf-calcul-de-fréquence",
    "href": "content/course/NLP/02_exoclean/index.html#tf-idf-calcul-de-fréquence",
    "title": "28  Nettoyer un texte: des exercices pour découvrir l’approche bag-of-words",
    "section": "28.4 TF-IDF: calcul de fréquence",
    "text": "28.4 TF-IDF: calcul de fréquence\nLe calcul tf-idf (term frequency–inverse document frequency)\npermet de calculer un score de proximité entre un terme de recherche et un\ndocument (c’est ce que font les moteurs de recherche).\n\nLa partie tf calcule une fonction croissante de la fréquence du terme de recherche dans le document à l’étude ;\nLa partie idf calcule une fonction inversement proportionnelle à la fréquence du terme dans l’ensemble des documents (ou corpus).\n\nLe score total, obtenu en multipliant les deux composantes,\npermet ainsi de donner un score d’autant plus élevé que le terme est surréprésenté dans un document\n(par rapport à l’ensemble des documents).\nIl existe plusieurs fonctions, qui pénalisent plus ou moins les documents longs,\nou qui sont plus ou moins smooth.\n\n\n Exercice 6 : TF-IDF: calcul de fréquence\n\nUtiliser le vectoriseur TF-IdF de scikit-learn pour transformer notre corpus en une matrice document x terms. Au passage, utiliser l’option stop_words pour ne pas provoquer une inflation de la taille de la matrice. Nommer le modèle tfidf et le jeu entraîné tfs.\nAprès avoir construit la matrice de documents x terms avec le code suivant, rechercher les lignes où les termes ayant la structure abandon sont non-nuls.\nTrouver les 50 extraits où le score TF-IDF est le plus élevé et l’auteur associé. Vous devriez obtenir le classement suivant:\n\n\n\n\nfeature_names = tfidf.get_feature_names_out()\ncorpus_index = [n for n in list(tfidf.vocabulary_.keys())]\nimport pandas as pd\ndf = pd.DataFrame(tfs.todense(), columns=feature_names)\n\ndf.head()\n\n\n\n\n\n\n\n\naaem\nab\naback\nabaft\nabandon\nabandoned\nabandoning\nabandonment\nabaout\nabased\n...\nzodiacal\nzoilus\nzokkar\nzone\nzones\nzopyrus\nzorry\nzubmizzion\nzuro\ná¼\n\n\n\n\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n0.0\n0.0\n0.0\n0.0\n0.0\n0.253506\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 24937 columns\n\n\n\nLes lignes où les termes de abandon sont non nuls\nsont les suivantes :\n\n\nIndex([    4,   116,   215,   571,   839,  1042,  1052,  1069,  2247,  2317,\n        2505,  3023,  3058,  3245,  3380,  3764,  3886,  4425,  5289,  5576,\n        5694,  6812,  7500,  9013,  9021,  9077,  9560, 11229, 11395, 11451,\n       11588, 11827, 11989, 11998, 12122, 12158, 12189, 13666, 15259, 16516,\n       16524, 16759, 17547, 18019, 18072, 18126, 18204, 18251],\n      dtype='int64')\n\n\n\n\n\n\n\n\n\naaem\nab\naback\nabaft\nabandon\nabandoned\nabandoning\nabandonment\nabaout\nabased\n...\nzodiacal\nzoilus\nzokkar\nzone\nzones\nzopyrus\nzorry\nzubmizzion\nzuro\ná¼\n\n\n\n\n4\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.253506\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n116\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.339101\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n215\n0.0\n0.0\n0.0\n0.0\n0.235817\n0.000000\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n571\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.143788\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n839\n0.0\n0.0\n0.0\n0.0\n0.285886\n0.000000\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 24937 columns\n\n\n\n\n\nAuthor\nMWS    22\nHPL    15\nEAP    13\nName: Text, dtype: int64\n\n\nLes 10 scores les plus élevés sont les suivants :\n\nprint(train.iloc[list_fear[:9]]['Text'].values)\n\n['We could not fear we did not.' '\"And now I do not fear death.'\n 'Be of heart and fear nothing.' 'I smiled, for what had I to fear?'\n 'Indeed I had no fear on her account.'\n 'I have not the slightest fear for the result.'\n 'At length, in an abrupt manner she asked, \"Where is he?\" \"O, fear not,\" she continued, \"fear not that I should entertain hope Yet tell me, have you found him?'\n '\"I fear you are right there,\" said the Prefect.'\n 'I went down to open it with a light heart, for what had I now to fear?']\n\n\nOn remarque que les scores les plus élévés sont soient des extraits courts où le mot apparait une seule fois, soit des extraits plus longs où le mot fear apparaît plusieurs fois.\n\n\n Note\nLa matrice document x terms est un exemple typique de matrice sparse puisque, dans des corpus volumineux, une grande diversité de vocabulaire peut être trouvée."
  },
  {
    "objectID": "content/course/NLP/02_exoclean/index.html#approche-contextuelle-les-n-gramms",
    "href": "content/course/NLP/02_exoclean/index.html#approche-contextuelle-les-n-gramms",
    "title": "28  Nettoyer un texte: des exercices pour découvrir l’approche bag-of-words",
    "section": "28.5 Approche contextuelle: les n-gramms",
    "text": "28.5 Approche contextuelle: les n-gramms\nPour être en mesure de mener cette analyse, il est nécessaire de télécharger un corpus supplémentaire :\n\nimport nltk\nnltk.download('genesis')\nnltk.corpus.genesis.words('english-web.txt')\n\n[nltk_data] Downloading package genesis to /github/home/nltk_data...\n[nltk_data]   Package genesis is already up-to-date!\n\n\n['In', 'the', 'beginning', 'God', 'created', 'the', ...]\n\n\nIl s’agit maintenant de raffiner l’analyse.\nOn s’intéresse non seulement aux mots et à leur fréquence, mais aussi aux mots qui suivent. Cette approche est essentielle pour désambiguiser les homonymes. Elle permet aussi d’affiner les modèles “bag-of-words”. Le calcul de n-grams (bigrams pour les co-occurences de mots deux-à-deux, tri-grams pour les co-occurences trois-à-trois, etc.) constitue la méthode la plus simple pour tenir compte du contexte.\nnltk offre des methodes pour tenir compte du contexte : pour ce faire, nous calculons les n-grams, c’est-à-dire l’ensemble des co-occurrences successives de mots n-à-n. En général, on se contente de bi-grams, au mieux de tri-grams :\n\nles modèles de classification, analyse du sentiment, comparaison de documents, etc. qui comparent des n-grams avec n trop grands sont rapidement confrontés au problème de données sparse, cela réduit la capacité prédictive des modèles ;\nles performances décroissent très rapidement en fonction de n, et les coûts de stockage des données augmentent rapidement (environ n fois plus élevé que la base de données initiale).\n\nOn va, rapidement, regarder dans quel contexte apparaît le mot fear dans\nl’oeuvre d’Edgar Allan Poe (EAP). Pour cela, on transforme d’abord\nle corpus EAP en tokens `nltk :\n\neap_clean = train[train[\"Author\"] == \"EAP\"]\neap_clean = ' '.join(eap_clean['Text'])\ntokens = eap_clean.split()\nprint(tokens[:10])\ntext = nltk.Text(tokens)\nprint(text)\n\n['This', 'process,', 'however,', 'afforded', 'me', 'no', 'means', 'of', 'ascertaining', 'the']\n&lt;Text: This process, however, afforded me no means of...&gt;\n\n\nVous aurez besoin des fonctions BigramCollocationFinder.from_words et BigramAssocMeasures.likelihood_ratio :\n\nfrom nltk.collocations import BigramCollocationFinder\nfrom nltk.metrics import BigramAssocMeasures\n\n\n\n Exercice 7  : n-grams et contexte du mot fear\n\nUtiliser la méthode concordance pour afficher le contexte dans lequel apparaît le terme fear.\nSélectionner et afficher les meilleures collocation, par exemple selon le critère du ratio de vraisemblance.\n\nLorsque deux mots sont fortement associés, cela est parfois dû au fait qu’ils apparaissent rarement. Il est donc parfois nécessaire d’appliquer des filtres, par exemple ignorer les bigrammes qui apparaissent moins de 5 fois dans le corpus.\n\nRefaire la question précédente en utilisant toujours un modèle BigramCollocationFinder suivi de la méthode apply_freq_filter pour ne conserver que les bigrammes présents au moins 5 fois. Puis, au lieu d’utiliser la méthode de maximum de vraisemblance, testez la méthode nltk.collocations.BigramAssocMeasures().jaccard.\nNe s’intéresser qu’aux collocations qui concernent le mot fear\n\n\n\nAvec la méthode concordance (question 1),\nla liste devrait ressembler à celle-ci:\n\n\nExemples d'occurences du terme 'fear' :\nDisplaying 13 of 13 matches:\nd quick unequal spoken apparently in fear as well as in anger. What he said wa\nhutters were close fastened, through fear of robbers, and so I knew that he co\nto details. I even went so far as to fear that, as I occasioned much trouble, \nyears of age, was heard to express a fear \"that she should never see Marie aga\nich must be entirely remodelled, for fear of serious accident I mean the steel\n my arm, and I attended her home. 'I fear that I shall never see Marie again.'\nclusion here is absurd. \"I very much fear it is so,\" replied Monsieur Maillard\nbt of ultimately seeing the Pole. \"I fear you are right there,\" said the Prefe\ner occurred before.' Indeed I had no fear on her account. For a moment there w\nerhaps so,\" said I; \"but, Legrand, I fear you are no artist. It is my firm int\n raps with a hammer. Be of heart and fear nothing. My daughter, Mademoiselle M\ne splendor. I have not the slightest fear for the result. The face was so far \narriers of iron that hemmed me in. I fear you have mesmerized\" adding immediat\n\n\n\n\nMême si on peut facilement voir le mot avant et après, cette liste est assez difficile à interpréter car elle recoupe beaucoup d’informations.\nLa collocation consiste à trouver les bi-grammes qui\napparaissent le plus fréquemment ensemble. Parmi toutes les paires de deux mots observées,\nil s’agit de sélectionner, à partir d’un modèle statistique, les “meilleures”.\nOn obtient donc avec cette méthode (question 2):\nSi on modélise les meilleures collocations:\nCette liste a un peu plus de sens,\non a des noms de personnages, de lieux mais aussi des termes fréquemment employés ensemble\n(Chess Player par exemple).\nEn ce qui concerne les collocations du mot fear:\nSi on mène la même analyse pour le terme love, on remarque que de manière logique, on retrouve bien des sujets généralement accolés au verbe :\n\ncollocations_word(\"love\")\n\n[('love', 'me'), ('love', 'he'), ('will', 'love'), ('I', 'love'), ('love', ','), ('you', 'love'), ('the', 'love')]"
  },
  {
    "objectID": "content/course/NLP/02_exoclean/index.html#références",
    "href": "content/course/NLP/02_exoclean/index.html#références",
    "title": "28  Nettoyer un texte: des exercices pour découvrir l’approche bag-of-words",
    "section": "28.6 Références",
    "text": "28.6 Références\n\n\n\n\nGaliana, Lino, and Milena Suarez Castillo. 2022. “Fuzzy Matching on Big-Data an Illustration with Scanner Data and Crowd-Sourced Nutritional Data.”\n\n\nGaliana, Lino, François Sémécurbe, Benjamin Sakarovitch, and Zbigniew Smoreda. 2020. “Residential Segregation, Daytime Segregation and Spatial Frictions: An Analysis from Mobile Phone Data.”"
  },
  {
    "objectID": "content/course/NLP/03_lda/index.html#librairies-nécessaires",
    "href": "content/course/NLP/03_lda/index.html#librairies-nécessaires",
    "title": "29  Latent Dirichlet Allocation (LDA)",
    "section": "29.1 Librairies nécessaires",
    "text": "29.1 Librairies nécessaires\nCette page évoquera les principales librairies pour faire du NLP, notamment :\n\nWordCloud\nNLTK\nSpaCy\nKeras\nTensorFlow\n\n\n\n Hint\nComme dans la partie précédente, il faut télécharger quelques\néléments pour que NTLK puisse fonctionner correctement. Pour cela, faire:\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('genesis')\nnltk.download('omw-1.4')\n\n\nLa liste des modules à importer est assez longue, la voici:\n\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('genesis')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n#from IPython.display import display\nimport base64\nimport string\nimport re\nimport nltk\n\nfrom collections import Counter\nfrom time import time\n# from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords\nfrom sklearn.metrics import log_loss\nimport matplotlib.pyplot as plt\n\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\n\n[nltk_data] Downloading package stopwords to /github/home/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /github/home/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package genesis to /github/home/nltk_data...\n[nltk_data]   Package genesis is already up-to-date!\n[nltk_data] Downloading package wordnet to /github/home/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /github/home/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!"
  },
  {
    "objectID": "content/course/NLP/03_lda/index.html#données-utilisées",
    "href": "content/course/NLP/03_lda/index.html#données-utilisées",
    "title": "29  Latent Dirichlet Allocation (LDA)",
    "section": "29.2 Données utilisées",
    "text": "29.2 Données utilisées\nSi vous avez déjà lu la section précédente et importé les données, vous\npouvez passer à la section suivante\nLe code suivant permet d’importer le jeu de données spooky:\n\nimport pandas as pd\n\nurl='https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv'\nimport pandas as pd\ntrain = pd.read_csv(url,\n                    encoding='latin-1')\ntrain.columns = train.columns.str.capitalize()\n                    \ntrain['ID'] = train['Id'].str.replace(\"id\",\"\")\ntrain = train.set_index('Id')\n\nLe jeu de données met ainsi en regard un auteur avec une phrase qu’il a écrite:\n\ntrain.head()\n\n\n\n\n\n\n\n\nText\nAuthor\nID\n\n\nId\n\n\n\n\n\n\n\nid26305\nThis process, however, afforded me no means of...\nEAP\n26305\n\n\nid17569\nIt never once occurred to me that the fumbling...\nHPL\n17569\n\n\nid11008\nIn his left hand was a gold snuff box, from wh...\nEAP\n11008\n\n\nid27763\nHow lovely is spring As we looked from Windsor...\nMWS\n27763\n\n\nid12958\nFinding nothing else, not even gold, the Super...\nHPL\n12958\n\n\n\n\n\n\n\nLes étapes de preprocessing sont expliquées dans le chapitre précédent. On applique les étapes suivantes :\n\nTokeniser\nRetirer la ponctuation et les stopwords\nLemmatiser le texte\n\n\nlemma = WordNetLemmatizer()\n\ntrain_clean = (train\n    .groupby([\"ID\",\"Author\"])\n    .apply(lambda s: nltk.word_tokenize(' '.join(s['Text'])))\n    .apply(lambda words: [word for word in words if word.isalpha()])\n)\n\nfrom nltk.corpus import stopwords  \nstop_words = set(stopwords.words('english'))\n\ntrain_clean = (train_clean\n    .apply(lambda words: [lemma.lemmatize(w) for w in words if not w in stop_words])\n    .reset_index(name='tokenized')\n)\n\ntrain_clean.head(2)\n\n\n\n\n\n\n\n\nID\nAuthor\ntokenized\n\n\n\n\n0\n00001\nMWS\n[Idris, well, content, resolve, mine]\n\n\n1\n00002\nHPL\n[I, faint, even, fainter, hateful, modernity, ..."
  },
  {
    "objectID": "content/course/NLP/03_lda/index.html#principe-de-la-lda-latent-dirichlet-allocation",
    "href": "content/course/NLP/03_lda/index.html#principe-de-la-lda-latent-dirichlet-allocation",
    "title": "29  Latent Dirichlet Allocation (LDA)",
    "section": "29.3 Principe de la LDA (Latent Dirichlet Allocation)",
    "text": "29.3 Principe de la LDA (Latent Dirichlet Allocation)\nLe modèle Latent Dirichlet Allocation (LDA) est un modèle probabiliste génératif qui permet\nde décrire des collections de documents de texte ou d’autres types de données discrètes. LDA fait\npartie d’une catégorie de modèles appelés “topic models”, qui cherchent à découvrir des structures\nthématiques cachées dans des vastes archives de documents.\nCeci permet d’obtenir des méthodes\nefficaces pour le traitement et l’organisation des documents de ces archives: organisation automatique\ndes documents par sujet, recherche, compréhension et analyse du texte, ou même résumer des\ntextes.\nAujourd’hui, ce genre de méthodes s’utilisent fréquemment dans le web, par exemple pour\nanalyser des ensemble d’articles d’actualité, les regrouper par sujet, faire de la recommandation\nd’articles, etc.\nLa LDA est une méthode qui considère les corpus comme des mélanges de sujets et\nde mots. Chaque document peut être représenté comme le résultat d’un mélange :\n\nde sujets\net, au sein de ces sujets, d’un choix de mots.\n\nL’estimation des\nparamètres de la LDA passe par l’estimation des distributions des variables\nlatentes à partir des données observées (posterior inference).\nMathématiquement, on peut se représenter la LDA comme une\ntechnique de maximisation de log vraisemblance avec un algorithme EM (expectation maximisation)\ndans un modèle de mélange.\nLa matrice termes-documents qui sert de point de départ est la suivante:\n\n\n\n\nword_1\nword_2\nword_3\n…\nword_J\n\n\n\n\ndoc_1\n3\n0\n1\n…\n0\n\n\n…\n…\n…\n…\n…\n…\n\n\ndoc_N\n1\n0\n0\n…\n5\n\n\n\nOn dit que cette matrice est sparse (creuse en Français) car elle contient principalement des 0. En effet, un document n’utilise qu’une partie mineure du vocabulaire complet.\nLa LDA consiste à transformer cette matrice sparsedocument-terme en deux matrices de moindre dimension:\n\nUne matrice document-sujet\nUne matrice sujet-mots\n\nEn notant \\(K_i\\) le sujet \\(i\\). On obtient donc\n\nUne matrice document-sujet ayant la structure suivante:\n\n\n\n\n\nK_1\nK_2\nK_3\n…\nK_M\n\n\n\n\ndoc_1\n1\n0\n1\n…\n0\n\n\n…\n…\n…\n…\n…\n…\n\n\ndoc_N\n1\n1\n1\n…\n0\n\n\n\n\nUne matrice sujets-mots ayant la structure suivante:\n\n\n\n\n\nword_1\nword_2\nword_3\n…\nword_J\n\n\n\n\nK_1\n1\n0\n0\n…\n0\n\n\n…\n…\n…\n…\n…\n…\n\n\nK_M\n1\n1\n1\n…\n0\n\n\n\nCes deux matrices ont l’interprétation suivante :\n\nLa première nous renseigne sur la présence d’un sujet dans un document\nLa seconde nous renseigne sur la présence d’un mot dans un sujet\n\nEn fait, le principe de la LDA est de construire ces deux matrices à partir des fréquences d’apparition des mots dans le texte.\nOn va se concentrer sur Edgar Allan Poe.\n\ncorpus = train_clean[train_clean[\"Author\"] == \"EAP\"]"
  },
  {
    "objectID": "content/course/NLP/03_lda/index.html#entraîner-une-lda",
    "href": "content/course/NLP/03_lda/index.html#entraîner-une-lda",
    "title": "29  Latent Dirichlet Allocation (LDA)",
    "section": "29.4 Entraîner une LDA",
    "text": "29.4 Entraîner une LDA\nIl existe plusieurs manières d’entraîner une LDA.\nNous allons utiliser Scikit ici avec la méthode LatentDirichletAllocation.\nComme expliqué dans la partie modélisation :\n\nOn initialise le modèle ;\nOn le met à jour avec la méthode fit.\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\n# Initialise the count vectorizer with the English stop words\ncount_vectorizer = CountVectorizer(stop_words='english')# Fit and transform the processed titles\ncount_data = count_vectorizer.fit_transform(corpus['tokenized'].apply(lambda s: ' '.join(s)))\n\n# Tweak the two parameters below\nnumber_topics = 5\nnumber_words = 10# Create and fit the LDA model\nlda = LatentDirichletAllocation(n_components=11, max_iter=5,\n                                learning_method = 'online',\n                                learning_offset = 50.,\n                                random_state = 0,\n                                n_jobs = 1)\nlda.fit(count_data)"
  },
  {
    "objectID": "content/course/NLP/03_lda/index.html#visualiser-les-résultats",
    "href": "content/course/NLP/03_lda/index.html#visualiser-les-résultats",
    "title": "29  Latent Dirichlet Allocation (LDA)",
    "section": "29.5 Visualiser les résultats",
    "text": "29.5 Visualiser les résultats\nOn peut déjà commencer par utiliser une fonction pour afficher les\nrésultats :\n\n# Helper function\ndef print_topics(model, count_vectorizer, n_top_words):\n    words = count_vectorizer.get_feature_names_out()\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"\\nTopic #%d:\" % topic_idx)\n        print(\" \".join([words[i]\n                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n\nprint_topics(lda, count_vectorizer, number_words)\n\n\nTopic #0:\narm looking thousand respect hour table woman rest ah seen\n\nTopic #1:\nsaid dupin ha end write smith chair phenomenon quite john\n\nTopic #2:\ntime thing say body matter course day place object immediately\n\nTopic #3:\nmere memory felt sat movement case sole green principle bone\n\nTopic #4:\ndoor room open small friend lady replied night window hand\n\nTopic #5:\nword man day idea good point house shall mind say\n\nTopic #6:\neye figure form left sea hour ordinary life deep world\n\nTopic #7:\nfoot great little earth let le year nature come nearly\n\nTopic #8:\nhand strange head color hair spoken read ear ghastly neck\n\nTopic #9:\ncame looked shadow low dream like death light spirit tree\n\nTopic #10:\neye know heart saw character far tell oh voice wall\n\n\nLa représentation sous forme de liste de mots n’est pas la plus pratique…\nOn peut essayer de se représenter un wordcloud de chaque sujet pour mieux voir si cette piste est pertinente :\n\ntf_feature_names = count_vectorizer.get_feature_names_out()\n\ndef wordcloud_lda(lda, tf_feature_names):\n\n  fig, axs = plt.subplots(len(lda.components_) // 3 + 1, 3)\n  \n  for i in range(len(lda.components_)):\n      corpus_lda = lda.components_[i]\n      first_topic_words = [tf_feature_names[l] for l in corpus_lda.argsort()[:-50-1:-1]]\n      k = i // 3\n      j = (i - k*3)\n      wordcloud = WordCloud(stopwords=stop_words, background_color=\"black\",width = 2500, height = 1800)\n      wordcloud = wordcloud.generate(\" \".join(first_topic_words))\n      axs[k][j].set_title(\"Wordcloud pour le \\nsujet {}\".format(i))\n      axs[k][j].axis('off')\n      axs[k][j].imshow(wordcloud)\n  \n  r = len(lda.components_) % 3\n  [fig.delaxes(axs[len(lda.components_) // 3,k-1]) for k in range(r+1, 3+1) if r != 0]\n\nwc = wordcloud_lda(lda, tf_feature_names)\nwc\n\n\n\n\n\n\n\n\n\nwc\n\nLe module pyLDAvis offre quelques visualisations bien pratiques lorsqu’on\ndésire représenter de manière synthétique les résultats d’une LDA et observer la distribution sujet x mots.\n\n\n Hint\nDans un notebook faire :\nimport pyLDAvis.sklearn\n\npyLDAvis.enable_notebook()\nPour les utilisateurs de Windows, il est nécessaire d’ajouter l’argument\nn_jobs = 1. Sinon, Python tente d’entraîner le modèle avec de la\nparallélisation. Le problème est que les processus sont des FORKs, ce que\nWindows ne supporte pas. Sur un système Unix (Linux, Mac OS), on peut se passer de cet\nargument.\n\n\n\n#!pip install pyLDAvis #à faire en haut du notebook sur colab\nimport pyLDAvis\nimport pyLDAvis.sklearn\n\n# pyLDAvis.enable_notebook()\nvis_data = pyLDAvis.sklearn.prepare(lda, count_data, count_vectorizer, n_jobs = 1)\npyLDAvis.display(vis_data)\n\n\nChaque bulle représente un sujet. Plus la bulle est grande, plus il y a de documents qui traitent de ce sujet.\n\nPlus les barres sont loin les unes des autres, plus elles sont différentes. Un bon modèle aura donc tendance à avoir de grandes bulles qui ne se recoupent pas. Ce n’est pas vraiment le cas ici…\n\nLes barres bleues représentent la fréquence de chaque mot dans le corpus.\nLes barres rouges représentent une estimation du nombre de termes générés dans un sujet précis. La barre rouge la plus longue correspond au mot le plus utilisé dans ce sujet."
  },
  {
    "objectID": "content/course/NLP/03_lda/index.html#références",
    "href": "content/course/NLP/03_lda/index.html#références",
    "title": "29  Latent Dirichlet Allocation (LDA)",
    "section": "29.6 Références",
    "text": "29.6 Références\n\nLe poly d’Alberto Brietti"
  },
  {
    "objectID": "content/course/NLP/04_word2vec/index.html#nettoyage-des-données",
    "href": "content/course/NLP/04_word2vec/index.html#nettoyage-des-données",
    "title": "30  Méthodes de vectorisation : comptages et word embeddings",
    "section": "30.1 Nettoyage des données",
    "text": "30.1 Nettoyage des données\nNous allons ainsi à nouveau utiliser le jeu de données spooky:\n\ndata_url = 'https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv'\nspooky_df = pd.read_csv(data_url)\n\nLe jeu de données met ainsi en regard un auteur avec une phrase qu’il a écrite:\n\nspooky_df.head()\n\n\n30.1.1 Preprocessing\nEn NLP, la première étape est souvent celle du preprocessing, qui inclut notamment les étapes de tokenization et de nettoyage du texte. Comme celles-ci ont été vues en détail dans le précédent chapitre, on se contentera ici d’un preprocessing minimaliste : suppression de la ponctuation et des stop words (pour la visualisation et les méthodes de vectorisation basées sur des comptages).\nJusqu’à présent, nous avons utilisé principalement nltk pour le\npreprocessing de données textuelles. Cette fois, nous proposons\nd’utiliser la librairie spaCy qui permet de mieux automatiser sous forme de\npipelines de preprocessing.\nPour initialiser le processus de nettoyage,\non va utiliser le corpus en_core_web_sm (voir plus\nhaut pour l’installation de ce corpus):\n\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n\nOn va utiliser un pipe spacy qui permet d’automatiser, et de paralléliser,\nun certain nombre d’opérations. Les pipes sont l’équivalent, en NLP, de\nnos pipelines scikit ou des pipes pandas. Il s’agit donc d’un outil\ntrès approprié pour industrialiser un certain nombre d’opérations de\npreprocessing :\n\ndef clean_docs(texts, remove_stopwords=False, n_process = 4):\n    \n    docs = nlp.pipe(texts, \n                    n_process=n_process,\n                    disable=['parser', 'ner',\n                             'lemmatizer', 'textcat'])\n    stopwords = nlp.Defaults.stop_words\n\n    docs_cleaned = []\n    for doc in docs:\n        tokens = [tok.text.lower().strip() for tok in doc if not tok.is_punct]\n        if remove_stopwords:\n            tokens = [tok for tok in tokens if tok not in stopwords]\n        doc_clean = ' '.join(tokens)\n        docs_cleaned.append(doc_clean)\n        \n    return docs_cleaned\n\nOn applique la fonction clean_docs à notre colonne pandas.\nLes pandas.Series étant itérables, elles se comportent comme des listes et\nfonctionnent ainsi très bien avec notre pipe spacy\n\nspooky_df['text_clean'] = clean_docs(spooky_df['text'])\n\n\nspooky_df.head()\n\n\n\n30.1.2 Encodage de la variable à prédire\nOn réalise un simple encodage de la variable à prédire :\nil y a trois catégories (auteurs), représentées par des entiers 0, 1 et 2.\nPour cela, on utilise le LabelEncoder de scikit déjà présenté\ndans la partie modélisation. On va utiliser la méthode\nfit_transform qui permet, en un tour de main, d’appliquer à la fois\nl’entraînement (fit), à savoir la création d’une correspondance entre valeurs\nnumériques et labels, et l’appliquer (transform) à la même colonne.\n\nle = LabelEncoder()\nspooky_df['author_encoded'] = le.fit_transform(spooky_df['author'])\n\nOn peut vérifier les classes de notre LabelEncoder :\n\nle.classes_\n\n\n\n30.1.3 Construction des bases d’entraînement et de test\nOn met de côté un échantillon de test (20 %) avant toute analyse (même descriptive).\nCela permettra d’évaluer nos différents modèles toute à la fin de manière très rigoureuse,\npuisque ces données n’auront jamais utilisées pendant l’entraînement.\nNotre échantillon initial n’est pas équilibré (balanced) : on retrouve plus d’oeuvres de\ncertains auteurs que d’autres. Afin d’obtenir un modèle qui soit évalué au mieux, nous allons donc stratifier notre échantillon de manière à obtenir une répartition similaire d’auteurs dans nos\nensembles d’entraînement et de test.\n\nX_train, X_test, y_train, y_test = train_test_split(spooky_df['text_clean'].values, \n                                                    spooky_df['author_encoded'].values, \n                                                    test_size=0.2, \n                                                    random_state=33,\n                                                    stratify = spooky_df['author_encoded'].values)\n\nPar exemple, les textes d’EAP représentent 40 % des échantillons d’entraînement et de test :\n\nprint(100*y_train.tolist().count(0)/(len(y_train)))\nprint(100*y_test.tolist().count(0)/(len(y_test)))\n\nAperçu du premier élément de X_train :\n\nX_train[0]\n\nOn peut aussi vérifier qu’on est capable de retrouver\nla correspondance entre nos auteurs initiaux avec\nla méthode inverse_transform\n\nprint(y_train[0], le.inverse_transform([y_train[0]])[0])"
  },
  {
    "objectID": "content/course/NLP/04_word2vec/index.html#statistiques-exploratoires",
    "href": "content/course/NLP/04_word2vec/index.html#statistiques-exploratoires",
    "title": "30  Méthodes de vectorisation : comptages et word embeddings",
    "section": "30.2 Statistiques exploratoires",
    "text": "30.2 Statistiques exploratoires\n\n30.2.1 Répartition des labels\nRefaisons un graphique que nous avons déjà produit précédemment pour voir\nla répartition de notre corpus entre auteurs:\n\nfig = pd.Series(le.inverse_transform(y_train)).value_counts().plot(kind='bar')\nfig\n\nOn observe une petite asymétrie : les passages des livres d’Edgar Allen Poe sont plus nombreux que ceux des autres auteurs dans notre corpus d’entraînement, ce qui peut être problématique dans le cadre d’une tâche de classification.\nL’écart n’est pas dramatique, mais on essaiera d’en tenir compte dans l’analyse en choisissant une métrique d’évaluation pertinente.\n\n\n30.2.2 Mots les plus fréquemment utilisés par chaque auteur\nOn va supprimer les stopwords pour réduire le bruit dans notre jeu\nde données.\n\n# Suppression des stop words\nX_train_no_sw = clean_docs(X_train, remove_stopwords=True)\nX_train_no_sw = np.array(X_train_no_sw)\n\nPour visualiser rapidement nos corpus, on peut utiliser la technique des\nnuages de mots déjà vue à plusieurs reprises.\nVous pouvez essayer de faire vous-même les nuages ci-dessous\nou cliquer sur la ligne ci-dessous pour afficher le code ayant\ngénéré les figures :\n\nCliquer pour afficher le code 👇\n\ndef plot_top_words(initials, ax, n_words=20):\n    # Calcul des mots les plus fréquemment utilisés par l'auteur\n    texts = X_train_no_sw[le.inverse_transform(y_train) == initials]\n    all_tokens = ' '.join(texts).split()\n    counts = Counter(all_tokens)\n    top_words = [word[0] for word in counts.most_common(n_words)]\n    top_words_counts = [word[1] for word in counts.most_common(n_words)]\n    \n    # Représentation sous forme de barplot\n    ax = sns.barplot(ax = ax, x=top_words, y=top_words_counts)\n    ax.set_title(f'Most Common Words used by {initials_to_author[initials]}')\n\n\ninitials_to_author = {\n    'EAP': 'Edgar Allen Poe',\n    'HPL': 'H.P. Lovecraft',\n    'MWS': 'Mary Wollstonecraft Shelley'\n}\n\nfig, axs = plt.subplots(3, 1, figsize = (12,12))\n\nplot_top_words('EAP', ax = axs[0])\nplot_top_words('HPL', ax = axs[1])\nplot_top_words('MWS', ax = axs[2])\n\n\n\nBeaucoup de mots se retrouvent très utilisés par les trois auteurs.\nIl y a cependant des différences notables : le mot “life”\nest le plus employé par MWS, alors qu’il n’apparaît pas dans les deux autres tops.\nDe même, le mot “old” est le plus utilisé par HPL\nlà où les deux autres ne l’utilisent pas de manière surreprésentée.\nIl semble donc qu’il y ait des particularités propres à chacun des auteurs\nen termes de vocabulaire,\nce qui laisse penser qu’il est envisageable de prédire les auteurs à partir\nde leurs textes dans une certaine mesure."
  },
  {
    "objectID": "content/course/NLP/04_word2vec/index.html#prédiction-sur-le-set-dentraînement",
    "href": "content/course/NLP/04_word2vec/index.html#prédiction-sur-le-set-dentraînement",
    "title": "30  Méthodes de vectorisation : comptages et word embeddings",
    "section": "30.3 Prédiction sur le set d’entraînement",
    "text": "30.3 Prédiction sur le set d’entraînement\nNous allons à présent vérifier cette conjecture en comparant\nplusieurs modèles de vectorisation,\ni.e. de transformation du texte en objets numériques\npour que l’information contenue soit exploitable dans un modèle de classification.\n\n30.3.1 Démarche\nComme nous nous intéressons plus à l’effet de la vectorisation qu’à la tâche de classification en elle-même,\nnous allons utiliser un algorithme de classification simple (un SVM linéaire), avec des paramètres non fine-tunés (c’est-à-dire des paramètres pas nécessairement choisis pour être les meilleurs de tous).\n\nclf = LinearSVC(max_iter=10000, C=0.1)\n\nCe modèle est connu pour être très performant sur les tâches de classification de texte, et nous fournira donc un bon modèle de référence (baseline). Cela nous permettra également de comparer de manière objective l’impact des méthodes de vectorisation sur la performance finale.\n\n\n\n\nPour les deux premières méthodes de vectorisation\n(basées sur des fréquences et fréquences relatives des mots),\non va simplement normaliser les données d’entrée, ce qui va permettre au SVM de converger plus rapidement, ces modèles étant sensibles aux différences d’échelle dans les données.\nOn va également fine-tuner via grid-search\ncertains hyperparamètres liés à ces méthodes de vectorisation :\n\non teste différents ranges de n-grams (unigrammes et unigrammes + bigrammes)\non teste avec et sans stop-words\n\nAfin d’éviter le surapprentissage,\non va évaluer les différents modèles via validation croisée, calculée sur 4 blocs.\nOn récupère à la fin le meilleur modèle selon une métrique spécifiée.\nOn choisit le score F1,\nmoyenne harmonique de la précision et du rappel,\nqui donne un poids équilibré aux deux métriques, tout en pénalisant fortement le cas où l’une des deux est faible.\nPrécisément, on retient le score F1 *micro-averaged* :\nles contributions des différentes classes à prédire sont agrégées,\npuis on calcule le score F1 sur ces données agrégées.\nL’avantage de ce choix est qu’il permet de tenir compte des différences\nde fréquences des différentes classes.\n\n\n30.3.2 Pipeline de prédiction\nOn va utiliser un pipeline scikit ce qui va nous permettre d’avoir\nun code très concis pour effectuer cet ensemble de tâches cohérentes.\nDe plus, cela va nous assurer de gérer de manière cohérentes nos différentes\ntransformations (cf. partie sur les pipelines)\nPour se faciliter la vie, on définit une fonction fit_vectorizers qui\nintègre dans un pipeline générique une méthode d’estimation scikit\net fait de la validation croisée en cherchant le meilleur modèle\n(en excluant/incluant les stopwords et avec unigrammes/bigrammes)\n\ndef fit_vectorizers(vectorizer):\n    pipeline = Pipeline(\n    [\n        (\"vect\", vectorizer()),\n        (\"scaling\", StandardScaler(with_mean=False)),\n        (\"clf\", clf),\n    ]\n    )\n\n    parameters = {\n        \"vect__ngram_range\": ((1, 1), (1, 2)),  # unigrams or bigrams\n        \"vect__stop_words\": (\"english\", None)\n    }\n\n    grid_search = GridSearchCV(pipeline, parameters, scoring='f1_micro',\n                               cv=4, n_jobs=4, verbose=1)\n    grid_search.fit(X_train, y_train)\n\n    best_parameters = grid_search.best_estimator_.get_params()\n    for param_name in sorted(parameters.keys()):\n        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n\n    print(f\"CV scores {grid_search.cv_results_['mean_test_score']}\")\n    print(f\"Mean F1 {np.mean(grid_search.cv_results_['mean_test_score'])}\")\n    \n    return grid_search"
  },
  {
    "objectID": "content/course/NLP/04_word2vec/index.html#approche-bag-of-words",
    "href": "content/course/NLP/04_word2vec/index.html#approche-bag-of-words",
    "title": "30  Méthodes de vectorisation : comptages et word embeddings",
    "section": "30.4 Approche bag-of-words",
    "text": "30.4 Approche bag-of-words\nOn commence par une approche “bag-of-words”,\ni.e. qui revient simplement à représenter chaque document par un vecteur\nqui compte le nombre d’apparitions de chaque mot du vocabulaire dans le document.\n\ncv_bow = fit_vectorizers(CountVectorizer)"
  },
  {
    "objectID": "content/course/NLP/04_word2vec/index.html#tf-idf",
    "href": "content/course/NLP/04_word2vec/index.html#tf-idf",
    "title": "30  Méthodes de vectorisation : comptages et word embeddings",
    "section": "30.5 TF-IDF",
    "text": "30.5 TF-IDF\nOn s’intéresse ensuite à l’approche TF-IDF,\nqui permet de tenir compte des fréquences relatives des mots.\nAinsi, pour un mot donné, on va multiplier la fréquence d’apparition du mot dans le document (calculé comme dans la méthode précédente) par un terme qui pénalise une fréquence élevée du mot dans le corpus. L’image ci-dessous, empruntée à Chris Albon, illustre cette mesure:\n\nSource: https://chrisalbon\nLa vectorisation TF-IDF permet donc de limiter l’influence des stop-words\net donc de donner plus de poids aux mots les plus salients d’un document.\nOn observe clairement que la performance de classification est bien supérieure,\nce qui montre la pertinence de cette technique.\n\ncv_tfidf = fit_vectorizers(TfidfVectorizer)"
  },
  {
    "objectID": "content/course/NLP/04_word2vec/index.html#word2vec-avec-averaging",
    "href": "content/course/NLP/04_word2vec/index.html#word2vec-avec-averaging",
    "title": "30  Méthodes de vectorisation : comptages et word embeddings",
    "section": "30.6 Word2vec avec averaging",
    "text": "30.6 Word2vec avec averaging\nOn va maintenant explorer les techniques de vectorisation basées sur les\nembeddings de mots, et notamment la plus populaire : Word2Vec.\nL’idée derrière est simple, mais a révolutionné le NLP :\nau lieu de représenter les documents par des\nvecteurs sparse de très grande dimension (la taille du vocabulaire)\ncomme on l’a fait jusqu’à présent,\non va les représenter par des vecteurs dense (continus)\nde dimension réduite (en général, autour de 100-300).\nChacune de ces dimensions va représenter un facteur latent,\nc’est à dire une variable inobservée,\nde la même manière que les composantes principales produites par une ACP.\n\n\n\n\n\nSource: https://medium.com/@zafaralibagh6/simple-tutorial-on-word-embedding-and-word2vec-43d477624b6d\nPourquoi est-ce intéressant ?\nPour de nombreuses raisons, mais pour résumer :\ncela permet de beaucoup mieux capturer la similarité sémantique entre les documents.\nPar exemple, un humain sait qu’un document contenant le mot “Roi”\net un autre document contenant le mot “Reine” ont beaucoup de chance\nd’aborder des sujets semblables.\nPourtant, une vectorisation de type comptage ou TF-IDF\nne permet pas de saisir cette similarité :\nle calcul d’une mesure de similarité (norme euclidienne ou similarité cosinus)\nentre les deux vecteurs ne prendra en compte la similarité des deux concepts, puisque les mots utilisés sont différents.\nA l’inverse, un modèle word2vec bien entraîné va capter\nqu’il existe un facteur latent de type “royauté”,\net la similarité entre les vecteurs associés aux deux mots sera forte.\nLa magie va même plus loin : le modèle captera aussi qu’il existe un\nfacteur latent de type “genre”,\net va permettre de construire un espace sémantique dans lequel les\nrelations arithmétiques entre vecteurs ont du sens ;\npar exemple :\n\\[\\text{king} - \\text{man} + \\text{woman} ≈ \\text{queen}\\]\nComment ces modèles sont-ils entraînés ?\nVia une tâche de prédiction résolue par un réseau de neurones simple.\nL’idée fondamentale est que la signification d’un mot se comprend\nen regardant les mots qui apparaissent fréquemment dans son voisinage.\nPour un mot donné, on va donc essayer de prédire les mots\nqui apparaissent dans une fenêtre autour du mot cible.\nEn répétant cette tâche de nombreuses fois et sur un corpus suffisamment varié,\non obtient finalement des embeddings pour chaque mot du vocabulaire,\nqui présentent les propriétés discutées précédemment.\n\nX_train_tokens = [text.split() for text in X_train]\nw2v_model = Word2Vec(X_train_tokens, vector_size=200, window=5, \n                     min_count=1, workers=4)\n\n\nw2v_model.wv.most_similar(\"mother\")\n\nOn voit que les mots les plus similaires à “mother”\nsont souvent des mots liés à la famille, mais pas toujours.\nC’est lié à la taille très restreinte du corpus sur lequel on entraîne le modèle,\nqui ne permet pas de réaliser des associations toujours pertinentes.\nL’embedding (la représentation vectorielle) de chaque document correspond à la moyenne des word-embeddings des mots qui le composent :\n\ndef get_mean_vector(w2v_vectors, words):\n    words = [word for word in words if word in w2v_vectors]\n    if words:\n        avg_vector = np.mean(w2v_vectors[words], axis=0)\n    else:\n        avg_vector = np.zeros_like(w2v_vectors['hi'])\n    return avg_vector\n\ndef fit_w2v_avg(w2v_vectors):\n    X_train_vectors = np.array([get_mean_vector(w2v_vectors, words)\n                                for words in X_train_tokens])\n    \n    scores = cross_val_score(clf, X_train_vectors, y_train, \n                         cv=4, scoring='f1_micro', n_jobs=4)\n\n    print(f\"CV scores {scores}\")\n    print(f\"Mean F1 {np.mean(scores)}\")\n    return scores\n\n\ncv_w2vec = fit_w2v_avg(w2v_model.wv)\n\nLa performance chute fortement ;\nla faute à la taille très restreinte du corpus, comme annoncé précédemment."
  },
  {
    "objectID": "content/course/NLP/04_word2vec/index.html#word2vec-pré-entraîné-averaging",
    "href": "content/course/NLP/04_word2vec/index.html#word2vec-pré-entraîné-averaging",
    "title": "30  Méthodes de vectorisation : comptages et word embeddings",
    "section": "30.7 Word2vec pré-entraîné + averaging",
    "text": "30.7 Word2vec pré-entraîné + averaging\nQuand on travaille avec des corpus de taille restreinte,\nc’est généralement une mauvaise idée d’entraîner son propre modèle word2vec.\nHeureusement, des modèles pré-entraînés sur de très gros corpus sont disponibles.\nIls permettent de réaliser du transfer learning,\nc’est-à-dire de bénéficier de la performance d’un modèle qui a été entraîné sur une autre tâche ou bien sur un autre corpus.\nL’un des modèles les plus connus pour démarrer est le glove_model de\nGensim (Glove pour Global Vectors for Word Representation)1:\n\nGloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\nSource: https://nlp.stanford.edu/projects/glove/\n\nOn peut le charger directement grâce à l’instruction suivante :\n\nglove_model = gensim.downloader.load('glove-wiki-gigaword-200')\n\nPar exemple, la représentation vectorielle de roi est l’objet\nmultidimensionnel suivant :\n\nglove_model['king']\n\nComme elle est peu intelligible, on va plutôt rechercher les termes les\nplus similaires. Par exemple,\n\nglove_model.most_similar('mother')\n\nOn peut retrouver notre formule précédente\n\\[\\text{king} - \\text{man} + \\text{woman} ≈ \\text{queen}\\]\ndans ce plongement de mots:\n\nglove_model.most_similar(positive = ['king', 'woman'], negative = ['man'])\n\nVous pouvez vous référer à ce tutoriel\npour en découvrir plus sur Word2Vec.\nFaisons notre apprentissage par transfert :\n\ncv_w2vec_transfert = fit_w2v_avg(glove_model)\n\nLa performance remonte substantiellement.\nCela étant, on ne parvient pas à faire mieux que les approches basiques,\non arrive à peine aux performances de la vectorisation par comptage.\nEn effet, pour rappel, les performances sont les suivantes:\n\nperfs = pd.DataFrame(\n    [np.mean(cv_bow.cv_results_['mean_test_score']),\n     np.mean(cv_tfidf.cv_results_['mean_test_score']),\n    np.mean(cv_w2vec),\n    np.mean(cv_w2vec_transfert)],\n    index = ['Bag-of-Words','TF-IDF', 'Word2Vec non pré-entraîné', 'Word2Vec pré-entraîné'],\n    columns = [\"Mean F1 score\"]\n).sort_values(\"Mean F1 score\",ascending = False)\nperfs\n\nLes performences limitées du modèle Word2Vec sont cette fois certainement dues à la manière dont\nles word-embeddings sont exploités : ils sont moyennés pour décrire chaque document.\nCela a plusieurs limites :\n\non ne tient pas compte de l’ordre et donc du contexte des mots\nlorsque les documents sont longs, la moyennisation peut créer\ndes représentation bruitées."
  },
  {
    "objectID": "content/course/NLP/04_word2vec/index.html#contextual-embeddings",
    "href": "content/course/NLP/04_word2vec/index.html#contextual-embeddings",
    "title": "30  Méthodes de vectorisation : comptages et word embeddings",
    "section": "30.8 Contextual embeddings",
    "text": "30.8 Contextual embeddings\nLes embeddings contextuels visent à pallier les limites des embeddings\ntraditionnels évoquées précédemment.\nCette fois, les mots n’ont plus de représentation vectorielle fixe,\ncelle-ci est calculée dynamiquement en fonction des mots du voisinage, et ainsi de suite.\nCela permet de tenir compte de la structure des phrases\net de tenir compte du fait que le sens d’un mot est fortement dépendant des mots\nqui l’entourent.\nPar exemple, dans les expressions “le président Macron” et “le camembert Président” le mot président n’a pas du tout le même rôle.\nCes embeddings sont produits par des architectures très complexes,\nde type Transformer (BERT, etc.).\n\nmodel = SentenceTransformer('all-mpnet-base-v2')\n\n\nX_train_vectors = model.encode(X_train)\n\n\nscores = cross_val_score(clf, X_train_vectors, y_train, \n                         cv=4, scoring='f1_micro', n_jobs=4)\n\nprint(f\"CV scores {scores}\")\nprint(f\"Mean F1 {np.mean(scores)}\")\n\n\nperfs = pd.concat(\n  [perfs,\n  pd.DataFrame(\n    [np.mean(scores)],\n    index = ['Contextual Embedding'],\n    columns = [\"Mean F1 score\"])]\n).sort_values(\"Mean F1 score\",ascending = False)\nperfs\n\nVerdict : on fait très légèrement mieux que la vectorisation TF-IDF.\nOn voit donc l’importance de tenir compte du contexte.\nMais pourquoi, avec une méthode très compliquée, ne parvenons-nous pas à battre une méthode toute simple ?\nOn peut avancer plusieurs raisons :\n\nle TF-IDF est un modèle simple, mais toujours très performant\n(on parle de “tough-to-beat baseline”).\nla classification d’auteurs est une tâche très particulière et très ardue,\nqui ne fait pas justice aux embeddings. Comme on l’a dit précédemment, ces derniers se révèlent particulièrement pertinents lorsqu’il est question de similarité sémantique entre des textes (clustering, etc.).\n\nDans le cas de notre tâche de classification, il est probable que\ncertains mots (noms de personnage, noms de lieux) soient suffisants pour classifier de manière pertinente,\nce que ne permettent pas de capter les embeddings qui accordent à tous les mots la même importance."
  },
  {
    "objectID": "content/course/NLP/04_word2vec/index.html#aller-plus-loin",
    "href": "content/course/NLP/04_word2vec/index.html#aller-plus-loin",
    "title": "30  Méthodes de vectorisation : comptages et word embeddings",
    "section": "30.9 Aller plus loin",
    "text": "30.9 Aller plus loin\n\nNous avons entraîné différents modèles sur l’échantillon d’entraînement par validation croisée, mais nous n’avons toujours pas utilisé l’échantillon test que nous avons mis de côté au début. Réaliser la prédiction sur les données de test, et vérifier si l’on obtient le même classement des méthodes de vectorisation.\nFaire un vrai split train/test : faire l’entraînement avec des textes de certains auteurs, et faire la prédiction avec des textes d’auteurs différents. Cela permettrait de neutraliser la présence de noms de lieux, de personnages, etc.\nComparer avec d’autres algorithmes de classification qu’un SVM\n(Avancé) : fine-tuner le modèle d’embeddings contextuels sur la tâche de classification"
  },
  {
    "objectID": "content/course/NLP/05_exo_supp/index.html",
    "href": "content/course/NLP/05_exo_supp/index.html",
    "title": "31  Exercices supplémentaires",
    "section": "",
    "text": "Cette page approfondit certains aspects présentés dans les autres tutoriels. Il s’agit d’une suite d’exercice, avec corrections, pour présenter d’autres aspects du NLP ou pratiquer sur des données différentes\n\n32 Exploration des libellés de l’openfood database\n{{% box status=“exercise” title=“Exercise: les noms de produits dans l’openfood database” icon=“fas fa-pencil-alt” %}}\nL’objectif de cet exercice est d’analyser les termes les plus fréquents\ndans les noms de produits de l’openfood database. Au passage, cela permet de réviser les étapes de preprocessing (LIEN XXXXX) et d’explorer les enjeux de reconnaissance d’entités nommées.\n{{% /box %}}\nDans cet exercice:\n\ntokenisation (nltk)\nretrait des stop words (nltk)\nnuage de mots (wordcloud)\nreconnaissance du langage (fasttext)\nreconnaissance d’entités nommées (spacy)\n\nle tout sur l’OpenFood Database, une base de données alimentaire qui est enrichie de manière collaborative.\n{{% box status=“hint” title=“Hint” icon=“fa fa-lightbulb” %}}\nPour pouvoir utiliser les modèles pré-entraînés de spaCy, il faut les télécharger. La méthode préconisée est d’utiliser, depuis un terminal, la commande suivante\npython -m spacy download fr_core_news_sm\nDans un notebook jupyter, il se peut qu’il soit nécessaire de relancer le kernel.\nSi l’accès à la ligne de commande n’est pas possible, ou si la commande échoue, il est possible de télécharger le modèle pré-entraîné directement depuis une session Python\nimport spacy\nspacy.cli.download('fr_core_news_sm')\n{{% /box %}}\n\nImporter le modèle de reconnaissance de langage qui sera utilisé par la suite\nainsi que le corpus Français utilisé par spacy\n\n\nimport tempfile\nimport os\nimport spacy\n\ntemp_dir = tempfile.NamedTemporaryFile()\ntemp_dir = temp_dir.name\n\nos.system(\"wget -O {} https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\".format( \"%s.model.bin\" % temp_dir))\nspacy.cli.download('fr_core_news_sm')\n\n\nImporter les données de l’openfood database à partir du code suivant\n\n\nimport pandas as pd\nimport urllib.request\n\n\nurllib.request.urlretrieve('https://static.openfoodfacts.org/data/en.openfoodfacts.org.products.csv', \"%s.openfood.csv\" % temp_dir)\ndf_openfood = pd.read_csv(\"%s.openfood.csv\" % temp_dir, delimiter=\"\\t\",\n                          usecols=['product_name'], encoding = 'utf-8', dtype = \"str\")\n\nCes données devraient avoir l’aspect suivant:\n\ndf_openfood.iloc[:2, :5]\n\n\nCréer une fonction de nettoyage des noms de produits effectuant les\nétapes suivantes:\n\n\ntokeniser le texte en question\nretirer la ponctuation et les stopwords\n\nAppliquer cette fonction à l’ensemble des noms de produits (variable\nproduct_name)\n\nEffectuer un nuage de mot sur les libellés avant et après nettoyage\npour comprendre la structure du corpus en question.\nLe résultat devrait avoir l’apparence suivante\n\n\nimport wordcloud as wc\nimport matplotlib.pyplot as plt\n\n\ndef graph_wordcloud(data, by = None, valueby = None, yvar = \"Text\"):\n    if (by is not None) & (valueby is not None):        \n        txt = data[data[by]==valueby][yvar].astype(str)\n    else:\n        txt = data[yvar].astype(str)\n    all_text = ' '.join([text for text in txt])\n    wordcloud = wc.WordCloud(width=800, height=500,\n                          random_state=21,\n                      max_words=2000).generate(all_text)\n    return wordcloud\n\ndef graph_wordcloud_by(data, by, yvar = \"Text\"):\n    n_topics = data[by].unique().tolist()\n    width=20\n    height=80\n    rows = len(n_topics)//2\n    cols = 2\n    fig=plt.figure(figsize=(width, height))\n    axes = []\n    for i in range(cols*rows):\n        b = graph_wordcloud(data, by = by, valueby = n_topics[i], yvar = yvar)\n        axes.append( fig.add_subplot(rows, cols, i+1) )\n        axes[-1].set_title(\"{}\".format(n_topics[i]))  \n        plt.imshow(b)\n        plt.axis('off')\n        plt.savefig('{}.png'.format(yvar), bbox_inches='tight')\n\n\ndef wordcount_words(data, yvar, by = None):\n    plt.figure( figsize=(15,15) )\n    if by is None:\n        wordcloud = graph_wordcloud(data, yvar = yvar, by = by)\n        plt.imshow(wordcloud)\n        plt.axis('off')\n        plt.savefig('{}.png'.format(yvar), bbox_inches='tight')\n    else:\n        graph_wordcloud_by(data, by = by, yvar = yvar)\n\nwordcount_words(df_openfood, yvar = \"product_name\")\nwordcount_words(df_openfood, \"tokenized\")\n\n\nUtiliser la librairie Fasttext pour extraire les noms de produits\nfrançais\n\n\nAppliquer le modèle téléchargé précedemment pour déterminer le langage\nNe récupérer que les libellés français\n\n\nimport fasttext\n\nPRETRAINED_MODEL_PATH = \"%s.model.bin\" % temp_dir\nmodel = fasttext.load_model(PRETRAINED_MODEL_PATH)\nnewcols = ['language','score_language']\ndf_openfood[newcols] = pd.DataFrame(df_openfood['product_name'].astype(str).apply(lambda s: list(model.predict(s))).apply(lambda l: [l[0][0],l[1][0]]).tolist(), columns = newcols)\ndf_openfood['language'] = df_openfood['language'].str.replace(\"__label__\",\"\")\ndf_openfood_french = df_openfood[df_openfood['language'] == \"fr\"]\ndf_openfood_french.head(2)\n\n\nVisualiser avec spacy.displacy le résultat d’une reconnaissance\nd’entités nommées sur 50 données aléatoires. Cela vous semble-t-il satisfaisant ?\n\n\nimport spacy\nimport fr_core_news_sm\n\nnlp = fr_core_news_sm.load()\n\nexample = \" \\n \".join(df_openfood_french['product_name'].astype(\"str\").sample(50))\n\nfrom spacy import displacy\nhtml = displacy.render(nlp(example), style='ent', page=True)\n\n\nprint(html)\n\n\nRécupérer dans un vecteur les entités nommées reconnues par spaCy.\nRegarder les entités reconnues dans les 20 premiers libellés de produits\n\n\nx = []\nfor doc in nlp.pipe(df_openfood_french.head(20)['product_name'].astype(\"unicode\"), disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n    # Do something with the doc here\n    x.append([(ent.text, ent.label_) for ent in doc.ents])\n    \nx"
  },
  {
    "objectID": "content/course/modern-ds/index.html#contenu-de-la-partie",
    "href": "content/course/modern-ds/index.html#contenu-de-la-partie",
    "title": "Partie 5: Introduction aux outils et méthodes à l’état de l’art",
    "section": "Contenu de la partie",
    "text": "Contenu de la partie"
  },
  {
    "objectID": "content/course/modern-ds/continuous_integration/index.html",
    "href": "content/course/modern-ds/continuous_integration/index.html",
    "title": "32  Intégration continue avec Python",
    "section": "",
    "text": "33 L’intégration continue: une opportunité pour les data-scientists\nOn retrouve régulièrement l’acronyme CI/CD\npour illustrer cette\nnouvelle méthode de travail dans le\nmonde du développement logiciel :\nCette pratique permet ainsi de détecter de manière précoce des possibilités\nde bug ou l’introduction d’un changement non anticipé. Tout comme Git,\ncette pratique devient un standard dans les domaines collaboratifs.\nL’intégration continue permet de sécuriser le travail, puisqu’elle offre un\nfilet de sécurité (par exemple un test sur une machine à la configuration\narbitraire), mais permet aussi de déployer en temps réel certaines\névolutions. On parle parfois de déploiement en continu, complémentaire de\nl’intégration continue. Cette approche réduit ainsi\nla muraille de Chine entre un\nanalyste de données et une équipe de développeurs d’application. Elle offre donc\nplus de contrôle, pour le producteur d’une analyse statistique, sur la\nvalorisation de celle-ci.\nCette approche consiste une excellente opportunité\npour les data-scientists d’être en mesure\nde valoriser leurs projets auprès de publics aux\nexigences différentes. Pour des développeurs, le\ndata-scientist pourra fournir une image Docker\n(environnement portable où l’ensemble des dépendances\net des configurations systèmes pour faire tourner un code\nsont contrôlés) permettant à d’autres d’exécuter\nfacilement le code d’un projet. Pour faciliter\nla réutilisation d’un modèle par d’autres data-scientists,\nil devient de plus en plus fréquent d’exposer\nun modèle sous forme d’API: les personnes désirant\nréutiliser le modèle peuvent directement l’appliquer\nen accédant à une prédiction par le biais d’une API\nce qui évite d’avoir à fournir le jeu d’entraînement\nsi ce dernier est sensible. Pour toucher\ndes publics moins\nfamiliers du code, la mise à disposition de sites web\ninteractifs valorisant certains résultats d’un projet\npeut être intéressante. Cette approche très exigeante\nd’utiliser un même projet pour toucher des cibles\ntrès différentes est grandement facilitée par le\ndéploiement en continu et la mise à disposition\nde librairies ou d’infrastructures\ndédiées dans le monde de l’open-source.\nTout en restant éco-responsable (voir partie XXX), cela\npermet de mieux valoriser des projets pour réduire\nles coûts à le maintenir et le faire évoluer.\nLe cours de dernière année de l’ENSAE que je développe\navec Romain Avouac (https://ensae-reproductibilite.netlify.app/)\nprésente beaucoup plus de détails sur cette question.\nL’intégration continue fonctionne très bien sur Gitlab et sur Github.\nA chaque interaction avec le dépôt distant (push), une série d’instruction\ndéfinie par l’utilisateur est exécutée. Python et R s’intègrent très bien dans ce paradigme grâce\nà un certain nombre d’images de base (concept sur lequel nous allons revenir)\nqui peuvent être customisées pour répondre à une certaine configuration\nnécessaire pour exécuter des codes\n(voir ici pour quelques éléments sur R.\nC’est une méthode idéale pour améliorer la reproductibilité d’un projet: les\ninstructions exécutées le sont dans un environnement isolé et contrôlé, ce qui\ndiffère d’une machine personnelle.\nL’intégration continue repose sur le système de la dockerisation ou conteneurisation.\nLa technologie sous jacente s’appelle Docker.\nIl s’agit d’une technologie qui permet la construction\nde machines autosuffisantes\n(que l’on nomme containeurs) répliquant un environnement\ncontrôlé (que l’on nomme image).\nOn parle de pipelines pour désigner une suite de tâches pour partir de 0\n(généralement une machine Linux à la configuration minimale) et aboutir\nà l’issue d’une série d’instructions définies par l’utilisateur.\nL’objectif est de trouver une image la plus\nparcimonieuse possible, c’est-à-dire à la configuration minimale, qui permet\nde faire tourner le code voulu.\nLes Actions Github\nconsistuent un modèle sur lequel il est facile\nde s’appuyer lorsqu’on a des connaissances limitées\nconcernant `Docker.\nIl est également très simple de construire son image\nde rien, ce qui est la démarche choisie dans\nl’autre cours de l’ENSAE que nous donnons avec Romain\nAvouac (https://ensae-reproductibilite.netlify.app/).\nQuand on utilise un dépôt Github \nou Gitlab ,\ndes services automatiques\nd’intégration continue peuvent être utilisés:\nHistoriquement, il existait d’autres services d’intégration continue, notamment\nTravis CI ou AppVeyor1\nLes projets de valorisation de données prennent des formes\ntrès variées et s’adressent à des publics multiples dont\nles attentes peuvent être très diverses.\nNe pas attendre la finalisation d’un projet pour mettre\nen oeuvre certains livrables est une méthode efficace\npour ne pas se retrouver noyé, au dernier moment,\nsous des demandes et de nouvelles contraintes.\nLa production en continu de livrables est donc une\nméthode très prisée dans le monde de la donnée.\nLes principaux fournisseurs de services\nd’intégration continue, à commencer par\nGithub et Gitlab proposent des services\npour le déploiement en continu. Cependant,\nceux-ci ne sont adaptés qu’à certains types\nde livrables, principalement la mise à disposition\nde sites internet, et il peut être intéressant\nd’utiliser des services externes ou une\ninfrastructures Kubernetes selon les\nmoyens à dispositon et les besoins des utilisateurs."
  },
  {
    "objectID": "content/course/modern-ds/continuous_integration/index.html#fonctionnement-des-actions-github",
    "href": "content/course/modern-ds/continuous_integration/index.html#fonctionnement-des-actions-github",
    "title": "32  Intégration continue avec Python",
    "section": "35.1 Fonctionnement des actions Github",
    "text": "35.1 Fonctionnement des actions Github\nLes actions Github fonctionnent par couches successives au sein desquelles\non effectue un certain nombre d’instructions.\nLa meilleure manière d’apprendre les actions Github est, certes, de lire la\ndocumentation officielle mais surtout,\nà mon avis, de regarder quelques pipelines pour comprendre la démarche.\nL’un des intérêts des Github Actions est la possibilité d’avoir un pipeline\nproposant une intrication de langages différents pour avoir une chaine de\nproduction qui propose les outils les plus efficaces pour répondre à un\nobjectif en limitant les verrous techniques.\nPar exemple, le pipeline de ce cours, disponible\nsur Github propose une intrication des langages\nPython et R avec des technologies Anaconda (pour contrôler\nl’environnement Python comme expliqué dans les chapitres précédents)\net Javascript (pour le déploiement d’un site web avec le service tiers\nNetlify)2. Cette chaîne de production multi-langage permet que\nles mêmes fichiers sources génèrent un site web et des notebooks disponibles\nsur plusieurs environnements.\n\n\nname: Production deployment\n\non:\n  push:\n    branches:\n      - main\n      - master\n\njobs:\n  pages:\n    name: Render-Blog\n    runs-on: ubuntu-latest\n    container: linogaliana/python-datascientist:latest\n    steps:\n      - uses: actions/checkout@v2\n        with:\n          #fetch-depth: 0\n          ref: ${{ github.event.pull_request.head.ref }}\n          repository: ${{github.event.pull_request.head.repo.full_name}}\n      - name: Configure safe.directory  # Workaround for actions/checkout#760\n        run: git config --global --add safe.directory /__w/python-datascientist/python-datascientist\n      - shell: bash\n        run: |\n          conda info\n          conda list\n      - name: Render website\n        run: |\n          python build/wc_website.py\n          quarto render --to html\n      - name: Publish to Pages\n        run: |\n          git config --global user.email quarto-github-actions-publish@example.com\n          git config --global user.name \"Quarto GHA Workflow Runner\"\n          quarto publish gh-pages . --no-render --no-browser\n      - uses: actions/upload-artifact@v2\n        with:\n          name: Website\n          path: public/\n\n\n\nLes couches qui constituent les étapes du pipeline\nportent ainsi le nom de steps. Un step peut comporter un certain\nnombre d’instructions ou exécuter des instructions pré-définies.\nL’une de ces instructions prédéfinies est, par exemple,\nl’installation de Python\nou l’initialisation d’un environnement conda.\nLa documentation officielle de Github propose un\nfichier qui peut servir de modèle\npour tester un script Python voire l’uploader de manière automatique\nsur Pypi."
  },
  {
    "objectID": "content/course/modern-ds/continuous_integration/index.html#intégration-continue-avec-python-tester-un-notebook",
    "href": "content/course/modern-ds/continuous_integration/index.html#intégration-continue-avec-python-tester-un-notebook",
    "title": "32  Intégration continue avec Python",
    "section": "35.2 Intégration continue avec Python: tester un notebook",
    "text": "35.2 Intégration continue avec Python: tester un notebook\nCette section n’est absolument pas exhaustive. Au contraire, elle ne fournit\nqu’un exemple minimal pour expliquer la logique de l’intégration continue. Il\nne s’agit ainsi pas d’une garantie absolue de reproductibilité d’un notebook.\nGithub propose une action officielle pour utiliser Python dans un\npipeline d’intégration continue. Elle est disponible sur le\nMarketPlace Github.\nIl s’agit d’un bon point de départ, à enrichir.\nLe fichier qui contrôle les instructions exécutées dans l’environnement Actions\ndoit se trouver dans le dossier .github/workflows/\n(:warning: ne pas oublier le point au début du\nnom du dossier). Il doit être au format YAML avec une extension .yml\nou .yaml.\nIl peut avoir n’importe quel nom néanmoins il\nvaut mieux lui donner un nom signifiant,\npar exemple prod.yml pour un fichier contrôlant une chaîne de production.\n\n35.2.1 Lister les dépendances\nAvant d’écrire les instructions à exécuter par Github, il faut définir un\nenvironnement d’exécution car Github ne connaît pas la configuration Python\ndont vous avez besoin.\nIl convient ainsi de lister les dépendances nécessaires dans un fichier\nrequirements.txt (si on utilise un environnement virtuel)\nou un fichier environment.yml (si on préfère\nutiliser un environnement conda).\nBien que le principe sous-jacent soit légèrement différent,\nces fichiers ont la même fonction:\npermettre la création d’un environnement ex-nihilo\navec un certain nombre de dépendances pré-installées3.\nSi on fait le choix de l’option environment.yml,\nle fichier prendra ainsi la forme\nsuivante, à enrichir en fonction de la\nrichesse de l’environnement souhaité. :\nchannels:\n  - conda-forge\n\ndependencies:\n  - python&gt;=3.10\n  - jupyter\n  - jupytext\n  - matplotlib\n  - nbconvert\n  - numpy\n  - pandas\n  - scipy\n  - seaborn\nLe même fichier sous le format requirements.txt aura\nla forme suivante:\njupyter\njupytext\nmatplotlib\nnbconvert\nnumpy\npandas\nscipy\nseaborn\nSous leur apparente équivalence, au-delà de\nla question du formatage, ces fichiers ont\ndeux différences principales :\n\nla version minimale de Python est définie dans\nle fichier environment.yml alors qu’elle ne l’est\npas dans un fichier requirements.txt. C’est\nparce que le second installe les dépendances dans\nun environnement déjà existant par ailleurs alors\nque le premier peut servir à créer l’environnement\navec une certaine configuration de Python ;\nle mode d’installation des packages n’est pas le\nmême. Avec un environment.yml on installera des\npackages via conda alors qu’avec un requirements.txt\non privilégiera plutôt pip4.\n\nDans le cas de l’environnement conda,\nle choix du channel conda-forge vise à contrôler le dépôt utilisé par\nAnaconda.\n{{% box status=“hint” title=“Conseil” icon=“fa fa-lightbulb” %}}\nLa conda forge est un dépôt de package alternatif\nau canal par défaut d’Anaconda qui est maintenu par\nl’équipe de développeurs officiels d’Anaconda.\nComme cette dernière cherche en priorité à\nassurer la stabilité de l’écosystème Anaconda,\nles versions de package évoluent moins vite\nque le rythme voulu par les développeurs de\npackages. Pour cette raison, un dépôt\nalternatif, où les montées de version sont\nplus simples parce qu’elles dépendent des\ndéveloppeurs de chaque package, a émergé.\nIl s’agit de la conda forge. Lorsqu’on\ndésire utiliser des fonctionalités récentes\nde l’écosystème de la data-science,\nil est conseillé de l’utiliser.\n{{% /box %}}\nNe pas oublier de mettre ce fichier sous contrôle de version et de l’envoyer\nsur le dépôt par un push.\n\n\n35.2.2 Créer un environnement reproductible dans Github Actions\nDeux approches sont possibles à ce niveau, selon le degré\nde reproductibilité désiré5:\n\nCréer l’environnement via une action existante. L’action\nconda-incubator/setup-miniconda@v2\nest un bon point de départ.\nCréer l’environnement dans une image Docker.\n\nLa deuxième solution permet de contrôler de manière\nbeaucoup plus fine l’environnement dans lequel\nPython s’éxécutera ainsi que la manière dont\nl’environnement sera créé6. Néanmoins, elle nécessite\ndes connaissances plus poussées dans la principe\nde la conteneurisation qui peuvent être coûteuses\nà acquérir. Selon l’ambition du projet, notamment\nles réutilisation qu’il désire,\nun data-scientist pourra privilégier\ntelle ou telle option. Les deux solutions sont présentées\ndans l’exemple fil-rouge du cours que nous\ndonnons avec Romain Avouac\n(https://ensae-reproductibilite.netlify.app/application/).\n\n\n35.2.3 Tester un notebook myfile.ipynb\nDans cette partie, on va supposer que le notebook à tester s’appelle myfile.ipynb\net se trouve à la racine du dépôt. Les\ndépendances pour l’exécuter sont\nlistées dans un fichier requirements.txt.\nLe modèle suivant, expliqué en dessous, fournit un modèle de recette pour\ntester un notebook. Supposons que ce fichier soit présent\ndans un chemin .github/workflows/test-notebook.yml\n\n\nEnvironnement virtuel\n\nname: Test notebook execution using Github Actions\n\non: [push]\n\njobs:\n  build-linux:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up Python 3.10\n      uses: actions/setup-python@v3\n      with:\n        python-version: '3.10'\n      - shell: bash\n      run: |\n        python --version\n    - name: Install dependencies\n      run:\n        pip install -r requirements.txt\n        pip install jupyter nbconvert\n    - name: Test jupyter from command line\n      run:\n        jupyter nbconvert --execute --to notebook --inplace myfile.ipynb\n    - uses: actions/upload-artifact@v3\n      with:\n        name: Notebook\n        path: myfile.ipynb\n        retention-days: 5\n\n\n\nEnvironnement conda\n\nname: Test notebook execution using Github Actions\n\non: [push]\n\njobs:\n  build-linux:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up Python 3.10\n      uses: actions/setup-python@v3\n      with:\n        python-version: '3.10'\n    - name: Add conda to system path\n      run: |\n        # $CONDA is an environment variable pointing to the root of the miniconda directory\n        echo $CONDA/bin &gt;&gt; $GITHUB_PATH\n    - name: Install dependencies\n      run: |\n        conda env update --file environment.yml --name base\n        conda install jupyter nbconvert\n    - name: Test jupyter from command line\n      run:\n        jupyter nbconvert --execute --to notebook --inplace myfile.ipynb\n    - uses: actions/upload-artifact@v3\n      with:\n        name: Notebook\n        path: myfile.ipynb\n        retention-days: 5\n\nDans les deux cas, la démarche est la même:\n\non récupère les fichiers présents dans le dépôt\n(action checkout) ;\non installe Python ;\non installe les dépendances pour exécuter le code.\nDans l’approche conda, il est également nécessaire\nde faire quelques configurations supplémentaires (notamment\najouter conda aux logiciels reconnus par la ligne\nde commande) ;\non teste le notebook en ligne de commande et remplace\ncelui existant, sur la machine temporaire, par la version\nproduite sur cet environnement neutre.\non rend possible le téléchargement du\nnotebook produit automatiquement pendant 5 jours7. Ceci\nrepose sur les artefacts qui sont un élément récupéré\ndes machines temporaires qui n’existent plus dès que le\ncode a fini d’être exécuté.\n\nCes actions sont exécutées à chaque interaction avec\nle dépôt distant (push), quelle que soit la\nbranche. A partir de ce modèle, il est possible de\nraffiner pour, par exemple, automatiquement\nfaire un commit du notebook validé et le pusher\nvia le robot Github8"
  },
  {
    "objectID": "content/course/modern-ds/continuous_integration/index.html#les-services-de-mise-à-disposition-de-github-et-gitlab",
    "href": "content/course/modern-ds/continuous_integration/index.html#les-services-de-mise-à-disposition-de-github-et-gitlab",
    "title": "32  Intégration continue avec Python",
    "section": "36.1 Les services de mise à disposition de Github et Gitlab",
    "text": "36.1 Les services de mise à disposition de Github et Gitlab\nGithub et Gitlab, les deux plateformes de partage\nde code, proposent non seulement des services\ngratuits d’intégration continue mais aussi des services\nde mise à disposition de sites web pleinement intégrés\naux services de stockage de code.\nCes services, Gitlab Pages et Github Pages, auxquels\non peut associer le service externe Netlify qui répond\nau même principe9 permettent, à chaque modification\ndu code source d’un projet, de reconstruire le site web (le livrable)\nqui peut être directement produit à partir de certains fichiers\n(des slides revealJS par exemple) ou qui\nsert d’output à l’intégration continue après compilation\nde fichiers plus complexes (des fichiers quarto par exemple).\nChaque dépôt sur Github ou Gitlab peut ainsi être associé\nà un URL de déploiement disponible sur internet. A chaque\ncommit sur le dépôt, le site web qui sert de livrable\nest ainsi mis à jour. La version déployée à partir de la\nbranche principale peut ainsi être considérée\ncomme la version de production alors que les branches\nsecondaires peuvent servir d’espace bac à sable pour\nvérifier que des changements dans le code source\nne mettent pas en péril le livrable. Cette méthode,\nqui sécurise la production d’un livrable sous forme\nde site web, est ainsi particulièrement appréciable."
  },
  {
    "objectID": "content/course/modern-ds/continuous_integration/index.html#les-services-externes-disponibles-sans-infrastructure-spéciale",
    "href": "content/course/modern-ds/continuous_integration/index.html#les-services-externes-disponibles-sans-infrastructure-spéciale",
    "title": "32  Intégration continue avec Python",
    "section": "36.2 Les services externes disponibles sans infrastructure spéciale",
    "text": "36.2 Les services externes disponibles sans infrastructure spéciale\nPour fonctionner, l’intégration continue\nnécessite de mettre en oeuvre des environnements normalisés.\nComme évoqué précédemment,\nla technologie sous-jacente est celle de la conteneurisation.\nLes images qui servent de point de départ au lancement\nd’un conteneur sont elles-mêmes mises à disposition\ndans des espaces communautaires (des registres d’images).\nIl en existe plusieurs, les plus connus étant\nle dockerhub ou le registry de Gitlab.\nCes registres servent d’espaces de stockage pour des images,\nqui sont des objets volumineux (potentiellement plusieurs\nGigas) mais aussi d’espace de mutualisation en permettant\nà d’autres de réutiliser une image prête à l’emploi ou,\nau contraire, à partir de\nlaquelle on peut ajouter un certain nombre de couches\npour obtenir l’environnement minimal\nde reproductibilité.\nIl est possible d’utiliser certaines actions Github\nprête à l’emploi pour constuire une image Docker\nà partir d’un fichier Dockerfile. Après avoir\ncrée une connexion entre un compte sur la\nplateforme Github et l’autre sur DockerHub,\nune mise à disposition automatisée d’un livrable\nsous forme d’image Docker est ainsi possible.\nUne image Docker peut offrir une grande variété\nd’output. Elle peut servir uniquement à\nmettre à disposition un environnement de\nreproductibilité mais elle peut servir à mettre\nà disposition, pour les personnes maîtrisant\nDocker, des output plus raffinés. Par exemple,\ndans le cours que nous donnons à l’ENSAE, nous\nmontrons comment docker peut servir à\nmettre à disposition à un utilisateur tiers\nune application minimaliste (construite avec flask)\nqu’il fera tourner\nsur son ordinateur.\nSi une image Docker peut être très utile pour la mise\nà disposition, elle nécessite pour sa réutilisation\nun niveau avancé d’expertise en programmation.\nCela ne conviendra pas à tous les publics. Certains\nne désireront que bénéficier d’une application interactive\noù ils pourrons visualiser certains résultats en fonction\nd’actions comme des filtres sur des sous-champs ou le choix\nde certaines plages de données. D’autres publics seront\nplutôt intéressé par la réutilisation d’un programme\nou des résultats d’un modèle sous forme d’API mais n’auront\npas l’infrastructure interne pour faire tourner le code\nd’origine ou une image Docker. C’est pour répondre à ces\nlimites qu’il peut devenir intéressant, pour une équipe\nde data-science de développer une architecture\nkubernetes interne, si l’organisation en a les moyens, ou\nde payer un fournisseur de service, comme AWS, qui permet\ncela."
  },
  {
    "objectID": "content/course/modern-ds/continuous_integration/index.html#kubernetes-le-sommet-de-la-pente-du-déploiement",
    "href": "content/course/modern-ds/continuous_integration/index.html#kubernetes-le-sommet-de-la-pente-du-déploiement",
    "title": "32  Intégration continue avec Python",
    "section": "36.3 Kubernetes: le sommet de la pente du déploiement",
    "text": "36.3 Kubernetes: le sommet de la pente du déploiement\nKubernetes est une technologie qui pousse la logique\nde la conteneurisation à son paroxysme.\nIl s’agit d’un système open-source, développé\npar Google, permettant\nd’automatiser le déploiement, la mise à l’échelle\net la gestion d’applications conteneurisées.\nGrâce à Kubernetes, une application, par exemple\nun site web proposant de la réactivité,\npeut être mise à disposition et reporter les calculs,\nlorsqu’ils sont nécessaires, sur\nun serveur. L’utilisation de Kubernetes dans\nun projet de data-science permet ainsi\nd’anticiper à la fois l’interface d’une application\nvalorisant un projet mais aussi le fonctionnement\ndu back-office, par exemple en testant la capacité\nde charge de cette application. Une introduction\nà Kubernetes orienté donnée peut être trouvée dans\nle cours dédié à la mise en production\nque nous donnons avec Romain Avouac et dans ce\npost de blog très bien fait.\nDans les grandes organisations, où les rôles sont\nplus spécialisés que dans les petites structures,\nce ne sont pas nécessairement les data-scientists\nqui devront maîtriser Kubernetes mais plutôt\nles data-architect ou les data-engineer. Néanmoins,\nles data-scientists devront être capable de\ndialoguer avec eux et mettre en oeuvre une méthode\nde travail adaptée (celle-ci reposera en principe sur\nl’approche CI/CD). Dans les petites structures, les\ndata-scientist peuvent être en mesure\nde mettre en oeuvre le déploiement en continu. En\nrevanche, il est plus rare, dans ces structures,\noù les moyens humains de maintenance sont limités,\nque les serveurs sur lesquels fonctionnent Kubernetes\nsoient détenus en propres. En général, ils sont loués\ndans des services de paiement à la demande de type\nAWS."
  },
  {
    "objectID": "content/course/modern-ds/dallE/index.html",
    "href": "content/course/modern-ds/dallE/index.html",
    "title": "33  Génération d’images avec Python, DALL-E et StableDiffusion",
    "section": "",
    "text": "34 Contexte\nLa publication en avril 2022 par l’organisation Open AI de\nson modèle de génération de contenu créatif Dall-E-2\n(un jeu de mot mélangeant Dali et Wall-E) a créé un bruit inédit dans\nle monde de la data-science.\nA propos de Dall-E, le bloggeur tech Casey Newton a pu parler d’une\nrévolution créative dans le monde de l’IA.\nThe Economist a par consacré\nun numéro au sujet de l’émergence de ces intelligences artificielles\ncréatrices de contenu.\nCe bruit sur la capacité des\nintelligences artificielle à générer du contenu créatif\na d’ailleurs été amplifié plus récemment\navec la publication du chatbot chatGPT\n(voir cet éditorial du Guardian).\nL’inconvénient principal de Dall-E\npour générer facilement du contenu\nest que le nombre de contenu pouvant être créé\navec un accès gratuit est limité (50 crédits gratuits par mois).\nDepuis le 22 Août 2022, un générateur de contenu\nsimilaire est disponible gratuitement,\navec une licence plus permissive1. Ce générateur, développé\npar une équipe de chercheurs (Rombach et al. 2022),\ns’appelle Stable Diffusion (dépôt Github pour le code source et\ndépôt HuggingFace pour le modèle mis à disposition2).\nUn excellent article de blog décrit la démarche de Stable Diffusion. La plupart des exemples originaux\ndans cette partie seront basés sur Stable Diffusion.\nHuggingFace\nHuggingface est une plateforme de partage de modèles de type réseau de neurone. Les utilisateurs de réseaux de neurone peuvent\nainsi mettre à disposition le résultat de leurs travaux sous forme d’API pour faciliter la réutilisation de leurs\nmodèles ou réutiliser facilement des modèles, ce qui évite de les ré-entraîner (ce qui aurait un coût écologique non\nnégligeable comme expliqué dans le chapitre introductif).\nDall-E-2 et StableDiffusion\nsont des modèles généralistes.\nD’autres modèles, plus spécialisés,\nexistent également.\nLe modèle Midjourney\n(produit propriétaire de la société du même nom)\npermet la production de contenu\nartistique, DreamBooth (développé par Google)\nest spécialisé dans la génération de contenu dans un nouveau\ncontexte.\nLe principe de tous ces modèles est le même: un utilisateur\ndonne une instruction (une ou plusieurs phrases) et l’intelligence\nartificielle l’interprète et génère une image censée être\ncohérente avec l’instruction.\nVoici par exemple l’une des productions possibles de DALL-E-2\n“A Shiba Inu dog wearing a beret and black turtleneck”\nMidjourney, spécialisé dans le contenu esthétique,\ngénèrera l’image suivante avec l’instruction “mechanical dove”:\nStableDiffusion, modèle généraliste comme Dall-E,\ncrééra le contenu suivant avec\nl’instruction “A photograph of an astronaut riding a horse”:\nEnfin, DreamBooth pourra lui introduire un chien dans une grande variété\nde contextes:\nUn compte Twitter (Weird AI Generations)\npropose de nombreuses générations de contenu drôles ou incongrues.\nVoici un premier exemple de production humoristique faite à partir de Mini Dall-E, la version\npublique:\npic.twitter.com/DIerJPtXGE— Weird Ai Generations (@weirddalle) August 6, 2022\nAinsi qu’un deuxième:\npic.twitter.com/Ju0Pdcokth— Weird Ai Generations (@weirddalle) August 8, 2022\nLes modèles Dall-E-2 et Stable Diffusion\ns’appuient sur des réseaux de neurone à différents niveaux :\nIllustration du fonctionnement de ce type de générateur d’image (ici à partir de Stable Diffusion)\nWarning\nLes services d’OpenAI ne sont gratuits que dans une certaine\nlimite. Votre clé d’API est donc assez précieuse car si elle\nest usurpée, elle peut permettre à certaines personnes\nd’épuiser vos crédits gratuits voire d’utiliser des crédits\npayants à votre place.\nSi vous êtes enregistrés récemment dans le service d’API\nd’OpenAI, vous avez accès à des crédits gratuits. Ne les\nutilisez néanmoins pas avec trop de légèreté en ne contrôlant\npas les paramètres de vos appels aux API car ces crédits\nsont pour l’ensemble des services d’OpenAI(chatGPT,\nDall-E, DaVinci…)\nLe contenu de cette partie s’appuie sur\nle tutoriel du site realpython\nL’utilisation de Dall-E sera faite via le package openai qui donne\naccès à l’API d’OpenAI.\nPour l’installer depuis la cellule d’un Notebook:\n!pip install openai\nAprès avoir obtenu votre clé d’API, on va supposer que celle-ci\nest stockée dans une variable key:\nkey = \"sk-XXXXXXXXXX\" #remplacer avec votre clé\nEnsuite, l’utilisation de l’API est assez directe:\nopenai.api_key = key\nopenai.Image.create(\n  prompt=\"Teddy bears working on new AI research underwater with 1990s technology\",\n  n=2,\n  size=\"1024x1024\"\n)\nL’output est un JSON avec les URL des images générées.\nVoici les deux images générées:\nPour aller plus loin, vous pouvez consulter\nle tutoriel de realpython\nStable Diffusion est\nune intelligence artificielle créatrice de contenu qui permet de\ngénérer du contenu à partir d’une phrase - ce pour quoi nous allons\nl’utiliser - mais aussi modifier des images à partir d’instructions.\nStable Diffusion est un modèle plus pratique à utiliser depuis Python\nque Dall-E. Celui-ci\nest open source et peut être téléchargé et réutilisé directement depuis Python.\nLa méthode la plus pratique est d’utiliser le modèle mis\nà disposition sur HuggingFace. Le modèle est implémenté\nà travers le framework PyTorch.\nPyTorch, librairie développée\npar Meta, n’est pas implementé directement en Python\npour des raisons de performance mais en C++ - Python étant un\nlangage lent, le revers de la médaille de sa facilité\nd’usage. A travers Python, on va utiliser une API haut niveau\nqui va contrôler la structure des réseaux de neurone ou\ncréer une interface entre des\ndonnées (sous forme d’array Numpy) et le modèle.\nPour ce type de packages qui utilisent un langage compilé,\nl’installation via Pandas\nConfiguration spécifique à Colab 👇\nSur Colab, conda n’est pas disponible par défaut.\nPour pouvoir\ninstaller un package en utilisant conda sur Colab,\non utilise donc l’astuce\nsuivante:\n!pip install -q condacolab\nimport condacolab\ncondacolab.install()\nOn va créer l’image suivante:\nPas mal comme scénario, non ?!\nNote\nPour que les résultats soient reproductibles entre différentes\nsessions,\nnous allons fixer\nla racine du générateur aléatoire.\nimport torch\ngenerator = torch.Generator(\"cuda\").manual_seed(123)\nSi vous voulez vous amuser à explorer différents résultats\npour un même texte, vous pouvez ne pas fixer de racine aléatoire.\nDans ce cas, retirer l’argument generator des codes présentés\nultérieurement.\nNous allons donc utiliser l’instruction suivante :\nprompt = \"Chuck Norris fighting against Zeus on Mount Olympus in an epic Mortal Kombat scene\"\nL’initialisation du modèle se fait de la manière\nsuivante:\nimport torch\nfrom torch import autocast\nfrom diffusers import StableDiffusionPipeline\n\nmodel_id = \"CompVis/stable-diffusion-v1-4\"\ndevice = \"cuda\"\n\ngenerator = torch.Generator(\"cuda\").manual_seed(1024)\nEnfin, pour générer l’image:\npipe = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=HF_TOKEN, generator=generator)\npipe = pipe.to(device)\n\nwith autocast(\"cuda\"):\n    image = pipe(prompt, guidance_scale=7.5, generator = generator)[\"images\"][0]  \n\n   \nimage.save(\"featured.png\")\nQui peut être visualisé avec le code suivant, dans un notebook:\nfrom IPython.display import Image \npil_img = Image(filename=\"featured.png\")\ndisplay(pil_img)\nC’est une représentation assez fidèle du\npitch “Chuck Norris fighting against Zeus on Mount Olympus in an epic Mortal Kombat scene” :boom:.\nY a un petit côté Les Dix Commandements que j’aime bien.\nEn voici une autre que j’aime bien (mais malheureusement je ne peux la reproduire car je n’ai pas\ngardé en mémoire la racine l’ayant généré :sob:)\nIl est également possible de générer plusieurs images du même texte (voir\nla note de blog de l’équipe\nà l’origine de Stable Diffusion). Cependant, c’est assez exigeant en\nmémoire et cela risque d’être impossible sur Colab (y compris\nen réduisant le poids des vecteurs numériques comme proposé dans le post)\nPour le plaisir, voici PuppyMan, le dernier né du Marvel Universe:\nprompt = \"In a new Marvel film we discover puppyman a new super hero that is half man half bulldog\"\nimport torch\nfrom torch import autocast\nfrom diffusers import StableDiffusionPipeline\n\nmodel_id = \"CompVis/stable-diffusion-v1-4\"\ndevice = \"cuda\"\n\ngenerator = torch.Generator(\"cuda\").manual_seed(1024)\n\npipe = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=HF_TOKEN, generator=generator)\npipe = pipe.to(device)\n\nwith autocast(\"cuda\"):\n    image = pipe(prompt, guidance_scale=7.5, generator = generator)[\"images\"][0]  \n\n   \nimage.save(\"puppyman.png\")\nLa moitié humain semble être son costume de super-héros, pas la bipédie.\nMais le rendu\nest quand même épatant !\nA vous de jouer :hugging_face:"
  },
  {
    "objectID": "content/course/modern-ds/dallE/index.html#installation-de-pytorch",
    "href": "content/course/modern-ds/dallE/index.html#installation-de-pytorch",
    "title": "33  Génération d’images avec Python, DALL-E et StableDiffusion",
    "section": "36.1 Installation de PyTorch",
    "text": "36.1 Installation de PyTorch\nPour installer PyTorch, la librairie de Deep Learning\ndéveloppée par Meta, il suffit de suivre les recommandations\nsur le site web officiel.\nDans un Notebook, cela prendra la forme suivante:\n\n!conda install mamba\n!mamba install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch\n\n\n\n Note\nJe propose ici d’utiliser mamba pour accélérer l’installation.\nDes éléments sur mamba sont disponibles dans l’introduction de ce cours."
  },
  {
    "objectID": "content/course/modern-ds/dallE/index.html#accès-à-huggingface",
    "href": "content/course/modern-ds/dallE/index.html#accès-à-huggingface",
    "title": "33  Génération d’images avec Python, DALL-E et StableDiffusion",
    "section": "36.2 Accès à HuggingFace",
    "text": "36.2 Accès à HuggingFace\nLa question - non négligeable - de l’accès à\nde la GPU mise à part,\nla réutilisation des modèles de Stable Diffusion est\ntrès facile car la documentation mise à disposition sur\nHuggingFace est très bien faite.\nLa première étape est de se créer un compte sur HuggingFace\net se créer un token3. Ce token sera donné à l’API\nde HuggingFace pour s’authentifier.\nL’API d’HuggingFace nécessite l’installation du\npackage diffusers.\nDans un Notebook, le code suivant permet d’installer la librairie\nrequise:\n\n!pip install --upgrade diffusers transformers scipy accelerate\n\n\n\n Note\nOn va supposer que le token est stocké dans une variable\nd’environnement HF_PAT. Cela évite d’écrire le token\ndans un Notebook qu’on va\npotentiellement partager, alors que le token\nest un élément à garder secret. Pour l’importer\ndans la session Python:\nSi vous n’avez pas la possibilité de rentrer le token dans les variables\nd’environnement, créez une cellule qui crée la variable\nHF_TOKEN et supprimez là de suite pour ne pas l’oublier avant\nde partager votre token.\n\n\n\nimport os\nHF_TOKEN = os.getenv('HF_PAT')"
  },
  {
    "objectID": "content/course/modern-ds/s3/index.html",
    "href": "content/course/modern-ds/s3/index.html",
    "title": "34  Les nouveaux modes d’accès aux données: le format parquet et les données sur le cloud",
    "section": "",
    "text": "35 Elements de contexte\nLe format CSV a rencontré un grand succès par sa simplicité: il\nest lisible par un humain (un bloc-note suffit pour l’ouvrir et\napercevoir les premières lignes), sa nature plate lui permet\nde bien correspondre au concept de données tabulées sans hiérarchie\nqui peuvent être rapidement valorisées, il est universel (il n’est\npas adhérent à un logiciel). Cependant, le CSV présente\nplusieurs inconvénients qui justifient l’émergence d’un format\nconcurrent:\nNote\nLa plupart des logiciels d’analyse de données proposent\nun format de fichier pour sauvegarder des bases de données. On\npeut citer le .pickle (Python), le .rda ou .RData (R),\nle .dta (Stata) ou le .sas7bdat (SAS). L’utilisation\nde ces formats est problématique car cela revient à se lier\nles mains pour l’analyse ultérieure des données, surtout\nlorsqu’il s’agit d’un format propriétaire (comme avec\nSAS ou Stata). Par exemple, Python ne\nsait pas nativement lire un .sas7bdat. Il existe des librairies\npour le faire (notamment Pandas) mais le format\nétant propriétaire, les développeurs de la librairie ont dû tâtonner et\non n’est ainsi jamais assuré qu’il n’y ait pas d’altération de la donnée.\nMalgré tous les inconvénients du .csv listés plus haut, il présente\nl’immense avantage, par rapport à ces formats, de l’universalité.\nIl vaut ainsi mieux privilégier un .csv à ces formats pour le stockage\nde la donnée. Ceci dit, comme vise à le montrer ce chapitre, il vaut\nmieux privilégier le format parquet au CSV.\nPour répondre à ces limites du CSV, le format parquet,\nqui est un projet open-source Apache, a émergé.\nLa première différence entre le format parquet et le CSV est\nque le premier repose sur un stockage orienté colonne là où\nle second est orienté ligne. Pour comprendre la différence, voici un\nexemple issu du blog d’upsolver:\nDans notre exemple précédent, cela donnera une information prenant\nla forme suivante (ignorez l’élément pyarrow.Table, nous\nreviendrons dessus) :\npyarrow.Table\nnom : string\nprofession: string\n----\nnom : [[\"Astérix \",\"Obélix \",\"Assurancetourix \"]]\nprofession: [[\"\",\"Tailleur de menhir\",\"Barde\"]]\nPour reprendre l’exemple fil rouge :point_up:, il sera ainsi beaucoup plus\nfacile de récupérer la deuxième ligne de la colonne profession:\non ne considère que le vecteur profession et on récupère la deuxième\nvaleur.\nLe requêtage d’échantillon de données ne nécessite donc pas l’import de\nl’ensemble des données. A cela s’ajoute des fonctionnalités supplémentaires\ndes librairies d’import de données parquet (par exemple pyarrow ou spark)\nqui vont faciliter des recherches complexes basées, par exemple, sur des\nrequêtes de type SQL, ou permettant l’utilisation de données plus volumineuses que la RAM.\nLe format parquet présente d’autres avantages par rapport au\nCSV:\nQu’on lise un ou plusieurs fichiers, on finira avec le schéma suivant:\nCes différents avantages expliquent le succès du format parquet dans le monde du\nbig-data. Le paragraphe suivant, extrait du post d’upsolver déjà cité,\nrésume bien l’intérêt:\nCependant, Parquet ne devrait pas intéresser que les producteurs ou utilisateurs de données big-data.\nC’est l’ensemble\ndes producteurs de données qui bénéficient des fonctionalités\nde Parquet.\nPour en savoir plus sur Arrow,\ndes éléments supplémentaires sur Parquet sont disponibles sur ce très bon\npost de blog d’upsolver\net sur la page officielle du projet Parquet.\nSi les fichiers parquet sont une\nsolution avantageuse pour\nles data-scientists, ils ne résolvent\npas tous les inconvénients de\nl’approche fichier.\nEn particulier, la question de la\nduplication des données pour la mise\nà disposition sécurisée des sources\nn’est pas résolue. Pour que\nl’utilisateur B n’altère pas les\ndonnées de l’utilisateur A, il est nécessaire\nqu’ils travaillent sur deux fichiers\ndifférents, dont l’un peut être une copie\nde l’autre.\nLa mise à disposition de données dans\nles systèmes de stockage cloud est\nune réponse à ce problème.\nLes data lake qui se sont développés dans les\ninstitutions et entreprises utilisatrices de données\nLe principe d’un stockage cloud\nest le même que celui d’une\nDropbox ou d’un Drive mais adapté à\nl’analyse de données. Un utilisateur de données\naccède à un fichier stocké sur un serveur distant\ncomme s’il était dans son file system local2.\nDonc, du point de vue de l’utilisateur Python,\nil n’y a pas de différence fondamentale. Cependant,\nles données ne sont pas hebergées dans un dossier\nlocal (par exemple Mes Documents/monsuperfichier)\nmais sur un serveur distant auquel l’utilisateur\nde Python accède à travers un échange réseau.\nDans l’univers du cloud, la hiérarchisation des données\ndans des dossiers et des fichiers bien rangés\nest d’ailleurs moins\nimportante que dans le monde du file system local.\nLorsque vous essayez de retrouver un fichier dans\nvotre arborescence de fichiers, vous utilisez parfois\nla barre de recherche de votre explorateur de fichiers,\navec des résultats mitigés3. Dans le monde du cloud,\nles fichiers sont parfois accumulés de manière plus\nchaotique car les outils de recherche sont plus\nefficaces4.\nEn ce qui concerne la sécurité des données,\nla gestion des droits de lecture et écriture peut être\nfine: on peut autoriser certains utilisateurs uniquement\nà la lecture, d’autres peuvent avoir les droits\nd’écriture pour modifier les données. Cela permet\nde concilier les avantages des bases de données (la sécurisation\ndes données) avec ceux des fichiers."
  },
  {
    "objectID": "content/course/modern-ds/s3/index.html#principe-du-stockage-de-la-donnée",
    "href": "content/course/modern-ds/s3/index.html#principe-du-stockage-de-la-donnée",
    "title": "34  Les nouveaux modes d’accès aux données: le format parquet et les données sur le cloud",
    "section": "35.1 Principe du stockage de la donnée",
    "text": "35.1 Principe du stockage de la donnée\nPour comprendre les apports du format Parquet, il est nécessaire\nde faire un détour pour comprendre la manière dont une information\nest stockée et accessible à un langage de traitement de la donnée.\nIl existe deux approches dans le monde du stockage de la donnée.\nLa première est celle de la base de données relationnelle. La seconde est le\nprincipe du fichier.\nLa différence entre les deux est dans la manière dont l’accès aux\ndonnées est organisé."
  },
  {
    "objectID": "content/course/modern-ds/s3/index.html#les-fichiers",
    "href": "content/course/modern-ds/s3/index.html#les-fichiers",
    "title": "34  Les nouveaux modes d’accès aux données: le format parquet et les données sur le cloud",
    "section": "35.2 Les fichiers",
    "text": "35.2 Les fichiers\nDans un fichier, les données sont organisées selon un certain format et\nle logiciel de traitement de la donnée va aller chercher et structurer\nl’information en fonction de ce format. Par exemple, dans un fichier\n.csv, les différentes informations seront stockées au même niveau\navec un caractère pour les séparer (la virgule , dans les .csv anglosaxons, le point virgule dans les .csv français, la tabulation dans les .tsv). Le fichier suivant\nnom ; profession \nAstérix ; \nObélix ; Tailleur de menhir ;\nAssurancetourix ; Barde\nsera ainsi organisé naturellement sous forme tabulée par Python\n\n\n\n\n\n\n\n\n\nnom\nprofession\n\n\n\n\n0\nAstérix\n\n\n\n1\nObélix\nTailleur de menhir\n\n\n2\nAssurancetourix\nBarde\n\n\n\n\n\n\n\nA propos des fichiers de ce type, on parle de fichiers plats car\nles enregistrements relatifs à une observation sont stockés ensemble,\nsans hiérarchie.\nCertains formats de données vont permettre d’organiser les informations\nde manière différente. Par exemple, le format JSON va\nhiérarchiser différemment la même information [^1]:\n[\n  {\n    \"nom\": \"Astérix\"\n  },\n  {\n    \"nom\": \"Obélix\",\n    \"profession\": \"Tailleur de menhir\"\n  },\n  {\n    \"nom\": \"Assurancetourix\",\n    \"profession\": \"Barde\"\n  }\n]\n\n\n Hint \nLa différence entre le CSV et le format JSON va au-delà d’un simple “formattage” des données.\nPar sa nature non tabulaire, le format JSON permet des mises à jour beaucoup plus facile de la donnée dans les entrepôts de données.\nPar exemple, un site web qui collecte de nouvelles données n’aura pas à mettre à jour l’ensemble de ses enregistrements antérieurs\npour stocker la nouvelle donnée (par exemple pour indiquer que pour tel ou tel client cette donnée n’a pas été collectée)\nmais pourra la stocker dans\nun nouvel item. Ce sera à l’outil de requête (Python ou un autre outil)\nde créer une relation entre les enregistrements stockés à des endroits\ndifférents.\nCe type d’approche flexible est l’un des fondements de l’approche NoSQL,\nsur laquelle nous allons revenir, qui a permis l’émergence de technologies au coeur de l’écosystème actuel du big-data comme Hadoop ou ElasticSearch.\n\n\nCette fois, quand on n’a pas d’information, on ne se retrouve pas avec nos deux séparateurs accolés (cf. la ligne “Astérix”) mais l’information\nn’est tout simplement pas collectée.\n\n\n Note\nIl se peut très bien que l’information sur une observation soit disséminée\ndans plusieurs fichiers dont les formats diffèrent.\nPar exemple, dans le domaine des données géographiques,\nlorsqu’une donnée est disponible sous format de fichier(s), elle peut l’être de deux manières!\n\nSoit la donnée est stockée dans un seul fichier qui mélange contours géographiques et valeurs attributaires\n(la valeur associée à cette observation géographique, par exemple le taux d’abstention). Ce principe est celui du geojson.\nSoit la donnée est stockée dans plusieurs fichiers qui sont spécialisés: un fichier va stocker les contours géographiques,\nl’autre les données attributaires et d’autres fichiers des informations annexes (comme le système de projection). Ce principe est celui du shapefile.\nC’est alors le logiciel qui requête\nles données (Python par exemple) qui saura où aller chercher l’information\ndans les différents fichiers et associer celle-ci de manière cohérente.\n\n\n\nUn concept supplémentaire dans le monde du fichier est celui du file system. Le file system est\nle système de localisation et de nommage des fichiers.\nPour simplifier, le file system est la manière dont votre ordinateur saura\nretrouver, dans son système de stockage, les bits présents dans tel ou tel fichier\nappartenant à tel ou tel dossier."
  },
  {
    "objectID": "content/course/modern-ds/s3/index.html#les-bases-de-données",
    "href": "content/course/modern-ds/s3/index.html#les-bases-de-données",
    "title": "34  Les nouveaux modes d’accès aux données: le format parquet et les données sur le cloud",
    "section": "35.3 Les bases de données",
    "text": "35.3 Les bases de données\nLa logique des bases de données est différente. Elle est plus systémique.\nUn système de gestion de base de données (Database Management System)\nest un logiciel qui gère à la fois le stockage d’un ensemble de données reliée,\npermet de mettre à jour celle-ci (ajout ou suppression d’informations, modification\ndes caractéristiques d’une table…)\net qui gère également\nles modalités d’accès à la donnée (type de requête, utilisateurs\nayant les droits en lecture ou en écriture…).\nLa relation entre les entités présentes dans une base de données\nprend généralement la forme d’un schéma en étoile. Une base va centraliser\nles informations disponibles qui seront ensuite détaillées dans des tables\ndédiées.\n\nSource: La documentation Databricks sur le schéma en étoile\nLe logiciel associé à la base de données fera ensuite le lien\nentre ces tables à partir de requêtes SQL. L’un des logiciels les plus efficaces dans ce domaine\nest PostgreSQL. Python est tout à fait\nutilisable pour passer une requête SQL à un gestionnaire de base de données.\nLes packages sqlalchemy et psycopg2\npeuvent servir à utiliser PostgreSQL pour requêter une\nbase de donnée ou la mettre à jour.\nLa logique de la base de données est donc très différente de celle du fichier.\nCes derniers sont beaucoup plus légers pour plusieurs raisons.\nD’abord, parce qu’ils sont moins adhérents à\nun logiciel gestionnaire. Là où le fichier ne nécessite, pour la gestion,\nqu’un file system, installé par défaut sur\ntout système d’exploitation, une base de données va nécessiter un\nlogiciel spécialisé. L’inconvénient de l’approche fichier, sous sa forme\nstandard, est qu’elle\nne permet pas une gestion fine des droits d’accès et amène généralement à une\nduplication de la donnée pour éviter que la source initiale soit\nré-écrite (involontairement ou de manière intentionnelle par un utilisateur malveillant).\nRésoudre ce problème est l’une des\ninnovations des systèmes cloud, sur lesquelles nous reviendrons en évoquant le\nsystème S3.\nUn deuxième inconvénient de l’approche base de données par\nrapport à l’approche fichier, pour un utilisateur de Python,\nest que les premiers nécessitent l’intermédiation du logiciel de gestion\nde base de données là où, dans le second cas, on va se contenter d’une\nlibrairie, donc un système beaucoup plus léger,\nqui sait comment transformer la donnée brute en DataFrame.\nPour ces raisons, entre autres, les bases de données sont donc moins à la\nmode dans l’écosystème récent de la data-science que les fichiers."
  },
  {
    "objectID": "content/course/modern-ds/s3/index.html#lire-un-parquet-en-python-la-librairie-pyarrow",
    "href": "content/course/modern-ds/s3/index.html#lire-un-parquet-en-python-la-librairie-pyarrow",
    "title": "34  Les nouveaux modes d’accès aux données: le format parquet et les données sur le cloud",
    "section": "36.1 Lire un parquet en Python: la librairie pyarrow",
    "text": "36.1 Lire un parquet en Python: la librairie pyarrow\nLa librairie pyarrow permet la lecture et l’écriture\nde fichiers parquet avec Python1. Elle repose\nsur un type particulier de dataframe, le pyarrow.Table\nqui peut être utilisé en substitut ou en complément\ndu DataFrame\nde pandas. Il est recommandé de régulièrement\nconsulter la documentation officielle de pyarrow\nconcernant la lecture et écriture de fichiers et celle relative\naux manipulations de données.\nPour illustrer les fonctionalités de pyarrow,\nrepartons de notre CSV initial que nous allons\nenrichir d’une nouvelle variable numérique\net que nous\nallons\nconvertir en objet pyarrow avant de l’écrire au format parquet:\n\nimport pandas as pd\nfrom io import StringIO \nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\ns = \"\"\"\nnom;cheveux;profession\nAstérix;blond;\nObélix;roux;Tailleur de menhir\nAssurancetourix;blond;Barde\n\"\"\"\n\nsource = StringIO(s)\n\ndf = pd.read_csv(source, sep = \";\", index_col=False)\ndf[\"taille\"] = [155, 190, 175]\ntable = pa.Table.from_pandas(df)\n\ntable\n\npq.write_table(table, 'example.parquet')\n\n\n\n Hint \nL’utilisation des noms pa pour pyarrow et pq pour\npyarrow.parquet est une convention communautaire\nqu’il est recommandé de suivre.\n\n\nPour importer et traiter ces données, on peut conserver\nles données sous le format pyarrow.Table\nou transformer en pandas.DataFrame. La deuxième\noption est plus lente mais présente l’avantage\nde permettre ensuite d’appliquer toutes les\nmanipulations offertes par l’écosystème\npandas qui est généralement mieux connu que\ncelui d’Arrow.\nSupposons qu’on ne s’intéresse qu’à la taille et à la couleur\nde cheveux de nos gaulois.\nIl n’est pas nécessaire d’importer l’ensemble de la base, cela\nferait perdre du temps pour rien. On appelle\ncette approche le column pruning qui consiste à\nne parcourir, dans le fichier, que les colonnes qui nous\nintéressent. Du fait du stockage orienté colonne du parquet,\nil suffit de ne considérer que les blocs qui nous\nintéressent (alors qu’avec un CSV il faudrait scanner tout\nle fichier avant de pouvoir éliminer certaines colonnes).\nCe principe du column pruning se matérialise avec\nl’argument columns dans parquet.\nEnsuite, avec pyarrow, on pourra utiliser pyarrow.compute pour\neffectuer des opérations directement sur une table\nArrow :\n\nimport pyarrow.compute as pc\n\ntable = pq.read_table('example.parquet', columns=['taille', 'cheveux'])\n\ntable.group_by(\"cheveux\").aggregate([(\"taille\", \"mean\")])\n\nLa manière équivalente de procéder en passant\npar l’intermédiaire de pandas est\n\ntable = pq.read_table('example.parquet', columns=['taille', 'cheveux'])\n\ntable.to_pandas().groupby(\"cheveux\")[\"taille\"].mean()\n\ncheveux\nblond    165.0\nroux     190.0\nName: taille, dtype: float64\n\n\nIci, comme les données sont peu volumineuses, deux des\navantages du parquet par rapport\nau CSV (données moins\nvolumineuses et vitesse de l’import)\nne s’appliquent pas vraiment.\n\n\n Note\nUn autre principe d’optimisation de la performance qui est\nau coeur de la librairie Arrow est le filter pushdown\n(ou predicate pushdown).\nQuand on exécute un filtre de sélection de ligne\njuste après avoir chargé un jeu de données,\nArrow va essayer de le mettre en oeuvre lors de l’étape de lecture\net non après. Autrement dit, Arrow va modifier le plan\nd’exécution pour pousser le filtre en amont de la séquence d’exécution\nafin de ne pas essayer de lire les lignes inutiles."
  },
  {
    "objectID": "content/course/modern-ds/s3/index.html#quest-ce-que-le-système-de-stockage-s3",
    "href": "content/course/modern-ds/s3/index.html#quest-ce-que-le-système-de-stockage-s3",
    "title": "34  Les nouveaux modes d’accès aux données: le format parquet et les données sur le cloud",
    "section": "38.1 Qu’est-ce que le système de stockage S3 ?",
    "text": "38.1 Qu’est-ce que le système de stockage S3 ?\nDans les entreprises et administrations,\nun nombre croissant de données sont\ndisponibles depuis un système de stockage\nnommé S3.\nLe système S3 (Simple Storage System) est un système de stockage développé\npar Amazon et qui est maintenant devenu une référence pour le stockage en ligne.\nIl s’agit d’une architecture à la fois\nsécurisée (données cryptées, accès restreints) et performante.\nLe concept central du système S3 est le bucket.\nUn bucket est un espace (privé ou partagé) où on peut stocker une\narborescence de fichiers. Pour accéder aux fichiers figurant\ndans un bucket privé, il faut des jetons d’accès (l’équivalent d’un mot de passe)\nreconnus par le serveur de stockage. On peut alors lire et écrire dans le bucket.\n\n\n Note\nLes exemples suivants seront réplicables pour les utilisateurs de la plateforme\nSSP-cloud\n\nIls peuvent également l’être pour des utilisateurs ayant un\naccès à AWS, il suffit de changer l’URL du endpoint\nprésenté ci-dessous."
  },
  {
    "objectID": "content/course/modern-ds/s3/index.html#comment-faire-avec-python",
    "href": "content/course/modern-ds/s3/index.html#comment-faire-avec-python",
    "title": "34  Les nouveaux modes d’accès aux données: le format parquet et les données sur le cloud",
    "section": "38.2 Comment faire avec Python ?",
    "text": "38.2 Comment faire avec Python ?\n\n38.2.1 Les librairies principales\nL’interaction entre ce système distant de fichiers et une session locale de Python\nest possible grâce à des API. Les deux principales librairies sont les suivantes:\n\nboto3, une librairie créée et maintenue par Amazon ;\ns3fs, une librairie qui permet d’interagir avec les fichiers stockés à l’instar d’un filesystem classique.\n\nLa librairie pyarrow que nous avons déjà présenté permet également\nde traiter des données stockées sur le cloud comme si elles\nétaient sur le serveur local. C’est extrêmement pratique\net permet de fiabiliser la lecture ou l’écriture de fichiers\ndans une architecture cloud.\nUn exemple, assez court, est disponible\ndans la documentation officielle\nIl existe également d’autres librairies permettant de gérer\ndes pipelines de données (chapitre à venir) de manière\nquasi indifférente entre une architecture locale et une architecture\ncloud. Parmi celles-ci, nous présenterons quelques exemples\navec snakemake.\nEn arrière-plan, snakemake\nva utiliser boto3 pour communiquer avec le système\nde stockage.\nEnfin, selon le même principe du comme si les données\nétaient en local, il existe l’outil en ligne de commande\nmc (Minio Client) qui permet de gérer par des lignes\nde commande Linux les dépôts distants comme s’ils étaient\nlocaux.\nToutes ces librairies offrent la possibilité de se connecter depuis Python,\nà un dépôt de fichiers distant, de lister les fichiers disponibles dans un\nbucket, d’en télécharger un ou plusieurs ou de faire de l’upload\nNous allons présenter quelques unes des opérations les plus fréquentes,\nen mode cheatsheet."
  },
  {
    "objectID": "content/course/modern-ds/s3/index.html#connexion-à-un-bucket",
    "href": "content/course/modern-ds/s3/index.html#connexion-à-un-bucket",
    "title": "34  Les nouveaux modes d’accès aux données: le format parquet et les données sur le cloud",
    "section": "38.3 Connexion à un bucket",
    "text": "38.3 Connexion à un bucket\nPar la suite, on va utiliser des alias pour les trois valeurs suivantes, qui servent\nà s’authentifier.\nkey_id = 'MY_KEY_ID'\naccess_key = 'MY_ACCESS_KEY'\ntoken = \"MY_TOKEN\"\nCes valeurs peuvent être également disponibles dans\nles variables d’environnement de Python. Comme il s’agit d’une information\nd’authentification personnelle, il ne faut pas stocker les vraies valeurs de ces\nvariables dans un projet, sous peine de partager des traits d’identité sans le\nvouloir lors d’un partage de code.\n\nboto3 👇\nAvec boto3, on créé d’abord un client puis on exécute des requêtes dessus.\nPour initialiser un client, il suffit, en supposant que l’url du dépôt S3 est\n\"https://minio.lab.sspcloud.fr\", de faire:\nimport boto3\ns3 = boto3.client(\"s3\",endpoint_url = \"https://minio.lab.sspcloud.fr\")\n\n\n\nS3FS 👇\nLa logique est identique avec s3fs.\nSi on a des jetons d’accès à jour et dans les variables d’environnement\nadéquates:\nimport s3fs\nfs = s3fs.S3FileSystem(\n  client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})\n\n\n\nArrow 👇\nLa logique d’Arrow est proche de celle de s3fs. Seuls les noms\nd’arguments changent\nSi on a des jetons d’accès à jour et dans les variables d’environnement\nadéquates:\nfrom pyarrow import fs\ns3 = fs.S3FileSystem(endpoint_override=\"http://\"+\"minio.lab.sspcloud.fr\")\n\n\n\nSnakemake 👇\nLa logique de Snakemake est, quant à elle,\nplus proche de celle de boto3. Seuls les noms\nd’arguments changent\nSi on a des jetons d’accès à jour et dans les variables d’environnement\nadéquates:\nfrom snakemake.remote.S3 import RemoteProvider as S3RemoteProvider\nS3 = S3RemoteProvider(host = \"https://\" + os.getenv('AWS_S3_ENDPOINT'))\n\n\nIl se peut que la connexion à ce stade soit refusée (HTTP error 403).\nCela peut provenir\nd’une erreur dans l’URL utilisé. Cependant, cela reflète plus généralement\ndes paramètres d’authentification erronés.\n\nboto3 👇\nLes paramètres d’authentification sont des arguments supplémentaires:\nimport boto3\ns3 = boto3.client(\"s3\",endpoint_url = \"https://minio.lab.sspcloud.fr\",\n                  aws_access_key_id=key_id, \n                  aws_secret_access_key=access_key, \n                  aws_session_token = token)\n\n\n\nS3FS 👇\nLa logique est la même, seuls les noms d’arguments diffèrent\nimport s3fs\nfs = s3fs.S3FileSystem(\n  client_kwargs={'endpoint_url': 'https://'+'minio.lab.sspcloud.fr'},\n  key = key_id, secret = access_key,\n  token = token)\n\n\n\nArrow 👇\nTout est en argument cette fois:\nfrom pyarrow import fs\n\ns3 = fs.S3FileSystem(\n    access_key = key_id,\n    secret_key = access_key,\n    session_token = token,\n    endpoint_override = 'https://'+'minio.lab.sspcloud.fr',\n    scheme = \"https\"\n    )\n\n\n\nSnakemake 👇\nLa logique est la même, seuls les noms d’arguments diffèrent\nfrom snakemake.remote.S3 import RemoteProvider as S3RemoteProvider\nS3 = S3RemoteProvider(host = \"https://\" + os.getenv('AWS_S3_ENDPOINT'), access_key_id=key_id, secret_access_key=access_key)\n\n\n\n\n Note\nDans le SSP-cloud,\nlorsque l’initialisation du service Jupyter du SSP-cloud est récente\n(moins de 12 heures), il est possible d’utiliser\nautomatiquement les jetons stockés automatiquement à la création du dépôt.\nSi on désire accéder aux données du SSP-cloud depuis une session python du\ndatalab (service VSCode, Jupyter…),\nil faut remplacer l’url par http://minio.lab.sspcloud.fr"
  },
  {
    "objectID": "content/course/modern-ds/s3/index.html#lister-les-fichiers",
    "href": "content/course/modern-ds/s3/index.html#lister-les-fichiers",
    "title": "34  Les nouveaux modes d’accès aux données: le format parquet et les données sur le cloud",
    "section": "38.4 Lister les fichiers",
    "text": "38.4 Lister les fichiers\nS’il n’y a pas d’erreur à ce stade, c’est que la connexion est bien effective.\nPour le vérifier, on peut essayer de faire la liste des fichiers disponibles\ndans un bucket auquel on désire accéder.\nPar exemple, on peut vouloir\ntester l’accès aux bases FILOSOFI (données de revenu localisées disponibles\nsur https://www.insee.fr) au sein du bucket donnees-insee.\n\nboto3 👇\nPour cela,\nla méthode list_objects offre toutes les options nécessaires:\nimport boto3\ns3 = boto3.client(\"s3\",endpoint_url = \"https://minio.lab.sspcloud.fr\")\nfor key in s3.list_objects(Bucket='donnees-insee', Prefix='diffusion/FILOSOFI')['Contents']:\n    print(key['Key'])\n\n\n\nS3FS 👇\nPour lister les fichiers, c’est la méthode ls (celle-ci ne liste pas par\ndéfaut les fichiers de manière récursive comme boto3):\nimport s3fs\nfs = s3fs.S3FileSystem(\n  client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})\nfs.ls(\"donnees-insee/diffusion\")\n\n\n\nArrow 👇\nfrom pyarrow import fs\ns3 = fs.S3FileSystem(endpoint_override='https://'+'minio.lab.sspcloud.fr')\ns3.get_file_info(fs.FileSelector('donnees-insee/diffusion', recursive=True))\n\n\n\nmc 👇\nmc ls -r"
  },
  {
    "objectID": "content/course/modern-ds/s3/index.html#télécharger-un-fichier-depuis-s3-pour-lenregistrer-en-local",
    "href": "content/course/modern-ds/s3/index.html#télécharger-un-fichier-depuis-s3-pour-lenregistrer-en-local",
    "title": "34  Les nouveaux modes d’accès aux données: le format parquet et les données sur le cloud",
    "section": "38.5 Télécharger un fichier depuis S3 pour l’enregistrer en local",
    "text": "38.5 Télécharger un fichier depuis S3 pour l’enregistrer en local\nCette méthode n’est en général pas recommandée car, comme on va le voir\npar la suite, il est possible de lire à la volée des fichiers. Cependant,\ntélécharger un fichier depuis le cloud pour l’écrire sur le disque\nlocal peut parfois être utile (par exemple, lorsqu’il est nécessaire\nde dézipper un fichier).\n\nboto3 👇\nOn utilise cette fois la méthode download_file\nimport boto3\ns3 = boto3.client(\"s3\",endpoint_url = \"https://minio.lab.sspcloud.fr\")\ns3.download_file('donnees-insee', \"diffusion/FILOSOFI/2014/FILOSOFI_COM.csv\", 'data.csv')\n\n\n\nS3FS 👇\nimport s3fs\nfs = s3fs.S3FileSystem(\n  client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})\nfs.download('donnees-insee/diffusion/FILOSOFI/2014/FILOSOFI_COM.csv','test.csv')\n\n\n\nSnakemake 👇\nfrom snakemake.remote.S3 import RemoteProvider as S3RemoteProvider\nS3 = S3RemoteProvider(host = \"https://\" + os.getenv('AWS_S3_ENDPOINT'))\nbucket = \"mon-bucket\"\n\nrule ma_super_regle_s3:\n    input:\n        fichier = S3.remote(f'{bucket}/moninput.csv')\n    output:\n        fichier='mon_dossier_local/monoutput.csv'\n    run:\n        shell(\"cp {input[0]} {output[0]}\")\n\n\n\nmc 👇\nmc cp \"donnees-insee/FILOSOFI/2014/FILOSOFI_COM.csv\" 'data.csv'"
  },
  {
    "objectID": "content/course/modern-ds/s3/index.html#lire-un-fichier-directement",
    "href": "content/course/modern-ds/s3/index.html#lire-un-fichier-directement",
    "title": "34  Les nouveaux modes d’accès aux données: le format parquet et les données sur le cloud",
    "section": "38.6 Lire un fichier directement",
    "text": "38.6 Lire un fichier directement\nLa méthode précédente n’est pas optimale. En effet, l’un des intérêts des API\nest qu’on peut traiter un fichier sur S3 comme s’il s’agissait d’un fichier\nsur son PC. Cela est d’ailleurs une manière plus sécurisée de procéder puisqu’on\nlit les données à la volée, sans les écrire dans un filesystem local.\n\nboto3 👇\nimport boto3\ns3 = boto3.client(\"s3\",endpoint_url = \"https://minio.lab.sspcloud.fr\")\nobj = s3.get_object(Bucket='donnees-insee', Key=\"diffusion/FILOSOFI/2014/FILOSOFI_COM.csv\")\ndf = pd.read_csv(obj['Body'], sep = \";\")\ndf.head(2)\n\n\n\nS3FS 👇\nLe code suivant devrait permettre d’effectuer la même opération avec s3fs\nimport pandas as pd\nimport s3fs\nfs = s3fs.S3FileSystem(\n  client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})\ndf = pd.read_csv(fs.open('{}/{}'.format('donnees-insee', \"diffusion/FILOSOFI/2014/FILOSOFI_COM.csv\"),\n                         mode='rb'), sep = \";\"\n                 )\n\ndf.head(2)\n\n\n\nSnakemake 👇\nfrom snakemake.remote.S3 import RemoteProvider as S3RemoteProvider\nS3 = S3RemoteProvider(host = \"https://\" + os.getenv('AWS_S3_ENDPOINT'))\nbucket = \"mon-bucket\"\n\nrule ma_super_regle_s3:\n    input:\n        fichier = S3.remote(f'{bucket}/moninput.csv')\n    run:\n        import pandas as pd\n        df = pd.read_csv(input.fichier)\n        # PLUS D'OPERATIONS\n\n\n\nArrow 👇\nArrow est une librairie qui permet de lire des CSV.\nIl est néanmoins\nbeaucoup plus pratique d’utiliser le format parquet avec arrow.\nDans un premier temps, on configure le filesystem avec les\nfonctionalités d’Arrow (cf. précédemment).\n\nfrom pyarrow import fs\n\ns3 = fs.S3FileSystem(endpoint_override='http://'+'minio.lab.sspcloud.fr')\n\nPour lire un csv, on fera:\nfrom pyarrow import fs\nfrom pyarrow import csv\n\ns3 = fs.S3FileSystem(endpoint_override='https://'+'minio.lab.sspcloud.fr')\n\nwith s3.open_input_file(\"donnees-insee/diffusion/FILOSOFI/2014/FILOSOFI_COM.csv\") as file:\n    df = csv.read_csv(file, parse_options=csv.ParseOptions(delimiter=\";\")).to_pandas()\nPour un fichier au format parquet, la démarche est plus simple grâce à l’argument\nfilesystem dans pyarrow.parquet.ParquetDataset :\nimport pyarrow.parquet as pq\n\n#bucket = \"\"\n#parquet_file=\"\"\ndf = pq.ParquetDataset(f'{bucket}/{parquet_file}', filesystem=s3).read_pandas().to_pandas()"
  },
  {
    "objectID": "content/course/modern-ds/s3/index.html#uploader-un-fichier",
    "href": "content/course/modern-ds/s3/index.html#uploader-un-fichier",
    "title": "34  Les nouveaux modes d’accès aux données: le format parquet et les données sur le cloud",
    "section": "38.7 Uploader un fichier",
    "text": "38.7 Uploader un fichier\n\nboto3 👇\ns3.upload_file(file_name, bucket, object_name)\n\n\n\nS3FS 👇\nfs.put(filepath, f\"{bucket}/{object_name}\", recursive=True)\n\n\n\nArrow 👇\nSupposons que df soit un pd.DataFrame\nDans un système local, on convertirait\nen table Arrow puis on écrirait en parquet\n(voir la documentation officielle).\nQuand on est sur un système S3, il s’agit seulement d’ajouter\nnotre connexion à S3 dans l’argument filesystem\n(voir la page sur ce sujet dans la documentation Arrow)\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\ntable = pa.Table.from_pandas(df)\npq.write_table(table, f\"{bucket}/{path}\", filesystem=s3)\n\n\n\nSnakemake 👇\nfrom snakemake.remote.S3 import RemoteProvider as S3RemoteProvider\nS3 = S3RemoteProvider(host = \"https://\" + os.getenv('AWS_S3_ENDPOINT'))\nbucket = \"mon-bucket\"\n\nrule ma_super_regle_s3:\n    input:\n        fichier='mon_dossier_local/moninput.csv'\n    output:\n        fichier=S3.remote(f'{bucket}/monoutput.csv')\n    run:\n        shell(\"cp output.fichier input.fichier\")\n\n\n\nmc 👇\nmc cp 'data.csv' \"MONBUCKET/monoutput.csv\""
  },
  {
    "objectID": "content/course/modern-ds/s3/index.html#pour-aller-plus-loin",
    "href": "content/course/modern-ds/s3/index.html#pour-aller-plus-loin",
    "title": "34  Les nouveaux modes d’accès aux données: le format parquet et les données sur le cloud",
    "section": "38.8 Pour aller plus loin",
    "text": "38.8 Pour aller plus loin\n\nLa documentation sur MinIO du SSPCloud"
  },
  {
    "objectID": "content/course/modern-ds/elastic_approfondissement/index.html",
    "href": "content/course/modern-ds/elastic_approfondissement/index.html",
    "title": "35  Approfondissement ElasticSearch pour des recherches de proximité géographique",
    "section": "",
    "text": "36 Introduction\nDans le cadre particulier de l’identification des entreprises, Elasticsearch fait partie de la solution retenue par\nl’API “Sirene données ouvertes” (DINUM) (cf https://annuaire-entreprises.data.gouv.fr/) l’API de recherche d’entreprises Française de la Fabrique numérique des Ministères Sociaux (cf https://api.recherche-entreprises.fabrique.social.gouv.fr/)le projet de l’Insee “Amélioration de l’identification de l’employeur dans le recensement”, pour faire une première sélection des établissements pertinents pour un bulletin de recensement donné.\nDans le cadre de l’identification des individus, Elasticsearch fait partie de la solution envisagée pour l’identification des individus au RNIPP (Répertoire national des personnes physiques) pour le projet CSNS (Code statistique non signifiant), et est la solution technique sous-jacente au projet matchID du ministère de l’intérieur.\nAu delà du secteur public, on peut citer qu’Amazon AWS fait partie des utilisateurs historiques d’Elasticsearch.\nLe jeu de données présente déjà\nl’identifiant\nde l’établissement, dit numéro siret.\nNous allons faire comme si nous étions\nen amont de cet appariement et que nous\ndésirons trouver ce numéro. La présence\ndans la base de ce numéro nous permettra d’évaluer la qualité\nde notre méthode de recherche avec\nElastic.\nimport requests\nimport zipfile\nimport pandas as pd\n\nurl = \"https://www.data.gouv.fr/fr/datasets/r/9af639b9-e2b6-4d7d-8c5f-0c4953c48663\"\nreq = requests.get(url)\n\nwith open(\"irep.zip\",'wb') as f:\n  f.write(req.content)\n\nwith zipfile.ZipFile(\"irep.zip\", 'r') as zip_ref:\n  zip_ref.extractall(\"irep\")\n\netablissements = pd.read_csv(\"irep/2019/etablissements.csv\", sep = \";\")\nDans un premier temps, on va combiner ensemble les différentes sources\nopen-data pour créer un référentiel fiable d’entreprises\ngéolocalisées.\nOn va supposer que l’utilisateur dispose déjà d’un serveur Elastic\nfonctionnel mais désire créer un nouvel index. Si vous utilisez\nle SSPCloud, la démarche de création d’un service\nElastic est disponible dans le chapitre précédent.\nfrom elasticsearch import Elasticsearch\nHOST = 'elasticsearch-master'\n\ndef elastic():\n    \"\"\"Connection avec Elastic sur le data lab\"\"\"\n    es = Elasticsearch([{'host': HOST, 'port': 9200, 'scheme': 'http'}], http_compress=True, request_timeout=200)\n    return es\n\nes = elastic()\nes\nPour se faciliter la création de cartes\nréactives, nous allons régulièrement\nutiliser la fonction suivante qui s’appuie\nsur un code déjà présenté dans un autre\nchapitre.\ndef plot_folium_sirene(df, yvar, xvar):\n\n    center = df[[yvar, xvar]].mean().values.tolist()\n    sw = df[[yvar, xvar]].min().values.tolist()\n    ne = df[[yvar, xvar]].max().values.tolist()\n\n    m = folium.Map(location = center, tiles='Stamen Toner')\n\n    # I can add marker one by one on the map\n    for i in range(0,len(df)):\n        folium.Marker(\n            [df.iloc[i][yvar], df.iloc[i][xvar]],\n            popup = df.iloc[i][\"_source.denom\"] + f'&lt;br&gt;(Score: {df.iloc[i][\"_score\"]:.2f})',\n            icon=folium.Icon(icon=\"home\")\n        ).add_to(m)\n\n    m.fit_bounds([sw, ne])\n\n    return m"
  },
  {
    "objectID": "content/course/modern-ds/elastic_approfondissement/index.html#objectif",
    "href": "content/course/modern-ds/elastic_approfondissement/index.html#objectif",
    "title": "35  Approfondissement ElasticSearch pour des recherches de proximité géographique",
    "section": "36.1 Objectif",
    "text": "36.1 Objectif\nCe chapitre vise à approfondir les éléments présentés sur Elastic précédemment. L’idée\nest de se placer dans un contexte opérationnel où on reçoit des informations\nsur des entreprises telles que l’adresse et la localisation et qu’on\ndésire associer à des données administratives considérées plus fliables."
  },
  {
    "objectID": "content/course/modern-ds/elastic_approfondissement/index.html#réplication-de-ce-chapitre",
    "href": "content/course/modern-ds/elastic_approfondissement/index.html#réplication-de-ce-chapitre",
    "title": "35  Approfondissement ElasticSearch pour des recherches de proximité géographique",
    "section": "36.2 Réplication de ce chapitre",
    "text": "36.2 Réplication de ce chapitre\nComme le précédent, ce chapitre est plus exigeant en termes d’infrastructures que les précédents.\nIl nécessite un serveur Elastic. Les utilisateurs du\nSSP Cloud pourront répliquer les exemples de ce cours\ncar cette technologie est disponible (que ce soit pour indexer une base ou\npour requêter une base existante).\nLa première partie de ce tutoriel, qui consiste à créer une base Sirene géolocalisée\nà partir des données open-data ne nécessite pas d’architecture particulière et\npeut ainsi être exécutée en utilisant les packages suivants:\n\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "content/course/modern-ds/elastic_approfondissement/index.html#sources",
    "href": "content/course/modern-ds/elastic_approfondissement/index.html#sources",
    "title": "35  Approfondissement ElasticSearch pour des recherches de proximité géographique",
    "section": "36.3 Sources",
    "text": "36.3 Sources\nCe chapitre va utiliser plusieurs sources de diffusion de\nl’Insee:\n\nLe stock des établissements présents dans les données de diffusion Sirene ;\nLes données Sirene géolocalisées\n\nLes données à siretiser sont celles du registre Français des émissions polluantes\nétabli par le Ministère de la Transition Energétique. Le jeu de données\nest disponible sur data.gouv"
  },
  {
    "objectID": "content/course/modern-ds/elastic_approfondissement/index.html#importer-la-base-déjà-créée",
    "href": "content/course/modern-ds/elastic_approfondissement/index.html#importer-la-base-déjà-créée",
    "title": "35  Approfondissement ElasticSearch pour des recherches de proximité géographique",
    "section": "38.1 Importer la base déjà créée",
    "text": "38.1 Importer la base déjà créée\nLes données à utiliser pour constuire une base Sirene géolocalisée\nsont trop volumineuses pour les serveurs mis à disposition\ngratuitement par Github pour la compilation de ce site web.\nNous proposons ainsi une version déjà construite, stockée\ndans l’espace de mise à disposition du SSP Cloud. Ce fichier est\nau format parquet et est ouvert à\ntous, même pour les personnes ne disposant pas d’un compte.\nLe code ayant construit cette base est présenté ci-dessous.\nPour importer cette base, on utilise les fonctionalités\nde pyarrow qui permettent d’importer un fichier sur\nun système de stockage cloud comme s’il était\nprésent sur le disque :\n\nfrom pyarrow import fs\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\nbucket = \"lgaliana\"\npath = \"diffusion/sirene_geolocalized.parquet\"\n\ns3 = fs.S3FileSystem(endpoint_override=\"http://\"+\"minio.lab.sspcloud.fr\")\n\ndf_geolocalized = pq.ParquetDataset(f'{bucket}/{path}', filesystem=s3).read_pandas().to_pandas()\ndf_geolocalized.head(3)"
  },
  {
    "objectID": "content/course/modern-ds/elastic_approfondissement/index.html#reproduire-la-construction-de-la-base",
    "href": "content/course/modern-ds/elastic_approfondissement/index.html#reproduire-la-construction-de-la-base",
    "title": "35  Approfondissement ElasticSearch pour des recherches de proximité géographique",
    "section": "38.2 Reproduire la construction de la base",
    "text": "38.2 Reproduire la construction de la base\nLa première base d’entrée à utiliser est disponible sur\ndata.gouv\n\nimport requests\nimport zipfile\n\nurl_download = \"https://www.data.gouv.fr/fr/datasets/r/0651fb76-bcf3-4f6a-a38d-bc04fa708576\"\nreq = requests.get(url_download)\n\nwith open(\"sirene.zip\",'wb') as f:\n  f.write(req.content)\n\nwith zipfile.ZipFile(\"sirene.zip\", 'r') as zip_ref:\n  zip_ref.extractall(\"sirene\")\n\nOn va importer seulement les colonnes utiles et simplifier la structure\npour être en mesure de ne garder que les informations qui nous\nintéressent (nom de l’entreprise, adresse, commune, code postal…)\n\nimport pandas as pd\nimport numpy as np\n\nlist_cols = [\n  'siren', 'siret',\n  'activitePrincipaleRegistreMetiersEtablissement',\n  'complementAdresseEtablissement',\n  'numeroVoieEtablissement',\n  'typeVoieEtablissement',\n  'libelleVoieEtablissement',\n  'codePostalEtablissement',\n  'libelleCommuneEtablissement',\n  'codeCommuneEtablissement',\n  'etatAdministratifEtablissement',\n  'denominationUsuelleEtablissement',\n  'activitePrincipaleEtablissement'\n]\n\ndf = pd.read_csv(\n  \"sirene/StockEtablissement_utf8.csv\",\n  usecols = list_cols)\n\ndf['numero'] = df['numeroVoieEtablissement']\\\n  .replace('-', np.NaN).str.split().str[0]\\\n  .str.extract('(\\d+)', expand=False)\\\n  .fillna(\"0\").astype(int)\n\ndf['numero'] = df['numero'].astype(str).replace(\"0\",\"\")\n\ndf['adresse'] = df['numero'] + \" \" + \\\n  df['typeVoieEtablissement'] + \" \" + \\\n  df['libelleVoieEtablissement']\n\ndf['adresse'] = df['adresse'].replace(np.nan, \"\")\n\ndf = df.loc[df['etatAdministratifEtablissement'] == \"A\"]\n\ndf.rename(\n  {\"denominationUsuelleEtablissement\": \"denom\",\n  \"libelleCommuneEtablissement\": \"commune\",\n  \"codeCommuneEtablissement\" : \"code_commune\",\n  \"codePostalEtablissement\": \"code_postal\"},\n  axis = \"columns\", inplace = True)\n\ndf['ape'] = df['activitePrincipaleEtablissement'].str.replace(\"\\.\", \"\", regex = True)\ndf['denom'] = df[\"denom\"].replace(np.nan, \"\")\n\ndf_siret = df.loc[:, ['siren', 'siret','adresse', 'ape', 'denom', 'commune', 'code_commune','code_postal']]\ndf_siret['code_postal'] = df_siret['code_postal'].replace(np.nan, \"0\").astype(int).astype(str).replace(\"0\",\"\")\n\nOn importe ensuite les données géolocalisées\n\nimport zipfile\nimport shutil\nimport os\n\n#os.remove(\"sirene.zip\")\n#shutil.rmtree('sirene/')\n\nurl_geoloc = \"https://files.data.gouv.fr/insee-sirene-geo/GeolocalisationEtablissement_Sirene_pour_etudes_statistiques_utf8.zip\"\nr = requests.get(url_geoloc)  \n\nwith open('geoloc.zip', 'wb') as f:\n    f.write(r.content)\n\nwith zipfile.ZipFile(\"geoloc.zip\", 'r') as zip_ref:\n  zip_ref.extractall(\"geoloc\")\n\ndf_geoloc = pd.read_csv(\n  \"geoloc/GeolocalisationEtablissement_Sirene_pour_etudes_statistiques_utf8.csv\",\n  usecols = [\"siret\", \"epsg\", \"x_longitude\", \"y_latitude\"] , sep = \";\")\n\nIl ne reste plus qu’à associer les deux jeux de données\n\ndf_geolocalized = df_siret.merge(df_geoloc, on = \"siret\") \ndf_geolocalized['code_commune'] = df_geolocalized['code_commune'].astype(str) \n\nSi vous avez accès à un espace de stockage cloud de type\nS3, il est possible d’utiliser pyarrow pour enregister\ncette base. Afin de l’enregistrer dans un espace de stockage\npublic, nous allons l’enregistrer dans un dossier diffusion1\n\nfrom pyarrow import fs\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\nbucket = \"lgaliana\"\npath = \"diffusion/sirene_geolocalized.parquet\"\n\ns3 = fs.S3FileSystem(endpoint_override=\"http://\"+\"minio.lab.sspcloud.fr\")\n\ntable = pa.Table.from_pandas(df_geolocalized)"
  },
  {
    "objectID": "content/course/modern-ds/elastic_approfondissement/index.html#définition-du-mapping",
    "href": "content/course/modern-ds/elastic_approfondissement/index.html#définition-du-mapping",
    "title": "35  Approfondissement ElasticSearch pour des recherches de proximité géographique",
    "section": "40.1 Définition du mapping",
    "text": "40.1 Définition du mapping\nOn va procéder par étape en essayant d’utiliser la structure la plus simple\npossible.\n:one: On s’occupe d’abord de définir le mapping\npour les variables textuelles.\n\nstring_var = [\"adresse\", \"denom\", \"ape\", \"commune\"]\nmap_string = {\"type\": \"text\", \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}}}\nmapping_string = {l: map_string for l in string_var}\n\n:two: Les variables catégorielles sont utilisées\npar le biais du type keyword:\n\n# keywords\nkeyword_var = [\"siren\",\"siret\",\"code_commune\",\"code_postal\"]\nmap_keywords = {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}\nmapping_keywords = {l: map_keywords for l in keyword_var}\n\n:three: La nouveauté par rapport à la partie\nprécédente est l’utilisation de la\ndimension géographique. Elastic propose\nle type geo_point pour cela.\n\n# geoloc\nmapping_geoloc = {\n  \"location\": {\n    \"type\": \"geo_point\"\n    }\n}    \n\nOn collecte tout cela ensemble dans un\ndictionnaire:\n\n# mapping\nmapping_elastic = {\"mappings\":\n  {\"properties\":\n    {**mapping_string, **mapping_geoloc, **mapping_keywords}\n  }\n}\n\nIl est tout à fait possible de définir un mapping\nplus raffiné. Ici, on va privilégier\nl’utilisation d’un mapping simple pour\nillustrer la recherche par distance\ngéographique en priorité."
  },
  {
    "objectID": "content/course/modern-ds/elastic_approfondissement/index.html#création-de-lindex",
    "href": "content/course/modern-ds/elastic_approfondissement/index.html#création-de-lindex",
    "title": "35  Approfondissement ElasticSearch pour des recherches de proximité géographique",
    "section": "40.2 Création de l’index",
    "text": "40.2 Création de l’index\nPour créer le nouvel index, on s’assure d’abord de ne pas\ndéjà l’avoir créé et on passe le mapping défini\nprécédemment.\n\nif es.indices.exists('sirene'):\n    es.indices.delete('sirene')\n\nes.indices.create(index = \"sirene\", body = mapping_elastic)"
  },
  {
    "objectID": "content/course/modern-ds/elastic_approfondissement/index.html#indexation-de-la-base-géolocalisée",
    "href": "content/course/modern-ds/elastic_approfondissement/index.html#indexation-de-la-base-géolocalisée",
    "title": "35  Approfondissement ElasticSearch pour des recherches de proximité géographique",
    "section": "40.3 Indexation de la base géolocalisée",
    "text": "40.3 Indexation de la base géolocalisée\nPour le moment, l’index est vide. Il convient de\nle peupler.\nIl est néanmoins nécessaire de créer le champ location\nau format attendu par elastic: lat, lon à partir\nde nos colonnes.\n\ndf_geolocalized['location'] = df_geolocalized['y_latitude'].astype(str) + \", \" + df_geolocalized['x_longitude'].astype(str)\n\nLa fonction suivante permet de structurer chaque\nligne du DataFrame telle qu’Elastic l’attend:\n\ndef gen_dict_from_pandas(index_name, df):\n    '''\n    Lit un dataframe pandas Open Food Facts, renvoi un itérable = dictionnaire des données à indexer, sous l'index fourni\n    '''\n    for i, row in df.iterrows():\n        header= {\"_op_type\": \"index\",\"_index\": index_name,\"_id\": i}\n        yield {**header,**row}\n\nEnfin, on peut industrialiser l’indexation\nde notre DataFrame en faisant tourner de\nmanière successive cette fonction:\n\nfrom elasticsearch.helpers import bulk, parallel_bulk\nfrom collections import deque\ndeque(parallel_bulk(client=es, actions=gen_dict_from_pandas(\"sirene\", df_geolocalized), chunk_size = 1000, thread_count = 4))\n\n\nes.count(index = 'sirene')\n\nObjectApiResponse({'count': 13059694, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}})"
  },
  {
    "objectID": "content/course/modern-ds/elastic_approfondissement/index.html#premier-exemple-de-requête-géographique",
    "href": "content/course/modern-ds/elastic_approfondissement/index.html#premier-exemple-de-requête-géographique",
    "title": "35  Approfondissement ElasticSearch pour des recherches de proximité géographique",
    "section": "41.1 Premier exemple de requête géographique",
    "text": "41.1 Premier exemple de requête géographique\n\nex1 = es.search(index = 'sirene', body = '''{\n  \"query\": {\n    \"bool\": {\n      \"must\":\n      { \"match\": { \"denom\":   \"institut national de la statistique\"}}\n      }\n  }\n}\n''')['hits']['hits']\n\necho_insee = pd.json_normalize(ex1)\necho_insee\n\nOn remarque déjà que les intitulés ne sont\npas bons. Quand est-il de leurs localisations ?\n\nplot_folium_sirene(\n  echo_insee, yvar = \"_source.y_latitude\",\n  xvar = \"_source.x_longitude\")\n\nCe premier essai nous suggère qu’il est\nnécessaire d’améliorer notre recherche.\nPlusieurs voies sont possibles:\n\nAméliorer le preprocessing de nos champs\ntextuels en excluant, par exemple, les\nstopwords ;\nEffectuer une restriction géographique\npour mieux cibler l’ensemble de recherche\nTrouver une variable catégorielle jouant\nle rôle de variable de blocage2 pour\nmieux cibler les paires pertinentes\n\nConcernant la restriction\ngéographique, Elastic fournit une approche\ntrès efficace de ciblage géographique.\nEn connaissant une position approximative\nde l’entreprise à rechercher,\nil est ainsi possible de\nrechercher dans un rayon\nd’une taille plus ou moins grande.\nEn supposant qu’on connaisse précisément\nla localisation de l’Insee, on peut\nchercher dans un rayon relativement\nrestreint. Si notre position était plus\napproximative, on pourrait rechercher\ndans un rayon plus large.\n\nex2 = es.search(index = 'sirene', body = '''{\n  \"query\": {\n    \"bool\": {\n      \"must\":\n      { \"match\": { \"denom\":   \"institut national de la statistique\"}}\n      ,\n      \"filter\":\n        {\"geo_distance\": {\n          \"distance\": \"1km\",\n          \"location\": {\n            \"lat\": \"48.8168\",\n            \"lon\": \"2.3099\"\n          }\n        }\n      }\n    }\n  }\n}\n''')['hits']['hits']\n\necho_insee = pd.json_normalize(ex2)\necho_insee\n\n{{% box status=“hint” title=“Conseil” icon=“fa fa-lightbulb” %}}\nConnaître la précision précise d’une\nentreprise\nnécessite déjà une bonne remontée\nd’information sur celle-ci.\nIl est plus plausible de supposer\nqu’on dispose, dans une phase amont\nde la chaine de production,\nde l’adresse de celle-ci.\nDans ce cas, il est utile\nd’utiliser un service de géocodage,\ncomme l’API Adresse\ndéveloppée par Etalab.\n{{% /box %}}\nLes résultats sont par construction mieux\nciblés. Néanmoins ils sont toujours décevants\npuisqu’on ne parvient pas à identifier l’Insee\ndans les dix meilleurs échos.\n\nspecificsearch = es.search(index = 'sirus_2020', body = \n'''{\n  \"query\": {\n    \"bool\": {\n      \"should\":\n          { \"match\": { \"rs_denom\":   \"CPCU - CENTRALE DE BERCY\"}},\n      \"filter\": [\n          {\"geo_distance\": {\n                  \"distance\": \"0.5km\",\n                  \"location\": {\n                        \"lat\": \"48.84329\", \n                        \"lon\": \"2.37396\"\n                              }\n                            }\n            }, \n            { \"prefix\":  { \"apet\": \"3530\" }}\n                ]\n            }\n          }\n}'''\n)"
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html",
    "href": "content/course/modern-ds/elastic_intro/index.html",
    "title": "36  Introduction à ElasticSearch pour la recherche textuelle",
    "section": "",
    "text": "37 Introduction\nPour évaluer la similarité entre deux données textuelles, il est\nnécessaire de transformer l’information qualitative qu’est le nom\ndu produit en information quantitative qui permettra de rapprocher\ndifférents types de produits.\nLes ordinateurs ont en effet besoin de transformer les informations\ntextuelles en information numérique pour être en mesure\nde les exploiter.\nOn appelle distance de Levenshtein entre deux chaînes de caractères\nle coût minimal (en nombre d’opérations)\npour transformer la première en la seconde par:\nLa distance de Levenshtein est une mesure très utilisée pour comparer la similarité entre deux\nchaînes de caractères. Il existe plusieurs packages pour calculer cette dernière.\nfuzzywuzzy est le plus connu mais ce dernier est assez lent (implémentation en pur Python).\nLe package rapidfuzz, présenté ici, propose les mêmes fonctionalités mais est plus rapide car implémenté\nen C++ qui est plus efficace.\nCependant, nous allons le voir, ce package ne nous\noffrira pas des performances\nassez bonnes pour que nous puissions\npasser à l’échelle.\nVoici trois exemples pour évaluer le coût de chaque\nopération:\nimport rapidfuzz\n\n[\n  rapidfuzz.distance.Levenshtein.distance('salut','slut', weights =(1,1,1)), # Suppression \n  rapidfuzz.distance.Levenshtein.distance('salut','saalut', weights =(1,1,1)), # Addition \n  rapidfuzz.distance.Levenshtein.distance('salut','selut', weights =(1,1,1)) # Substitution\n]\nA partir de maintenant, commence, à proprement parler, la démonstration Elastic.\nCette\npartie développe les éléments les plus techniques, à savoir l’indexation d’une base.\nTous les utilisateurs d’Elastic n’ont pas nécessairement à passer par là, ils peuvent\ntrouver une base déjà indexée, idéalement par un data engineer qui aura optimisé\nles traitements.\nLes utilisateurs du SSP Cloud, architecture qui\nrepose sur la technologie Kubernetes peuvent\nrépliquer les éléments de la suite du document.\nOn peut spécifier la façon dont l’on souhaite analyser le texte.\nPar exemple, on peut préciser que l’on souhaite enlever des stopwords, raciniser, analyser les termes via des n-grammes\npour rendre la recherche plus robuste aux fautes de frappes…\nCes concepts sont présentés dans la partie NLP.\nPour une présentation plus complète, voir\nla documentation officielle d’Elastic\nOn propose les analyseurs stockés dans un fichier schema.json\nLes n-grammes sont des séquences de n caractères ou plus généralement n éléments qui s’enchaînent séquentiellement.\nPar exemple, NOI et OIX sont des tri-grammes de caractères dans NOIX.\nComparer les n-grammes composant des libellés peut permettre d’avoir dans des comparaisons à fautes de frappe/abbréviations près.\nCela fait aussi plus de comparaisons à opérer ! D’où également, l’intérêt d’Elastic, qui intégre facilement et efficacement ces comparaisons.\nOn va préciser un peu le schéma de données qu’on souhaite indexer, et aussi préciser comment les différents champs seront analysés."
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#réplication-de-ce-chapitre",
    "href": "content/course/modern-ds/elastic_intro/index.html#réplication-de-ce-chapitre",
    "title": "36  Introduction à ElasticSearch pour la recherche textuelle",
    "section": "37.1 Réplication de ce chapitre",
    "text": "37.1 Réplication de ce chapitre\nCe chapitre est plus exigeant en termes d’infrastructures que les précédents.\nSi la première partie de ce chapitre peut être menée avec une\ninstallation standard de Python, ce n’est pas le cas de la\ndeuxième qui nécessite un serveur ElasticSearch. Les utilisateurs du\nSSP Cloud pourront répliquer les exemples de ce cours\ncar cette technologie est disponible (que ce soit pour indexer une base ou\npour requêter une base existante).\n:warning: Ce\nchapitre nécessite une version particulière du\npackage ElasticSearch pour tenir compte de l’héritage de la version 7 du moteur Elastic.\nPour cela, faire\n\n!pip install elasticsearch==8.2.0\n!pip install unidecode\n!pip install rapidfuzz\n!pip install xlrd\n\nLa première partie de ce tutoriel ne nécessite pas d’architecture particulière et\npeut ainsi être exécutée en utilisant les packages suivants:\n\nimport time\nimport pandas as pd\n\nLe script functions.py, disponible sur Github,\nregroupe un certain nombre de fonctions utiles permettant\nd’automatiser certaines tâches de nettoyage classiques\nen NLP.\n{{% box status=“hint” title=“Hint” icon=“fa fa-lightbulb” %}}\nPlusieurs méthodes peuvent être mises en oeuvre pour récupérer\nle script d’utilitaires. Vous pouvez trouver en dessous\nde cet encadré une méthode qui va chercher la dernière\nversion sur le dépôt Github du cours\n{{% /box %}}\n\nimport requests\nbaseurl = \"https://raw.githubusercontent.com/linogaliana/python-datascientist\"\nbranch = \"master\"\npath = \"content/course/modern-ds/elastic_intro/functions.py\"\n\nurl = f\"{baseurl}/{branch}/{path}\"\nr = requests.get(url, allow_redirects=True)\n\nopen('functions.py', 'wb').write(r.content)\n\nAprès l’avoir récupéré (cf. encadré dédié),\nil convient d’importer les fonctions sous forme de module:\n\nimport functions as fc"
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#cas-dusage",
    "href": "content/course/modern-ds/elastic_intro/index.html#cas-dusage",
    "title": "36  Introduction à ElasticSearch pour la recherche textuelle",
    "section": "37.2 Cas d’usage",
    "text": "37.2 Cas d’usage\nCe notebook recense et propose d’appréhender quelques outils utilisés\npour le papier présenté aux\nJournées de Méthodologie Statistiques 2022: Galiana and Suarez-Castillo, “Fuzzy matching on big-data: an illustration with scanner data and crowd-sourced nutritional data”\n(travail en cours!)\nOn va partir du cas d’usage suivant:\n\nCombien de calories dans ma recette de cuisine de ce soir? Combien de calories dans mes courses de la semaine?\n\nL’objectif est de reconstituer, à partir de libellés de produits, les caractéristiques nutritionnelles d’une recette.\nLe problème est que les libellés des tickets de caisse ne sont pas des champs textuels très propres, ils contiennent,\npar exemple, beaucoup d’abbréviations, toutes n’étant pas évidentes.\nVoici par exemple une série de noms de produits qu’on va utiliser par la suite:\n\nticket = ['CROISSANTS X6 400G',\n          'MAQUEREAUX MOUTAR.',\n          'IGP OC SAUVIGNON B',\n          'LAIT 1/2 ECRM UHT',\n          '6 OEUFS FRAIS LOCA',\n          'ANANAS C2',\n          'L POMME FUDJI X6 CAL 75/80 1KG ENV',\n          'PLT MIEL',\n          'STELLA ARTOIS X6',\n          'COTES DU LUBERON AIGUEBRUN 75C']\n\nA ces produits, s’ajoutent les ingrédients suivants, issus de la\nrecette du velouté de potiron et carottes de Marmiton\nqui sera notre plat principal :\n\ningredients = ['500 g de carottes',\n '2 pommes de terre',\n \"1 gousse d'ail\",\n '1/2 l de lait',\n '1/2 l de bouillon de volaille',\n \"1 cuillère à soupe de huile d'olive\",\n '1 kg de potiron',\n '1 oignon',\n '10 cl de crème liquide (facultatif)']\n\nEssayer de récupérer par webscraping cette liste est un bon exercice pour réviser\nles concepts vus précedemment\nOn va donc créer une liste de course compilant\nces deux\nlistes hétérogènes de noms de produits:\n\nlibelles = ticket + ingredients\n\nOn part avec cette liste dans notre supermarché virtuel. L’objectif sera de trouver\nune méthode permettant de passer à l’échelle:\nautomatiser les traitements, effectuer des recherches efficaces, garder une certaine généralité et flexibilité.\nCe chapitre montrera par l’exemple l’intérêt d’Elastic par rapport à une solution\nqui n’utiliserait que du Python."
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#les-bases-offrant-des-informations-nutritionnelles",
    "href": "content/course/modern-ds/elastic_intro/index.html#les-bases-offrant-des-informations-nutritionnelles",
    "title": "36  Introduction à ElasticSearch pour la recherche textuelle",
    "section": "38.1 Les bases offrant des informations nutritionnelles",
    "text": "38.1 Les bases offrant des informations nutritionnelles\nPour un nombre restreint de produits, on pourrait bien-sûr chercher à\nla main les caractéristiques des produits en utilisant les\nfonctionalités d’un moteur de recherche:\n\n\n\n\n\nCependant, cette approche serait très fastidieuse et\nnécessiterait de récuperer, à la main, chaque caractéristique\npour chaque produit. Ce n’est donc pas envisageable.\nLes données disponibles sur Google viennent de l’USDA,\nl’équivalent américain de notre Ministère de l’Agriculture.\nCependant, pour des recettes comportant des noms de produits français, ainsi que\ndes produits potentiellement transformés, ce n’est pas très pratique d’utiliser\nune base de données de produits agricoles en Français. Pour cette raison,\nnous proposons d’utiliser les deux bases suivantes,\nqui servent de base au travail de\nGaliana and Suarez Castillo (2022)\n\nL’OpenFoodFacts database qui est une base\ncollaborative française de produits alimentaires. Issue d’un projet Data4Good, il s’agit d’une\nalternative opensource et opendata à la base de données de l’application Yuka.\nLa table de composition nutritionnelle Ciqual produite par l’Anses. Celle-ci\npropose la composition nutritionnelle moyenne des aliments les plus consommés en France. Il s’agit d’une base de données\nenrichie par rapport à celle de l’USDA puisqu’elle ne se cantonne pas aux produits agricoles non transformés.\nAvec cette base, il ne s’agit pas de trouver un produit exact mais essayer de trouver un produit type proche du produit\ndont on désire connaître les caractéristiques."
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#import",
    "href": "content/course/modern-ds/elastic_intro/index.html#import",
    "title": "36  Introduction à ElasticSearch pour la recherche textuelle",
    "section": "38.2 Import",
    "text": "38.2 Import\nQuelques fonctions utiles sont regroupées dans le script functions.py et importées dans le notebook.\nLa base OpenFood peut être récupérée en ligne\nvia la fonction fc.import_openfood. Néanmoins, cette opération nécessitant\nun certain temps (les données brutes faisant autour de 2Go), nous proposons une méthode\npour les utilisateurs du SSP-Cloud où une version est disponible sur\nl’espace de stockage.\nLa base Ciqual, qui plus légère, est récupérée elle directement en ligne\nvia la fonction fc.import_ciqual.\n\n# Pour les utilisateurs du SSP-Cloud\nopenfood = fc.import_openfood_s3()\n# Pour les utilisateurs hors du SSP-Cloud\n# openfood = fc.import_openfood()\nciqual = fc.import_ciqual()\n\n\nopenfood.head()\n\n\nciqual.head()"
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#quest-ce-quelastic",
    "href": "content/course/modern-ds/elastic_intro/index.html#quest-ce-quelastic",
    "title": "36  Introduction à ElasticSearch pour la recherche textuelle",
    "section": "39.1 Qu’est-ce qu’Elastic ?",
    "text": "39.1 Qu’est-ce qu’Elastic ?\nElasticSearch c’est un logiciel qui fournit un moteur de recherche installé sur\nun serveur (ou une machine personnelle) qu’il est possible de requêter depuis un client\n(une session Python par exemple).\nC’est un moteur de recherche\ntrès performant, puissant et flexible, extrêmement utilisé dans le domaine de la datascience\nsur données textuelles.\nUn cas d’usage est par exemple de trouver,\ndans un corpus de grande dimension\n(plusieurs sites web, livres…), un certain texte en s’autorisant des termes voisins\n(verbes conjugués, fautes de frappes…).\nUn index est une collection de documents dans lesquels on souhaite chercher, préalablement ingérés dans un moteur de recherche les documents sont les établissements.\nL’indexation consiste à pré-réaliser les traitements des termes des documents pour gagner en efficacité lors de la phase de recherche.\nL’indexation est faite une fois pour de nombreuses recherches potentielles, pour lesquelles la rapidité de réponse peut être cruciale.\nAprès avoir indexé une base, on effectuera des requêtes qui sont des recherches\nd’un document dans la base indexé (équivalent de notre web) à partir de\ntermes de recherche normalisés.\nLe principe est le même que celui d’un moteur de recherche du web comme Google.\nD’un côté, l’ensemble à parcourir est indexé pour être en\nmesure de parcourir de manière efficace l’ensemble du corpus.\nDe l’autre côté, la phase de recherche permet de retrouver l’élément du corpus le\nplus cohérent avec la requête de recherche.\nL’indexation consiste, par exemple,\nà pré-définir des traitements des termes du corpus pour gagner en efficacité\nlors de la phase de recherche. En effet, l’indexation est une opération peu fréquente\npar rapport à la recherche. Pour cette dernière, l’efficacité est cruciale (un site web\nqui prend plusieurs secondes à interpréter une requête simple ne sera pas utilisé). Mais, pour\nl’indexation, ceci est moins crucial.\nLes documents sont constitués de variables, les champs (‘fields’),\ndont le type est spécifié (“text”, “keywoard”, “geo_point”, “numeric”…) à l’indexation.\nElasticSearch propose une interface graphique nommée Kibana.\nCelle-ci est pratique\npour tester des requêtes et pour superviser le serveur Elastic. Cependant,\npour le passage à l’échelle, notamment pour mettre en lien une base indexée dans\nElastic avec une autre source de données, les API proposées par ElasticSearch\nsont beaucoup plus pratiques. Ces API permettent de connecter une session Python (idem pour R)\nà un serveur Elastic afin de communiquer avec lui\n(échanger des flux via une API REST)."
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#elasticsearch-et-python",
    "href": "content/course/modern-ds/elastic_intro/index.html#elasticsearch-et-python",
    "title": "36  Introduction à ElasticSearch pour la recherche textuelle",
    "section": "39.2 ElasticSearch et Python",
    "text": "39.2 ElasticSearch et Python\nEn Python, le package officiel est elasticsearch.\nCe dernier permet de configurer les paramètres pour interagir avec un serveur, indexer\nune ou plusieurs bases, envoyer de manière automatisée un ensemble de requêtes\nau serveur, récupérer les résultats directement dans une session Python…"
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#premier-essai-les-produits-ciqual-les-plus-similaires-aux-produits-de-la-recette",
    "href": "content/course/modern-ds/elastic_intro/index.html#premier-essai-les-produits-ciqual-les-plus-similaires-aux-produits-de-la-recette",
    "title": "36  Introduction à ElasticSearch pour la recherche textuelle",
    "section": "40.1 Premier essai: les produits Ciqual les plus similaires aux produits de la recette",
    "text": "40.1 Premier essai: les produits Ciqual les plus similaires aux produits de la recette\nOn pourrait écrire une fonction qui prend en argument\nune liste de libellés d’intérêt et une liste de candidat au match et\nrenvoie le libellé le plus proche.\nCependant, le risque est que cet algorithme soit relativement lent s’il n’est pas codé\nparfaitement.\nIl est, à mon avis, plus simple, quand\non est habitué à la logique Pandas,\nde faire un produit cartésien pour obtenir un vecteur mettant en miroir\nchaque produit de notre recette avec l’ensembles des produits Ciqual et ensuite comparer les deux vecteurs pour prendre,\npour chaque produit, le meilleur match.\nLes bases étant de taille limitée, le produit cartésien n’est pas problématique.\nAvec des bases plus conséquentes, une stratégie plus parcimonieuse en mémoire devrait être envisagée.\nPour faire cette opération, on va utiliser la fonction match_product de\nnote script d’utilitaires.\n\ndist_leven = fc.match_product(libelles, ciqual)\ndist_leven\n\nCette première étape naïve est décevante à plusieurs égards:\n\nCertes, on a des matches cohérent (par exemple “Oignon rouge, cru” et “1 oignon”)\nmais on a plus de couples incohérents ;\nLe temps de calcul peut apparaître faible mais le passage à l’échelle risque d’être compliqué ;\nLes besoins mémoires sont potentiellement importants lors de l’appel à\nrapidfuzz.process.extract ce qui peut bloquer le passage à l’échelle ;\nLa distance textuelle n’est pas nécessairement la plus pertinente.\n\nOn a, en fait, négligé une étape importante: la normalisation (ou nettoyage des textes) présentée dans la\npartie NLP, notamment:\n\nharmonisation de la casse, suppression des accents…\nsuppressions des mots outils (e.g. ici on va d’abord négliger les quantités pour trouver la nature de l’aliment, en particulier pour Ciqual)\n\n\n\n\n\n\nScanner-data avant nettoyage\n\n\n\n\n\nOpenFood data avant nettoyage\n\n\n\n\n\n\n\nScanner-data après nettoyage\n\n\n\n\n\nOpenFood data après nettoyage\n\n\n\n\nFaisons donc en apparence un retour en arrière qui sera\nnéanmoins salvateur pour améliorer\nla pertinence des liens faits entre nos\nbases de données."
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#objectif",
    "href": "content/course/modern-ds/elastic_intro/index.html#objectif",
    "title": "36  Introduction à ElasticSearch pour la recherche textuelle",
    "section": "41.1 Objectif",
    "text": "41.1 Objectif\nLe preprocessing correspond à l’ensemble des opérations\nayant lieu avant l’analyse à proprement parler.\nIci, ce preprocessing est intéressant à plusieurs\négards:\n\nIl réduit le bruit dans nos jeux de données (par exemple des mots de liaisons) ;\nIl permet de normaliser et harmoniser les syntaxes dans nos différentes sources.\n\nL’objectif est ainsi de réduire nos noms de produits à la substantifique moelle\npour améliorer la pertinence de la recherche.\nPour être pertinent, le preprocessing comporte généralement deux types de\ntraitements. En premier lieu, ceux qui sont généraux et applicables\nà tous types de corpus textuels: retrait des stopwords, de la ponctuation, etc.\nles méthodes disponibles dans la partie NLP.\nEnsuite, il est nécessaire de mettre en oeuvre des nettoyages plus spécifiques à chaque corpus.\nPar exemple dans la source Ciqual,\nla cuisson est souvent renseignée et bruite les appariemments."
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#démarche",
    "href": "content/course/modern-ds/elastic_intro/index.html#démarche",
    "title": "36  Introduction à ElasticSearch pour la recherche textuelle",
    "section": "41.2 Démarche",
    "text": "41.2 Démarche\n{{% box status=“exercise” title=“Exercice 1” icon=“fas fa-pencil-alt” %}}\nExercice 1: preprocessing\n\nPour transformer les lettres avec accents en leur équivalent\nsans accent, la fonction unidecode\n(du package du même nom) est pratique.\nLa tester sur le jeu de données ciqual en créant une nouvelle\ncolonne nommée libel_clean\nLa casse différente selon les jeux de données peut être pénalisante\npour trouver des produits similaires. Pour éviter ces problèmes,\nmettre tout en majuscule.\nLes informations sur les quantités ou le packaging peuvent apporter\ndu bruit dans notre comparaison. Nous allons retirer ces mots,\nà travers la liste ['KG','CL','G','L','CRUE?S?', 'PREEMBALLEE?S?'],\nqu’on peut considérer comme un dictionnaire de stop-words métier.\nPour cela, il convient d’utiliser une expression régulière dans la méthode\nstr.replace de Pandas.\nAvec ceux-ci, on va utiliser la liste des stop-words de\nla librairie nltk pour retirer les stop-words classiques (_“le”,“la”, etc.).\nLa librairie SpaCy, plus riche, pourrait être utilisée ; nous laissons\ncela sous la forme d’exercice supplémentaire.\nOn a encore des signes de ponctuation ou des chiffres qui peuvent\npoluer la comparaison. Les retirer grâce à la méthode replace et\nune regex [^a-zA-Z]\nEnfin, par sécurité, on peut supprimer les espaces multiples.\nUtiliser la regex '([ ]{2,})' pour cela. Observer le résultat\nfinal.\n(Optionnel). Comme exercice supplémentaire, faire la même chose avec les\npipelines SpaCy.\n\n{{% /box %}}\nA l’issue de la question 1, le jeu de données ciqual devrait\nressembler à celui-ci:\nAprès avoir mis en majuscule, on se retrouve avec le jeu de données\nsuivant:\nAprès retrait des stop-words, nos libellés prennent\nla forme suivante:\nLa regex pour éliminer les caractères de ponctuation permet ainsi d’obtenir:\nEnfin, à l’issue de la question 5, le DataFrame obtenu est le suivant:\nCes étapes de nettoyage ont ainsi permis de concentrer l’information\ndans les noms de produits sur ce qui l’identifie vraiment."
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#approche-systématique",
    "href": "content/course/modern-ds/elastic_intro/index.html#approche-systématique",
    "title": "36  Introduction à ElasticSearch pour la recherche textuelle",
    "section": "41.3 Approche systématique",
    "text": "41.3 Approche systématique\nPour systématiser cette approche à nos différents DataFrame, rien de mieux\nqu’une fonction. Celle-ci est présente dans le module functions\nsous le nom clean_libelle.\n\nfrom functions import clean_libelle\n\nPour résumer l’exercice précédent, cette fonction va :\n\nHarmoniser la casse et retirer les accents (voir functions.py) ;\nRetirer tout les caractères qui ne sont pas des lettres (chiffres, ponctuations) ;\nRetirer les caractères isolés.\n\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\n\nstop_words = ['KG','CL','G','L','CRUE?S?', 'PREEMBALLEE?S?']\nstop_words += [l.upper() for l in stopwords.words('french')]\n\nreplace_regex = {r'[^A-Z]': ' ', r'\\b[A-Z0-9]{1,2}?\\b':' '} # \n\nCela permet d’obtenir les bases nettoyées suivantes:\n\nciqual = clean_libelle(ciqual, yvar = 'alim_nom_fr', replace_regex = replace_regex, stopWords = stop_words)\nciqual.sample(10)\n\n\nopenfood = clean_libelle(openfood, yvar = 'product_name', replace_regex = replace_regex, stopWords = stop_words)\nopenfood.sample(10)\n\n\ncourses = pd.DataFrame(libelles, columns = ['libel'])\ncourses = clean_libelle(courses, yvar = 'libel', replace_regex = replace_regex, stopWords = stop_words)\ncourses.sample(10)\n\nLes noms de produits sont déjà plus harmonisés.\nVoyons voir si cela permet de trouver un\nmatch dans l’Openfood database:\n\ndist_leven_openfood = fc.match_product(courses[\"libel_clean\"], openfood, \"libel_clean\")\ndist_leven_openfood.sample(10)\n\nPas encore parfait, mais on progresse sur les produits appariés!\nConcernant le temps de calcul, les quelques secondes nécessaires à\nce calcul peuvent apparaître un faible prix à payer. Cependant,\nil convient de rappeler que le nombre de produits dans l’ensemble\nde recherche est faible. Cette solution n’est donc pas généralisable."
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#réduire-les-temps-de-recherche",
    "href": "content/course/modern-ds/elastic_intro/index.html#réduire-les-temps-de-recherche",
    "title": "36  Introduction à ElasticSearch pour la recherche textuelle",
    "section": "41.4 Réduire les temps de recherche",
    "text": "41.4 Réduire les temps de recherche\nFinalement, l’idéal serait de disposer d’un moteur de recherche adapté à notre besoin,\ncontenant les produits candidats, que l’on pourrait interroger, rapide en lecture, capable de classer les échos renvoyés par pertinence, que l’on pourrait requêter de manière flexible.\nPar exemple, on pourrait vouloir signaler qu’un\nécho nous intéresse seulement si la donnée calorique n’est pas manquante.\nOn pourrait même vouloir qu’il effectue pour nous des prétraitements sur les données.\nCela paraît beaucoup demander. Mais c’est exactement ce que fait ElasticSearch."
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#créer-un-cluster-elastic-sur-le-datalab",
    "href": "content/course/modern-ds/elastic_intro/index.html#créer-un-cluster-elastic-sur-le-datalab",
    "title": "36  Introduction à ElasticSearch pour la recherche textuelle",
    "section": "42.1 Créer un cluster Elastic sur le DataLab",
    "text": "42.1 Créer un cluster Elastic sur le DataLab\nPour lancer un service Elastic, il faut cliquer sur ce lien.\nUne fois créé, vous pouvez explorer l’interface graphique Kibana.\nCependant, grâce à l’API Elastic\nde Python, on se passera de celle-ci. Donc, en pratique,\nune fois lancé, pas besoin d’ouvrir ce service Elastic pour continuer à suivre1.\nDans un terminal, vous pouvez aussi vérifier que vous êtes en mesure de dialoguer avec votre cluster Elastic,\nqui est prêt à vous écouter:\nkubectl get statefulset\nPasser par la ligne de commande serait peu commode pour industrialiser notre\nrecherche.\nNous allons utiliser la librairie elasticsearch pour dialoguer avec notre moteur de recherche Elastic.\nLes instructions ci-dessous indiquent comment établir la connection.\n\nfrom elasticsearch import Elasticsearch\nHOST = 'elasticsearch-master'\n\ndef elastic():\n    \"\"\"Connection avec Elastic sur le data lab\"\"\"\n    es = Elasticsearch([{'host': HOST, 'port': 9200, 'scheme': 'http'}], http_compress=True, request_timeout=200)\n    return es\n\nes = elastic()\n\n&lt;Elasticsearch([{'host': 'elasticsearch-master', 'port': 9200}])&gt;\nMaintenant que la connection est établie, deux étapes nous attendent:\n\nIndexation Envoyer les documents parmi lesquels on veut chercher des echos pertinents dans notre elastic. Un index est une collection de document. Nous pourrions en créer deux: un pour les produits ciqual, un pour les produits openfood\nRequête Chercher les documents les plus pertinents suivant une recherche textuelle flexible. Nous allons rechercher les libellés de notre recette et de notre liste de course."
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#première-indexation",
    "href": "content/course/modern-ds/elastic_intro/index.html#première-indexation",
    "title": "36  Introduction à ElasticSearch pour la recherche textuelle",
    "section": "42.2 Première indexation",
    "text": "42.2 Première indexation\nOn crée donc nos deux index:\n\nif not es.indices.exists(index = 'openfood'):\n    es.indices.create(index = 'openfood')\nif not es.indices.exists(index = 'ciqual'):\n    es.indices.create(index = 'ciqual')\n\nPour l’instant, nos index sont vides! Ils contiennent 0 documents.\n\nes.count(index = 'openfood')\n\n{'count': 0, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}}\nNous allons en rajouter quelques uns !\n\nes.create(index = 'openfood',  id = 1, body = {'product_name': 'Tarte noix de coco', 'product_name_clean': 'TARTE NOIX COCO'})\nes.create(index = 'openfood',  id = 2, body = {'product_name': 'Noix de coco', 'product_name_clean': 'NOIX COCO'})\nes.create(index = 'openfood',  id = 3, body = {'product_name': 'Beurre doux', 'product_name_clean': 'BEURRE DOUX'})\n\n\nes.count(index = 'openfood')\n\n{'count': 3, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}}\nDans l’interface graphique Kibana,\non peut vérifier que l’indexation\na bien eue lieu en allant dans Management &gt; Stack Management"
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#première-recherche",
    "href": "content/course/modern-ds/elastic_intro/index.html#première-recherche",
    "title": "36  Introduction à ElasticSearch pour la recherche textuelle",
    "section": "42.3 Première recherche",
    "text": "42.3 Première recherche\nFaisons notre première recherche: cherchons des noix de pécan!\n\nes.search(index = 'openfood', q = 'noix de pécan')\n\nObjectApiResponse({'took': 116, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 2, 'relation': 'eq'}, 'max_score': 0.9400072, 'hits': [{'_index': 'openfood', '_type': '_doc', '_id': '2', '_score': 0.9400072, '_source': {'product_name': 'Noix de coco', 'product_name_clean': 'NOIX COCO'}}, {'_index': 'openfood', '_type': '_doc', '_id': '1', '_score': 0.8272065, '_source': {'product_name': 'Tarte noix de coco', 'product_name_clean': 'TARTE NOIX COCO'}}]}})\nIntéressons nous aux hits (résultats pertinents, ou echos) : nous en avons 2.\nLe score maximal parmi les hits est mentionné dans max_score et correspond à celui du deuxième document indexé.\nElastic nous fournit ici un score de pertinence dans notre recherche d’information, et classe ainsi les documents renvoyés.\nIci nous utilisons la configuration par défaut. Mais comment est calculé ce score? Demandons à Elastic de nous expliquer le score du document 2 dans la requête \"noix de pécan\".\n\nes.explain(index = 'openfood', id = 2, q = 'noix de pécan')\n\nObjectApiResponse({'_index': 'openfood', '_type': '_doc', '_id': '2', 'matched': True, 'explanation': {'value': 0.9400072, 'description': 'max of:', 'details': [{'value': 0.49917626, 'description': 'sum of:', 'details': [{'value': 0.49917626, 'description': 'weight(product_name_clean:noix in 1) [PerFieldSimilarity], result of:', 'details': [{'value': 0.49917626, 'description': 'score(freq=1.0), computed as boost * idf * tf from:', 'details': [{'value': 2.2, 'description': 'boost', 'details': []}, {'value': 0.47000363, 'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:', 'details': [{'value': 2, 'description': 'n, number of documents containing term', 'details': []}, {'value': 3, 'description': 'N, total number of documents with field', 'details': []}]}, {'value': 0.48275858, 'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:', 'details': [{'value': 1.0, 'description': 'freq, occurrences of term within document', 'details': []}, {'value': 1.2, 'description': 'k1, term saturation parameter', 'details': []}, {'value': 0.75, 'description': 'b, length normalization parameter', 'details': []}, {'value': 2.0, 'description': 'dl, length of field', 'details': []}, {'value': 2.3333333, 'description': 'avgdl, average length of field', 'details': []}]}]}]}]}, {'value': 0.9400072, 'description': 'sum of:', 'details': [{'value': 0.4700036, 'description': 'weight(product_name:noix in 1) [PerFieldSimilarity], result of:', 'details': [{'value': 0.4700036, 'description': 'score(freq=1.0), computed as boost * idf * tf from:', 'details': [{'value': 2.2, 'description': 'boost', 'details': []}, {'value': 0.47000363, 'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:', 'details': [{'value': 2, 'description': 'n, number of documents containing term', 'details': []}, {'value': 3, 'description': 'N, total number of documents with field', 'details': []}]}, {'value': 0.45454544, 'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:', 'details': [{'value': 1.0, 'description': 'freq, occurrences of term within document', 'details': []}, {'value': 1.2, 'description': 'k1, term saturation parameter', 'details': []}, {'value': 0.75, 'description': 'b, length normalization parameter', 'details': []}, {'value': 3.0, 'description': 'dl, length of field', 'details': []}, {'value': 3.0, 'description': 'avgdl, average length of field', 'details': []}]}]}]}, {'value': 0.4700036, 'description': 'weight(product_name:de in 1) [PerFieldSimilarity], result of:', 'details': [{'value': 0.4700036, 'description': 'score(freq=1.0), computed as boost * idf * tf from:', 'details': [{'value': 2.2, 'description': 'boost', 'details': []}, {'value': 0.47000363, 'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:', 'details': [{'value': 2, 'description': 'n, number of documents containing term', 'details': []}, {'value': 3, 'description': 'N, total number of documents with field', 'details': []}]}, {'value': 0.45454544, 'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:', 'details': [{'value': 1.0, 'description': 'freq, occurrences of term within document', 'details': []}, {'value': 1.2, 'description': 'k1, term saturation parameter', 'details': []}, {'value': 0.75, 'description': 'b, length normalization parameter', 'details': []}, {'value': 3.0, 'description': 'dl, length of field', 'details': []}, {'value': 3.0, 'description': 'avgdl, average length of field', 'details': []}]}]}]}]}]}})\nElastic nous explique donc que le score 0.9400072 est le maximum entre deux sous-scores, 0.4991 et 0.9400072.\nPour chacun de ces sous-scores, le détail de son calcul est donné.\nLe premier sous-score n’a accordé un score que par rapport au premier mot (noix), tandis que le second a accordé un score sur la base des deux mots déjà connu dans les documents (“noix” et “de”). Il a ignoré pécan! Jusqu’à présent, ce terme n’est pas connu dans l’index.\nLa pertinence d’un mot pour notre recherche est construite sur une variante de la TF-IDF,\nconsidérant qu’un terme est pertinent s’il est souvent présent dans le document (Term Frequency)\nalors qu’il est peu fréquent dans les autres document (inverse document frequency).\nIci les notations des documents 1 et 2 sont très proches, la différence est dûe à des IDF plus faibles dans le document 1,\nqui est pénalisé pour être légérement plus long.\nBref, tout ça est un peu lourd, mais assez efficace,\nen tout cas moins rudimentaire que les distances caractères à caractères pour ramener des echos pertinents."
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#limite-de-cette-première-indexation",
    "href": "content/course/modern-ds/elastic_intro/index.html#limite-de-cette-première-indexation",
    "title": "36  Introduction à ElasticSearch pour la recherche textuelle",
    "section": "42.4 Limite de cette première indexation",
    "text": "42.4 Limite de cette première indexation\nPour l’instant, Elastic n’a pas l’air de gérer les fautes de frappes!\nPas le droit à l’erreur dans la requête:\n\nes.search(index = 'openfood',q = 'TART NOI')\n\nObjectApiResponse({'took': 38, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 0, 'relation': 'eq'}, 'max_score': None, 'hits': []}})\nCela s’explique par la représentation des champs (‘product_name’ par exemple) qu’Elastic a inféré,\npuisque nous n’avons rien spécifié.\nLa représentation d’une variable conditionne la façon dont les champs sont analysés pour calculer la pertinence.\nPar exemple, regardons la représentation du champ product_name\n\nes.indices.get_field_mapping(index = 'openfood', fields = 'product_name')\n\nObjectApiResponse({'openfood': {'mappings': {'product_name': {'full_name': 'product_name', 'mapping': {'product_name': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}}}}}})\nElastic a compris qu’il s’agissait d’un champ textuel.\nEn revanche, le type est keyword n’autorise pas des analyses approximatives donc\nne permet pas de tenir compte de fautes de frappes.\nPour qu’un echo remonte, un des termes doit matcher exactement. Dommage !\nMais c’est parce qu’on a utilisé le mapping par défaut.\nEn réalité, il est assez simple de préciser un mapping plus riche,\nautorisant une analyse “fuzzy” ou “flou”."
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#nos-premières-requêtes",
    "href": "content/course/modern-ds/elastic_intro/index.html#nos-premières-requêtes",
    "title": "36  Introduction à ElasticSearch pour la recherche textuelle",
    "section": "43.1 Nos premières requêtes",
    "text": "43.1 Nos premières requêtes\nVérifions qu’on recupère quelques tartes aux noix même si l’on fait plein de fautes:\n\nes.search(index = 'openfood', q = 'TART NOI', size = 3)\n\nObjectApiResponse({'took': 60, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 10000, 'relation': 'gte'}, 'max_score': 22.837925, 'hits': [{'_index': 'openfood', '_type': '_doc', '_id': '405332', '_score': 22.837925, '_source': {'product_name': 'Tarte noix', 'libel_clean': 'TARTE NOIX', 'energy_100g': 1833.0, 'nutriscore_score': 23.0}}, {'_index': 'openfood', '_type': '_doc', '_id': '1103594', '_score': 22.82367, '_source': {'product_name': 'Tarte aux noix', 'libel_clean': 'TARTE NOIX', 'energy_100g': 4.0, 'nutriscore_score': 4.0}}, {'_index': 'openfood', '_type': '_doc', '_id': '1150755', '_score': 22.82367, '_source': {'product_name': 'Tarte aux noix', 'libel_clean': 'TARTE NOIX', 'energy_100g': 1929.0, 'nutriscore_score': 21.0}}]}})\nSi on préfère sous une forme de DataFrame:\n\ndf = pd.json_normalize(\n    es.search(index = 'openfood', q = 'TART NOI', size = 3)['hits']['hits']\n)\ndf.columns = df.columns.str.replace(\"_source.\", \"\", regex = False)\ndf.head(2)\n\n\\n  \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n  \\n  \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n  \\n_index_type_id_scoreproduct_namelibel_cleanenergy_100gnutriscore_score0openfood_doc40533222.837925Tarte noixTARTE NOIX1833.023.01openfood_doc110359422.823670Tarte aux noixTARTE NOIX4.04.02openfood_doc115075522.823670Tarte aux noixTARTE NOIX1929.021.0\nPour automatiser l’envoi de requêtes et la récupération du meilleur\nécho, on peut définir la fonction suivante\n\ndef matchElastic(libelles):\n    start_time = time.time()\n    matches = {}\n    for l in libelles:\n        response = es.search(index = 'openfood', q = l, size = 1)\n        if len(response['hits']['hits'])&gt;0:\n            matches[l] = pd.json_normalize(\n              response['hits']['hits']\n            )\n    print(80*'-')\n    print(f\"Temps d'exécution total : {(time.time() - start_time):.2f} secondes ---\")\n    \n    return matches\n\n\nmatches = matchElastic(courses['libel_clean'])\nmatches = pd.concat(matches)\nmatches.sample(3)\n\n\\n  \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n  \\n  \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n  \\n_index_type_id_score_source.product_name_source.libel_clean_source.energy_100g_source.nutriscore_scoreGOUSSE AIL0openfood_doc198206257.93140Gousse d\\'ailGOUSSE AIL498.05.0IGP SAUVIGNON0openfood_doc180140696.55756vin blanc SauvignonVIN BLANC SAUVIGNON66.31.0POTIRON0openfood_doc104396175.96385PotironPOTIRON172.00.0\nEt voilà, on a un outil très rapide de requête !\nLa pertinence des résultats est encore douteuse.\nPour cela, il conviendrait de préciser des requêtes plus sophistiquées!2\n\nreq = {\n    \"bool\": {\n      \"should\": [\n        { \"match\": { \"libel_clean\":  { \"query\":  \"HUILE OLIVE\" , \"boost\" : 10}}},\n        { \"match\": { \"libel_clean.ngr\":   \"HUILE OLIVE\" }}\n        ],\n      \"minimum_should_match\": 1,\n      \"filter\": [\n      { \n            \"range\" : {\n                \"nutriscore_score\" : {\n                    \"gte\" : 10,\n                    \"lte\" : 20\n                    }\n                    }\n                    }\n      ]\n    }\n}\n\n\nout = es.search(index = 'openfood', query = req, size = 1)\npd.json_normalize(out['hits']['hits'])\n\n\\n  \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n  \\n  \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n  \\n_index_type_id_score_source.product_name_source.libel_clean_source.energy_100g_source.nutriscore_score0openfood_doc960041174.27896Huile d oliveHUILE OLIVE3761.011.0\nQu’a-t-on demandé ici?\n- De renvoyer 1 et 1 seul echo (\"size\":\"1\") et seulement si celui ci a:\n+ \"should\": Au moins un (\"minimum_should_match\":\"1\") des termes des deux champs libel_clean et libel_clean.ngr qui matche sur un terme de HUILE OLIVE, l’analyse (la définition du “terme”) étant réalisé soit en tant que text (“libel_clean”) soit en tant que n-gramme ngr (“libel_clean.ngr”, une analyse que nous avons spécifié dans le mapping)\n+ \"filter\": Le champ float nutriscore_score doit être compris entre 10 et 20 (“filter”).\nA noter :\n\nLes clauses (\"should\"+\"minimum_should_match\":\"1\") peuvent être remplacé par un \"must\". Auquel cas, l’écho doit obligatoirement matcher sur chaque clause.\nPréciser dans \"filter\" (plutôt que dans \"should\") une condition signifie que celle-ci ne participe pas au score de pertinence.\n\nOn n’a pas encore un appariemment très satisfaisant, en particulier sur les boissons. Comment faire ? La réponse est dans Galiana and Suarez Castillo (2022)\n\nA vous, de calculer le nombre de calories de notre recette de course !\n\n\n\n\n\nGaliana, Lino, and Milena Suarez Castillo. 2022. “Fuzzy Matching on Big-Data: An Illustration with Scanner and Crowd-Sourced Nutritional Datasets.” In Proceedings of the 2022 ACM Conference on Information Technology for Social Good, 331–37. GoodIT ’22. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3524458.3547244."
  }
]