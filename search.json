[
  {
    "objectID": "content/annexes/corrections.html",
    "href": "content/annexes/corrections.html",
    "title": "Corrections",
    "section": "",
    "text": "Seules les chapitres dont les corrections ne sont pas apparentes sont listés sur cette page."
  },
  {
    "objectID": "content/annexes/corrections.html#partie-1-manipuler-des-données",
    "href": "content/annexes/corrections.html#partie-1-manipuler-des-données",
    "title": "Corrections",
    "section": "Partie 1: manipuler des données",
    "text": "Partie 1: manipuler des données\n\nRetour sur Numpy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercices Pandas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercices Geopandas:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWebscraping\n\n\n\n\n\n\n\n\n\n\n\n\n\nExpressions régulières:\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapitre sur les API:"
  },
  {
    "objectID": "content/annexes/corrections.html#partie-2-visualiser-les-données",
    "href": "content/annexes/corrections.html#partie-2-visualiser-les-données",
    "title": "Corrections",
    "section": "Partie 2: visualiser les données",
    "text": "Partie 2: visualiser les données"
  },
  {
    "objectID": "content/annexes/corrections.html#partie-3-modéliser",
    "href": "content/annexes/corrections.html#partie-3-modéliser",
    "title": "Corrections",
    "section": "Partie 3: modéliser",
    "text": "Partie 3: modéliser"
  },
  {
    "objectID": "content/annexes/corrections.html#partie-4-natural-language-processing-nlp",
    "href": "content/annexes/corrections.html#partie-4-natural-language-processing-nlp",
    "title": "Corrections",
    "section": "Partie 4: Natural Language Processing (NLP)",
    "text": "Partie 4: Natural Language Processing (NLP)"
  },
  {
    "objectID": "content/annexes/corrections.html#partie-5-introduction-aux-outils-et-méthodes-à-létat-de-lart",
    "href": "content/annexes/corrections.html#partie-5-introduction-aux-outils-et-méthodes-à-létat-de-lart",
    "title": "Corrections",
    "section": "Partie 5: Introduction aux outils et méthodes à l’état de l’art",
    "text": "Partie 5: Introduction aux outils et méthodes à l’état de l’art"
  },
  {
    "objectID": "content/git/exogit.html",
    "href": "content/git/exogit.html",
    "title": "Un cadavre exquis pour découvrir Git",
    "section": "",
    "text": "Les exercices suivants sont inspirés d’un cours de Git que j’ai\nparticipé à construire\nà l’Insee et dont les ressources sont disponibles\nici. L’idée\ndu cadavre exquis est inspirée de\ncette ressource et de celle-ci.\nCette partie part du principe que les concepts généraux de Git sont\nmaîtrisés et qu’un environnement de travail fonctionnel avec Git est\ndisponible. Un exemple de tel environnement est le JupyterLab ou l’environnement VSCode du\nSSPCloud où une extension\nGit est pré-installée :\nOutre le chapitre précédent, il existe de\nnombreuses ressources sur internet sur le sujet,\nnotamment une série de ressources construites\npour l’Insee sur ce site\net des ressources de la documentation collaborative sur R qu’est utilitR\n(des éléments sur la configuration\net pratique sur RStudio). Toutes\nles ressources ne sont donc pas du Python car Git est un outil transversal\nqui doit servir quel que soit le langage de prédilection.\nGit fait parti des pratiques collaboratives\ndevenues standards dans le domaine de l’open-source\nmais également de plus en plus communes dans les administrations et entreprises\nde la data science.\nCe chapitre propose, pour simplifier l’apprentissage,\nd’utiliser l’ extension Git de JupyterLab ou de VSCode.\nUn tutoriel présentant l’extension JupyterLab est disponible\nici.\nVSCode propose\nprobablement, à l’heure actuelle, l’ensemble le plus complet.\nCertains passages de ce TD nécessitent d’utiliser la ligne de commande.\nIl est tout à fait possible de réaliser ce TD entièrement avec celle-ci.\nCependant, pour une personne débutante en Git, l’utilisation d’une\ninterface graphique peut constituer un élément important pour\nla compréhension et l’adoption de Git. Une fois à l’aise avec\nGit, on peut tout à fait se passer des interfaces graphiques\npour les routines quotidiennes et ne les utiliser que\npour certaines opérations où elles s’avèrent fort pratiques\n(notamment la comparaison de deux fichiers avant de devoir fusionner)."
  },
  {
    "objectID": "content/git/exogit.html#rappels-sur-la-notion-de-dépôt-distant",
    "href": "content/git/exogit.html#rappels-sur-la-notion-de-dépôt-distant",
    "title": "Un cadavre exquis pour découvrir Git",
    "section": "Rappels sur la notion de dépôt distant",
    "text": "Rappels sur la notion de dépôt distant\nComme expliqué dans le chapitre précédent,\nil convient de distinguer\nle dépôt distant (remote) et la copie ou les copies locales (les clones)\nd’un dépôt. Le dépôt distant est généralement stocké sur une forge\nlogicielle (Github ou Gitlab) et sert à centraliser la version\ncollective d’un projet. Les copies locales sont des copies de travail.\nGit est un système de contrôle de version asynchrone, c’est-à-dire\nqu’on n’interagit pas en continu avec le dépôt distant (comme c’est le\ncas dans le système SVN) mais qu’il est possible d’avoir une version\nlocale qui se différencie du dépôt commun et qu’on rend cohérente\nde temps en temps.\nBien qu’il soit possible d’avoir une utilisation hors-ligne de Git,\nc’est-à-dire un pur contrôle de version local sans dépôt\ndistant, cela est une utilisation\nrare et qui comporte un intérêt limité. L’intérêt de Git est\nd’offrir une manière robuste et efficace d’interagir avec un\ndépôt distant facilitant ainsi la collaboration en équipe ou en\nsolitaire.\nPour ces exercices, il est proposé\nd’utiliser Github, la forge la plus visible.\nL’avantage de Github par rapport à son principal concurrent, Gitlab,\nest que le premier est plus visible, car\nmieux indexé par Google et concentre, en partie pour des raisons historiques, plus\nde développeurs Python et R (ce qui est important dans des domaines comme\nle code où les externalités de réseau jouent)."
  },
  {
    "objectID": "content/git/exogit.html#première-étape-créer-un-compte-github",
    "href": "content/git/exogit.html#première-étape-créer-un-compte-github",
    "title": "Un cadavre exquis pour découvrir Git",
    "section": "Première étape: créer un compte Github",
    "text": "Première étape: créer un compte Github\nLes deux premières étapes se font sur Github.\n\n\n Exercice 1 : Créer un compte Github\n\nSi vous n’en avez pas déjà un, créer un compte sur https://github.com\nCréer un dépôt en suivant les consignes ci-dessous.\n\n\nCréez ce dépôt privé, cela permettra\ndans l’exercice 2 d’activer notre jeton. Vous pourrez le rendre public\naprès l’exercice 2, c’est comme vous le souhaitez.\nCréer ce dépôt avec un README.md en cliquant sur la case Add a README file\nAjouter un .gitignore en sélectionnant le modèle Python\n\nConnexion sur https://github.com &gt; + (en haut de la page) &gt; New repository &gt; Renseigner le “Repository name” &gt; Cocher “private” &gt; “Create repository”"
  },
  {
    "objectID": "content/git/exogit.html#deuxième-étape-créer-un-token-jeton-https",
    "href": "content/git/exogit.html#deuxième-étape-créer-un-token-jeton-https",
    "title": "Un cadavre exquis pour découvrir Git",
    "section": "Deuxième étape: créer un token (jeton) HTTPS",
    "text": "Deuxième étape: créer un token (jeton) HTTPS"
  },
  {
    "objectID": "content/git/exogit.html#principe",
    "href": "content/git/exogit.html#principe",
    "title": "Un cadavre exquis pour découvrir Git",
    "section": "Principe",
    "text": "Principe\nGit est un système décentralisé de contrôle de version :\nles codes sont modifiés par chaque personne sur son poste de travail,\npuis sont mis en conformité avec la version collective disponible\nsur le dépôt distant au moment où le contributeur le décide.\nIl est donc nécessaire que la forge connaisse l’identité de chacun des\ncontributeurs, afin de déterminer qui est l’auteur d’une modification apportée\naux codes stockés dans le dépôt distant.\nPour que Github reconnaisse un utilisateur proposant des modifications,\nil est nécessaire de s’authentifier (un dépôt distant, même public, ne peut pas être modifié par n’importe qui). L’authentification consiste ainsi à fournir un élément que seul vous et la forge êtes censés connaître : un mot de passe, une clé compliquée, un jeton d’accès…\nPlus précisément, il existe deux modalités pour faire connaître son identité à Github :\n\nune authentification HTTPS (décrite ici) : l’authentification se fait avec un login et un mot de passe ou avec un token (un mot de passe compliqué généré automatiquement par Github et connu exclusivement du détenteur du compte Github) ;\nune authentification SSH : l’authentification se fait par une clé cryptée disponible sur le poste de travail et que GitHub ou GitLab connaît. Une fois configurée, cette méthode ne nécessite plus de faire connaître son identité : l’empreinte digitale que constitue la clé suffit à reconnaître un utilisateur.\n\nLa documentation collaborative utilitR présente les raisons pour lesquelles il convient de favoriser\nla méthode HTTPS sur la méthode SSH.\n\n\n Note\nDepuis Août 2021, Github n’autorise plus l’authentification par mot de passe\nlorsqu’on interagit (pull/push) avec un dépôt distant\n(raisons ici).\nIl est nécessaire d’utiliser un token (jeton d’accès) qui présente l’avantage\nd’être révoquable (on peut à tout moment supprimer un jeton si, par exemple,\non suspecte qu’il a été diffusé par erreur) et à droits limités\n(le jeton permet certaines opérations standards mais\nn’autorise pas certaines opérations déterminantes comme la suppression\nd’un dépôt).\nÀ partir de mars 2023 et jusqu’à la fin de 2023, GitHub commencera progressivement à exiger que tous les utilisateurs de GitHub activent une ou plusieurs formes d’authentification à deux facteurs (2FA). Pour plus d’informations sur le déploiement de l’inscription 2FA, consultez cet article de blog. Concrètement, cela signifie que vous devrez au choix :\n\nRenseigner votre numéro de portable pour valider certaines connexions grâce à un code que vous recevrez par sms ;\nInstaller une application d’authentification (Ex : Microsoft Authenticator) installée sur votre téléphone qui génèrera un QR code que vous pourrez scanner depuis github, ce qui ne nécessite pas que vous ayez à fournir votre numéro de téléphone\nUtiliser une clef USB de sécurité\n\nPour choisir entre ces différentes options, vous pouvez vous rendre sur Settings &gt; Password and authentication &gt; Enable two-factor authentication.\n\n\n\n\n Note\nIl est important de ne jamais stocker un token, et encore moins son mot de passe, dans un projet.\nIl est possible de stocker un mot de passe ou token de manière sécurisée et durable\navec le credential helper de Git. Celui-ci est présenté par la suite.\nS’il n’est pas possible d’utiliser le credential helper de Git, un mot de passe\nou token peut être stocké de manière sécurisé dans\nun système de gestion de mot de passe comme Keepass.\nNe jamais stocker un jeton Github, ou pire un mot de passe, dans un fichier\ntexte non crypté. Les logiciels de gestion de mot de passe\n(comme Keepass, recommandé par l’Anssi)\nsont simples\nd’usage et permettent de ne conserver sur l’ordinateur qu’une version\nhashée du mot de passe qui ne peut être décryptée qu’avec un mot de passe\nconnu de vous seuls."
  },
  {
    "objectID": "content/git/exogit.html#créer-un-jeton",
    "href": "content/git/exogit.html#créer-un-jeton",
    "title": "Un cadavre exquis pour découvrir Git",
    "section": "Créer un jeton",
    "text": "Créer un jeton\nLa documentation officielle comporte un certain nombre de captures d’écran expliquant\ncomment procéder.\nNous allons utiliser le credential helper associé à Git pour stocker\nce jeton. Ce credential helper permet de conserver de manière pérenne\nun jeton (on peut aussi faire en sorte que le mot de passe soit automatiquement\nsupprimé de la mémoire de l’ordinateur au bout, par\nexemple, d’une heure).\nL’inconvénient de cette méthode est que Git écrit en clair le jeton dans\nun fichier de configuration. C’est pour cette raison qu’on utilise des jetons\npuisque, si ces derniers sont révélés, on peut toujours les révoquer et éviter\nles problèmes (pour ne pas stocker en clair un jeton il faudrait utiliser\nune librairie supplémentaire comme libsecrets qui est au-delà du programme\nde ce cours).\nSi vous désirez conserver de manière plus durable ou plus sécurisée votre jeton\n(en ne conservant pas le jeton en clair mais de manière hashée),\nest d’utiliser un gestionnaire de mot de passe comme\nKeepass (recommandé par l’Anssi). Néanmoins,\nil est recommandé de tout de même fixer une date d’expéritation\naux jetons pour limiter les risques de sécurité d’un token qui fuite\nsans s’en rendre compte.\n\n\n Exercice 2.0 : Créer un service sur le SSPCloud\nEn amont de l’exercice 2, pour les utilisateurs\ndu SSPCloud,\nil est recommandé d’ouvrir un service Jupyter\nen suivant les consignes suivantes:\n\nDans la page Mes services, cliquer sur le bouton Nouveau service ;\nChoisir Jupyter-Python ;\nCliquer sur Configuration Jupyter-Python. ⚠️ ne pas lancer le service\ntout de suite !\nFaire défiler les onglets pour arriver sur l’onglet Git ;\nRemplacer la valeur sous Cache par un nombre important,\npar exemple 36000 pour que le jeton que vous utiliserez soit\nvalable 10 heures ;\nLancer le service.\n\n\n\n\n\n Exercice 2 : Créer et stocker un token\n1️⃣ Suivre la\ndocumentation officielle en ne donnant que les droits repo au jeton (ajouter les droits\nworkflow si vous désirez que votre jeton soit utilisable pour des projets\noù l’intégration continue est nécessaire).\nPour résumer les étapes devraient être les suivantes :\nSettings &gt; Developers Settings &gt; Personal Access Token &gt; Generate a new token &gt; “My bash script” &gt; Expiration “30 days” &gt; cocher juste “repo” &gt; Generate token &gt; Le copier\n2️⃣ Ouvrir un terminal depuis Jupyter (par exemple File &gt; New &gt; Terminal) ou VSCode (Terminal &gt; New Terminal).\n3️⃣ [Optionnel] Taper dans le terminal la commande\nqui convient selon votre système d’exploitation pour activer le\ncredential helper:\n# Sous mac et linux et le datalab\ngit config --global credential.helper store\n\n# Sous windows\ngit config --global credential.helper manager-core\n4️⃣ Récupérer, sur la page d’accueil de votre dépôt, l’url du dépôt distant.\nIl prend la forme suivante\nhttps://github.com/&lt;username&gt;/&lt;reponame&gt;.git\nVous pouvez utiliser l’icone à droite pour copier l’url.\n5️⃣ Retournez dans le Terminal. Taper\ngit clone repo_url\noù repo_url est l’url du dépôt en question (vous pouvez utiliser\nMAJ+Inser pour coller l’url précédemment copié)\nTapez Entrée. Dans le cas d’un répertoire privé et sans credential helper, renseignez ensuite votre identifiant, faites Entrée, puis votre personal access token, Entrée. Si vous n’avez pas d’erreur, cela signifie\nque l’authentification a bien fonctionné et donc que tout va\nbien. Sinon, il vous suffit de réécrire l’instruction git clone et de retenter de taper votre personal access token. Normalement, si vous avez créé un dépôt vide dans l’exercice 1,\nvous avez un message de Git:\n\nwarning: You appear to have cloned an empty repository.\n\nCe n’est pas une erreur mais il est préférable de suivre la\nconsigne de l’exercice 1 et de créer un projet non vide.\nLe dossier de votre projet a bien\nété créé.\nSi vous avez une erreur, suivez la consigne présentée ci-après\npour réinitialiser\nvotre credential helper\n6️⃣ Si vous le désirez, vous pouvez changer la visibilité de votre dépôt\nen le rendant public.\n7️⃣ Stocker le token sur le SSP Cloud (ou un gestionnaire de mot de passe) :\n\nMon Compte -&gt; Services externes -&gt; Jeton d’accès personnel GitHub\n\n\n\n\n\n Note\nSi vous avez fait une faute de frappe dans le mot de passe ou dans le jeton, il est possible de vider la mémoire\nde la manière suivante, sous Mac ou Linux :\ngit config --global --unset credential.helper\nSous Windows, si vous avez utilisé l’option manager-core évoquée ci-dessus, vous pouvez utiliser une interface graphique pour effacer le mot de passe ou jeton erroné. Pour cela, dans le menu démarrer, taper Gestionnaire d'identification (ou Credential Manager si Windows ne trouve pas). Dans l’interface graphique qui s’ouvre, il est possible de supprimer le mot de passe ou jeton en question. Après cela, vous devriez à nouveau avoir l’opportunité de taper un mot de passe ou jeton lors d’une authentification HTTPS."
  },
  {
    "objectID": "content/git/exogit.html#envoyer-des-modifications-sur-le-dépôt-distant-push",
    "href": "content/git/exogit.html#envoyer-des-modifications-sur-le-dépôt-distant-push",
    "title": "Un cadavre exquis pour découvrir Git",
    "section": "Envoyer des modifications sur le dépôt distant: push",
    "text": "Envoyer des modifications sur le dépôt distant: push\n\n\n Exercice 6 : Interagir avec Github\nIl convient maintenant d’envoyer les fichiers sur le dépôt distant.\n\n1️⃣\nL’objectif est d’envoyer vos modifications vers origin.\nOn va passer par la ligne de commande car les boutons push/pull\nde l’extension Jupyter ne fonctionnent pas de manière systématique.\nTaper\ngit push origin main\nCela signifie: “git envoie (push) mes modifications sur la\nbranche main (la branche sur laquelle on a travaillé, on reviendra\ndessus) vers mon dépôt (alias\norigin)”\nRemarque : Si vous obtenez l’erreur suivante error: src refspec hello does not match any, c’est probablement que vous avez indiqué le mauvais nom de branche. La confusion se fait souvent entre le nom main ou master (ancienne norme de branche par défaut).\nNormalement, si vous avez utilisé le credential helper, Git ne\nvous demande pas vos identifiants de connexion. Sinon,\nil faut taper\nvotre identifiant Github et votre mot de passe correspond au personal access token nouvellement créé !\n2️⃣ Retournez voir le dépôt sur Github, vous devriez maintenant voir le fichier\n.gitignore s’afficher en page d’accueil."
  },
  {
    "objectID": "content/git/exogit.html#la-fonctionnalité-pull",
    "href": "content/git/exogit.html#la-fonctionnalité-pull",
    "title": "Un cadavre exquis pour découvrir Git",
    "section": "La fonctionnalité pull",
    "text": "La fonctionnalité pull\nLa deuxième manière d’interagir avec le dépôt est de récupérer des\nrésultats disponibles en ligne sur sa copie de travail. On appelle\ncela pull.\nPour le moment, vous êtes tout seul sur le dépôt. Il n’y a donc pas de\npartenaire pour modifier un fichier dans le dépôt distant. On va simuler ce\ncas en utilisant l’interface graphique de Github pour modifier\ndes fichiers. On rapatriera les résultats en local dans un deuxième temps.\n\n\n Exercice 7 : Rapatrier des modifs en local\n1️⃣ Se rendre sur votre dépôt depuis l’interface https://github.com\n\nSe placer sur le fichier README.md et cliquer sur le bouton Edit this file, qui prend la forme d’un icône de crayon.\n\n2️⃣ L’objectif est de\ndonner au README.md un titre en ajoutant, au début du document, la ligne suivante :\n# Mon oeuvre d'art surréaliste \nSautez une ligne et entrez le texte que vous désirez, sans ponctuation. Par exemple,\nle chêne un jour dit au roseau\n3️⃣ Cliquez sur l’onglet Preview pour voir le texte mis en forme au format Markdown\n4️⃣ Rédiger un titre et un message complémentaire pour faire le commit. Conserver\nl’option par défaut Commit directly to the main branch\n5️⃣ Editer à nouveau le README en cliquant sur le crayon juste au dessus\nde l’affichage du contenu du README.\nAjouter une deuxième phrase et corrigez la\nponctuation de la première. Ecrire un message de commit et valider.\nLe Chêne un jour dit au roseau :\nVous avez bien sujet d'accuser la Nature\n6️⃣ Au dessus de l’aborescence des fichiers, vous devriez voir s’afficher le\ntitre du dernier commit. Vous pouvez cliquer dessus pour voir la modification\nque vous avez faite.\n7️⃣ Les résultats sont sur le dépôt distant mais ne sont pas sur votre\ndossier de travail dans Jupyter ou VSCode. Il faut re-synchroniser votre copie locale\navec le dépôt distant :\n\nSur VSCode, cliquez simplement sur ... &gt; Pull à côté du bouton qui permet\nde visualiser le graphe Git.\nAvec l’interface Jupyter, si cela est possible, appuyez tout simplement sur la petite\nflèche vers le bas, qui est celle qui a désormais la pastille orange.\nSi cette flèche n’est pas disponible ou si vous travaillez dans un autre\nenvironnement, vous pouvez utiliser la ligne de\ncommande et taper\n\ngit pull origin main\nCela signifie : “git récupère (pull) les modifications sur la\nbranche main vers mon dépôt (alias\norigin)”\n8️⃣ Regarder à nouveau l’historique des commits. Cliquez sur le\ndernier commit et affichez les changements sur le fichier. Vous pouvez\nremarquer la finesse du contrôle de version : Git détecte au sein de\nla première ligne de votre texte que vous avez mis des majuscules\nou de la ponctuation.\n\n\nL’opération pull permet :\n\nA votre système local de vérifier les modifications sur le dépôt distant\nque vous n’auriez pas faites (cette opération s’appelle fetch)\nDe les fusionner s’il n’y a pas de conflit de version ou si les conflits de\nversion sont automatiquement fusionnables (deux modifications d’un fichier mais\nqui ne portent pas sur le même emplacement)."
  },
  {
    "objectID": "content/git/exogit.html#le-workflow-adopté",
    "href": "content/git/exogit.html#le-workflow-adopté",
    "title": "Un cadavre exquis pour découvrir Git",
    "section": "Le workflow adopté",
    "text": "Le workflow adopté\nNous allons adopter le mode de travail le plus simple, le Github Flow.\nIl correspond à cette forme caractéristique d’arbre:\n\nLa branche main constitue le tronc\nLes branches partent de main et divergent\nLorsque les modifications aboutissent, elles sont intégrées à main ;\nla branche en question disparaît :\n\n\n\n\n\n\nIl existe des workflows plus complexes, notamment le Git Flow que j’utilise\npour développer ce cours. Ce tutoriel, très bien fait,\nillustre avec un graphique la complexité accrue de ce flow :\n\n\n\n\n\nCette fois, une branche intermédiaire, par exemple une branche development\nintègre des modifications à tester avant de les intégrer dans la version\nofficielle (main).\n\n\n Hint\nVous pourrez trouvez des dizaines d’articles et d’ouvrages sur ce sujet dont chacun prétend avoir trouvé la meilleure organisation du travail (Git flow, GitHub flow, GitLab flow…). Ne lisez pas trop ces livres et articles sinon vous serez perdus (un peu comme avec les magazines destinés aux jeunes parents…).\nLa méthode de travail la plus simple est le Github flow qu’on vous a proposé d’adopter. L’arborescence est reconnaissable : des branches divergent et reviennent systématiquement vers main.\nPour des projets plus complexes dans des équipes développant des applications, on pourra utiliser d’autres méthodes de travail, notamment le Git flow. Il n’existe pas de règles universelles pour déterminer la méthode de travail ; l’important c’est, avant tout, de se mettre d’accord sur des règles communes de travail avec votre équipe."
  },
  {
    "objectID": "content/git/exogit.html#méthode-pour-les-merges",
    "href": "content/git/exogit.html#méthode-pour-les-merges",
    "title": "Un cadavre exquis pour découvrir Git",
    "section": "Méthode pour les merges",
    "text": "Méthode pour les merges\nLes merges vers main doivent impérativement passer par Github (ou Gitlab). Cela permet de garder une trace explicite de ceux-ci (par exemple ici), sans avoir à chercher dans l’arborescence, parfois complexe, d’un projet.\nLa bonne pratique veut qu’on fasse un squash commit pour éviter une inflation du nombre de commits dans main: les branches ont vocation à proposer une multitude de petits commits, les modifications dans main doivent être simples à tracer d’où le fait de modifier des petits bouts de code.\nComme on l’a fait dans un exercice précédent, il est très pratique d’ajouter dans le corps du message close #xx où xx est le numéro d’une issue associée à la pull request. Lorsque la pull request sera fusionnée, l’issue sera automatiquement fermée et un lien sera créé entre l’issue et la pull request. Cela vous permettra de comprendre, plusieurs mois ou années plus tard comment et pourquoi telle ou telle fonctionnalité a été implémentée.\nEn revanche, l’intégration des dernières modifications de main vers une branche se fait en local. Si votre branche est en conflit, le conflit doit être résolu dans la branche et pas dans main.\nmain doit toujours rester propre."
  },
  {
    "objectID": "content/git/exogit.html#mise-en-pratique",
    "href": "content/git/exogit.html#mise-en-pratique",
    "title": "Un cadavre exquis pour découvrir Git",
    "section": "Mise en pratique",
    "text": "Mise en pratique\n\n\n Exercice 9 : Interactions avec le dépôt distant\nCet exercice se fait par groupe de trois ou quatre. Il y aura deux rôles dans ce scénario :\n\nUne personne aura la responsabilité d’être mainteneur\nDeux à trois personnes seront développeurs.\n\n1️⃣ Le mainteneur crée un dépôt sur Github. Il/Elle donne des droits au(x) développeur(s) du projet (Settings &gt; Manage Access &gt; Invite a collaborator).\n2️⃣ Chaque membre du projet, crée une copie locale du projet grâce à la commande git clone ou\navec le bouton Clone a repository de JupyterLab.\nPour cela, récupérer l’url HTTPS du dépôt en copiant l’url du dépôt que vous pouvez trouver, par exemple, dans la page d’accueil du dépôt, en dessous de Quick setup — if you’ve done this kind of thing before\nEn ligne de commande, cela donnera :\ngit clone https://github.com/&lt;username&gt;/&lt;reponame&gt;.git\n3️⃣ Chaque membre du projet crée un fichier avec son nom et son prénom, selon cette structure nom-prenom.md en évitant les caractères spéciaux. Il écrit dedans trois phrases de son choix sans ponctuation ni majuscules (pour pouvoir effectuer une correction ultérieurement). Enfin, il commit sur le projet.\nPour rappel, en ligne de commande cela donnera les commandes suivantes à modifier\ngit add nom-prenom.md\ngit commit -m \"C'est l'histoire de XXXXX\"\n4️⃣ Chacun essaie d’envoyer (push) ses modifications locales sur le dépôt:\ngit push origin main\n5️⃣ A ce stade, une seule personne (la plus rapide) devrait ne pas avoir rencontré de rejet du push. C’est normal, avant d’accepter une modification Git vérifie en premier lieu la cohérence de la branche avec le dépôt distant. Le premier ayant fait un push a modifié le dépôt commun ; les autres doivent intégrer ces modifications dans leur version locale (pull) avant d’avoir le droit de proposer un changement.\nPour celui/celle/ceux dont le push a été refusé, faire\ngit pull origin main\npour ramener les modifications distantes en local.\n6️⃣ Taper git log et regarder la manière dont a été intégré la modification de votre camarade ayant pu faire son push\nVous remarquerez que les commits de vos camarades sont intégrés tels quels à\nl’histoire du dépôt.\n7️⃣ Faire à nouveau\ngit pull origin main\nLe dernier doit refaire, à nouveau, les étapes 5 à 7 (dans une équipe de quatre\nil faudra encore le refaire une fois).\n\n\n\n\n Warning à nouveau: ne JAMAIS FAIRE git push force\nQuand on fait face à un rejet du push, on est tenté de faire passer en force le push malgré la mise en garde précédente.\nIl faut immédiatement oublier cette solution, elle crée de nombreux problèmes et, en fait, ne résout rien. L’un des risques est de réécrire entièrement l’historique rendant les copies locales, et donc les modifications de vos collaborateurs, caduques. Cela vous vaudra, à raison, des remontrances de vos partenaires qui perdent le bénéfice de leur historique Git qui, s’ils ont des versions sans push depuis longtemps peuvent avoir diverger fortement du dépôt maître.\n\n\n\n\n Exercice 10 : Gérer les conflits quand on travaille sur le même fichier\nDans la continuité de l’exercice précédent, chaque personne va travailler sur les fichiers des autres membres de l’équipe.\n1️⃣ Les deux ou trois développeurs ajoutent la ponctuation et les majuscules du fichier du premier développeur.\n2️⃣ Ils sautent une ligne et ajoutent une phrase (pas tous la même).\n3️⃣ Valider les résultats (git add . et commit) et faire un push\n4️⃣ La personne la plus rapide n’a, normalement, rencontré aucune difficulté (elle peut s’arrêter temporairement pour regarder ce qui va se passer chez les voisins). Les autres voient leur push refusé et doivent faire un pull.\n💥 Il y a conflit, ce qui doit être signalé par un message du type :\nAuto-merging XXXXXX\nCONFLICT (content): Merge conflict in XXXXXX.md\nAutomatic merge failed; fix conflicts and then commit the result.\n5️⃣ Etudier le résultat de git status\n6️⃣ Si vous ouvrez les fichiers incriminés, vous devriez voir des balises du type\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nthis is some content to mess with\ncontent to append\n=======\ntotally different content to merge later\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; new_branch_to_merge_later\n7️⃣ Corriger à la main les fichiers en choisissant, pour chaque ligne, la version qui vous convient et en retirant les balises. Valider en faisant:\ngit add . && git commit -m \"Résolution du conflit par XXXX\"\nRemplacer XXXX par votre nom. La balise && permet d’enchaîner, en une seule ligne de code, les deux commandes.\n8️⃣ Faire un push. Pour la dernière personne, refaire les opérations 4 à 8\n\n\nGit permet donc de travailler, en même temps, sur le même fichier et de limiter le nombre de gestes manuels nécessaires pour faire la fusion. Lorsqu’on travaille sur des bouts différents du même fichier, on n’a même pas besoin de faire de modification manuelle, la fusion peut être automatique.\nGit est un outil très puissant. Mais, il ne remplace pas une bonne organisation du travail. Vous l’avez vu, ce mode de travail uniquement sur main peut être pénible. Les branches prennent tout leur sens dans ce cas.\n\n\n Exercice 11 : Gestion des branches\n1️⃣ Le mainteneur va contribuer directement dans main et ne crée pas de branche. Chaque développeur crée une branche, en local nommée contrib-XXXXX où XXXXX est le prénom:\ngit checkout -b contrib-XXXXX\n2️⃣ Chaque membre du groupe crée un fichier README.md où il écrit une phrase sujet-verbe-complément. Le mainteneur est le seul à ajouter un titre dans le README (qu’il commit dans main).\n3️⃣ Chacun push le produit de son subconscient sur le dépôt.\n4️⃣ Les développeurs ouvrent, chacun, une pull request sur Github de leur branche vers main. Ils lui donnent un titre explicite.\n5️⃣ Dans la discussion de chaque pull request, le mainteneur demande au développeur d’intégrer le titre qu’il a écrit.\n6️⃣ Chaque développeur, en local, intègre cette modification en faisant\n# Pour être sûr d'être sur sa propre branche\ngit checkout branche-XXXX\ngit merge main\nRégler le conflit et valider (add et commit). Pousser le résultat. Le mainteneur choisit une des pull request et la valide avec l’option squash commits. Vérifier sur la page d’accueil le résultat.\n7️⃣ L’auteur (si 2 développeurs) ou les deux auteurs (si 3 développeurs) de la pull request non validée doivent à nouveau répéter l’opération 6.\n8️⃣ Une fois le conflit de version réglé et poussé, le mainteneur valide la pull request selon la même procédure que précédemment.\n9️⃣ Vérifier l’arborescence du dépôt dans Insights &gt; Network. Votre arbre doit avoir une forme caractéristique de ce qu’on appelle le Github flow:"
  },
  {
    "objectID": "content/git/exogit.html#footnotes",
    "href": "content/git/exogit.html#footnotes",
    "title": "Un cadavre exquis pour découvrir Git",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCe cas de figure arrive lorsqu’on contribue à des projets\nsur lesquels on n’a pas de droit d’écriture. Il est alors\nnécessaire d’effectuer un fork, une copie de ce dépôt sur laquelle\non dispose de droits.\nDans ce cas de figure, on rencontre généralement un nouvel alias à côté d’origin.\nnommé upstream (cf.\nle tutoriel Github pour mettre à jour un fork\net qui pointe vers le dépôt source à l’origine du fork.\nLa création du bouton Fetch upstream par Github facilite grandement\nla mise en cohérence d’upstream et origin et constitue la méthode\nrecommandée.↩︎\nLa commande checkout est un couteau-suisse de la gestion de branche en Git. Elle permet en effet de basculer d’une branche à l’autre, mais aussi d’en créer, etc.↩︎"
  },
  {
    "objectID": "content/git/index.html",
    "href": "content/git/index.html",
    "title": "Git: un outil nécessaire pour les data scientists",
    "section": "",
    "text": "Cette partie du site présente un élément qui n’est pas propre à\nPython mais qui est néanmoins indispensable : la pratique de Git.\nUne grande partie du contenu de la partie provient\nd’un cours dédié fait avec Romain Avouac.\nLe chapitre de présentation de Git propose\nune introduction visant à présenter l’intérêt d’utiliser\ncet outil. Une mise en pratique est proposée\navec un cadavre exquis."
  },
  {
    "objectID": "content/git/index.html#utilisation-de-git-avec-python",
    "href": "content/git/index.html#utilisation-de-git-avec-python",
    "title": "Git: un outil nécessaire pour les data scientists",
    "section": "Utilisation de Git avec Python",
    "text": "Utilisation de Git avec Python\nGit est à la fois un outil et un langage. Il\nest donc nécessaire d’installer, dans un premier\ntemps Git Bash, puis de connecter\nson outil préféré pour faire du Python (qu’il\ns’agisse de Jupyter, VSCode ou PyCharm).\nCette partie est structurée en 2 temps :\n\nPrésentation de Git\nExercice pour découvrir Git"
  },
  {
    "objectID": "content/modern-ds/elastic_approfondissement.html",
    "href": "content/modern-ds/elastic_approfondissement.html",
    "title": "Approfondissement ElasticSearch pour des recherches de proximité géographique",
    "section": "",
    "text": "Pour essayer les exemples présents dans ce tutoriel :\nCe chapitre est issu du travail produit\ndans le cadre d’un hackathon de l’Insee avec\nRaphaële Adjerad\net présente quelques éléments qui peuvent être utiles\npour l’enrichissement de données d’entreprises\nà partir d’un répertoire officiel.\n:warning: Il nécessite une version particulière du package elasticsearch pour tenir compte de l’héritage de la version 7 du moteur Elastic. Pour cela, faire"
  },
  {
    "objectID": "content/modern-ds/elastic_approfondissement.html#objectif",
    "href": "content/modern-ds/elastic_approfondissement.html#objectif",
    "title": "Approfondissement ElasticSearch pour des recherches de proximité géographique",
    "section": "Objectif",
    "text": "Objectif\nCe chapitre vise à approfondir les éléments présentés sur Elastic précédemment. L’idée\nest de se placer dans un contexte opérationnel où on reçoit des informations\nsur des entreprises telles que l’adresse et la localisation et qu’on\ndésire associer à des données administratives considérées plus fliables."
  },
  {
    "objectID": "content/modern-ds/elastic_approfondissement.html#réplication-de-ce-chapitre",
    "href": "content/modern-ds/elastic_approfondissement.html#réplication-de-ce-chapitre",
    "title": "Approfondissement ElasticSearch pour des recherches de proximité géographique",
    "section": "Réplication de ce chapitre",
    "text": "Réplication de ce chapitre\nComme le précédent, ce chapitre est plus exigeant en termes d’infrastructures que les précédents.\nIl nécessite un serveur Elastic. Les utilisateurs du\nSSP Cloud pourront répliquer les exemples de ce cours\ncar cette technologie est disponible (que ce soit pour indexer une base ou\npour requêter une base existante).\nLa première partie de ce tutoriel, qui consiste à créer une base Sirene géolocalisée\nà partir des données open-data ne nécessite pas d’architecture particulière et\npeut ainsi être exécutée en utilisant les packages suivants :\n\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "content/modern-ds/elastic_approfondissement.html#sources",
    "href": "content/modern-ds/elastic_approfondissement.html#sources",
    "title": "Approfondissement ElasticSearch pour des recherches de proximité géographique",
    "section": "Sources",
    "text": "Sources\nCe chapitre va utiliser plusieurs sources de diffusion de\nl’Insee:\n\nLe stock des établissements présents dans les données de diffusion Sirene ;\nLes données Sirene géolocalisées\n\nLes données à siretiser sont celles du registre Français des émissions polluantes\nétabli par le Ministère de la Transition Energétique. Le jeu de données\nest disponible sur data.gouv"
  },
  {
    "objectID": "content/modern-ds/elastic_approfondissement.html#importer-la-base-déjà-créée",
    "href": "content/modern-ds/elastic_approfondissement.html#importer-la-base-déjà-créée",
    "title": "Approfondissement ElasticSearch pour des recherches de proximité géographique",
    "section": "Importer la base déjà créée",
    "text": "Importer la base déjà créée\nLes données à utiliser pour constuire une base Sirene géolocalisée\nsont trop volumineuses pour les serveurs mis à disposition\ngratuitement par Github pour la compilation de ce site web.\nNous proposons ainsi une version déjà construite, stockée\ndans l’espace de mise à disposition du SSP Cloud. Ce fichier est\nau format parquet et est ouvert à\ntous, même pour les personnes ne disposant pas d’un compte.\nLe code ayant construit cette base est présenté ci-dessous.\nPour importer cette base, on utilise les fonctionalités\nde pyarrow qui permettent d’importer un fichier sur\nun système de stockage cloud comme s’il était\nprésent sur le disque :\n\nfrom pyarrow import fs\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\nbucket = \"lgaliana\"\npath = \"diffusion/sirene_geolocalized.parquet\"\n\ns3 = fs.S3FileSystem(endpoint_override=\"http://\"+\"minio.lab.sspcloud.fr\")\n\ndf_geolocalized = pq.ParquetDataset(f'{bucket}/{path}', filesystem=s3).read_pandas().to_pandas()\ndf_geolocalized.head(3)"
  },
  {
    "objectID": "content/modern-ds/elastic_approfondissement.html#reproduire-la-construction-de-la-base",
    "href": "content/modern-ds/elastic_approfondissement.html#reproduire-la-construction-de-la-base",
    "title": "Approfondissement ElasticSearch pour des recherches de proximité géographique",
    "section": "Reproduire la construction de la base",
    "text": "Reproduire la construction de la base\nLa première base d’entrée à utiliser est disponible sur\ndata.gouv\n\nimport requests\nimport zipfile\n\nurl_download = \"https://www.data.gouv.fr/fr/datasets/r/0651fb76-bcf3-4f6a-a38d-bc04fa708576\"\nreq = requests.get(url_download)\n\nwith open(\"sirene.zip\",'wb') as f:\n  f.write(req.content)\n\nwith zipfile.ZipFile(\"sirene.zip\", 'r') as zip_ref:\n  zip_ref.extractall(\"sirene\")\n\nOn va importer seulement les colonnes utiles et simplifier la structure\npour être en mesure de ne garder que les informations qui nous\nintéressent (nom de l’entreprise, adresse, commune, code postal…)\n\nimport pandas as pd\nimport numpy as np\n\nlist_cols = [\n  'siren', 'siret',\n  'activitePrincipaleRegistreMetiersEtablissement',\n  'complementAdresseEtablissement',\n  'numeroVoieEtablissement',\n  'typeVoieEtablissement',\n  'libelleVoieEtablissement',\n  'codePostalEtablissement',\n  'libelleCommuneEtablissement',\n  'codeCommuneEtablissement',\n  'etatAdministratifEtablissement',\n  'denominationUsuelleEtablissement',\n  'activitePrincipaleEtablissement'\n]\n\ndf = pd.read_csv(\n  \"sirene/StockEtablissement_utf8.csv\",\n  usecols = list_cols)\n\ndf['numero'] = df['numeroVoieEtablissement']\\\n  .replace('-', np.NaN).str.split().str[0]\\\n  .str.extract('(\\d+)', expand=False)\\\n  .fillna(\"0\").astype(int)\n\ndf['numero'] = df['numero'].astype(str).replace(\"0\",\"\")\n\ndf['adresse'] = df['numero'] + \" \" + \\\n  df['typeVoieEtablissement'] + \" \" + \\\n  df['libelleVoieEtablissement']\n\ndf['adresse'] = df['adresse'].replace(np.nan, \"\")\n\ndf = df.loc[df['etatAdministratifEtablissement'] == \"A\"]\n\ndf.rename(\n  {\"denominationUsuelleEtablissement\": \"denom\",\n  \"libelleCommuneEtablissement\": \"commune\",\n  \"codeCommuneEtablissement\" : \"code_commune\",\n  \"codePostalEtablissement\": \"code_postal\"},\n  axis = \"columns\", inplace = True)\n\ndf['ape'] = df['activitePrincipaleEtablissement'].str.replace(\"\\.\", \"\", regex = True)\ndf['denom'] = df[\"denom\"].replace(np.nan, \"\")\n\ndf_siret = df.loc[:, ['siren', 'siret','adresse', 'ape', 'denom', 'commune', 'code_commune','code_postal']]\ndf_siret['code_postal'] = df_siret['code_postal'].replace(np.nan, \"0\").astype(int).astype(str).replace(\"0\",\"\")\n\nOn importe ensuite les données géolocalisées\n\nimport zipfile\nimport shutil\nimport os\n\n#os.remove(\"sirene.zip\")\n#shutil.rmtree('sirene/')\n\nurl_geoloc = \"https://files.data.gouv.fr/insee-sirene-geo/GeolocalisationEtablissement_Sirene_pour_etudes_statistiques_utf8.zip\"\nr = requests.get(url_geoloc)  \n\nwith open('geoloc.zip', 'wb') as f:\n    f.write(r.content)\n\nwith zipfile.ZipFile(\"geoloc.zip\", 'r') as zip_ref:\n  zip_ref.extractall(\"geoloc\")\n\ndf_geoloc = pd.read_csv(\n  \"geoloc/GeolocalisationEtablissement_Sirene_pour_etudes_statistiques_utf8.csv\",\n  usecols = [\"siret\", \"epsg\", \"x_longitude\", \"y_latitude\"] , sep = \";\")\n\nIl ne reste plus qu’à associer les deux jeux de données\n\ndf_geolocalized = df_siret.merge(df_geoloc, on = \"siret\") \ndf_geolocalized['code_commune'] = df_geolocalized['code_commune'].astype(str) \n\nSi vous avez accès à un espace de stockage cloud de type\nS3, il est possible d’utiliser pyarrow pour enregister\ncette base. Afin de l’enregistrer dans un espace de stockage\npublic, nous allons l’enregistrer dans un dossier diffusion1\n\nfrom pyarrow import fs\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\nbucket = \"lgaliana\"\npath = \"diffusion/sirene_geolocalized.parquet\"\n\ns3 = fs.S3FileSystem(endpoint_override=\"http://\"+\"minio.lab.sspcloud.fr\")\n\ntable = pa.Table.from_pandas(df_geolocalized)"
  },
  {
    "objectID": "content/modern-ds/elastic_approfondissement.html#définition-du-mapping",
    "href": "content/modern-ds/elastic_approfondissement.html#définition-du-mapping",
    "title": "Approfondissement ElasticSearch pour des recherches de proximité géographique",
    "section": "Définition du mapping",
    "text": "Définition du mapping\nOn va procéder par étape en essayant d’utiliser la structure la plus simple\npossible.\n:one: On s’occupe d’abord de définir le mapping\npour les variables textuelles.\n\nstring_var = [\"adresse\", \"denom\", \"ape\", \"commune\"]\nmap_string = {\"type\": \"text\", \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}}}\nmapping_string = {l: map_string for l in string_var}\n\n:two: Les variables catégorielles sont utilisées\npar le biais du type keyword:\n\n# keywords\nkeyword_var = [\"siren\",\"siret\",\"code_commune\",\"code_postal\"]\nmap_keywords = {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}\nmapping_keywords = {l: map_keywords for l in keyword_var}\n\n:three: La nouveauté par rapport à la partie\nprécédente est l’utilisation de la\ndimension géographique. Elastic propose\nle type geo_point pour cela.\n\n# geoloc\nmapping_geoloc = {\n  \"location\": {\n    \"type\": \"geo_point\"\n    }\n}    \n\nOn collecte tout cela ensemble dans un\ndictionnaire:\n\n# mapping\nmapping_elastic = {\"mappings\":\n  {\"properties\":\n    {**mapping_string, **mapping_geoloc, **mapping_keywords}\n  }\n}\n\nIl est tout à fait possible de définir un mapping\nplus raffiné. Ici, on va privilégier\nl’utilisation d’un mapping simple pour\nillustrer la recherche par distance\ngéographique en priorité."
  },
  {
    "objectID": "content/modern-ds/elastic_approfondissement.html#création-de-lindex",
    "href": "content/modern-ds/elastic_approfondissement.html#création-de-lindex",
    "title": "Approfondissement ElasticSearch pour des recherches de proximité géographique",
    "section": "Création de l’index",
    "text": "Création de l’index\nPour créer le nouvel index, on s’assure d’abord de ne pas\ndéjà l’avoir créé et on passe le mapping défini\nprécédemment.\n\nif es.indices.exists('sirene'):\n    es.indices.delete('sirene')\n\nes.indices.create(index = \"sirene\", body = mapping_elastic)"
  },
  {
    "objectID": "content/modern-ds/elastic_approfondissement.html#indexation-de-la-base-géolocalisée",
    "href": "content/modern-ds/elastic_approfondissement.html#indexation-de-la-base-géolocalisée",
    "title": "Approfondissement ElasticSearch pour des recherches de proximité géographique",
    "section": "Indexation de la base géolocalisée",
    "text": "Indexation de la base géolocalisée\nPour le moment, l’index est vide. Il convient de\nle peupler.\nIl est néanmoins nécessaire de créer le champ location\nau format attendu par elastic: lat, lon à partir\nde nos colonnes.\n\ndf_geolocalized['location'] = df_geolocalized['y_latitude'].astype(str) + \", \" + df_geolocalized['x_longitude'].astype(str)\n\nLa fonction suivante permet de structurer chaque\nligne du DataFrame telle qu’Elastic l’attend:\n\ndef gen_dict_from_pandas(index_name, df):\n    '''\n    Lit un dataframe pandas Open Food Facts, renvoi un itérable = dictionnaire des données à indexer, sous l'index fourni\n    '''\n    for i, row in df.iterrows():\n        header= {\"_op_type\": \"index\",\"_index\": index_name,\"_id\": i}\n        yield {**header,**row}\n\nEnfin, on peut industrialiser l’indexation\nde notre DataFrame en faisant tourner de\nmanière successive cette fonction:\n\nfrom elasticsearch.helpers import bulk, parallel_bulk\nfrom collections import deque\ndeque(parallel_bulk(client=es, actions=gen_dict_from_pandas(\"sirene\", df_geolocalized), chunk_size = 1000, thread_count = 4))\n\n\nes.count(index = 'sirene')\n\nObjectApiResponse({'count': 13059694, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}})"
  },
  {
    "objectID": "content/modern-ds/elastic_approfondissement.html#premier-exemple-de-requête-géographique",
    "href": "content/modern-ds/elastic_approfondissement.html#premier-exemple-de-requête-géographique",
    "title": "Approfondissement ElasticSearch pour des recherches de proximité géographique",
    "section": "Premier exemple de requête géographique",
    "text": "Premier exemple de requête géographique\n\nex1 = es.search(index = 'sirene', body = '''{\n  \"query\": {\n    \"bool\": {\n      \"must\":\n      { \"match\": { \"denom\":   \"institut national de la statistique\"}}\n      }\n  }\n}\n''')['hits']['hits']\n\necho_insee = pd.json_normalize(ex1)\necho_insee\n\nOn remarque déjà que les intitulés ne sont\npas bons. Quand est-il de leurs localisations ?\n\nplot_folium_sirene(\n  echo_insee, yvar = \"_source.y_latitude\",\n  xvar = \"_source.x_longitude\")\n\nCe premier essai nous suggère qu’il est\nnécessaire d’améliorer notre recherche.\nPlusieurs voies sont possibles:\n\nAméliorer le preprocessing de nos champs\ntextuels en excluant, par exemple, les\nstopwords ;\nEffectuer une restriction géographique\npour mieux cibler l’ensemble de recherche\nTrouver une variable catégorielle jouant\nle rôle de variable de blocage2 pour\nmieux cibler les paires pertinentes\n\nConcernant la restriction\ngéographique, Elastic fournit une approche\ntrès efficace de ciblage géographique.\nEn connaissant une position approximative\nde l’entreprise à rechercher,\nil est ainsi possible de\nrechercher dans un rayon\nd’une taille plus ou moins grande.\nEn supposant qu’on connaisse précisément\nla localisation de l’Insee, on peut\nchercher dans un rayon relativement\nrestreint. Si notre position était plus\napproximative, on pourrait rechercher\ndans un rayon plus large.\n\nex2 = es.search(index = 'sirene', body = '''{\n  \"query\": {\n    \"bool\": {\n      \"must\":\n      { \"match\": { \"denom\":   \"institut national de la statistique\"}}\n      ,\n      \"filter\":\n        {\"geo_distance\": {\n          \"distance\": \"1km\",\n          \"location\": {\n            \"lat\": \"48.8168\",\n            \"lon\": \"2.3099\"\n          }\n        }\n      }\n    }\n  }\n}\n''')['hits']['hits']\n\necho_insee = pd.json_normalize(ex2)\necho_insee\n\n\n\n Hint\nConnaître la localisation précise d’une\nentreprise\nnécessite déjà une bonne remontée\nd’information sur celle-ci.\nIl est plus plausible de supposer\nqu’on dispose, dans une phase amont\nde la chaine de production,\nde l’adresse de celle-ci.\nDans ce cas, il est utile\nd’utiliser un service de géocodage,\ncomme l’API Adresse\ndéveloppée par Etalab.\n\n\nLes résultats sont par construction mieux\nciblés. Néanmoins ils sont toujours décevants\npuisqu’on ne parvient pas à identifier l’Insee\ndans les dix meilleurs échos.\n\nspecificsearch = es.search(index = 'sirus_2020', body = \n'''{\n  \"query\": {\n    \"bool\": {\n      \"should\":\n          { \"match\": { \"rs_denom\":   \"CPCU - CENTRALE DE BERCY\"}},\n      \"filter\": [\n          {\"geo_distance\": {\n                  \"distance\": \"0.5km\",\n                  \"location\": {\n                        \"lat\": \"48.84329\", \n                        \"lon\": \"2.37396\"\n                              }\n                            }\n            }, \n            { \"prefix\":  { \"apet\": \"3530\" }}\n                ]\n            }\n          }\n}'''\n)"
  },
  {
    "objectID": "content/modern-ds/elastic_approfondissement.html#footnotes",
    "href": "content/modern-ds/elastic_approfondissement.html#footnotes",
    "title": "Approfondissement ElasticSearch pour des recherches de proximité géographique",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nConcernant la première piste, il aurait\nfallu mieux définir notre mapping pour\nautoriser des analyzers. A défaut,\nnous pourrons\nutiliser nltk ou spacy pour transformer\nles champs textuels avant d’envoyer la requête.\nCette solution présente l’inconvénient\nde ne pas formatter de la même manière l’ensemble\nindexé mais pourrait malgré tout améliorer la pertinence\ndes recherches.↩︎\nConcernant la première piste, il aurait\nfallu mieux définir notre mapping pour\nautoriser des analyzers. A défaut,\nnous pourrons\nutiliser nltk ou spacy pour transformer\nles champs textuels avant d’envoyer la requête.\nCette solution présente l’inconvénient\nde ne pas formatter de la même manière l’ensemble\nindexé mais pourrait malgré tout améliorer la pertinence\ndes recherches.↩︎"
  },
  {
    "objectID": "content/modern-ds/dallE.html",
    "href": "content/modern-ds/dallE.html",
    "title": "Génération d’images avec Python, DALL-E et StableDiffusion",
    "section": "",
    "text": "Pour tester les exemples présentés dans ce chapitre:\nNote\nL’utilisation de ce tutoriel est assez exigeante en termes d’infrastructure\ncar il est nécessaire de disposer de GPU.\nLes GPU sont des ressources rares et assez chères, elle ne sont donc pas mises à disposition de façon\naussi aisées que les CPU dans les cloud providers. Il s’agit de plus\nde ressources plus polluantes que les CPU.\nDes GPU sont disponibles sur Google Colab, la procédure pour les activer\nest expliquée ci-dessous. Des GPU sont également disponibles sur le SSPCloud\nmais sont à utiliser avec parcimonie. Elles ne sont pas mises à disposition\npar défaut car il s’agit d’une ressource rare. Ce chapitre, lancé depuis le\nbouton en début de chapitre, active cette option pour permettre la réplication\ndes exemples.\nHint\nPar défaut, Colab n’utilise pas de GPU mais de la CPU. Il est donc nécessaire\nd’éditer les paramètres d’exécution du Notebook\n\nDans le menu Exécution, cliquer sur Modifier le type d'exécution ;\nSélectionner GPU sous Accélérateur matériel."
  },
  {
    "objectID": "content/modern-ds/dallE.html#installation-de-pytorch",
    "href": "content/modern-ds/dallE.html#installation-de-pytorch",
    "title": "Génération d’images avec Python, DALL-E et StableDiffusion",
    "section": "Installation de PyTorch",
    "text": "Installation de PyTorch\nPour installer PyTorch, la librairie de Deep Learning\ndéveloppée par Meta, il suffit de suivre les recommandations\nsur le site web officiel.\nDans un Notebook, cela prendra la forme suivante :\n\n!conda install mamba\n!mamba install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch\n\n\n\n Note\nJe propose ici d’utiliser mamba pour accélérer l’installation.\nDes éléments sur mamba sont disponibles dans l’introduction de ce cours."
  },
  {
    "objectID": "content/modern-ds/dallE.html#accès-à-huggingface",
    "href": "content/modern-ds/dallE.html#accès-à-huggingface",
    "title": "Génération d’images avec Python, DALL-E et StableDiffusion",
    "section": "Accès à HuggingFace",
    "text": "Accès à HuggingFace\nLa question - non négligeable - de l’accès à\nde la GPU mise à part,\nla réutilisation des modèles de Stable Diffusion est\ntrès facile car la documentation mise à disposition sur\nHuggingFace est très bien faite.\nLa première étape est de se créer un compte sur HuggingFace\net se créer un token3. Ce token sera donné à l’API\nde HuggingFace pour s’authentifier.\nL’API d’HuggingFace nécessite l’installation du\npackage diffusers.\nDans un Notebook, le code suivant permet d’installer la librairie\nrequise:\n\n!pip install --upgrade diffusers transformers scipy accelerate\n\n\n\n Note\nOn va supposer que le token est stocké dans une variable\nd’environnement HF_PAT. Cela évite d’écrire le token\ndans un Notebook qu’on va\npotentiellement partager, alors que le token\nest un élément à garder secret. Pour l’importer\ndans la session Python:\nSi vous n’avez pas la possibilité de rentrer le token dans les variables\nd’environnement, créez une cellule qui crée la variable\nHF_TOKEN et supprimez là de suite pour ne pas l’oublier avant\nde partager votre token.\n\n\n\nimport os\nHF_TOKEN = os.getenv('HF_PAT')"
  },
  {
    "objectID": "content/modern-ds/dallE.html#footnotes",
    "href": "content/modern-ds/dallE.html#footnotes",
    "title": "Génération d’images avec Python, DALL-E et StableDiffusion",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIl est notamment possible de réutiliser l’image générée à des fins commerciales. En revanche, il est interdit de chercher à nuire à une personne. Pour cette raison, il est fréquent que les visages de personnes célèbres soient floutés pour éviter la création de contenu nuisant à leur réputation.↩︎\nComme les autres plateformes du monde de la data science,\nHuggingFace a adopté l’utilisation standardisée des\njetons (token) comme méthode d’authentification. Le jeton est\ncomme un mot de passe sauf qu’il n’est pas inventé par l’utilisateur\n(ce qui permet qu’il ne soit pas partagé avec d’autres sites web potentiellement\nmoins sécurisés), est révocable (date d’expiration ou choix de l’utilisateur)\net dispose de droits moins importants qu’un\nmot de passe qui vous permet, potentiellement,\nde changer tous les paramètres de votre compte. Je recommande vivement l’utilisation\nd’un gestionnaire de mot de passe pour\nstocker vos token (si vous utilisez Git, Docker, etc.\nvous en avez potentiellement beaucoup) plutôt que\nstocker ces jetons dans des fichiers non sécurisés.↩︎\nComme les autres plateformes du monde de la data science,\nHuggingFace a adopté l’utilisation standardisée des\njetons (token) comme méthode d’authentification. Le jeton est\ncomme un mot de passe sauf qu’il n’est pas inventé par l’utilisateur\n(ce qui permet qu’il ne soit pas partagé avec d’autres sites web potentiellement\nmoins sécurisés), est révocable (date d’expiration ou choix de l’utilisateur)\net dispose de droits moins importants qu’un\nmot de passe qui vous permet, potentiellement,\nde changer tous les paramètres de votre compte. Je recommande vivement l’utilisation\nd’un gestionnaire de mot de passe pour\nstocker vos token (si vous utilisez Git, Docker, etc.\nvous en avez potentiellement beaucoup) plutôt que\nstocker ces jetons dans des fichiers non sécurisés.↩︎"
  },
  {
    "objectID": "content/modern-ds/index.html",
    "href": "content/modern-ds/index.html",
    "title": "Partie 5: Introduction aux outils et méthodes à l’état de l’art",
    "section": "",
    "text": "Les parties précédentes étaient très tournées sur l’acquisition\nde compétences minimales dans chaque domaine de l’analyse de données.\nCette partie propose des éléments plus avancés mais plus représentatifs\ndu travail quotidien du data scientist. Cette partie\nprésente la manière dont Python peut être utilisé dans une architecture\nmoderne de type cloud. Elle illustre la manière dont Python peut\nservir de couteau-suisse faisant l’interface entre différents\nlangages plus efficaces ou plusieurs types de données.\nCette partie est en cours de construction et présentera les\néléments suivants :"
  },
  {
    "objectID": "content/modern-ds/index.html#contenu-de-la-partie",
    "href": "content/modern-ds/index.html#contenu-de-la-partie",
    "title": "Partie 5: Introduction aux outils et méthodes à l’état de l’art",
    "section": "Contenu de la partie",
    "text": "Contenu de la partie"
  },
  {
    "objectID": "content/NLP/04_word2vec.html",
    "href": "content/NLP/04_word2vec.html",
    "title": "Méthodes de vectorisation : comptages et word embeddings",
    "section": "",
    "text": "Cette page approfondit certains aspects présentés dans la\npartie introductive. Après avoir travaillé sur le\nComte de Monte Cristo, on va continuer notre exploration de la littérature\navec cette fois des auteurs anglophones:\nLes données sont disponibles ici : spooky.csv et peuvent être requétées via l’url\nhttps://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv.\nLe but va être dans un premier temps de regarder dans le détail les termes les plus fréquents utilisés par les auteurs, de les représenter graphiquement puis on va ensuite essayer de prédire quel texte correspond à quel auteur à partir de différents modèles de vectorisation, notamment les word embeddings.\nCe notebook est librement inspiré de :\nWarning\nComme dans la partie précédente, il faut télécharger quelques éléments\npour que nos librairies de NLP puissent fonctionner correctement.\nEn premier lieu, il convient d’installer les librairies adéquates\n(spacy, gensim et sentence_transformers):\n!pip install spacy gensim sentence_transformers\nEnsuite, comme nous allons utiliser également spacy, il convient de télécharger\nle corpus Anglais. Pour cela, on peut se référer à\nla documentation de spacy,\nextrêmement bien faite.\n\nIdéalement, il faut installer le module via la ligne de commande. Dans\nune cellule de notebook Jupyter, faire :\n\n!python -m spacy download en_core_web_sm\n\nSans accès à la ligne de commande (depuis une instance Docker par exemple),\nfaire :\n\nimport spacy\nspacy.cli.download(\"en_core_web_sm\")\n\nSinon, il est également possible d’installer le module en faisant pointer\npip install vers le fichier adéquat sur\nGithub. Pour cela, taper\n\npip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gensim\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n\nfrom gensim.models.word2vec import Word2Vec\nimport gensim.downloader\nfrom sentence_transformers import SentenceTransformer"
  },
  {
    "objectID": "content/NLP/04_word2vec.html#nettoyage-des-données",
    "href": "content/NLP/04_word2vec.html#nettoyage-des-données",
    "title": "Méthodes de vectorisation : comptages et word embeddings",
    "section": "Nettoyage des données",
    "text": "Nettoyage des données\nNous allons ainsi à nouveau utiliser le jeu de données spooky:\n\ndata_url = 'https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv'\nspooky_df = pd.read_csv(data_url)\n\nLe jeu de données met ainsi en regard un auteur avec une phrase qu’il a écrite:\n\nspooky_df.head()\n\n\nPreprocessing\nEn NLP, la première étape est souvent celle du preprocessing, qui inclut notamment les étapes de tokenization et de nettoyage du texte. Comme celles-ci ont été vues en détail dans le précédent chapitre, on se contentera ici d’un preprocessing minimaliste : suppression de la ponctuation et des stop words (pour la visualisation et les méthodes de vectorisation basées sur des comptages).\nJusqu’à présent, nous avons utilisé principalement nltk pour le\npreprocessing de données textuelles. Cette fois, nous proposons\nd’utiliser la librairie spaCy qui permet de mieux automatiser sous forme de\npipelines de preprocessing.\nPour initialiser le processus de nettoyage,\non va utiliser le corpus en_core_web_sm (voir plus\nhaut pour l’installation de ce corpus):\n\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n\nOn va utiliser un pipe spacy qui permet d’automatiser, et de paralléliser,\nun certain nombre d’opérations. Les pipes sont l’équivalent, en NLP, de\nnos pipelines scikit ou des pipes pandas. Il s’agit donc d’un outil\ntrès approprié pour industrialiser un certain nombre d’opérations de\npreprocessing :\n\ndef clean_docs(texts, remove_stopwords=False, n_process = 4):\n    \n    docs = nlp.pipe(texts, \n                    n_process=n_process,\n                    disable=['parser', 'ner',\n                             'lemmatizer', 'textcat'])\n    stopwords = nlp.Defaults.stop_words\n\n    docs_cleaned = []\n    for doc in docs:\n        tokens = [tok.text.lower().strip() for tok in doc if not tok.is_punct]\n        if remove_stopwords:\n            tokens = [tok for tok in tokens if tok not in stopwords]\n        doc_clean = ' '.join(tokens)\n        docs_cleaned.append(doc_clean)\n        \n    return docs_cleaned\n\nOn applique la fonction clean_docs à notre colonne pandas.\nLes pandas.Series étant itérables, elles se comportent comme des listes et\nfonctionnent ainsi très bien avec notre pipe spacy\n\nspooky_df['text_clean'] = clean_docs(spooky_df['text'])\n\n\nspooky_df.head()\n\n\n\nEncodage de la variable à prédire\nOn réalise un simple encodage de la variable à prédire :\nil y a trois catégories (auteurs), représentées par des entiers 0, 1 et 2.\nPour cela, on utilise le LabelEncoder de scikit déjà présenté\ndans la partie modélisation. On va utiliser la méthode\nfit_transform qui permet, en un tour de main, d’appliquer à la fois\nl’entraînement (fit), à savoir la création d’une correspondance entre valeurs\nnumériques et labels, et l’appliquer (transform) à la même colonne.\n\nle = LabelEncoder()\nspooky_df['author_encoded'] = le.fit_transform(spooky_df['author'])\n\nOn peut vérifier les classes de notre LabelEncoder :\n\nle.classes_\n\n\n\nConstruction des bases d’entraînement et de test\nOn met de côté un échantillon de test (20 %) avant toute analyse (même descriptive).\nCela permettra d’évaluer nos différents modèles toute à la fin de manière très rigoureuse,\npuisque ces données n’auront jamais utilisées pendant l’entraînement.\nNotre échantillon initial n’est pas équilibré (balanced) : on retrouve plus d’oeuvres de\ncertains auteurs que d’autres. Afin d’obtenir un modèle qui soit évalué au mieux, nous allons donc stratifier notre échantillon de manière à obtenir une répartition similaire d’auteurs dans nos\nensembles d’entraînement et de test.\n\nX_train, X_test, y_train, y_test = train_test_split(spooky_df['text_clean'].values, \n                                                    spooky_df['author_encoded'].values, \n                                                    test_size=0.2, \n                                                    random_state=33,\n                                                    stratify = spooky_df['author_encoded'].values)\n\nPar exemple, les textes d’EAP représentent 40 % des échantillons d’entraînement et de test :\n\nprint(100*y_train.tolist().count(0)/(len(y_train)))\nprint(100*y_test.tolist().count(0)/(len(y_test)))\n\nAperçu du premier élément de X_train :\n\nX_train[0]\n\nOn peut aussi vérifier qu’on est capable de retrouver\nla correspondance entre nos auteurs initiaux avec\nla méthode inverse_transform\n\nprint(y_train[0], le.inverse_transform([y_train[0]])[0])"
  },
  {
    "objectID": "content/NLP/04_word2vec.html#statistiques-exploratoires",
    "href": "content/NLP/04_word2vec.html#statistiques-exploratoires",
    "title": "Méthodes de vectorisation : comptages et word embeddings",
    "section": "Statistiques exploratoires",
    "text": "Statistiques exploratoires\n\nRépartition des labels\nRefaisons un graphique que nous avons déjà produit précédemment pour voir\nla répartition de notre corpus entre auteurs :\n\nfig = pd.Series(le.inverse_transform(y_train)).value_counts().plot(kind='bar')\nfig\n\nOn observe une petite asymétrie : les passages des livres d’Edgar Allen Poe sont plus nombreux que ceux des autres auteurs dans notre corpus d’entraînement, ce qui peut être problématique dans le cadre d’une tâche de classification.\nL’écart n’est pas dramatique, mais on essaiera d’en tenir compte dans l’analyse en choisissant une métrique d’évaluation pertinente.\n\n\nMots les plus fréquemment utilisés par chaque auteur\nOn va supprimer les stopwords pour réduire le bruit dans notre jeu\nde données.\n\n# Suppression des stop words\nX_train_no_sw = clean_docs(X_train, remove_stopwords=True)\nX_train_no_sw = np.array(X_train_no_sw)\n\nPour visualiser rapidement nos corpus, on peut utiliser la technique des\nnuages de mots déjà vue à plusieurs reprises.\nVous pouvez essayer de faire vous-même les nuages ci-dessous\nou cliquer sur la ligne ci-dessous pour afficher le code ayant\ngénéré les figures :\n\nCliquer pour afficher le code 👇\n\ndef plot_top_words(initials, ax, n_words=20):\n    # Calcul des mots les plus fréquemment utilisés par l'auteur\n    texts = X_train_no_sw[le.inverse_transform(y_train) == initials]\n    all_tokens = ' '.join(texts).split()\n    counts = Counter(all_tokens)\n    top_words = [word[0] for word in counts.most_common(n_words)]\n    top_words_counts = [word[1] for word in counts.most_common(n_words)]\n    \n    # Représentation sous forme de barplot\n    ax = sns.barplot(ax = ax, x=top_words, y=top_words_counts)\n    ax.set_title(f'Most Common Words used by {initials_to_author[initials]}')\n\n\ninitials_to_author = {\n    'EAP': 'Edgar Allen Poe',\n    'HPL': 'H.P. Lovecraft',\n    'MWS': 'Mary Wollstonecraft Shelley'\n}\n\nfig, axs = plt.subplots(3, 1, figsize = (12,12))\n\nplot_top_words('EAP', ax = axs[0])\nplot_top_words('HPL', ax = axs[1])\nplot_top_words('MWS', ax = axs[2])\n\n\n\nBeaucoup de mots se retrouvent très utilisés par les trois auteurs.\nIl y a cependant des différences notables : le mot “life”\nest le plus employé par MWS, alors qu’il n’apparaît pas dans les deux autres tops.\nDe même, le mot “old” est le plus utilisé par HPL\nlà où les deux autres ne l’utilisent pas de manière surreprésentée.\nIl semble donc qu’il y ait des particularités propres à chacun des auteurs\nen termes de vocabulaire,\nce qui laisse penser qu’il est envisageable de prédire les auteurs à partir\nde leurs textes dans une certaine mesure."
  },
  {
    "objectID": "content/NLP/04_word2vec.html#prédiction-sur-le-set-dentraînement",
    "href": "content/NLP/04_word2vec.html#prédiction-sur-le-set-dentraînement",
    "title": "Méthodes de vectorisation : comptages et word embeddings",
    "section": "Prédiction sur le set d’entraînement",
    "text": "Prédiction sur le set d’entraînement\nNous allons à présent vérifier cette conjecture en comparant\nplusieurs modèles de vectorisation,\ni.e. de transformation du texte en objets numériques\npour que l’information contenue soit exploitable dans un modèle de classification.\n\nDémarche\nComme nous nous intéressons plus à l’effet de la vectorisation qu’à la tâche de classification en elle-même,\nnous allons utiliser un algorithme de classification simple (un SVM linéaire), avec des paramètres non fine-tunés (c’est-à-dire des paramètres pas nécessairement choisis pour être les meilleurs de tous).\n\nclf = LinearSVC(max_iter=10000, C=0.1)\n\nCe modèle est connu pour être très performant sur les tâches de classification de texte, et nous fournira donc un bon modèle de référence (baseline). Cela nous permettra également de comparer de manière objective l’impact des méthodes de vectorisation sur la performance finale.\n\n\n\n\nPour les deux premières méthodes de vectorisation\n(basées sur des fréquences et fréquences relatives des mots),\non va simplement normaliser les données d’entrée, ce qui va permettre au SVM de converger plus rapidement, ces modèles étant sensibles aux différences d’échelle dans les données.\nOn va également fine-tuner via grid-search\ncertains hyperparamètres liés à ces méthodes de vectorisation :\n\non teste différents ranges de n-grams (unigrammes et unigrammes + bigrammes)\non teste avec et sans stop-words\n\nAfin d’éviter le surapprentissage,\non va évaluer les différents modèles via validation croisée, calculée sur 4 blocs.\nOn récupère à la fin le meilleur modèle selon une métrique spécifiée.\nOn choisit le score F1,\nmoyenne harmonique de la précision et du rappel,\nqui donne un poids équilibré aux deux métriques, tout en pénalisant fortement le cas où l’une des deux est faible.\nPrécisément, on retient le score F1 *micro-averaged* :\nles contributions des différentes classes à prédire sont agrégées,\npuis on calcule le score F1 sur ces données agrégées.\nL’avantage de ce choix est qu’il permet de tenir compte des différences\nde fréquences des différentes classes.\n\n\nPipeline de prédiction\nOn va utiliser un pipeline scikit ce qui va nous permettre d’avoir\nun code très concis pour effectuer cet ensemble de tâches cohérentes.\nDe plus, cela va nous assurer de gérer de manière cohérentes nos différentes\ntransformations (cf. partie sur les pipelines)\nPour se faciliter la vie, on définit une fonction fit_vectorizers qui\nintègre dans un pipeline générique une méthode d’estimation scikit\net fait de la validation croisée en cherchant le meilleur modèle\n(en excluant/incluant les stopwords et avec unigrammes/bigrammes)\n\ndef fit_vectorizers(vectorizer):\n    pipeline = Pipeline(\n    [\n        (\"vect\", vectorizer()),\n        (\"scaling\", StandardScaler(with_mean=False)),\n        (\"clf\", clf),\n    ]\n    )\n\n    parameters = {\n        \"vect__ngram_range\": ((1, 1), (1, 2)),  # unigrams or bigrams\n        \"vect__stop_words\": (\"english\", None)\n    }\n\n    grid_search = GridSearchCV(pipeline, parameters, scoring='f1_micro',\n                               cv=4, n_jobs=4, verbose=1)\n    grid_search.fit(X_train, y_train)\n\n    best_parameters = grid_search.best_estimator_.get_params()\n    for param_name in sorted(parameters.keys()):\n        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n\n    print(f\"CV scores {grid_search.cv_results_['mean_test_score']}\")\n    print(f\"Mean F1 {np.mean(grid_search.cv_results_['mean_test_score'])}\")\n    \n    return grid_search"
  },
  {
    "objectID": "content/NLP/04_word2vec.html#approche-bag-of-words",
    "href": "content/NLP/04_word2vec.html#approche-bag-of-words",
    "title": "Méthodes de vectorisation : comptages et word embeddings",
    "section": "Approche bag-of-words",
    "text": "Approche bag-of-words\nOn commence par une approche “bag-of-words”,\ni.e. qui revient simplement à représenter chaque document par un vecteur\nqui compte le nombre d’apparitions de chaque mot du vocabulaire dans le document.\n\ncv_bow = fit_vectorizers(CountVectorizer)"
  },
  {
    "objectID": "content/NLP/04_word2vec.html#tf-idf",
    "href": "content/NLP/04_word2vec.html#tf-idf",
    "title": "Méthodes de vectorisation : comptages et word embeddings",
    "section": "TF-IDF",
    "text": "TF-IDF\nOn s’intéresse ensuite à l’approche TF-IDF,\nqui permet de tenir compte des fréquences relatives des mots.\nAinsi, pour un mot donné, on va multiplier la fréquence d’apparition du mot dans le document (calculé comme dans la méthode précédente) par un terme qui pénalise une fréquence élevée du mot dans le corpus. L’image ci-dessous, empruntée à Chris Albon, illustre cette mesure:\n\nSource: Chris Albon\nLa vectorisation TF-IDF permet donc de limiter l’influence des stop-words\net donc de donner plus de poids aux mots les plus salients d’un document.\nOn observe clairement que la performance de classification est bien supérieure,\nce qui montre la pertinence de cette technique.\n\ncv_tfidf = fit_vectorizers(TfidfVectorizer)"
  },
  {
    "objectID": "content/NLP/04_word2vec.html#word2vec-avec-averaging",
    "href": "content/NLP/04_word2vec.html#word2vec-avec-averaging",
    "title": "Méthodes de vectorisation : comptages et word embeddings",
    "section": "Word2vec avec averaging",
    "text": "Word2vec avec averaging\nOn va maintenant explorer les techniques de vectorisation basées sur les\nembeddings de mots, et notamment la plus populaire : Word2Vec.\nL’idée derrière est simple, mais a révolutionné le NLP :\nau lieu de représenter les documents par des\nvecteurs sparse de très grande dimension (la taille du vocabulaire)\ncomme on l’a fait jusqu’à présent,\non va les représenter par des vecteurs dense (continus)\nde dimension réduite (en général, autour de 100-300).\nChacune de ces dimensions va représenter un facteur latent,\nc’est à dire une variable inobservée,\nde la même manière que les composantes principales produites par une ACP.\n\n\n\n\n\nSource: https://medium.com/@zafaralibagh6/simple-tutorial-on-word-embedding-and-word2vec-43d477624b6d\nPourquoi est-ce intéressant ?\nPour de nombreuses raisons, mais pour résumer :\ncela permet de beaucoup mieux capturer la similarité sémantique entre les documents.\nPar exemple, un humain sait qu’un document contenant le mot “Roi”\net un autre document contenant le mot “Reine” ont beaucoup de chance\nd’aborder des sujets semblables.\nPourtant, une vectorisation de type comptage ou TF-IDF\nne permet pas de saisir cette similarité :\nle calcul d’une mesure de similarité (norme euclidienne ou similarité cosinus)\nentre les deux vecteurs ne prendra en compte la similarité des deux concepts, puisque les mots utilisés sont différents.\nA l’inverse, un modèle word2vec bien entraîné va capter\nqu’il existe un facteur latent de type “royauté”,\net la similarité entre les vecteurs associés aux deux mots sera forte.\nLa magie va même plus loin : le modèle captera aussi qu’il existe un\nfacteur latent de type “genre”,\net va permettre de construire un espace sémantique dans lequel les\nrelations arithmétiques entre vecteurs ont du sens ;\npar exemple :\n\\[\\text{king} - \\text{man} + \\text{woman} ≈ \\text{queen}\\]\nComment ces modèles sont-ils entraînés ?\nVia une tâche de prédiction résolue par un réseau de neurones simple.\nL’idée fondamentale est que la signification d’un mot se comprend\nen regardant les mots qui apparaissent fréquemment dans son voisinage.\nPour un mot donné, on va donc essayer de prédire les mots\nqui apparaissent dans une fenêtre autour du mot cible.\nEn répétant cette tâche de nombreuses fois et sur un corpus suffisamment varié,\non obtient finalement des embeddings pour chaque mot du vocabulaire,\nqui présentent les propriétés discutées précédemment.\n\nX_train_tokens = [text.split() for text in X_train]\nw2v_model = Word2Vec(X_train_tokens, vector_size=200, window=5, \n                     min_count=1, workers=4)\n\n\nw2v_model.wv.most_similar(\"mother\")\n\nOn voit que les mots les plus similaires à “mother”\nsont souvent des mots liés à la famille, mais pas toujours.\nC’est lié à la taille très restreinte du corpus sur lequel on entraîne le modèle,\nqui ne permet pas de réaliser des associations toujours pertinentes.\nL’embedding (la représentation vectorielle) de chaque document correspond à la moyenne des word-embeddings des mots qui le composent :\n\ndef get_mean_vector(w2v_vectors, words):\n    words = [word for word in words if word in w2v_vectors]\n    if words:\n        avg_vector = np.mean(w2v_vectors[words], axis=0)\n    else:\n        avg_vector = np.zeros_like(w2v_vectors['hi'])\n    return avg_vector\n\ndef fit_w2v_avg(w2v_vectors):\n    X_train_vectors = np.array([get_mean_vector(w2v_vectors, words)\n                                for words in X_train_tokens])\n    \n    scores = cross_val_score(clf, X_train_vectors, y_train, \n                         cv=4, scoring='f1_micro', n_jobs=4)\n\n    print(f\"CV scores {scores}\")\n    print(f\"Mean F1 {np.mean(scores)}\")\n    return scores\n\n\ncv_w2vec = fit_w2v_avg(w2v_model.wv)\n\nLa performance chute fortement ;\nla faute à la taille très restreinte du corpus, comme annoncé précédemment."
  },
  {
    "objectID": "content/NLP/04_word2vec.html#word2vec-pré-entraîné-averaging",
    "href": "content/NLP/04_word2vec.html#word2vec-pré-entraîné-averaging",
    "title": "Méthodes de vectorisation : comptages et word embeddings",
    "section": "Word2vec pré-entraîné + averaging",
    "text": "Word2vec pré-entraîné + averaging\nQuand on travaille avec des corpus de taille restreinte,\nc’est généralement une mauvaise idée d’entraîner son propre modèle word2vec.\nHeureusement, des modèles pré-entraînés sur de très gros corpus sont disponibles.\nIls permettent de réaliser du transfer learning,\nc’est-à-dire de bénéficier de la performance d’un modèle qui a été entraîné sur une autre tâche ou bien sur un autre corpus.\nL’un des modèles les plus connus pour démarrer est le glove_model de\nGensim (Glove pour Global Vectors for Word Representation)1:\n\nGloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\nSource : https://nlp.stanford.edu/projects/glove/\n\nOn peut le charger directement grâce à l’instruction suivante :\n\nglove_model = gensim.downloader.load('glove-wiki-gigaword-200')\n\nPar exemple, la représentation vectorielle de roi est l’objet\nmultidimensionnel suivant :\n\nglove_model['king']\n\nComme elle est peu intelligible, on va plutôt rechercher les termes les\nplus similaires. Par exemple,\n\nglove_model.most_similar('mother')\n\nOn peut retrouver notre formule précédente\n\\[\\text{king} - \\text{man} + \\text{woman} ≈ \\text{queen}\\]\ndans ce plongement de mots:\n\nglove_model.most_similar(positive = ['king', 'woman'], negative = ['man'])\n\nVous pouvez vous référer à ce tutoriel\npour en découvrir plus sur Word2Vec.\nFaisons notre apprentissage par transfert :\n\ncv_w2vec_transfert = fit_w2v_avg(glove_model)\n\nLa performance remonte substantiellement.\nCela étant, on ne parvient pas à faire mieux que les approches basiques,\non arrive à peine aux performances de la vectorisation par comptage.\nEn effet, pour rappel, les performances sont les suivantes :\n\nperfs = pd.DataFrame(\n    [np.mean(cv_bow.cv_results_['mean_test_score']),\n     np.mean(cv_tfidf.cv_results_['mean_test_score']),\n    np.mean(cv_w2vec),\n    np.mean(cv_w2vec_transfert)],\n    index = ['Bag-of-Words','TF-IDF', 'Word2Vec non pré-entraîné', 'Word2Vec pré-entraîné'],\n    columns = [\"Mean F1 score\"]\n).sort_values(\"Mean F1 score\",ascending = False)\nperfs\n\nLes performences limitées du modèle Word2Vec sont cette fois certainement dues à la manière dont\nles word-embeddings sont exploités : ils sont moyennés pour décrire chaque document.\nCela a plusieurs limites :\n\non ne tient pas compte de l’ordre et donc du contexte des mots\nlorsque les documents sont longs, la moyennisation peut créer\ndes représentation bruitées."
  },
  {
    "objectID": "content/NLP/04_word2vec.html#contextual-embeddings",
    "href": "content/NLP/04_word2vec.html#contextual-embeddings",
    "title": "Méthodes de vectorisation : comptages et word embeddings",
    "section": "Contextual embeddings",
    "text": "Contextual embeddings\nLes embeddings contextuels visent à pallier les limites des embeddings\ntraditionnels évoquées précédemment.\nCette fois, les mots n’ont plus de représentation vectorielle fixe,\ncelle-ci est calculée dynamiquement en fonction des mots du voisinage, et ainsi de suite.\nCela permet de tenir compte de la structure des phrases\net de tenir compte du fait que le sens d’un mot est fortement dépendant des mots\nqui l’entourent.\nPar exemple, dans les expressions “le président Macron” et “le camembert Président” le mot président n’a pas du tout le même rôle.\nCes embeddings sont produits par des architectures très complexes,\nde type Transformer (BERT, etc.).\n\nmodel = SentenceTransformer('all-mpnet-base-v2')\n\n\nX_train_vectors = model.encode(X_train)\n\n\nscores = cross_val_score(clf, X_train_vectors, y_train, \n                         cv=4, scoring='f1_micro', n_jobs=4)\n\nprint(f\"CV scores {scores}\")\nprint(f\"Mean F1 {np.mean(scores)}\")\n\n\nperfs = pd.concat(\n  [perfs,\n  pd.DataFrame(\n    [np.mean(scores)],\n    index = ['Contextual Embedding'],\n    columns = [\"Mean F1 score\"])]\n).sort_values(\"Mean F1 score\",ascending = False)\nperfs\n\nVerdict : on fait très légèrement mieux que la vectorisation TF-IDF.\nOn voit donc l’importance de tenir compte du contexte.\nMais pourquoi, avec une méthode très compliquée, ne parvenons-nous pas à battre une méthode toute simple ?\nOn peut avancer plusieurs raisons :\n\nle TF-IDF est un modèle simple, mais toujours très performant\n(on parle de “tough-to-beat baseline”).\nla classification d’auteurs est une tâche très particulière et très ardue,\nqui ne fait pas justice aux embeddings. Comme on l’a dit précédemment, ces derniers se révèlent particulièrement pertinents lorsqu’il est question de similarité sémantique entre des textes (clustering, etc.).\n\nDans le cas de notre tâche de classification, il est probable que\ncertains mots (noms de personnage, noms de lieux) soient suffisants pour classifier de manière pertinente,\nce que ne permettent pas de capter les embeddings qui accordent à tous les mots la même importance."
  },
  {
    "objectID": "content/NLP/04_word2vec.html#aller-plus-loin",
    "href": "content/NLP/04_word2vec.html#aller-plus-loin",
    "title": "Méthodes de vectorisation : comptages et word embeddings",
    "section": "Aller plus loin",
    "text": "Aller plus loin\n\nNous avons entraîné différents modèles sur l’échantillon d’entraînement par validation croisée, mais nous n’avons toujours pas utilisé l’échantillon test que nous avons mis de côté au début. Réaliser la prédiction sur les données de test, et vérifier si l’on obtient le même classement des méthodes de vectorisation.\nFaire un vrai split train/test : faire l’entraînement avec des textes de certains auteurs, et faire la prédiction avec des textes d’auteurs différents. Cela permettrait de neutraliser la présence de noms de lieux, de personnages, etc.\nComparer avec d’autres algorithmes de classification qu’un SVM\n(Avancé) : fine-tuner le modèle d’embeddings contextuels sur la tâche de classification"
  },
  {
    "objectID": "content/NLP/04_word2vec.html#footnotes",
    "href": "content/NLP/04_word2vec.html#footnotes",
    "title": "Méthodes de vectorisation : comptages et word embeddings",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nJeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation.↩︎"
  },
  {
    "objectID": "content/NLP/02_exoclean.html",
    "href": "content/NLP/02_exoclean.html",
    "title": "Nettoyer un texte: des exercices pour découvrir l’approche bag-of-words",
    "section": "",
    "text": "Cette page approfondit certains aspects présentés dans la\npartie introductive. Après avoir travaillé sur le\nComte de Monte Cristo, on va continuer notre exploration de la littérature\navec cette fois des auteurs anglophones :\nLes données sont disponibles ici : spooky.csv et peuvent être requétées via l’url\nhttps://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv.\nLe but va être dans un premier temps de regarder dans le détail les termes les plus fréquemment utilisés par les auteurs, de les représenter graphiquement.\nOn prendra appui sur l’approche bag of words présentée dans le chapitre précédent1.\nCe notebook est librement inspiré de :\nLes chapitres suivants permettront d’introduire aux enjeux de modélisation\nde corpus textuels. Dans un premier temps, le modèle LDA permettra d’explorer\nle principe des modèles bayésiens à couche cachées pour modéliser les sujets (topics)\nprésents dans un corpus et segmenter ces topics selon les mots qui les composent.\nLe dernier chapitre de la partie visera à\nprédire quel texte correspond à quel auteur à partir d’un modèle Word2Vec.\nCela sera un pas supplémentaire dans la formalisation puisqu’il s’agira de\nreprésenter chaque mot d’un texte sous forme d’un vecteur de grande dimension, ce\nqui nous permettra de rapprocher les mots entre eux dans un espace complexe.\nCette technique, dite des plongements de mots (Word Embedding),\npermet ainsi de transformer une information complexe difficilement quantifiable\ncomme un mot\nen un objet numérique qui peut ainsi être rapproché d’autres par des méthodes\nalgébriques. Pour découvrir ce concept, ce post de blog\nest particulièrement utile. En pratique, la technique des\nplongements de mots permet d’obtenir des tableaux comme celui-ci:"
  },
  {
    "objectID": "content/NLP/02_exoclean.html#librairies-nécessaires",
    "href": "content/NLP/02_exoclean.html#librairies-nécessaires",
    "title": "Nettoyer un texte: des exercices pour découvrir l’approche bag-of-words",
    "section": "Librairies nécessaires",
    "text": "Librairies nécessaires\nCette page évoquera les principales librairies pour faire du NLP, notamment :\n\nWordCloud\nnltk\nSpaCy\nKeras\nTensorFlow\n\nIl faudra également installer les librairies gensim et pywaffle\n\n\n Hint\nComme dans la partie précédente, il faut télécharger quelques éléments pour que NTLK puisse fonctionner correctement. Pour cela, faire :\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('genesis')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\n\nLa liste des modules à importer est assez longue, la voici :\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport base64\nimport string\nimport re\nimport nltk\n\nfrom collections import Counter\nfrom time import time\n# from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords\nfrom sklearn.metrics import log_loss\nimport matplotlib.pyplot as plt\n#!pip install pywaffle\nfrom pywaffle import Waffle\n\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\n\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('genesis')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\n[nltk_data] Downloading package stopwords to /github/home/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /github/home/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package genesis to /github/home/nltk_data...\n[nltk_data]   Unzipping corpora/genesis.zip.\n[nltk_data] Downloading package wordnet to /github/home/nltk_data...\n[nltk_data] Downloading package omw-1.4 to /github/home/nltk_data...\n\n\nTrue"
  },
  {
    "objectID": "content/NLP/02_exoclean.html#données-utilisées",
    "href": "content/NLP/02_exoclean.html#données-utilisées",
    "title": "Nettoyer un texte: des exercices pour découvrir l’approche bag-of-words",
    "section": "Données utilisées",
    "text": "Données utilisées\n\n\n Exercice 1 : Importer les données spooky\nPour ceux qui ont envie de tester leurs connaissances en pandas\n\nImporter le jeu de données spooky à partir de l’URL https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv sous le nom train. L’encoding est latin-1\nMettre des majuscules au nom des colonnes.\nRetirer le prefix id de la colonne Id et appeler la nouvelle colonne ID.\nMettre l’ancienne colonne Id en index.\n\n\n\nSi vous ne faites pas l’exercice 1, pensez à charger les données en executant la fonction get_data.py :\n\nimport requests\n\nurl = 'https://raw.githubusercontent.com/linogaliana/python-datascientist/master/content/NLP/get_data.py'\nr = requests.get(url, allow_redirects=True)\nopen('getdata.py', 'wb').write(r.content)\n\nimport getdata\ntrain = getdata.create_train_dataframes()\n\nCe code introduit une base nommée train dans l’environnement.\nLe jeu de données met ainsi en regard un auteur avec une phrase qu’il a écrite :\n\ntrain.head()\n\n\n\n\n\n\n\n\nText\nAuthor\nID\n\n\nId\n\n\n\n\n\n\n\nid26305\nThis process, however, afforded me no means of...\nEAP\n26305\n\n\nid17569\nIt never once occurred to me that the fumbling...\nHPL\n17569\n\n\nid11008\nIn his left hand was a gold snuff box, from wh...\nEAP\n11008\n\n\nid27763\nHow lovely is spring As we looked from Windsor...\nMWS\n27763\n\n\nid12958\nFinding nothing else, not even gold, the Super...\nHPL\n12958\n\n\n\n\n\n\n\nOn peut se rendre compte que les extraits des 3 auteurs ne sont\npas forcément équilibrés dans le jeu de données.\nIl faudra en tenir compte dans la prédiction.\n\nfig = plt.figure()\ng = sns.barplot(x=['Edgar Allen Poe', 'Mary W. Shelley', 'H.P. Lovecraft'], y=train['Author'].value_counts())\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1765: FutureWarning:\n\nunique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n\n\n\n\n\n\n\n\n\n Note\nL’approche bag of words est présentée de\nmanière plus extensive dans le chapitre précédent.\nL’idée est d’étudier la fréquence des mots d’un document et la\nsurreprésentation des mots par rapport à un document de\nréférence (appelé corpus).\nCette approche un peu simpliste mais très\nefficace : on peut calculer des scores permettant par exemple de faire\nde classification automatique de document par thème, de comparer la\nsimilarité de deux documents. Elle est souvent utilisée en première analyse,\net elle reste la référence pour l’analyse de textes mal\nstructurés (tweets, dialogue tchat, etc.).\nLes analyses tf-idf (term frequency-inverse document frequency) ou les\nconstructions d’indices de similarité cosinus reposent sur ce type d’approche.\n\n\n\nFréquence d’un mot\nAvant de s’adonner à une analyse systématique du champ lexical de chaque\nauteur, on se focaliser dans un premier temps sur un unique mot, le mot fear.\n\n\n Note\nL’exercice ci-dessous présente une représentation graphique nommée\nwaffle chart. Il s’agit d’une approche préférable aux\ncamemberts qui sont des graphiques manipulables car l’oeil humain se laisse\nfacilement berner par cette représentation graphique qui ne respecte pas\nles proportions.\n\n\n\n\n Exercice 2 : Fréquence d'un mot\n\nCompter le nombre de phrases, pour chaque auteur, où apparaît le mot fear.\nUtiliser pywaffle pour obtenir les graphiques ci-dessous qui résument\nde manière synthétique le nombre d’occurrences du mot “fear” par auteur.\nRefaire l’analyse avec le mot “horror”.\n\n\n\nA l’issue de la question 1, vous devriez obtenir le tableau\nde fréquence suivant :\n\n\n\n\n\n\n\n\n\nText\nID\nwordtoplot\n\n\nAuthor\n\n\n\n\n\n\n\nEAP\nThis process, however, afforded me no means of...\n2630511008096741351519322166071718908441148621...\n70\n\n\nHPL\nIt never once occurred to me that the fumbling...\n1756912958197641888620836080752790708121117330...\n160\n\n\nMWS\nHow lovely is spring As we looked from Windsor...\n2776322965009121673712799131170076400683052582...\n211\n\n\n\n\n\n\n\nCeci permet d’obtenir le waffle chart suivant :\n\n\n\n\n\nFigure 2: Répartition du terme fear dans le corpus de nos trois auteurs\n\n\n\n\nOn remarque ainsi de manière très intuitive\nle déséquilibre de notre jeu de données\nlorsqu’on se focalise sur le terme “peur”\noù Mary Shelley représente près de 50%\ndes observations.\nSi on reproduit cette analyse avec le terme “horror”, on peut\nen conclure que la peur est plus évoquée par Mary Shelley\n(sentiment assez naturel face à la créature du docteur Frankenstein) alors\nque Lovecraft n’a pas volé sa réputation d’écrivain de l’horreur !\n\n\n\n\n\n\n\n\n\n\n\nPremier wordcloud\nPour aller plus loin dans l’analyse du champ lexical de chaque auteur,\non peut représenter un wordcloud qui permet d’afficher chaque mot avec une\ntaille proportionnelle au nombre d’occurrence de celui-ci.\n\n\n Exercice 3 : Wordcloud\n\nEn utilisant la fonction wordCloud, faire trois nuages de mot pour représenter les mots les plus utilisés par chaque auteur.\nCalculer les 25 mots plus communs pour chaque auteur et représenter les trois histogrammes des décomptes.\n\n\n\nLe wordcloud pour nos différents auteurs est le suivant :\n\n\n\n\n\n\n\n\n\nEnfin, si on fait un histogramme des fréquences,\ncela donnera :\n\n\n\n\n\n\n\n\n\nOn voit ici que ce sont des mots communs, comme “the”, “of”, etc. sont très\nprésents. Mais ils sont peu porteurs d’information, on peut donc les éliminer\navant de faire une analyse syntaxique poussée.\nCeci est une démonstration par l’exemple qu’il vaut mieux nettoyer le texte avant de\nl’analyser (sauf si on est intéressé\npar la loi de Zipf, cf. exercice suivant).\n\n\nAparté: la loi de Zipf\n\n\n La loi de Zipf\nDans son sens strict, la loi de Zipf prévoit que\ndans un texte donné, la fréquence d’occurrence \\(f(n_i)\\) d’un mot est\nliée à son rang \\(n_i\\) dans l’ordre des fréquences par une loi de la forme\n\\(f(n_i) = c/n_i\\) où \\(c\\) est une constante. Zipf, dans les années 1930, se basait sur l’oeuvre\nde Joyce, Ulysse pour cette affirmation.\nPlus généralement, on peut dériver la loi de Zipf d’une distribution exponentielle des fréquences : \\(f(n_i) = cn_{i}^{-k}\\). Cela permet d’utiliser la famille des modèles linéaires généralisés, notamment les régressions poissonniennes, pour mesurer les paramètres de la loi. Les modèles linéaire traditionnels en log souffrent en effet, dans ce contexte, de biais (la loi de Zipf est un cas particulier d’un modèle gravitaire, où appliquer des OLS est une mauvaise idée, cf. Galiana et al. (2020) pour les limites).\n\n\nUn modèle exponentiel peut se représenter par un modèle de Poisson ou, si\nles données sont très dispersées, par un modèle binomial négatif. Pour\nplus d’informations, consulter l’annexe de Galiana et al. (2020).\nLa technique économétrique associée pour l’estimation est\nles modèles linéaires généralisés (GLM) qu’on peut\nutiliser en Python via le\npackage statsmodels2:\n\\[\n\\mathbb{E}\\bigg( f(n_i)|n_i \\bigg) = \\exp(\\beta_0 + \\beta_1 \\log(n_i))\n\\]\nPrenons les résultats de l’exercice précédent et enrichissons les du rang et de la fréquence d’occurrence d’un mot :\n\ncount_words = pd.DataFrame({'counter' : train\n    .groupby('Author')\n    .apply(lambda s: ' '.join(s['Text']).split())\n    .apply(lambda s: Counter(s))\n    .apply(lambda s: s.most_common())\n    .explode()}\n)\ncount_words[['word','count']] = pd.DataFrame(count_words['counter'].tolist(), index=count_words.index)\ncount_words = count_words.reset_index()\n\ncount_words = count_words.assign(\n    tot_mots_auteur = lambda x: (x.groupby(\"Author\")['count'].transform('sum')),\n    freq = lambda x: x['count'] /  x['tot_mots_auteur'],\n    rank = lambda x: x.groupby(\"Author\")['count'].transform('rank', ascending = False)\n)\n\nCommençons par représenter la relation entre la fréquence et le rang:\nNous avons bien, graphiquement, une relation log-linéaire entre les deux:\n\ng.figure.get_figure()\n\n\n\n\n\n\n\n\nAvec statsmodels, vérifions plus formellement cette relation:\n\nimport statsmodels.api as sm\n\nexog = sm.add_constant(np.log(count_words['rank'].astype(float)))\n\nmodel = sm.GLM(count_words['freq'].astype(float), exog, family = sm.families.Poisson()).fit()\n\n# Afficher les résultats du modèle\nprint(model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                   freq   No. Observations:                69301\nModel:                            GLM   Df Residuals:                    69299\nModel Family:                 Poisson   Df Model:                            1\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -23.011\nDate:                Tue, 21 Nov 2023   Deviance:                     0.065676\nTime:                        15:27:18   Pearson chi2:                   0.0656\nNo. Iterations:                     5   Pseudo R-squ. (CS):          0.0002431\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.4388      1.089     -2.239      0.025      -4.574      -0.303\nrank          -0.9831      0.189     -5.196      0.000      -1.354      -0.612\n==============================================================================\n\n\nLe coefficient de la régression est presque 1 ce qui suggère bien une relation\nquasiment log-linéaire entre le rang et la fréquence d’occurrence d’un mot.\nDit autrement, le mot le plus utilisé l’est deux fois plus que le deuxième\nmois le plus fréquent qui l’est trois plus que le troisième, etc."
  },
  {
    "objectID": "content/NLP/02_exoclean.html#nettoyage-dun-texte",
    "href": "content/NLP/02_exoclean.html#nettoyage-dun-texte",
    "title": "Nettoyer un texte: des exercices pour découvrir l’approche bag-of-words",
    "section": "Nettoyage d’un texte",
    "text": "Nettoyage d’un texte\nLes premières étapes dans le nettoyage d’un texte, qu’on a\ndéveloppé au cours du chapitre précédent, sont :\n\nsuppression de la ponctuation\nsuppression des stopwords\n\nCela passe par la tokenisation d’un texte, c’est-à-dire la décomposition\nde celui-ci en unités lexicales (les tokens).\nCes unités lexicales peuvent être de différentes natures,\nselon l’analyse que l’on désire mener.\nIci, on va définir les tokens comme étant les mots utilisés.\nPlutôt que de faire soi-même ce travail de nettoyage,\navec des fonctions mal optimisées,\non peut utiliser la librairie nltk comme détaillé précédemment.\n\n\n Exercice 4 : Nettoyage du texte\nRepartir de train, notre jeu de données d’entraînement. Pour rappel, train a la structure suivante :\n\nTokeniser chaque phrase avec nltk.\nRetirer les stopwords avec nltk.\n\n\n\nPour rappel, au début de l’exercice, le DataFrame présente l’aspect suivant :\n\n\n\n\n\n\n\n\n\nText\nAuthor\nID\nwordtoplot\n\n\nId\n\n\n\n\n\n\n\n\nid26305\nThis process, however, afforded me no means of...\nEAP\n26305\n0\n\n\nid17569\nIt never once occurred to me that the fumbling...\nHPL\n17569\n0\n\n\n\n\n\n\n\nAprès tokenisation, il devrait avoir cet aspect :\n\n\nID     Author\n00001  MWS       [Idris, was, well, content, with, this, resolv...\n00002  HPL       [I, was, faint, even, fainter, than, the, hate...\ndtype: object\n\n\nAprès le retrait des stopwords, cela donnera :\n\n\n Hint\nLa méthode apply est très pratique ici car nous avons une phrase par ligne. Plutôt que de faire un DataFrame par auteur, ce qui n’est pas une approche très flexible, on peut directement appliquer la tokenisation\nsur notre DataFrame grâce à apply, sans le diviser.\n\n\nCe petit nettoyage permet d’arriver à un texte plus intéressant en termes d’analyse lexicale. Par exemple, si on reproduit l’analyse précédente… :\n\n\n\n\n\n\n\n\n\nPour aller plus loin dans l’harmonisation d’un texte, il est possible de\nmettre en place les classes d’équivalence développées dans la\npartie précédente afin de remplacer différentes variations d’un même\nmot par une forme canonique :\n\nla racinisation (stemming) assez fruste mais rapide, notamment\nen présence de fautes d’orthographe. Dans ce cas, chevaux peut devenir chev\nmais être ainsi confondu avec chevet ou cheveux.\nCette méthode est généralement plus simple à mettre en oeuvre, quoique\nplus fruste.\nla lemmatisation qui requiert la connaissance des statuts\ngrammaticaux (exemple : chevaux devient cheval).\nElle est mise en oeuvre, comme toujours avec nltk, à travers un\nmodèle. En l’occurrence, un WordNetLemmatizer (WordNet est une base\nlexicographique ouverte). Par exemple, les mots “women”, “daughters”\net “leaves” seront ainsi lemmatisés de la manière suivante :\n\n\nfrom nltk.stem import WordNetLemmatizer\nlemm = WordNetLemmatizer()\n\nfor word in [\"women\",\"daughters\", \"leaves\"]:\n    print(\"The lemmatized form of %s is: {}\".format(lemm.lemmatize(word)) % word)\n\nThe lemmatized form of women is: woman\nThe lemmatized form of daughters is: daughter\nThe lemmatized form of leaves is: leaf\n\n\n\n\n Note\nPour disposer du corpus nécessaire à la lemmatisation, il faut, la première fois,\ntélécharger celui-ci grâce aux commandes suivantes :\nimport nltk\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\n\nOn va se restreindre au corpus d’Edgar Allan Poe et repartir de la base de données\nbrute:\n\neap_clean = train[train[\"Author\"] == \"EAP\"]\neap_clean = ' '.join(eap_clean['Text'])\n#Tokenisation naïve sur les espaces entre les mots =&gt; on obtient une liste de mots\n#tokens = eap_clean.split()\nword_list = nltk.word_tokenize(eap_clean)\n\n\n\n Exercice 5 : Lemmatisation avec nltk\nUtiliser un WordNetLemmatizer et observer le résultat.\nOptionnel: Effectuer la même tâche avec spaCy\n\n\nLe WordNetLemmatizer donnera le résultat suivant :"
  },
  {
    "objectID": "content/NLP/02_exoclean.html#tf-idf-calcul-de-fréquence",
    "href": "content/NLP/02_exoclean.html#tf-idf-calcul-de-fréquence",
    "title": "Nettoyer un texte: des exercices pour découvrir l’approche bag-of-words",
    "section": "TF-IDF: calcul de fréquence",
    "text": "TF-IDF: calcul de fréquence\nLe calcul tf-idf (term frequency–inverse document frequency)\npermet de calculer un score de proximité entre un terme de recherche et un\ndocument (c’est ce que font les moteurs de recherche).\n\nLa partie tf calcule une fonction croissante de la fréquence du terme de recherche dans le document à l’étude ;\nLa partie idf calcule une fonction inversement proportionnelle à la fréquence du terme dans l’ensemble des documents (ou corpus).\n\nLe score total, obtenu en multipliant les deux composantes,\npermet ainsi de donner un score d’autant plus élevé que le terme est surréprésenté dans un document\n(par rapport à l’ensemble des documents).\nIl existe plusieurs fonctions, qui pénalisent plus ou moins les documents longs,\nou qui sont plus ou moins smooth.\n\n\n Exercice 6 : TF-IDF: calcul de fréquence\n\nUtiliser le vectoriseur TF-IdF de scikit-learn pour transformer notre corpus en une matrice document x terms. Au passage, utiliser l’option stop_words pour ne pas provoquer une inflation de la taille de la matrice. Nommer le modèle tfidf et le jeu entraîné tfs.\nAprès avoir construit la matrice de documents x terms avec le code suivant, rechercher les lignes où les termes ayant la structure abandon sont non-nuls.\nTrouver les 50 extraits où le score TF-IDF est le plus élevé et l’auteur associé. Vous devriez obtenir le classement suivant :\n\n\n\n\nfeature_names = tfidf.get_feature_names_out()\ncorpus_index = [n for n in list(tfidf.vocabulary_.keys())]\nimport pandas as pd\ndf = pd.DataFrame(tfs.todense(), columns=feature_names)\n\ndf.head()\n\n\n\n\n\n\n\n\naaem\nab\naback\nabaft\nabandon\nabandoned\nabandoning\nabandonment\nabaout\nabased\n...\nzodiacal\nzoilus\nzokkar\nzone\nzones\nzopyrus\nzorry\nzubmizzion\nzuro\ná¼\n\n\n\n\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n0.0\n0.0\n0.0\n0.0\n0.0\n0.253506\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 24937 columns\n\n\n\nLes lignes où les termes de abandon sont non nuls\nsont les suivantes :\n\n\nIndex([    4,   116,   215,   571,   839,  1042,  1052,  1069,  2247,  2317,\n        2505,  3023,  3058,  3245,  3380,  3764,  3886,  4425,  5289,  5576,\n        5694,  6812,  7500,  9013,  9021,  9077,  9560, 11229, 11395, 11451,\n       11588, 11827, 11989, 11998, 12122, 12158, 12189, 13666, 15259, 16516,\n       16524, 16759, 17547, 18019, 18072, 18126, 18204, 18251],\n      dtype='int64')\n\n\n\n\n\n\n\n\n\naaem\nab\naback\nabaft\nabandon\nabandoned\nabandoning\nabandonment\nabaout\nabased\n...\nzodiacal\nzoilus\nzokkar\nzone\nzones\nzopyrus\nzorry\nzubmizzion\nzuro\ná¼\n\n\n\n\n4\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.253506\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n116\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.339101\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n215\n0.0\n0.0\n0.0\n0.0\n0.235817\n0.000000\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n571\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.143788\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n839\n0.0\n0.0\n0.0\n0.0\n0.285886\n0.000000\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 24937 columns\n\n\n\n\n\nAuthor\nMWS    22\nHPL    15\nEAP    13\nName: Text, dtype: int64\n\n\nLes 10 scores les plus élevés sont les suivants :\n\nprint(train.iloc[list_fear[:9]]['Text'].values)\n\n['We could not fear we did not.' '\"And now I do not fear death.'\n 'Be of heart and fear nothing.' 'I smiled, for what had I to fear?'\n 'Indeed I had no fear on her account.'\n 'I have not the slightest fear for the result.'\n 'At length, in an abrupt manner she asked, \"Where is he?\" \"O, fear not,\" she continued, \"fear not that I should entertain hope Yet tell me, have you found him?'\n '\"I fear you are right there,\" said the Prefect.'\n 'I went down to open it with a light heart, for what had I now to fear?']\n\n\nOn remarque que les scores les plus élévés sont soient des extraits courts où le mot apparait une seule fois, soit des extraits plus longs où le mot fear apparaît plusieurs fois.\n\n\n Note\nLa matrice document x terms est un exemple typique de matrice sparse puisque, dans des corpus volumineux, une grande diversité de vocabulaire peut être trouvée."
  },
  {
    "objectID": "content/NLP/02_exoclean.html#approche-contextuelle-les-n-gramms",
    "href": "content/NLP/02_exoclean.html#approche-contextuelle-les-n-gramms",
    "title": "Nettoyer un texte: des exercices pour découvrir l’approche bag-of-words",
    "section": "Approche contextuelle: les n-gramms",
    "text": "Approche contextuelle: les n-gramms\nPour être en mesure de mener cette analyse, il est nécessaire de télécharger un corpus supplémentaire :\n\nimport nltk\nnltk.download('genesis')\nnltk.corpus.genesis.words('english-web.txt')\n\n[nltk_data] Downloading package genesis to /github/home/nltk_data...\n[nltk_data]   Package genesis is already up-to-date!\n\n\n['In', 'the', 'beginning', 'God', 'created', 'the', ...]\n\n\nIl s’agit maintenant de raffiner l’analyse.\nOn s’intéresse non seulement aux mots et à leur fréquence, mais aussi aux mots qui suivent. Cette approche est essentielle pour désambiguiser les homonymes. Elle permet aussi d’affiner les modèles “bag-of-words”. Le calcul de n-grams (bigrams pour les co-occurences de mots deux-à-deux, tri-grams pour les co-occurences trois-à-trois, etc.) constitue la méthode la plus simple pour tenir compte du contexte.\nnltk offre des methodes pour tenir compte du contexte : pour ce faire, nous calculons les n-grams, c’est-à-dire l’ensemble des co-occurrences successives de mots n-à-n. En général, on se contente de bi-grams, au mieux de tri-grams :\n\nles modèles de classification, analyse du sentiment, comparaison de documents, etc. qui comparent des n-grams avec n trop grands sont rapidement confrontés au problème de données sparse, cela réduit la capacité prédictive des modèles ;\nles performances décroissent très rapidement en fonction de n, et les coûts de stockage des données augmentent rapidement (environ n fois plus élevé que la base de données initiale).\n\nOn va, rapidement, regarder dans quel contexte apparaît le mot fear dans\nl’oeuvre d’Edgar Allan Poe (EAP). Pour cela, on transforme d’abord\nle corpus EAP en tokens `nltk :\n\neap_clean = train[train[\"Author\"] == \"EAP\"]\neap_clean = ' '.join(eap_clean['Text'])\ntokens = eap_clean.split()\nprint(tokens[:10])\ntext = nltk.Text(tokens)\nprint(text)\n\n['This', 'process,', 'however,', 'afforded', 'me', 'no', 'means', 'of', 'ascertaining', 'the']\n&lt;Text: This process, however, afforded me no means of...&gt;\n\n\nVous aurez besoin des fonctions BigramCollocationFinder.from_words et BigramAssocMeasures.likelihood_ratio :\n\nfrom nltk.collocations import BigramCollocationFinder\nfrom nltk.metrics import BigramAssocMeasures\n\n\n\n Exercice 7  : n-grams et contexte du mot fear\n\nUtiliser la méthode concordance pour afficher le contexte dans lequel apparaît le terme fear.\nSélectionner et afficher les meilleures collocation, par exemple selon le critère du ratio de vraisemblance.\n\nLorsque deux mots sont fortement associés, cela est parfois dû au fait qu’ils apparaissent rarement. Il est donc parfois nécessaire d’appliquer des filtres, par exemple ignorer les bigrammes qui apparaissent moins de 5 fois dans le corpus.\n\nRefaire la question précédente en utilisant toujours un modèle BigramCollocationFinder suivi de la méthode apply_freq_filter pour ne conserver que les bigrammes présents au moins 5 fois. Puis, au lieu d’utiliser la méthode de maximum de vraisemblance, testez la méthode nltk.collocations.BigramAssocMeasures().jaccard.\nNe s’intéresser qu’aux collocations qui concernent le mot fear\n\n\n\nAvec la méthode concordance (question 1),\nla liste devrait ressembler à celle-ci:\n\n\nExemples d'occurences du terme 'fear' :\nDisplaying 13 of 13 matches:\nd quick unequal spoken apparently in fear as well as in anger. What he said wa\nhutters were close fastened, through fear of robbers, and so I knew that he co\nto details. I even went so far as to fear that, as I occasioned much trouble, \nyears of age, was heard to express a fear \"that she should never see Marie aga\nich must be entirely remodelled, for fear of serious accident I mean the steel\n my arm, and I attended her home. 'I fear that I shall never see Marie again.'\nclusion here is absurd. \"I very much fear it is so,\" replied Monsieur Maillard\nbt of ultimately seeing the Pole. \"I fear you are right there,\" said the Prefe\ner occurred before.' Indeed I had no fear on her account. For a moment there w\nerhaps so,\" said I; \"but, Legrand, I fear you are no artist. It is my firm int\n raps with a hammer. Be of heart and fear nothing. My daughter, Mademoiselle M\ne splendor. I have not the slightest fear for the result. The face was so far \narriers of iron that hemmed me in. I fear you have mesmerized\" adding immediat\n\n\n\n\nMême si on peut facilement voir le mot avant et après, cette liste est assez difficile à interpréter car elle recoupe beaucoup d’informations.\nLa collocation consiste à trouver les bi-grammes qui\napparaissent le plus fréquemment ensemble. Parmi toutes les paires de deux mots observées,\nil s’agit de sélectionner, à partir d’un modèle statistique, les “meilleures”.\nOn obtient donc avec cette méthode (question 2):\nSi on modélise les meilleures collocations:\nCette liste a un peu plus de sens,\non a des noms de personnages, de lieux mais aussi des termes fréquemment employés ensemble\n(Chess Player par exemple).\nEn ce qui concerne les collocations du mot fear:\nSi on mène la même analyse pour le terme love, on remarque que de manière logique, on retrouve bien des sujets généralement accolés au verbe :\n\ncollocations_word(\"love\")\n\n[('love', 'me'), ('love', 'he'), ('will', 'love'), ('I', 'love'), ('love', ','), ('you', 'love'), ('the', 'love')]"
  },
  {
    "objectID": "content/NLP/02_exoclean.html#footnotes",
    "href": "content/NLP/02_exoclean.html#footnotes",
    "title": "Nettoyer un texte: des exercices pour découvrir l’approche bag-of-words",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nL’approche bag of words est déjà, si on la pousse à ses limites, très intéressante. Elle peut notamment\nfaciliter la mise en cohérence de différents corpus\npar la méthode des appariements flous\n(cf. Galiana and Castillo (2022).\nLe chapitre sur ElasticSearch présent dans cette partie du cours présente quelques\néléments de ce travail sur les données de l’OpenFoodFacts↩︎\nLa littérature sur les modèles gravitaires, présentée dans Galiana et al. (2020),\ndonne quelques arguments pour privilégier les modèles GLM à des modèles log-linéaires\nestimés par moindres carrés ordinaires.↩︎"
  },
  {
    "objectID": "content/NLP/index.html",
    "href": "content/NLP/index.html",
    "title": "Partie 4 : Natural Language Processing (NLP)",
    "section": "",
    "text": "Les parties précédentes étaient consacrées à l’acquisition de compétences\ntransversales pour la valorisation des données. De manière naturelle,\nnous nous sommes jusqu’à présent plutôt consacrés\nà la valorisation de données structurées, d’une\ndimension certes modeste mais qui ouvraient déjà énormément de\nproblématiques à creuser. Cette partie propose maintenant de se\nconsacrer à un sujet dont il n’est pas évident a priori que\nles ordinateurs s’emparent: le langage humain et sa richesse.\nEn effet, si la linguistique propose certes de représenter\nde manière conceptuelle le langage sous une forme de données, comment\ndes ordinateurs, qui au fond ne connaissent que le 0 et le 1, peuvent-ils\ns’approprier cet objet éminemment complexe qu’est le langage et qu’un\nhumain met lui-même des années à comprendre et s’approprier ?1\nLe traitement du langage naturel - traduction française du concept de\nnatural language processing (NLP) - est l’ensemble des techniques\npermettant aux ordinateurs de comprendre, analyser synthétiser et\ngénérer le langage humain2.\nIl s’agit d’un champ disciplinaire à l’intersection de la statistique\net de la linguistique qui connait depuis quelques années un engouement\nimportant que ce soit d’un point de vue académique ou opérationnel.\nCertaines des applications de ces techniques sont devenues incontournables\ndans nos tâches quotidiennes, notamment les moteurs de recherche, la traduction\nautomatique et plus récemment les chatbots.\nCette partie du cours est consacrée à l’analyse des données textuelles avec\ndes exemples de 📖 pour s’amuser. Elle est une introduction progressive\nà ce sujet en se concentrant sur des concepts de base, nécessaires à\nla compréhension ultérieure de principes plus avancés et de techniques\nsophistiquées3."
  },
  {
    "objectID": "content/NLP/index.html#résumé-de-la-partie",
    "href": "content/NLP/index.html#résumé-de-la-partie",
    "title": "Partie 4 : Natural Language Processing (NLP)",
    "section": "Résumé de la partie",
    "text": "Résumé de la partie\nPython est un excellent outil pour l’analyse de données textuelles.\nLes méthodes de base ou les librairies spécialisées\ncomme NLTK et SpaCy permettent d’effectuer ces tâches de manière\ntrès efficace. Les ressources en ligne sur le sujet sont très\nnombreuses. Python est bien mieux outillé que R pour l’analyse de\ndonnées textuelles.\nDans un premier temps, cette partie propose\nde revenir sur la manière de structurer et nettoyer un corpus\ntextuel au travers de l’approche bag of words (sac de mots).\nElle vise à montrer comment transformer un corpus en outil propre à une\nanalyse statistique :\n\nElle propose d’abord une introduction aux enjeux du nettoyage des données\ntextuelles à travers l’analyse du Comte de Monte Cristo d’Alexandre Dumas\nici qui permet de synthétiser rapidement l’information disponible\ndans un large volume de données (à l’image de la ?@fig-wordcloud-dumas)\nElle propose ensuite une série d’exercices sur le nettoyage de textes à partir des\noeuvres d’Edgar Allan Poe, Mary Shelley et H.P. Lovecraft visant à distinguer la\nspécificité du vocabulaire employé par chaque auteurs (par exemple ?@fig-waffle-fear). Ces exercices sont\ndisponibles dans le deuxième chapitre de la partie.\n\nEnsuite, nous proposerons d’explorer une approche alternative, prenant en compte\nle contexte d’apparition d’un mot. L’introduction à la\nLatent Dirichlet Allocation (LDA) sera l’occasion de présenter la modélisation\nde documents sous la forme de topics.\nEnfin, nous introduirons aux enjeux de la transformation de champs textuels\nsous forme de vecteurs numériques. Pour cela, nous présenterons le principe\nde Word2Vec qui permet ainsi, par exemple,\nmalgré une distance syntaxique importante,\nde dire que sémantiquement Homme et Femme sont proches.\nCe chapitre est une passerelle vers le concept d’embedding, véritable\nrévolution récente du NLP, et qui permet de rapprocher des corpus\nnon seulement sur leur proximité syntaxique (partagent-ils par exemple des mots\ncommuns ?) mais aussi sur leur proximité sémantique (partagent-ils un thème ou un sens commun ?).4\nComme l’illustre la figure suivante, empruntée à Sebastian Raschka, les concepts que nous allons aborder\ndans cette partie sont indispensables pour être en mesure\nd’entraîner ou réutiliser ultérieurement un modèle\nsophistiqué d’analyse du langage:\n\n\n\n\n\n(a) Illustration transformer architecture\n\n\nFigure 1: Illustration of the original transformer architecture proposed in Attention Is All You Need, 2017\n(source: Sebastien Raschka)"
  },
  {
    "objectID": "content/NLP/index.html#pour-aller-plus-loin",
    "href": "content/NLP/index.html#pour-aller-plus-loin",
    "title": "Partie 4 : Natural Language Processing (NLP)",
    "section": "Pour aller plus loin",
    "text": "Pour aller plus loin\nLa recherche dans le domaine du NLP est très active. Il est donc recommandé\nde faire preuve de curiosité pour en apprendre plus car une ressource\nunique ne compilera pas l’ensemble des connaissances, a fortiori dans\nun champ de recherche aussi dynamique que le NLP.\nPour approfondir les compétences évoquées dans ce cours, je recommande vivement\nce cours d’HuggingFace.\nPour comprendre l’architecture interne d’un LLM,\nce post de Sebastian Raschka\nest très utile."
  },
  {
    "objectID": "content/NLP/index.html#footnotes",
    "href": "content/NLP/index.html#footnotes",
    "title": "Partie 4 : Natural Language Processing (NLP)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEn écrivant ces lignes j’ai demandé à ChatGPT de me faire une blague, la communication entre l’humain et la machine est perfectible :\n\nPourquoi les programmeurs détestent-ils la plage ?\nParce que le sable change tout le temps de paradigme !\n\nNe l’ayant pas comprise du fait de mon esprit obtus, je lui\nai demandé de me l’expliquer. Voici sa réponse\n\nBien sûr, je serais ravi de l’expliquer ! Cette blague joue sur le double sens du mot “paradigme” et fait un jeu de mots avec les notions liées à la programmation et à la plage.\nDans le domaine de la programmation, un “paradigme” désigne un modèle ou une méthode de programmation, comme la programmation orientée objet, la programmation fonctionnelle, etc. Les programmeurs sont souvent familiers avec différents paradigmes de programmation.\nD’autre part, en contexte de plage, “paradigme” est utilisé pour signifier “marée qui monte et descend”, se référant à l’action naturelle du sable et de l’eau sur la plage.\nLa blague joue sur la frustration humoristique que peuvent ressentir les programmeurs lorsqu’ils doivent s’adapter à de nouveaux paradigmes de programmation, tout comme le sable de la plage qui change constamment avec les marées. En associant les deux sens du mot “paradigme”, la blague crée une situation absurde et amusante, faisant sourire ceux qui comprennent les références à la programmation et à la plage.\n\nFinalement, j’ai trouvé son explication plus drôle que sa blague.↩︎\nLe concept de natural language processing tend à remplacer celui\nde text mining dont le périmètre était plus spécifique. Le text mining\nrenvoie à la recherche d’information - la fouille - dans un ensemble de\ntextes. C’est l’un des champs d’applications\ndu natural language processing mais ce n’est pas l’unique.↩︎\nPar exemple, le concept d’embedding - transformation d’un champ\ntextuel en un vecteur numérique multidimensionnel - aujourd’hui central\ndans le NLP n’est évoqué qu’à quelques reprises. Avant d’en arriver\nau sujet des embeddings, il est précieux de comprendre les apports et les\nlimites de concepts comme\nle sac de mot (bag of words) ou la distance\nTF-IDF (term frequency - inverse document frequency). Dans une\noptique introductive, ce cours se focalise donc sur ces derniers pour faciliter\nl’ouverture ultérieure de la boite de Pandore que sont les embeddings.↩︎\nUn exemple d’intérêt de ce type d’approche est la ?@fig-relevanc-table-embedding.↩︎"
  },
  {
    "objectID": "content/modelisation/5_clustering.html",
    "href": "content/modelisation/5_clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Nous allons continuer avec le même jeu de données que précédemment,\nc’est-à-dire les résultats des élections US 2020 présentés dans l’introduction\nde cette partie: les données de vote aux élections présidentielles américaines\ncroisées à des variables sociodémographiques.\nLe code\nest disponible sur Github.\n#!pip install --upgrade xlrd #colab bug verson xlrd\n#!pip install geopandas\n\nimport requests\n\nurl = 'https://raw.githubusercontent.com/linogaliana/python-datascientist/master/content/modelisation/get_data.py'\nr = requests.get(url, allow_redirects=True)\nopen('getdata.py', 'wb').write(r.content)\n\nimport getdata\nvotes = getdata.create_votes_dataframes()"
  },
  {
    "objectID": "content/modelisation/5_clustering.html#introduction-sur-le-clustering",
    "href": "content/modelisation/5_clustering.html#introduction-sur-le-clustering",
    "title": "Clustering",
    "section": "Introduction sur le clustering",
    "text": "Introduction sur le clustering\nJusqu’à présent, nous avons fait de l’apprentissage supervisé puisque nous\nconnaissions la vraie valeur de la variable à expliquer/prédire (y). Ce n’est plus le cas avec\nl’apprentissage non supervisé.\nLe clustering est un champ d’application de l’apprentissage non-supervisé.\nIl s’agit d’exploiter l’information disponible pour regrouper des observations\nqui se ressemblent.\nL’objectif est de créer des clusters d’observations pour lesquels :\n\nau sein de chaque cluster, les observations sont homogènes (variance intra-cluster minimale)\nles clusters ont des profils hétérogènes, c’est-à-dire qu’ils se distinguent les uns des autres (variance inter-cluster maximale)\n\nEn Machine Learning, les méthodes de clustering sont très utilisées pour\nfaire de la recommandation. En faisant, par exemple, des classes homogènes de\nconsommateurs, il est plus facile d’identifier et cibler des comportements\npropres à chaque classe de consommateurs.\nCes méthodes ont également un intérêt en économie et sciences sociales parce qu’elles permettent\nde regrouper des observations sans a priori et ainsi interpréter une variable\nd’intérêt à l’aune de ces résultats. Cette publication sur la ségrégation spatiale utilisant des données de téléphonie mobile\nutilise par exemple cette approche.\nLes méthodes de clustering peuvent aussi intervenir en amont d’un problème de classification (dans des\nproblèmes d’apprentissage semi-supervisé).\nLe manuel Hands-on machine learning with scikit-learn, Keras et TensorFlow présente dans le\nchapitre dédié à l’apprentissage non supervisé quelques exemples.\nDans certaines bases de données, on peut se retrouver avec quelques exemples labellisés mais la plupart sont\nnon labellisés. Les labels ont par exemple été faits manuellement par des experts.\nPar exemple, supposons que dans la base MNIST des chiffres manuscrits, les chiffres ne soient pas labellisés\net que l’on se demande quelle est la meilleure stratégie pour labelliser cette base.\nOn pourrait regarder des images de chiffres manuscrits au hasard de la base et les labelliser.\nLes auteurs du livre montrent qu’il existe toutefois une meilleure stratégie.\nIl vaut mieux appliquer un algorithme de clustering en amont pour regrouper les images ensemble et avoir une\nimage représentative par groupe, et labelliser ces images représentatives au lieu de labelliser au hasard.\nLes méthodes de clustering sont nombreuses.\nNous allons nous pencher sur la plus intuitive : les k-means."
  },
  {
    "objectID": "content/modelisation/5_clustering.html#les-k-means",
    "href": "content/modelisation/5_clustering.html#les-k-means",
    "title": "Clustering",
    "section": "Les k-means",
    "text": "Les k-means\n\nPrincipe\nL’objectif des k-means est de partitionner l’espace des observations en trouvant des points (centroids) jouant le rôle de centres de gravité pour lesquels les observations proches peuvent être regroupées dans une classe homogène.\nL’algorithme k-means fonctionne par itération, en initialisant les centroïdes puis en les mettant à jour à chaque\nitération, jusqu’à ce que les centroïdes se stabilisent. Quelques exemples de clusters issus de la méthode k-means :\n\n\n\n\n\n\n\n Hint\nL’objectif des k-means est de trouver une partition des données \\(S=\\{S_1,...,S_K\\}\\) telle que\n\\[\n\\arg\\min_{S} \\sum_{i=1}^K \\sum_{x \\in S_i} ||x - \\mu_i||^2\n\\]\navec \\(\\mu_i\\) la moyenne des \\(x_i\\) dans l’ensemble de points \\(S_i\\)\n\n\n\n# packages utiles\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nfrom sklearn.cluster import KMeans #pour kmeans\nimport seaborn as sns #pour scatterplots\n\n\n\n Exercice 1 : Principe des k-means\n\nImporter les données et se restreindre aux variables 'Unemployment_rate_2019', 'Median_Household_Income_2019', 'Percent of adults with less than a high school diploma, 2015-19', \"Percent of adults with a bachelor's degree or higher, 2015-19\" et bien sûr 'per_gop'. Appelez cette base restreinte df2 et enlevez les valeurs manquantes.\nFaire un k-means avec \\(k=4\\).\nCréer une variable label dans votes stockant le résultat de la typologie\nAfficher cette typologie sur une carte.\nChoisir les variables Median_Household_Incomme_2019 et Unemployment_rate_2019 et représenter le nuage de points en colorant différemment\nen fonction du label obtenu.\nReprésenter la distribution du vote pour chaque cluster\n\n\n\nLa carte obtenue à la question 4, qui permet de\nreprésenter spatialement nos groupes, est\nla suivante :\n\n\n\n\n\n\n\n\n\nLe nuage de point de la question 5, permettant de représenter\nla relation entre Median_Household_Incomme_2019\net Unemployment_rate_2019, aura l’aspect suivant :\n\n\n\n\n\n\n\n\n\nEnfin, l’histogramme des votes pour chaque cluster est :\n\n\n\n\n\n\n\n\n\n\n\n Hint\nIl faut noter plusieurs points sur l’algorithme implémenté par défaut par scikit-learn, que l’on peut lire dans\nla documentation :\n- l’algorithme implémenté par défaut est kmeans ++ (cf. paramètre init). Cela signifie que\nl’initialisation des centroïdes est faite de manière intelligente pour que les centroïdes initiaux soient choisis\nafin de ne pas être trop proches.\n- l’algorithme va être démarré avec n_init centroïdes différents et le modèle va choisir la meilleure initialisation\nen fonction de l’inertia du modèle, par défaut égale à 10.\nLe modèle renvoie les cluster_centers_, les labels labels_, l’inertia inertia_ et le nombre d’itérations\nn_iter_.\n\n\n\n\nChoisir le nombre de clusters\nLe nombre de clusters est fixé par le modélisateur.\nIl existe plusieurs façons de fixer ce nombre :\n\nconnaissance a priori du problème ;\nanalyse d’une métrique spécifique pour définir le nombre de clusters à choisir ;\netc.\n\nIl y a un arbitrage à faire\nentre biais et variance :\nun trop grand nombre de clusters implique une variance\nintra-cluster très faible (sur-apprentissage, même s’il n’est jamais possible de déterminer\nle vrai type d’une observation puisqu’on est en apprentissage non supervisé).\nSans connaissance a priori du nombre de clusters, on peut recourir à deux familles de méthodes :\n\nLa méthode du coude (elbow method): On prend le point d’inflexion de la courbe\nde performance du modèle. Cela représente le moment où ajouter un cluster\n(complexité croissante du modèle) n’apporte que des gains modérés dans la\nmodélisation des données.\nLe score de silhouette : On mesure la similarité entre un point et les autres points\ndu cluster par rapport aux autres clusters. Plus spécifiquement :\n\n\nSilhouette value is a measure of how similar an object is to its own cluster\n(cohesion) compared to other clusters (separation). The silhouette ranges\nfrom −1 to +1, where a high value indicates that the object is\nwell matched to its own cluster and poorly matched to neighboring\nclusters. If most objects have a high value, then the clustering\nconfiguration is appropriate. If many points have a low or negative\nvalue, then the clustering configuration may have too many or too few clusters\nSource: Wikipedia\n\nLe score de silhouette d’une observation est donc égal à\n(m_nearest_cluster - m_intra_cluster)/max( m_nearest_cluster,m_intra_cluster)\noù m_intra_cluster est la moyenne des distances de l’observation aux observations du même cluster\net m_nearest_cluster est la moyenne des distances de l’observation aux observations du cluster le plus proche.\nLe package yellowbrick fournit une extension utile à scikit pour représenter\nfacilement la performance en clustering.\n\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\n\nvisualizer = KElbowVisualizer(model, k=(2,12))\nvisualizer.fit(df2[xvars])        # Fit the data to the visualizer\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\nKElbowVisualizer(ax=&lt;Axes: &gt;, estimator=KMeans(n_clusters=11), k=(2, 12))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KElbowVisualizerKElbowVisualizer(ax=&lt;Axes: &gt;, estimator=KMeans(n_clusters=11), k=(2, 12))estimator: KMeansKMeans(n_clusters=11)KMeansKMeans(n_clusters=11)\n\n\n\n\n\n\n\n\n\n\n\n&lt;Axes: title={'center': 'Distortion Score Elbow for KMeans Clustering'}, xlabel='k', ylabel='distortion score'&gt;\n\n\n&lt;Figure size 768x528 with 0 Axes&gt;\n\n\n\n\n\n\n\nPour la méthode du coude, la courbe\nde performance du modèle marque un coude léger à \\(k=4\\). Le modèle initial\nsemblait donc approprié.\nyellowbrick permet également de représenter des silhouettes mais\nl’interprétation est moins aisée et le coût computationnel plus élevé :\n\nfrom yellowbrick.cluster import SilhouetteVisualizer\n\nfig, ax = plt.subplots(2, 2, figsize=(15,8))\nj=0\nfor i in [3, 4, 6, 10]:\n    j += 1\n    '''\n    Create KMeans instance for different number of clusters\n    '''\n    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)\n    q, mod = divmod(j, 2)\n    '''\n    Create SilhouetteVisualizer instance with KMeans instance\n    Fit the visualizer\n    '''\n    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])\n    ax[q-1][mod].set_title(\"k = \" + str(i))\n    visualizer.fit(df2[xvars])\n\n\n\n\n\n\nLe score de silhouette offre une représentation plus riche que la courbe coudée.\nSur ce graphique, les barres verticales en rouge et en pointillé représentent le score de silhouette\nglobal pour chaque k choisi. On voit par exemple que pour tous les k représentés ici, le\nscore de silhouette se situe entre 0.5 et 0.6 et varie peu.\nEnsuite, pour un k donné, on va avoir la représentation des scores de silhouette de chaque\nobservation, regroupées par cluster.\nPar exemple, pour k = 4, ici, on voit bien quatre couleurs différentes qui sont les 4 clusters modélisés.\nLes ordonnées sont toutes les observations clusterisées et en abscisses on a le score de silhouette de\nchaque observation. Si au sein d’un cluster, les observations ont un score de silhouette plus faible que le\nscore de silhouette global (ligne verticale en rouge), cela signifie que les observations du clusters sont\ntrop proches des autres clusters.\nGrâce à cette représentation, on peut aussi se rendre compte de la taille relative des clusters. Par exemple,\navec k = 3, on voit qu’on a deux clusters conséquents et un plus “petit” cluster relativement aux deux autres.\nCela peut nous permettre de choisir des clusters de tailles homogènes ou non.\nEnfin, quand le score de silhouette est négatif, cela signifie que la moyenne des distances de l’observation\naux observations du cluster le plus proche est inférieure à la moyenne des distances de l’observation aux\nobservations de son cluster. Cela signifie que l’observation est mal classée.\n\n\nAutres méthodes de clustering\nIl existe de nombreuses autres méthodes de clustering. Parmi les plus connues, on peut citer deux exemples en particulier :\n\nDBSCAN\nles mélanges de Gaussiennes\n\n\nDBSCAN\nL’algorithme DBSCAN est implémenté dans sklearn.cluster.\nIl peut être utilisé pour faire de la détection d’anomalies\nnotamment.\nEn effet, cette méthode repose sur le clustering en régions où la densité\ndes observations est continue, grâce à la notion de voisinage selon une certaine distance epsilon.\nPour chaque observation, on va regarder si dans son voisinage selon une distance epsilon, il y a des voisins. S’il y a au\nmoins min_samples voisins, alors l’observation sera une core instance.\nLes observations qui ne sont pas des core instances et qui n’en ont pas dans leur voisinage selon une distance espilon\nvont être détectées comme des anomalies.\n\n\nLes mélanges de gaussiennes\nEn ce qui concerne la théorie, voir le cours Probabilités numériques et statistiques computationnelles, M1 Jussieu, V.Lemaire et T.Rebafka\nSe référer notamment aux notebooks pour l’algorithme EM pour mélange gaussien.\nDans sklearn, les mélanges gaussiens sont implémentés dans sklearn.mixture comme GaussianMixture.\nLes paramètres importants sont alors le nombre de gaussiennes n_components et le nombre d’initiatisations n_init.\nIl est possible de faire de la détection d’anomalie savec les mélanges de gaussiennes.\n\n\n Pour aller plus loin\nIl existe de nombreuses autres méthodes de clustering :\n\nLocal outlier factor ;\nbayesian gaussian mixture models ;\ndifférentes méthodes de clustering hiérarchique ;\netc."
  },
  {
    "objectID": "content/modelisation/3_regression.html",
    "href": "content/modelisation/3_regression.html",
    "title": "Régression : une introduction",
    "section": "",
    "text": "Nous allons partir du même jeu de données que précédemment,\nc’est-à-dire les résultats des élections US 2020 présentés dans l’introduction\nde cette partie: les données de vote aux élections présidentielles américaines\ncroisées à des variables sociodémographiques.\nLe code\nest disponible sur Github.\n#!pip install --upgrade xlrd #colab bug verson xlrd\n#!pip install geopandas\n\nimport requests\n\nurl = 'https://raw.githubusercontent.com/linogaliana/python-datascientist/master/content/modelisation/get_data.py'\nr = requests.get(url, allow_redirects=True)\nopen('getdata.py', 'wb').write(r.content)\n\nimport getdata\nvotes = getdata.create_votes_dataframes()\nLe précédent chapitre visait à proposer un premier modèle pour comprendre\nles comtés où le parti Républicain l’emporte. La variable d’intérêt étant\nbimodale (victoire ou défaite), on était dans le cadre d’un modèle de\nclassification.\nMaintenant, sur les mêmes données, on va proposer un modèle de régression\npour expliquer le score du parti Républicain. La variable est donc continue.\nNous ignorerons le fait que ses bornes se trouvent entre 0 et 100 et donc\nqu’il faudrait, pour être rigoureux, transformer l’échelle afin d’avoir\ndes données dans cet intervalle.\nCe chapitre utilise toujours le même jeu de données, présenté dans l’introduction\nde cette partie: les données de vote aux élections présidentielles américaines\ncroisées à des variables sociodémographiques.\nLe code\nest disponible sur Github."
  },
  {
    "objectID": "content/modelisation/3_regression.html#principe-général",
    "href": "content/modelisation/3_regression.html#principe-général",
    "title": "Régression : une introduction",
    "section": "Principe général",
    "text": "Principe général\nLe principe général de la régression consiste à trouver une loi \\(h_\\theta(X)\\)\ntelle que\n\\[\nh_\\theta(X) = \\mathbb{E}_\\theta(Y|X)\n\\]\nCette formalisation est extrêmement généraliste et ne se restreint d’ailleurs\npar à la régression linéaire.\nEn économétrie, la régression offre une alternative aux méthodes de maximum\nde vraisemblance et aux méthodes des moments. La régression est un ensemble\ntrès vaste de méthodes, selon la famille de modèles\n(paramétriques, non paramétriques, etc.) et la structure de modèles.\n\nLa régression linéaire\nC’est la manière la plus simple de représenter la loi \\(h_\\theta(X)\\) comme\ncombinaison linéaire de variables \\(X\\) et de paramètres \\(\\theta\\). Dans ce\ncas,\n\\[\n\\mathbb{E}_\\theta(Y|X) = X\\beta\n\\]\nCette relation est encore, sous cette formulation, théorique. Il convient\nde l’estimer à partir des données observées \\(y\\). La méthode des moindres\ncarrés consiste à minimiser l’erreur quadratique entre la prédiction et\nles données observées (ce qui explique qu’on puisse voir la régression comme\nun problème de Machine Learning). En toute généralité, la méthode des\nmoindres carrés consiste à trouver l’ensemble de paramètres \\(\\theta\\)\ntel que\n\\[\n\\theta = \\arg \\min_{\\theta \\in \\Theta} \\mathbb{E}\\bigg[ \\left( y - h_\\theta(X) \\right)^2 \\bigg]\n\\]\nCe qui, dans le cadre de la régression linéaire, s’exprime de la manière suivante :\n\\[\n\\beta = \\arg\\min \\mathbb{E}\\bigg[ \\left( y - X\\beta \\right)^2 \\bigg]\n\\]\nLorsqu’on amène le modèle théorique (\\(\\mathbb{E}_\\theta(Y|X) = X\\beta\\)) aux données,\non formalise le modèle de la manière suivante :\n\\[\nY = X\\beta + \\epsilon\n\\]\nAvec une certaine distribution du bruit \\(\\epsilon\\) qui dépend\ndes hypothèses faites. Par exemple, avec des\n\\(\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)\\) i.i.d., l’estimateur \\(\\beta\\) obtenu\nest équivalent à celui du Maximum de Vraisemblance dont la théorie asymptotique\nnous assure l’absence de biais, la variance minimale (borne de Cramer-Rao).\n\n# packages utiles\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n\n\n Exercice 1a : Régression linéaire avec scikit\nCet exercice vise à illustrer la manière d’effectuer une régression linéaire avec scikit.\nDans ce domaine,\nstatsmodels est nettement plus complet, ce que montrera l’exercice suivant.\nL’intérêt principal de faire\ndes régressions avec scikit est de pouvoir comparer les résultats d’une régression linéaire\navec d’autres modèles de régression. Cependant, le chapitre sur les\npipelines montrera qu’on peut très bien insérer, avec quelques efforts\nde programmation orientée objet, une régression statsmodels dans\nun pipeline scikit.\nL’objectif est d’expliquer le score des Républicains en fonction de quelques\nvariables. Contrairement au chapitre précédent, où on se focalisait sur\nun résultat binaire (victoire/défaite des Républicains), cette\nfois on va chercher à modéliser directement le score des Républicains.\n\nA partir de quelques variables, par exemple, ‘Unemployment_rate_2019’, ‘Median_Household_Income_2019’, ‘Percent of adults with less than a high school diploma, 2015-19’, “Percent of adults with a bachelor’s degree or higher, 2015-19”, expliquer la variable per_gop à l’aide d’un échantillon d’entraînement X_train constitué au préalable.\n\n:warning: utiliser la variable Median_Household_Income_2019\nen log sinon son échelle risque d’écraser tout effet.\n\nAfficher les valeurs des coefficients, constante comprise\nEvaluer la pertinence du modèle avec le \\(R^2\\) et la qualité du fit avec le MSE.\nReprésenter un nuage de points des valeurs observées\net des erreurs de prédiction. Observez-vous\nun problème de spécification ?\n\n\n\n\n# packages utiles\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n\n\n Exercice 1b : Régression linéaire avec statsmodels\nCet exercice vise à illustrer la manière d’effectuer une régression linéaire avec statsmodels qui offre des fonctionnalités plus proches de celles de R, et moins orientées Machine Learning.\nL’objectif est toujours d’expliquer le score des Républicains en fonction de quelques\nvariables.\n\nA partir de quelques variables, par exemple, ‘Unemployment_rate_2019’, ‘Median_Household_Income_2019’, ‘Percent of adults with less than a high school diploma, 2015-19’, “Percent of adults with a bachelor’s degree or higher, 2015-19”, expliquer la variable per_gop. :warning: utiliser la variable Median_Household_Income_2019\nen log sinon son échelle risque d’écraser tout effet.\nAfficher un tableau de régression.\nEvaluer la pertinence du modèle avec le R^2.\nUtiliser l’API formula pour régresser le score des républicains en fonction de la variable Unemployment_rate_2019, de Unemployment_rate_2019 au carré et du log de\nMedian_Household_Income_2019.\n\n\n\n\n\n Hint\nPour sortir une belle table pour un rapport sous \\(\\LaTeX\\), il est possible d’utiliser\nla méthode Summary.as_latex. Pour un rapport HTML, on utilisera Summary.as_html\n\n\n\n\n Note\nLes utilisateurs de R retrouveront des éléments très familiers avec statsmodels,\nnotamment la possibilité d’utiliser une formule pour définir une régression.\nLa philosophie de statsmodels est similaire à celle qui a influé sur la construction\ndes packages stats et MASS de R: offrir une librairie généraliste, proposant\nune large gamme de modèles. Néanmoins, statsmodels bénéficie de sa jeunesse\npar rapport aux packages R. Depuis les années 1990, les packages R visant\nà proposer des fonctionalités manquantes dans stats et MASS se sont\nmultipliés alors que statsmodels, enfant des années 2010, n’a eu qu’à\nproposer un cadre général (les generalized estimating equations) pour\nenglober ces modèles.\n\n\n\n\nLa régression logistique\nCe modèle s’applique à une distribution binaire.\nDans ce cas, \\(\\mathbb{E}\\_{\\theta}(Y|X) = \\mathbb{P}\\_{\\theta}(Y = 1|X)\\).\nLa régression logistique peut être vue comme un modèle linéaire en probabilité:\n\\[\n\\text{logit}\\bigg(\\mathbb{E}\\_{\\theta}(Y|X)\\bigg) = \\text{logit}\\bigg(\\mathbb{P}\\_{\\theta}(Y = 1|X)\\bigg) = X\\beta\n\\]\nLa fonction \\(\\text{logit}\\) est \\(]0,1[ \\to \\mathbb{R}: p \\mapsto \\log(\\frac{p}{1-p})\\).\nElle permet ainsi de transformer une probabilité dans \\(\\mathbb{R}\\).\nSa fonction réciproque est la sigmoïde (\\(\\frac{1}{1 + e^{-x}}\\)),\nobjet central du Deep Learning.\nIl convient de noter que les probabilités ne sont pas observées, c’est l’outcome\nbinaire (0/1) qui l’est. Cela amène à voir la régression logistique de deux\nmanières différentes :\n\nEn économétrie, on s’intéresse au modèle latent qui détermine le choix de\nl’outcome. Par exemple, si on observe les choix de participer ou non au marché\ndu travail, on va modéliser les facteurs déterminant ce choix ;\nEn Machine Learning, le modèle latent n’est nécessaire que pour classifier\ndans la bonne catégorie les observations\n\nL’estimation des paramètres \\(\\beta\\) peut se faire par maximum de vraisemblance\nou par régression, les deux solutions sont équivalentes sous certaines\nhypothèses.\n\n\n Note\nPar défaut, scikit applique une régularisation pour pénaliser les modèles\npeu parcimonieux (comportement différent\nde celui de statsmodels). Ce comportement par défaut est à garder à l’esprit\nsi l’objectif n’est pas de faire de la prédiction.\n\n\n\n# packages utiles\nfrom sklearn.linear_model import LogisticRegression\nimport sklearn.metrics\n\n\n\n Exercice 2a : Régression logistique avec scikit\nAvec scikit, en utilisant échantillons d’apprentissage et d’estimation :\n\nEvaluer l’effet des variables déjà utilisées sur la probabilité des Républicains\nde gagner. Affichez la valeur des coefficients.\nDéduire une matrice de confusion et\nune mesure de qualité du modèle.\nSupprimer la régularisation grâce au paramètre penalty. Quel effet sur les paramètres estimés ?\n\n\n\n\n# packages utiles\nfrom scipy import stats\n\n\n\n Exercice 2b : Régression logistique avec statmodels\nEn utilisant échantillons d’apprentissage et d’estimation :\n\nEvaluer l’effet des variables déjà utilisées sur la probabilité des Républicains\nde gagner.\nFaire un test de ratio de vraisemblance concernant l’inclusion de la variable de (log)-revenu.\n\n\n\n\n\n Hint\nLa statistique du test est :\n\\[\nLR = -2\\log\\bigg(\\frac{\\mathcal{L}_{\\theta}}{\\mathcal{L}_{\\theta_0}}\\bigg) = -2(\\mathcal{l}_{\\theta} - \\mathcal{l}_{\\theta_0})\n\\]"
  },
  {
    "objectID": "content/modelisation/1_modelevaluation.html",
    "href": "content/modelisation/1_modelevaluation.html",
    "title": "Evaluer la qualité d’un modèle",
    "section": "",
    "text": "Nous allons ici voir des méthodes générales permettant de s’assurer que le modèle\nde Machine Learning mobilisé est de qualité. Ce chapitre ne présente pas\nd’exercice ou de code, il est là pour présenter certains concepts\nque nous appliquerons dans les prochains chapitres."
  },
  {
    "objectID": "content/modelisation/1_modelevaluation.html#découper-léchantillon",
    "href": "content/modelisation/1_modelevaluation.html#découper-léchantillon",
    "title": "Evaluer la qualité d’un modèle",
    "section": "Découper l’échantillon",
    "text": "Découper l’échantillon\nLe chapitre précédent présentait le pipeline simple ci-dessous\npour introduire à la notion d’entraînement d’un modèle:\n\n\n\n\n\nCe pipeline fait abstraction d’hypothèses exogènes à l’estimation\nmais qui sont à faire sur des paramètres\ncar elles affectent la performance de la prédiction.\nPar exemple, de nombreux modèles proposent une pénalisation des modèles\nnon parcimonieux pour éviter le sur-apprentissage. Le choix de la pénalisation\nidéale dépend de la structure des données et n’est jamais connue, ex-ante\npar le modélisateur. Faut-il pénaliser fortement ou non le modèle ? En l’absence\nd’argument théorique, on aura tendance à tester plusieurs paramètres de\npénalisation et choisir celui qui permet la meilleure prédiction.\nLa notion de validation croisée permettra de généraliser cette approche. Ces paramètres\nqui affectent la prédiction seront pas la suite appelés des\nhyperparamètres. Comme nous allons le voir, nous allons aboutir à un\nraffinement de l’approche pour obtenir un pipeline ayant plutôt cet aspect:"
  },
  {
    "objectID": "content/modelisation/1_modelevaluation.html#le-problème-du-sur-apprentissage",
    "href": "content/modelisation/1_modelevaluation.html#le-problème-du-sur-apprentissage",
    "title": "Evaluer la qualité d’un modèle",
    "section": "Le problème du sur-apprentissage",
    "text": "Le problème du sur-apprentissage\nLe but du Machine Learning est de calibrer l’algorithme sur des exemples\nconnus (données labellisées) afin de généraliser à des\nexemples nouveaux (éventuellement non labellisés).\nOn vise donc de bonnes qualités\nprédictives et non un ajustement parfait\naux données historiques.\nIl existe un arbitrage biais-variance dans la qualité d’estimation[^1]. Soit \\(h(X,\\theta)\\) un modèle statistique. On\npeut décomposer l’erreur d’estimation en deux parties :\n\\[\n\\mathbb{E}\\bigg[(y - h(\\theta,X))^2 \\bigg] = \\underbrace{ \\bigg( y - \\mathbb{E}(h_\\theta(X)) \\bigg)^2}_{\\text{biais}^2} + \\underbrace{\\mathbb{V}\\big(h(\\theta,X)\\big)}_{\\text{variance}}\n\\]\nIl y a ainsi un compromis à faire entre biais et variance. Un modèle peu parcimonieux, c’est-à-dire proposant un grand nombre de paramètres, va, en général, avoir un faible biais mais une grande variance. En effet, le modèle va tendre à se souvenir d’une combinaison de paramètres à partir d’un grand nombre d’exemples sans être capable d’apprendre la règle qui permette de structurer les données.\n[^1]! Cette formule permet de bien comprendre la théorie statistique asymptotique, notamment le théorème de Cramer-Rao. Dans la classe des estimateurs sans biais, c’est-à-dire dont le premier terme est nul, trouver l’estimateur à variance minimale revient à trouver l’estimateur qui minimise \\(\\mathbb{E}\\bigg[(y - h_\\theta(X))^2 \\bigg]\\). C’est la définition même de la régression, ce qui, quand on fait des hypothèses supplémentaires sur le modèle statistique, explique le théorème de Cramer-Rao.\nPar exemple, la ligne verte ci-dessous est trop dépendante des données et risque de produire une erreur plus importante que la ligne noire (qui moyennise plus) sur de nouvelles données.\n\n\n\n\n\nPour renforcer la validité externe d’un modèle, il est ainsi commun, en Machine Learning:\n\nd’estimer un modèle sur un jeu de données (jeu d’apprentissage ou training set) mais d’évaluer la performance, et donc la pertinence du modèle, sur d’autres données, qui n’ont pas été mobilisées lors de la phase d’estimation (jeu de validation, de test ou testing set) ;\nd’avoir des mesures de performances qui pénalisent fortement les modèles peu parcimonieux (BIC) ou conduire une première phase de sélection de variable (par des méthodes de LASSO…)\n\nPour décomposer un modèle en jeu d’estimation et de test,\nla meilleure méthode est d’utiliser les fonctionnalités de scikit de la manière suivante :\n\nfrom sklearn.model_selection import train_test_split\nxTrain, xTest, yTrain, yTest = train_test_split(\n  x, y,\n  test_size = 0.2,\n  random_state = 0\n  )\n\nLa proportion d’observations dans le jeu de test est contrôlée par l’argument test_size.\nLa proportion optimale n’existe pas.\nLa règle du pouce habituelle est d’assigner aléatoirement 20 % des observations\ndans l’échantillon de test pour garder suffisamment d’observations\ndans l’échantillon d’estimation.\n\n\n Hint \nLorsqu’on travaille avec des séries temporelles, l’échantillonnage aléatoire des observations n’a pas vraiment de sens. Il vaut mieux tester la qualité de l’observation sur des périodes distinguées.\n\n\n\n\n Note\nAvec des données multi-niveaux,\ncomme c’est le cas de données géographiques ou de données individuelles avec des variables de classe,\nil peut être intéressant d’utiliser un échantillonnage stratifié.\nCela permet de garder une proportion équivalente de chaque groupe dans les deux jeux de données de test ou d’apprentissage.\nCe type d’échantillonnage stratifié est également possible avec scikit.\n\n\nL’exercice sur les SVM illustre cette construction et la manière\ndont elle facilite l’évaluation de la qualité d’un modèle."
  },
  {
    "objectID": "content/modelisation/1_modelevaluation.html#validation-croisée",
    "href": "content/modelisation/1_modelevaluation.html#validation-croisée",
    "title": "Evaluer la qualité d’un modèle",
    "section": "Validation croisée",
    "text": "Validation croisée\nCertains algorithmes font intervenir des hyperparamètres,\nc’est-à-dire des paramètres exogènes qui déterminent la prédiction mais ne sont pas estimés.\nLa validation croisée est une méthode permettant de choisir la valeur du paramètre\nqui optimise la qualité de la prédiction en agrégeant\ndes scores de performance sur des découpages différents de l’échantillon d’apprentissage.\nLa validation croisée permet d’évaluer les performances de modèles différents (SVM, random forest, etc.) ou, couplé à une stratégie de grid search de trouver les valeurs des hyperparamètres qui aboutissent à la meilleure prédiction.\n\n\n Note\nL’étape de découpage de l’échantillon de validation croisée est à distinguer de l’étape split_sample_test. A ce stade, on a déjà partitionné les données en échantillon d’apprentissage et test. C’est l’échantillon d’apprentissage qu’on découpe en sous-morceaux.\n\n\nLa méthode la plus commune est la validation croisée k-fold.\nOn partitionne les données en \\(K\\) morceaux et on considère chaque pli, tour à tour, comme un échantillon\nde test en apprenant sur les \\(K-1\\) échantillons restants. Les \\(K\\) indicateurs ainsi calculés sur les \\(K\\) échantillons de test peuvent être moyennés et\ncomparés pour plusieurs valeurs des hyperparamètres.\n\n\n\n\n\nIl existe d’autres types de validation croisée, notamment la leave one out qui consiste à considérer une fois\nexactement chaque observation comme l’échantillon de test (une n-fold cross validation)."
  },
  {
    "objectID": "content/modelisation/1_modelevaluation.html#mesurer-la-performance",
    "href": "content/modelisation/1_modelevaluation.html#mesurer-la-performance",
    "title": "Evaluer la qualité d’un modèle",
    "section": "Mesurer la performance",
    "text": "Mesurer la performance\nJusqu’à présent, nous avons passé sous silence la question du support de \\(y\\), c’est-à-dire\nde l’étendue des valeurs de notre variable d’intérêt.\nEn pratique, la distribution des \\(y\\)\nva néanmoins déterminer deux questions cruciales : la méthode et l’indicateur de performance.\nEn apprentissage supervisé, on distingue en général les problèmes de:\n\nClassification : la variable \\(y\\) est discrète\nRégression : la variable \\(y\\) est continue\n\nLes deux approches ne sont pas sans lien. On peut par exemple voir le modèle économétrique de choix d’offre de travail comme un problème de classification (participation ou non au marché du travail) ou de régression (régression sur un modèle à variable latente)\n\nClassification\nLa plupart des critères de performance sont construits à partir de la matrice de confusion:\n\n\n\nImage empruntée à https://www.lebigdata.fr/confusion-matrix-definition\n\n\nA partir des 4 coins de cette matrice, il existe plusieurs mesure de performance\n\n\n\n\n\n\n\n\nCritère\nMesure\nCalcul\n\n\n\n\nAccuracy\nTaux de classification correcte\nDiagonale du tableau: \\(\\frac{TP+TN}{TP+FP+FN+FP}\\)\n\n\nPrecision\nTaux de vrais positifs\nLigne des prédictions positives : \\(\\frac{TP}{TP+FP}\\)\n\n\nRecall (rappel)\nCapacité à identifier les labels positifs\nColonne des prédictions positives : \\(\\frac{TP}{TP+FN}\\)\n\n\nF1 Score\nMesure synthétique (moyenne harmonique) de la précision et du rappel\n\\(2 \\frac{precision \\times recall}{precision + recall}\\)\n\n\n\nEn présence de classes désequilibrées, la\nF-mesure est plus pertinente pour évaluer les\nperformances mais l’apprentissage restera\nmauvais si l’algorithme est sensible à ce\nproblème. Notamment, si on désire avoir une performance équivalente sur les classes minoritaires, il faut généralement les sur-pondérer (ou faire un échantillonnage stratifié) lors de la constitution de l’échantillon d’observation.\nIl est possible de construire des modèles à partir des probabilités prédites d’appartenir à la classe d’intérêt. Pour cela, on fixe un seuil \\(c\\) tel que\n\\[\n\\mathbb{P}(y_i=1|X_i) &gt; c \\Rightarrow \\widehat{y}_i = 1\n\\]\nPlus on augmente \\(c\\), plus on est sélectif sur le critère d’appartenance à la classe.\nLe rappel, i.e. le taux de faux négatifs, diminue. Mais on augmente le nombre de positifs manqués. Pour chaque valeur de \\(c\\) correspond une matrice de confusion et donc des mesures de performances.\nLa courbe ROC est un outil classique pour représenter en un graphique l’ensemble de ces\ninformations en faisant varier \\(c\\) de 0 à 1:\n\n\n\n\n\nL’aire sous la courbe (AUC) permet d’évaluer quantitativement le meilleur modèle au\nsens de ce critère. L’AUC représente la probabilité que le modèle soit capable de distinguer entre la classe positive et négative.\n\n\nRégression\nEn Machine Learning, les indicateurs de performance en régression sont les suivants :\n\n\n\n\n\n\n\nNom\nFormule\n\n\n\n\nMean squared error\n\\(MSE = \\mathbb{E}\\left[(y - h_\\theta(X))^2\\right]\\)\n\n\nRoot Mean squared error\n\\(RMSE = \\sqrt{\\mathbb{E}\\left[(y - h_\\theta(X))^2\\right]}\\)\n\n\nMean Absolute Error\n\\(MAE = \\mathbb{E} \\bigg[ \\lvert y - h_\\theta(X) \\rvert \\bigg]\\)\n\n\nMean Absolute Percentage Error\n\\(MAE = \\mathbb{E}\\left[ \\left\\lvert \\frac{y - h_\\theta(X)}{y} \\right\\rvert \\right]\\)\n\n\n\nL’économètre se focalise moins sur la qualité de la prédiction et utilisera\nd’autres critères pour évaluer la qualité d’un modèle (certains, comme le BIC, sont\nà regarder aussi dans une optique Machine Learning): \\(R^2\\), \\(BIC\\),\n\\(AIC\\), log-likelihood, etc."
  },
  {
    "objectID": "content/modelisation/index.html",
    "href": "content/modelisation/index.html",
    "title": "Partie 3: modéliser",
    "section": "",
    "text": "Les data scientists sont souvent associés à la mise en oeuvre\nde modèles complexes d’intelligence artificielle.\nLe succès médiatique de ce type d’outils, notamment ChatGPT,\nn’y est pas pour rien. Cependant, la modélisation n’est souvent\nqu’une\nphase du travail du data scientist, un peu comme la visualisation.\nD’ailleurs, dans certaines organisations où la division des tâches\nest plus poussée, les data engineers sont au moins aussi\nimpliqués dans la phase de modélisation que les data scientists.\nC’est souvent un péché de jeunesse de penser qu’on peut résumer\nle travail du data scientist exclusivement à la phase de modélisation.\nCette dernière dépend très fortement de la qualité du travail de\nnettoyage et structuration des données mis en oeuvre en amont. La\nmise en oeuvre de modèles complexes, qui s’accomodent de données\npeu structurées, est gourmande en ressources et coûteuse. Ce ne sont\ndonc qu’un nombre limité d’acteurs qui peuvent entraîner, ex nihilo,\ndes grands modèles de langage1, capables de dépenser au moins 300 000 dollars\ndans l’entraînement d’un modèle, avant même toute phase d’inférence (Izsak, Berchansky, and Levy 2021).\nCes besoins computationnels pour entraîner de grands modèles de langage sont\nd’ailleurs assez gourmands en énergie, ce qui peut amener à\ndes empreintes carbones non négligeables (Strubell, Ganesh, and McCallum 2019; Arcep 2019).\nHeureusement, il est possible de mettre en oeuvre des modélisations plus\nlégères (celles que nous présenterons dans les prochains chapitres)\nou de réutiliser des modèles pré-entraînés pour les spécialiser\nsur un nouveau jeu de données (principe du fine tuning2).\nEn fait, pour être plus pertinent que des approches plus parcimonieuses,\nles techniques de deep learning, notamment\nles réseaux de neurones, nécessitent soit des volumes de données très\nimportants (des millions voire dizaine de millions d’observations) soit\ndes données à la structure complexe comme le langage naturel ou les images.\nDans de nombreux cas, des modèles plus simples comme les techniques d’apprentissage\nautomatique (machine learning) suffisent largement."
  },
  {
    "objectID": "content/modelisation/index.html#la-modélisation-une-approche-au-coeur-de-la-statistique",
    "href": "content/modelisation/index.html#la-modélisation-une-approche-au-coeur-de-la-statistique",
    "title": "Partie 3: modéliser",
    "section": "La modélisation, une approche au coeur de la statistique",
    "text": "La modélisation, une approche au coeur de la statistique\nUn modèle statistique\nest une représentation simplifiée et structurée d’un phénomène réel,\nconstruite à partir d’observations regroupées dans un ensemble partiel de données.\nUn modèle vise à capturer les relations et les schémas sous-jacents au sein de ces données, permettant ainsi de formuler des hypothèses, d’effectuer des prédictions et d’extrapoler des conclusions au-delà\nde l’ensemble de données mesurées.\nLes modèles statistiques fournissent ainsi un cadre analytique pour explorer, comprendre et interpréter les informations contenues dans les données.\nDans le domaine de la recherche économique, ils peuvent servir à\nassocier certains paramètres structurants des modèles de comportement\néconomique à des valeurs quantitatives.\nLes modèles statistiques, comme les modèles économiques\nprésentent néanmoins toujours une part d’irréalisme (Friedman 1953; Salmon 2010)\net accepter de manière trop littérale les implications d’un modèle, même s’il\na de bonnes performances prédictives, peut être dangereux et relever d’un biais\nscientiste. On sélectionne plutôt le moins mauvais modèle\nque le vrai processus générateur des données.\nReprésenter la réalité sous la forme d’un modèle est un principe à la\nbase de la statistique comme discipline scientifique et ayant des\napplications dans de nombreux champs disciplinaires : économie,\nsociologie, géographique, biologie, physique, etc.\nSelon les disciplines, le nom donné peut varier mais on retrouve\nrégulièrement la même approche scientifique : le modélisateur\nconstruit des relations entre plusieurs variables théoriques\nayant des contreparties empiriques afin d’expliquer tel ou tel\nprocessus.\nDans l’enseignement de l’ENSAE ce type d’approche empirique se retrouve\nprincipalement dans deux types d’approches : le machine learning et\nl’économétrie. La différence est certes\nsémantique - la régression linéaire peut être considérée comme une\ntechnique de machine learning ou d’économétrie - mais elle est\négalement conceptuelle :\n\nDans le domaine du machine learning,\nla structure imposée par le modélisateur est minimale et ce sont plutôt\nles algorithmes qui, sur des critères de performance statistique, vont\namener à choisir une loi mathématique qui correspond au mieux aux données ;\nEn économétrie,\nles hypothèses de structure des lois sont plus fortes (même dans un cadre semi ou non-paramétrique) et sont plus souvent imposées\npar le modélisateur.\n\nDans cette partie du cours, nous allons principalement\nparler de machine learning car il s’agit d’une perspective\nplus opérationnelle que l’économétrie qui est plus directement associée\nà des concepts statistiques complexes comme la théorie asymptotique.\nL’adoption du machine learning dans la littérature économique a été longue\ncar la structuration des données est souvent le\npendant empirique d’hypothèses théoriques sur le comportement des acteurs ou des marchés (Athey and Imbens 2019; Charpentier, Flachaire, and Ly 2018).\nPour caricaturer, l’économétrie s’attacherait à comprendre la causalité de certaines variables sur une autre.\nCela implique que ce qui intéresse l’économètre\nest principalement de l’estimation des paramètres (et l’incertitude\nsur l’estimation de ceux-ci) qui permettent de quantifier l’effet d’une\nvariation d’une variable sur une autre.\nToujours pour caricaturer,\nle machine learning se focaliserait\nsur un simple objectif prédictif en exploitant les relations de corrélations entre les variables.\nDans cette perspective, l’important n’est pas la causalité mais le fait qu’une variation\nde \\(x\\)% d’une variable permette d’anticiper un changement de \\(\\beta x\\) de la variable\nd’intérêt ; peu importe la raison.\nMullainathan and Spiess (2017) ont ainsi, pour simplifier, proposé la différence fondamentale qui\nsuit : l’économétrie se préoccupe de \\(\\widehat{\\beta}\\) là où le machine learning\nse focalise sur \\(\\widehat{y}\\). Les deux sont bien sûr reliés dans un cadre\nlinéaire mais cette différence d’approche a des implications importantes\nsur la structure des modèles étudiés, notamment leur parcimonie3."
  },
  {
    "objectID": "content/modelisation/index.html#quelques-définitions",
    "href": "content/modelisation/index.html#quelques-définitions",
    "title": "Partie 3: modéliser",
    "section": "Quelques définitions",
    "text": "Quelques définitions\nDans cette partie du cours nous allons employer un certain nombre\nde termes devenus familiers aux praticiens du machine learning\nmais qui méritent d’être explicités.\n\nMachine learning et deep learning\nJusqu’à présent nous avons beaucoup utilisé, sans le définir, le\nconcept de machine learning, dont la traduction française est\napprentissage automatique mais le terme anglo-saxon est suffisamment\nutilisé pour être considéré comme standard.\nLe machine learning est un ensemble de techniques algorithmiques\nqui permettent aux ordinateurs d’apprendre, à partir d’exemples, à ajuster un modèle\nsans avoir explicitement défini celui-ci. A partir d’algorithmes itératifs et d’une\nmétrique de performance, des règles de classification ou de prédiction vont permettre\nde mettre en relation des caractéristiques (features) avec une variable d’intérêt (label)4.\nDe nombreux algorithmes existent et se distinguent sur la manière d’introduire une structure plus ou\nmoins formelle dans la relation entre les variables observées. Nous n’allons voir que quelques-uns\nde ces algorithmes : support vector machine (SVM), régression logistique, arbres de décision, forêts\naléatoires, etc. Simples à mettre en oeuvre grâce à la librairie Scikit-Learn, ils permettront\ndéjà de comprendre la démarche originale du machine learning que vous pourrez approfondir\nultérieurement.\nAu sein de la grande famille des algorithmes de machine learning, tendent de plus à plus à devenir\nautonomes les techniques de réseaux de neurone. Les techniques qui s’appuient sur les réseaux de neurones sont regroupés\ndans une famille qu’on\nappelle deep learning (apprentissage profond en Français).\nCes réseaux sont inspirés du fonctionnement du cerveau humain et sont composés de nombreuses couches de neurones interconnectés.\nLa structure canonique bien connue est illustrée dans la Figure 1.\nLe deep learning est intéressant pour créer des modèles capables d’apprendre de représentations\nde données complexes et abstraites à partir de données brutes,\nce qui évite parfois la complexe tâche de définir manuellement des caractéristiques spécifiques à cibler.\nLes champs de l’analyse d’image (computer vision) ou du traitement du langage naturel sont les principaux\ncas d’application de ces méthodes.\n\n\n\n\n\n\n\n\n\nFigure 1: Exemple de structure d’un réseau de neurones (source: lebigdata.fr)\n\n\nNous n’allons pas vraiment parler dans ce cours de deep learning car ces modèles, pour être pertinents, nécessitent\nsoit des données structurées d’un volume important (ce qui est rarement disponible\nen open data) soit des cas d’usage spécifiques, plus avancés que ne le permet\nun cours d’introduction. L’organisation HuggingFace, créatrice de la\nplateforme du même nom facilitant la réutilisation de modèles de deep learning\npropose d’excellents cours sur le sujet, notamment sur\nle traitement du langage naturel (NLP).\nNous ferons du traitement du langage naturel dans la prochaine partie de ce cours mais\nde manière plus modeste en revenant sur les concepts nécessaires avant de mettre en oeuvre\nune modélisation sophistiquée du langage.\n\n\nApprentissage supervisé ou non supervisé\nUne ligne de clivage importante entre les méthodes à mettre en oeuvre est le fait d’observer ou non\nle label (la variable \\(y\\)) qu’on désire modéliser.\nPrenons par exemple un site de commerce qui dispose\nd’informations sur ses clients comme l’âge, le sexe, le lieu de résidence.\nCe site peut désirer\nexploiter cette information de différentes manières pour modéliser le comportement d’achat.\nEn premier lieu, ce site peut désirer\nanticiper le volume d’achat d’un nouveau client ayant certaines caractéristiques.\nDans ce cas, il est possible d’utiliser les montants dépensés par d’autres clients en fonction de leurs\ncaractéristiques. L’information pour notre nouveau client n’est pas mesurée mais elle peut s’appuyer\nsur un ensemble d’observations de la même variable.\nMais il est tout à fait possible d’entraîner un modèle sur un label qu’on ne mesure pas, en supposant\nqu’il fasse sens. Par exemple notre site de commerce peut désirer déterminer, en fonction des\ncaractéristiques de notre nouveau client et de sa clientèle existante, s’il appartient à tel ou\ntel groupe de consommateurs : les dépensiers, les économes… Bien sûr on ne sait jamais a priori\nà quel groupe appartient un consommateur mais le rapprochement entre consommateurs ayant un comportement\nsimilaire permettra de donner du sens à cette catégorie. Dans ce cas, l’algorithme apprendra à reconnaître\nquelles caractéristiques sont structurantes dans la constitution de groupes au comportement similaire et\npermettra d’associer tout nouveau consommateur à un groupe.\nCes deux exemples illustrent l’approche différente selon qu’on essaie de construire des modèles\nsur un label observé ou non. Cela constitue même l’une des dualités fondamentale dans les\ntechniques de machine learning :\n\nApprentissage supervisé : la valeur cible est connue et peut être utilisée pour évaluer la qualité d’un modèle ;\nApprentissage non supervisé : la valeur cible est inconnue et ce sont des critères statistiques qui vont amener\nà sélectionner la structure de données la plus plausible.\n\nCette partie du cours illustrera ces deux approches de manière différente à partir du même\njeu de données, les résultats des élections américaines.\nDans le cas de l’apprentissage supervisé, nous chercherons à modéliser directement\nle résultat des candidats aux élections (soit le score soit le gagnant). Dans\nle cas de l’apprentissage non supervisé, nous essaierons de regrouper les\nterritoires au comportement de vote similaire en fonction de facteurs\nsocio-démographiques.\n\n\nClassification et régression\nUne deuxième dualité fondamentale qui est déterminante dans le choix de la méthode de machine learning\nà mettre en oeuvre est la nature du label. S’agit-il d’une variable continue ou d’une variable\ndiscrète, c’est-à-dire prenant un nombre limité de modalités ?\nCette différence de nature entre les données amène à distinguer deux types d’approche :\n\nDans les problématiques de classification, où notre label \\(y\\) a un nombre fini de valeurs5,\non cherche à prédire dans quelle classe ou à quel groupe il est possible de rattacher nos données.\nPar exemple, si vous prenez du café le matin, faites-vous partie du groupe des personnes ronchons au lever ?\nLes métriques de performance utilisent généralement la proportion de bonnes ou mauvaises classifications\npour estimer la qualité d’un modèle.\nDans les problématiques de régression, où notre label est une grandeur numérique, on\ncherche à prédire directement la valeur de notre variable dans le modèle. Par exemple, si vous\navez tel ou tel âge, quel est votre dépense quotidienne en fast food. Les métriques\nde performance sont généralement des moyennes plus ou moins sophistiquées d’écarts entre\nla prédiction et la valeur observée.\n\nEn résumé, l’aide-mémoire suivante, issue de l’aide de Scikit-Learn, peut déjà donner de premiers enseignements sur les différentes familles de modèles:\n\n\n\n\n\n\n\n\n\nFigure 2: Une cheatsheet des algorithmes disponibles dans Scikit-Learn"
  },
  {
    "objectID": "content/modelisation/index.html#données",
    "href": "content/modelisation/index.html#données",
    "title": "Partie 3: modéliser",
    "section": "Données",
    "text": "Données\nLa plupart des exemples de cette partie s’appuient sur les résultats des\nélections US 2020 au niveau comtés. Plusieurs bases sont utilisées pour\ncela :\n\nLes données électorales sont une reconstruction à partir des données du MIT election lab\nproposées sur Github par tonmcg\nou directement disponibles sur le site du MIT Election Lab\nLes données socioéconomiques (population, données de revenu et de pauvreté,\ntaux de chômage, variables d’éducation) proviennent de l’USDA (source)\nLe shapefile vient des données du Census Bureau. Le fichier peut\nêtre téléchargé directement depuis cet url:\nhttps://www2.census.gov/geo/tiger/GENZ2019/shp/cb_2019_us_county_20m.zip\n\nLe code pour construire une base unique à partir de ces sources diverses\nest disponible ci-dessous :\n\n\nimport urllib\nimport urllib.request\nimport os\nimport zipfile\nfrom urllib.request import Request, urlopen\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\n\ndef download_url(url, save_path):\n    with urllib.request.urlopen(url) as dl_file:\n        with open(save_path, 'wb') as out_file:\n            out_file.write(dl_file.read())\n\n\ndef create_votes_dataframes():\n    \n  Path(\"data\").mkdir(parents=True, exist_ok=True)\n  \n  \n  download_url(\"https://www2.census.gov/geo/tiger/GENZ2019/shp/cb_2019_us_county_20m.zip\", \"data/shapefile\")\n  with zipfile.ZipFile(\"data/shapefile\", 'r') as zip_ref:\n      zip_ref.extractall(\"data/counties\")\n  \n  shp = gpd.read_file(\"data/counties/cb_2019_us_county_20m.shp\")\n  shp = shp[~shp[\"STATEFP\"].isin([\"02\", \"69\", \"66\", \"78\", \"60\", \"72\", \"15\"])]\n  shp\n  \n  df_election = pd.read_csv(\"https://raw.githubusercontent.com/tonmcg/US_County_Level_Election_Results_08-20/master/2020_US_County_Level_Presidential_Results.csv\")\n  df_election.head(2)\n  population = pd.read_excel(\"https://www.ers.usda.gov/webdocs/DataFiles/48747/PopulationEstimates.xls?v=290.4\", header = 2).rename(columns = {\"FIPStxt\": \"FIPS\"})\n  education = pd.read_excel(\"https://www.ers.usda.gov/webdocs/DataFiles/48747/Education.xls?v=290.4\", header = 4).rename(columns = {\"FIPS Code\": \"FIPS\", \"Area name\": \"Area_Name\"})\n  unemployment = pd.read_excel(\"https://www.ers.usda.gov/webdocs/DataFiles/48747/Unemployment.xls?v=290.4\", header = 4).rename(columns = {\"fips_txt\": \"FIPS\", \"area_name\": \"Area_Name\", \"Stabr\": \"State\"})\n  income = pd.read_excel(\"https://www.ers.usda.gov/webdocs/DataFiles/48747/PovertyEstimates.xls?v=290.4\", header = 4).rename(columns = {\"FIPStxt\": \"FIPS\", \"Stabr\": \"State\", \"Area_name\": \"Area_Name\"})\n  \n  \n  dfs = [df.set_index(['FIPS', 'State']) for df in [population, education, unemployment, income]]\n  data_county = pd.concat(dfs, axis=1)\n  df_election = df_election.merge(data_county.reset_index(), left_on = \"county_fips\", right_on = \"FIPS\")\n  df_election['county_fips'] = df_election['county_fips'].astype(str).str.lstrip('0')\n  shp['FIPS'] = shp['GEOID'].astype(str).str.lstrip('0')\n  votes = shp.merge(df_election, left_on = \"FIPS\", right_on = \"county_fips\")\n  \n  req = Request('https://dataverse.harvard.edu/api/access/datafile/3641280?gbrecs=false')\n  req.add_header('User-Agent', 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:77.0) Gecko/20100101 Firefox/77.0')\n  content = urlopen(req)\n  df_historical = pd.read_csv(content, sep = \"\\t\")\n  #df_historical = pd.read_csv('https://dataverse.harvard.edu/api/access/datafile/3641280?gbrecs=false', sep = \"\\t\")\n  \n  df_historical = df_historical.dropna(subset = [\"FIPS\"])\n  df_historical[\"FIPS\"] = df_historical[\"FIPS\"].astype(int)\n  df_historical['share'] = df_historical['candidatevotes']/df_historical['totalvotes']\n  df_historical = df_historical[[\"year\", \"FIPS\", \"party\", \"candidatevotes\", \"share\"]]\n  df_historical['party'] = df_historical['party'].fillna(\"other\")\n  \n  df_historical_wide = df_historical.pivot_table(index = \"FIPS\", values=['candidatevotes',\"share\"], columns = [\"year\",\"party\"])\n  df_historical_wide.columns = [\"_\".join(map(str, s)) for s in df_historical_wide.columns.values]\n  df_historical_wide = df_historical_wide.reset_index()\n  df_historical_wide['FIPS'] = df_historical_wide['FIPS'].astype(str).str.lstrip('0')\n  votes['FIPS'] = votes['GEOID'].astype(str).str.lstrip('0')\n  votes = votes.merge(df_historical_wide, on = \"FIPS\")\n  votes[\"winner\"] =  np.where(votes['votes_gop'] &gt; votes['votes_dem'], 'republican', 'democrats') \n\n  return votes\n\n\nCette partie n’est absolument pas exhaustive. Elle constitue un point\nd’entrée dans le sujet à partir d’une série d’exemples sur un fil rouge.\nDe nombreux modèles plus appronfondis, que ce soit en économétrie ou en machine learning\nmériteraient d’être évoqués. Pour les personnes désirant en savoir plus sur les\nmodèles économétriques, qui seront moins évoqués que ceux de machine learning,\nje recommande la lecture de Turrell and contributors (2021)."
  },
  {
    "objectID": "content/modelisation/index.html#références",
    "href": "content/modelisation/index.html#références",
    "title": "Partie 3: modéliser",
    "section": "Références",
    "text": "Références\n\n\nArcep. 2019. “L’empreinte Carbone Du Numérique.” Rapport de l’Arcep.\n\n\nAthey, Susan, and Guido W Imbens. 2019. “Machine Learning Methods That Economists Should Know About.” Annual Review of Economics 11: 685–725.\n\n\nCharpentier, Arthur, Emmanuel Flachaire, and Antoine Ly. 2018. “Econometrics and Machine Learning.” Economie Et Statistique 505 (1): 147–69.\n\n\nFriedman, Milton. 1953. “The Methodology of Positive Economics.” In Essays in Positive Economics. Chicago: The University of Chicago Press.\n\n\nIzsak, Peter, Moshe Berchansky, and Omer Levy. 2021. “How to Train BERT with an Academic Budget.” https://arxiv.org/abs/2104.07705.\n\n\nMullainathan, Sendhil, and Jann Spiess. 2017. “Machine Learning: An Applied Econometric Approach.” Journal of Economic Perspectives 31 (2): 87–106. https://doi.org/10.1257/jep.31.2.87.\n\n\nSalmon, Pierre. 2010. “Le Problème Du réalisme Des Hypothèses En économie Politique.”\n\n\nStrubell, Emma, Ananya Ganesh, and Andrew McCallum. 2019. “Energy and Policy Considerations for Deep Learning in NLP.” https://arxiv.org/abs/1906.02243.\n\n\nTurrell, Arthur, and contributors. 2021. Coding for Economists. Online. https://aeturrell.github.io/coding-for-economists."
  },
  {
    "objectID": "content/modelisation/index.html#footnotes",
    "href": "content/modelisation/index.html#footnotes",
    "title": "Partie 3: modéliser",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNous reviendrons de manière épisodique\nsur ce principe des grands modèles de langage\nqui sont devenus, en quelques années,\ncentraux dans l’écosystème de la data science mais sont également\namenés à devenir des outils grands publics, à la manière de ChatGPT.↩︎\nHistoriquement, cette approche nécessitait de disposer de données labellisées donc d’être\ndans un cadre d’apprentissage supervisé.\nCependant, avec l’utilisation de plus en plus\nfréquente de données non structurées, sans labels, a émergé une approche intéressante\nqui ne nécessite plus forcément de labelliser des volumes importants de données en amont :\nle reinforcement learning with human feedback.\nCet article d’Andrew Ng revient sur la manière dont cette approche\nchange la donne dans l’entraînement ou le ré-entraînement de modèles.↩︎\nComme nous l’avons dit, cette différenciation est un peu\ncaricaturale, notamment maintenant que les économistes sont\nplus familiarisés aux concepts d’évaluation de performance\nprédictive sur des sous-ensembles d’apprentissage et de test (mais\nl’évolution est lente).\nLa recherche en machine learning est quant à elle très dynamique\nsur la question de l’explicabilité et de l’interprétabilité\ndes modèles de machine learning, notamment autour du concept\nde valeurs de Shapley.↩︎\nPour faire l’analogie avec le cadre économétrique, les features sont les variables explicatives\nou covariates (la matrice \\(X\\)) et le label est la variable expliquée (\\(y\\)).↩︎\nNous allons nous focaliser sur le cas binaire, le plus simple. Dans ce type de problèmes,\nla variable \\(y\\) a deux modalités: gagnant-perdant, 0-1, oui-non… Néanmoins il existe de\nnombreux cas d’usage où la variable dispose de plus de modalités, par exemples des\nscores de satisfaction entre 0 et 5 ou A et D. La mise en oeuvre de modèles est plus\ncomplexe mais l’idée générale est souvent de se ramener à un ensemble de modèles dichotomiques\npour pouvoir appliquer des métriques simples et stables.↩︎"
  },
  {
    "objectID": "content/visualisation/matplotlib.html",
    "href": "content/visualisation/matplotlib.html",
    "title": "De beaux graphiques avec python : mise en pratique",
    "section": "",
    "text": "La pratique de la visualisation se fera, dans ce cours, en répliquant des graphiques qu’on peut trouver sur\nla page de l’open data de la ville de Paris\nici.\nCe TP vise à initier :\nNous verrons par la suite la manière de construire des cartes facilement avec\ndes formats équivalents.\nSi vous êtes intéressés par R ,\nune version très proche de ce TP est\ndisponible dans ce cours d’introduction à R pour l’ENS.\nNote\nÊtre capable de construire des visualisations de données\nintéressantes est une compétence nécessaire à tout\ndata scientist ou chercheur. Pour améliorer\nla qualité de ces visualisations, il est recommandé\nde suivre certains conseils donnés par des spécialistes\nde la dataviz sur la sémiologie graphique.\nLes bonnes visualisations de données, comme celles du New York Times,\nreposent certes sur des outils adaptés (des librairies JavaScript)\nmais aussi sur certaines règles de représentation qui permettent\nde comprendre en quelques secondes le message d’une visualisation.\nCe post de blog\nest une ressource qu’il est utile de consulter régulièrement.\nCe post de blog d’Albert Rapp montre bien comment construire graduellement une bonne visualisation\nde données."
  },
  {
    "objectID": "content/visualisation/matplotlib.html#données",
    "href": "content/visualisation/matplotlib.html#données",
    "title": "De beaux graphiques avec python : mise en pratique",
    "section": "Données",
    "text": "Données\nUn sous-ensemble des données de Paris Open Data a été mis à disposition\npour faciliter l’import.\nIl s’agit d’une extraction, qui commence à dater, des données disponibles\nsur le site où seules les colonnes\nqui servent à cet exercice ont été conservées."
  },
  {
    "objectID": "content/visualisation/matplotlib.html#premières-productions-graphiques-avec-lapi-matplotlib-de-pandas",
    "href": "content/visualisation/matplotlib.html#premières-productions-graphiques-avec-lapi-matplotlib-de-pandas",
    "title": "De beaux graphiques avec python : mise en pratique",
    "section": "Premières productions graphiques avec l’API Matplotlib de Pandas",
    "text": "Premières productions graphiques avec l’API Matplotlib de Pandas\nChercher à produire une visualisation parfaite du premier coup est\nillusoire. Il est beaucoup plus réaliste d’améliorer graduellement\nune représentation graphique afin, petit à petit, de mettre en\navant les effets de structure dans un jeu de données.\nNous allons donc commencer par nous représenter la distribution\ndes passages aux principales stations de mesure.\nPour cela nous allons produire\nrapidement un barplot puis l’améliorer graduellement.\nDans cette partie, nous allons ainsi\nreproduire les deux premiers graphiques de la\npage d’analyse des données :\nLes 10 compteurs avec la moyenne horaire la plus élevée et Les 10 compteurs ayant comptabilisé le plus de vélos. Les valeurs chiffrées des graphiques seront différentes de celles de la page en ligne, c’est normal, nous travaillons sur des données plus anciennes.\n\n\n Exercice 1 : Importer les données et produire un premier graphique\nLes données comportent plusieurs dimensions pouvant faire l’objet d’une\nanalyse statistique. Il est donc nécessaire dans un premier temps\nde synthétiser celles-ci par des agrégations afin d’avoir un\ngraphique lisible.\n\nImporter les données de compteurs de vélos. Vous pouvez utiliser l’url https://minio.lab.sspcloud.fr/projet-formation/diffusion/python-datascientist/bike.csv.\n\n\n\n⚠️ Warning sur le format de données\n\nIl s’agit de données\ncompressées au format gzip, il faut donc utiliser l’option compression = 'gzip'\n\n\nGarder les dix bornes à la moyenne la plus élevée.\nComme pour obtenir un graphique ordonné du plus grand au plus petit,\nil faut avoir les données ordonnées du plus petit au\nplus grand (oui c’est bizarre mais c’est comme ça…), réordonner\nles données ;\nEn premier lieu, sans se préoccuper des éléments de style ni de la beauté\ndu graphique, créer la structure du barplot (diagramme en batons) de la\npage d’analyse des données.\nPour préparer le travail sur la deuxième figure, ne conserver\nque les 10 compteurs ayant comptabilisés le plus de vélos\nComme pour la question 3, créer un barplot\npour reproduire la figure 2 de l’open data parisien\n\n\n\n\n\n\nPremières lignes nécessaires pour cet exercice :\n\n\n\n\n\n\n\n\n\n\nIdentifiant du compteur\nNom du compteur\nIdentifiant du site de comptage\nNom du site de comptage\nComptage horaire\nDate et heure de comptage\nDate d'installation du site de comptage\n\n\n\n\n0\n100003096-SC\n97 avenue Denfert Rochereau SO-NE\n100003096\n97 avenue Denfert Rochereau SO-NE\n1\n2019-08-01T02:00:00Z\n2012-02-22\n\n\n1\n100003096-SC\n97 avenue Denfert Rochereau SO-NE\n100003096\n97 avenue Denfert Rochereau SO-NE\n0\n2019-08-01T01:00:00Z\n2012-02-22\n\n\n2\n100003096-SC\n97 avenue Denfert Rochereau SO-NE\n100003096\n97 avenue Denfert Rochereau SO-NE\n0\n2019-08-01T04:00:00Z\n2012-02-22\n\n\n\n\n\n\n\n\n\n\n\n\nLes 10 principales stations à l'issue de la question 2\n\n\n\n\n\n\n\n\n\n\nComptage horaire\n\n\nNom du compteur\n\n\n\n\n\n26 boulevard de Ménilmontant SE-NO\n109.462847\n\n\n35 boulevard de Menilmontant NO-SE\n117.180643\n\n\n21 boulevard Saint Michel S-N\n117.730884\n\n\n67 boulevard Voltaire SE-NO\n119.208018\n\n\n72 boulevard Voltaire NO-SE\n124.391365\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1 sans travail sur le style:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2 sans travail sur le style:\n\n\n\n\n\n\n\n\n\n\n\n\nOn commence à avoir quelque chose qui commence à transmettre\nun message synthétique sur la nature des données.\nOn peut néanmoins remarquer plusieurs éléments problématiques\n(par exemple les labels) mais\naussi des éléments ne correspondant pas (les titres des axes, etc.) ou\nmanquants (le nom du graphique…).\nComme les graphiques produits par Pandas suivent la logique très flexible\nde matplotlib, il est possible de les customiser. Cependant, c’est\nsouvent beaucoup de travail et la grammaire matplotlib n’est\npas aussi normalisée que celle de ggplot en R.\nIl peut être préférable de directement\nutiliser seaborn, qui offre quelques arguments prêts à l’emploi."
  },
  {
    "objectID": "content/visualisation/matplotlib.html#utiliser-directement-seaborn",
    "href": "content/visualisation/matplotlib.html#utiliser-directement-seaborn",
    "title": "De beaux graphiques avec python : mise en pratique",
    "section": "Utiliser directement seaborn",
    "text": "Utiliser directement seaborn\nVous pouvez repartir des deux dataframes précédents. On va supposer qu’ils se\nnomment df1 et df2.\nLa figure comporte maintenant un message mais il est encore peu\nlisible. Il y a plusieurs manières de faire un barplot en seaborn. Les deux principales\nsont :\n\nsns.catplot ;\nsns.barplot.\n\nOn propose d’utiliser sns.catplot pour cet exercice.\n\n\n Exercice 2 : Un peu de style !\n\nRéinitialiser l’index des dataframes df1 et df2\npour avoir une colonne ‘Nom du compteur’. Réordonner les données\nde manière décroissante pour obtenir un graphique ordonné dans\nle bon sens avec seaborn.\nRefaire le graphique précédent avec la fonction catplot de seaborn. Pour\ncontrôler la taille du graphique vous pouvez utiliser les arguments height et\naspect.\nAjouter les titres des axes et le titre du graphique pour le premier graphique\nEssayez de colorer en rouge l’axe des x. Vous pouvez pré-définir un\nstyle avec sns.set_style(\"ticks\", {\"xtick.color\": \"red\"})\n\n\n\nA l’issue de la question 2, c’est-à-dire en utilisant\nseaborn pour reproduire de manière minimale\nun barplot, on obtient :\n\n\n\n\n\n\n\n\n\nAprès quelques réglages esthétiques, à l’issue des questions 3 et 4,\non obtient une figure proche de celle de l’open data parisien.\n\n\n\n\n\n\n\n\n\nOn comprend\nainsi que le boulevard de Sébastopol est le plus emprunté,\nce qui ne vous suprendra pas si vous faites du vélo à Paris.\nNéanmoins, si vous n’êtes pas familiers avec la géographie parisienne,\ncela sera peu informatif pour vous, vous allez avoir besoin d’une\nreprésentation graphique supplémentaire: une carte ! Nous verrons\nceci lors d’un prochain chapitre.\n\n\n Exercice 3 : reproduire la figure 2\nEn suivant l’approche graduelle de l’exercice 2,\nrefaire le graphique Les 10 compteurs ayant comptabilisé le plus de vélos.\n\n\nLes diagrammes en batons (barplot) sont extrêmement communs mais\nqu’ils transmettent. Sur le plan sémiologique,\nles lollipop charts sont préférables : ils\ntransmettent la même information mais avec moins de bruit\n(la largeur des barres du barplot noie un peu l’information).\n\n\n Exercice 3 (bis) : reproduire la figure 2 avec un lollipop chart\nEn suivant l’approche graduelle de l’exercice 2,\nrefaire le graphique Les 10 compteurs ayant comptabilisé le plus de vélos.\n\n\n\n\nText(0, 0.5, 'La somme des vélos comptabilisés sur la période sélectionnée')"
  },
  {
    "objectID": "content/visualisation/matplotlib.html#premières-agrégations-temporelles",
    "href": "content/visualisation/matplotlib.html#premières-agrégations-temporelles",
    "title": "De beaux graphiques avec python : mise en pratique",
    "section": "Premières agrégations temporelles",
    "text": "Premières agrégations temporelles\nOn va maintenant se concentrer sur la dimension spatiale de notre\njeu de données à travers deux approches :\n\nUn diagramme en barre synthétisant l’information de notre jeu de données\nde manière mensuelle ;\nDes séries instructives sur la dynamique temporelle. Cela sera l’objet de la prochaine partie.\n\nPour commencer, reproduisons la troisième figure qui est, encore une fois,\nun barplot. La première question implique une première rencontre avec\nune donnée temporelle à travers une opération assez classique en séries\ntemporelles : changer le format d’une date pour pouvoir faire une agrégation\nà un pas de temps plus large.\n\n\n Exercice 4: barplot des comptages mensuels\n\nUtiliser to_datetime du package Pandas pour transformer la variable Date et heure de comptage\nen horodatage car le type de celle-ci a été mal interprété à la lecture du fichier.\nLe format à utiliser est %Y-%m-%dT%H:%M:%SZ. Il peut être nécessaire d’utiliser également l’option errors='coerce'\nCréer une variable month\ndont le format respecte, par exemple, le schéma 2019-08 grâce à la bonne option de la méthode dt.to_period\nAppliquer les conseils précédents pour construire et améliorer\ngraduellement un graphique afin d’obtenir une figure similaire\nà la 3e production sur la page de l’open data parisien.\nQuestion optionnelle: représenter la même information sous forme de lollipop\n\n\n\n\n\n\n\n\n\n\n\n\nmonth\nvalue\n\n\n\n\n0\n2019-08\n33.637536\n\n\n1\n2019-09\n55.831038\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\nSi vous préférez représenter cela sous forme de lollipop1:\n\n\n\n\n\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;"
  },
  {
    "objectID": "content/visualisation/matplotlib.html#première-série-temporelle",
    "href": "content/visualisation/matplotlib.html#première-série-temporelle",
    "title": "De beaux graphiques avec python : mise en pratique",
    "section": "Première série temporelle",
    "text": "Première série temporelle\nIl est plus commun de représenter sous forme de série\nles données ayant une dimension temporelle.\n\n\n Exercice 5: barplot des comptages mensuels\n\nCréer une variable day qui transforme l’horodatage en format journalier\ndu type 2021-05-01 avec dt.day.\nReproduire la figure de la page d’open data\n\n\n\n\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n\n&lt;matplotlib.collections.PolyCollection at 0x7fb390f88220&gt;"
  },
  {
    "objectID": "content/visualisation/matplotlib.html#des-graphiques-dynamiques-avec-plotly",
    "href": "content/visualisation/matplotlib.html#des-graphiques-dynamiques-avec-plotly",
    "title": "De beaux graphiques avec python : mise en pratique",
    "section": "Des graphiques dynamiques avec Plotly",
    "text": "Des graphiques dynamiques avec Plotly\n\nContexte\nL’inconvénient des figures avec ggplot est que celles-ci ne permettent\npas d’interaction avec le lecteur. Toute l’information doit donc être\ncontenue dans la figure ce qui peut la rendre difficile à lire.\nSi la figure est bien faite, avec différents niveaux d’information, cela\npeut bien fonctionner.\nIl est néanmoins plus simple, grâce aux technologies web, de proposer des\nvisualisations à plusieurs niveaux. Un premier niveau d’information, celui du\ncoup d’oeil, peut suffire à assimiler les principaux messages de la\nvisualisation. Ensuite, un comportement plus volontaire de recherche\nd’information secondaire peut permettre d’en savoir plus. Les visualisations\nréactives, qui sont maintenant la norme dans le monde de la dataviz,\npermettent ce type d’approche : le lecteur d’une visualisation peut passer\nsa souris à la recherche d’information complémentaire (par exemple les\nvaleurs exactes) ou cliquer pour faire apparaître des informations complémentaires\nsur la visualisation ou autour.\nCes visualisations reposent sur le même triptyque que l’ensemble de l’écosystème\nweb : HTML, CSS et JavaScript. Les utilisateurs de Python\nne vont jamais manipuler directement ces langages, qui demandent une\ncertaine expertise, mais vont utiliser des librairies au niveau de R qui génèreront automatiquement tout le code HTML, CSS et JavaScript\npermettant de créer la figure.\n\n\nLa librairie Plotly\nLe package Plotly est une surcouche à la librairie Javascript\nPlotly.js qui permet de créer et manipuler des objets graphiques de manière\ntrès flexible afin de produire des objets réactifs sans avoir à recourir\nà Javascript.\nLe point d’entrée recommandé est le module plotly.express\n(documentation ici) qui offre une arborescence\nriche mais néanmoins intuitive pour construire des graphiques\n(objets plotly.graph_objects.Figure) pouvant être modifiés a posteriori\nsi besoin (par exemple pour customiser les axes).\n\n\n Visualiser les figures produites par Plotly\nDans un notebook Jupyter classique, les lignes suivantes de code permettent\nd’afficher le résultat d’une commande Plotly sous un bloc de code :\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected = True)\nPour JupyterLab, l’extension jupyterlab-plotly s’avère nécessaire:\n!jupyter labextension install jupyterlab-plotly\n\n\n\n\nRéplication de l’exemple précédent avec Plotly\nLes représentations figées comme celles ci-dessus\nsont approriées pour des rapports ou articles.\nNéanmoins\nLes modules suivants seront nécessaires pour construire des graphiques\navec plotly:\n\n\n Exercice 7: un barplot avec Plotly\nL’objectif est de reconstuire le premier diagramme en barre rouge avec Plotly.\n\nRéalisez le graphique en utilisant la fonction adéquate avec plotly.express et…\n\nNe pas prendre le\nthème par défaut mais un à fond blanc, pour avoir un résultat ressemblant\nà celui proposé sur le site de l’open-data.\nPour la couleur rouge,\nvous pouvez utiliser l’argument color_discrete_sequence.\nNe pas oublier de nommer les axes\nPensez à la couleur du texte de l’axe inférieur\n\nTester un autre thème, à fond sombre. Pour les couleurs, faire un\ngroupe stockant les trois plus fortes valeurs puis les autres.\n\n\n\nLa première question permet de construire le graphique suivant :\n\n\n\n                                                \n\n\nAlors qu’avec le thème sombre (question 2), on obtient :\n\n\n\n                                                \n\n\nCette représentation montre bien le caractère spécial de l’année 2020. Pour\nrappeller au lecteur distrait la nature particulière de la période, marquée\npar un premier confinement qu’on voit bien dans les données, on pourrait,\navec l’aide de la documentation,\najouter deux barres verticales pour marquer les dates de début et\nde fin de cette période."
  },
  {
    "objectID": "content/visualisation/matplotlib.html#bonus",
    "href": "content/visualisation/matplotlib.html#bonus",
    "title": "De beaux graphiques avec python : mise en pratique",
    "section": "Bonus",
    "text": "Bonus\nEn bonus, l’état d’esprit des habitués de ggplot2\nquand ils découvrent plotnine:"
  },
  {
    "objectID": "content/visualisation/matplotlib.html#footnotes",
    "href": "content/visualisation/matplotlib.html#footnotes",
    "title": "De beaux graphiques avec python : mise en pratique",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nJ’ai retiré la couleur sur l’axe des ordonnées qui, je trouve,\napporte peu à la figure voire dégrade la compréhension du message.↩︎"
  },
  {
    "objectID": "content/manipulation/07_dask.html",
    "href": "content/manipulation/07_dask.html",
    "title": "Introduction à dask grâce aux données DVF",
    "section": "",
    "text": "Pour essayer les exemples présents dans ce tutoriel :\nLa documentation complète sur Dask se trouve sur https://docs.dask.org/.\nLe projet requiert l’installation de dask. Afin d’avoir\nla distribution complète on utilise la commande suivante :"
  },
  {
    "objectID": "content/manipulation/07_dask.html#pourquoi-utiliser-dask",
    "href": "content/manipulation/07_dask.html#pourquoi-utiliser-dask",
    "title": "Introduction à dask grâce aux données DVF",
    "section": "Pourquoi utiliser Dask ?",
    "text": "Pourquoi utiliser Dask ?\nOn peut se référer à la page https://docs.dask.org/en/stable/why.html\nPlusieurs points sont mis en avant dans la documentation officielle et sont résumés ci-dessous:\n- Dask ressemble fortement en termes de syntaxe à pandas et numpy ;\n- Dask peut être utilisé sur un ordinateur seul ou sur un cloud cluster. Avec Dask, on peut traiter des bases de 100GB sur un ordinateur portable, voire même 1TB sans même avoir besoin d’un cluster big data ;\n- Dask requiert peu de temps d’installation puisqu’il peut être installé avec le gestionnaire de packages conda (il est même livré dans la distribution par défaut d’Anaconda)\n\nComment Dask se compare à Spark ?\nDans le monde du big-data, un écosystème concurrent existe: Spark. Globalement, lorsqu’on a compris la logique\nde l’un, il est très facile de faire la transition vers l’autre si besoin1.\n\n\nSpark est écrit en Scala à l’origine. Le package pyspark permet d’écrire en Python et s’assure de la traduction en Python afin d’interagir avec les machines virtuelles Java (JVM) nécessaires pour la parallélisation des opérations Spark. Dask est quant à lui écrit en Python, ce qui est un écosystème plus léger. Pour gagner en performance, il permet d’interagir avec du code C/C++ entre autres ;\nL’installation de Spark est plus lourde que celle de Dask\nSpark est un projet Apache en lui-même alors que Dask intervient comme une composante de l’univers Python;\nSpark est un peu plus vieux (2010 versus 2014 pour Dask) ;\nSpark permet de très bien faire des opérations classiques SQL et des ETLs, et proposer ses propres librairies de parallélisation de modèles de machine learning. Pour faire du machine learning avec Spark il faut aller piocher dans Spark MLLib. Dask permet quant à lui de bien interagir avec scikit-learn et de faire de la modélisation.\n\nGlobalement, il faut retenir que Dask comme Spark ne sont intéressants que pour des données dont le traitement engendre des problèmes de RAM. Autrement, il\nvaut mieux se contenter de pandas."
  },
  {
    "objectID": "content/manipulation/07_dask.html#démonstration-de-quelques-features-de-dask",
    "href": "content/manipulation/07_dask.html#démonstration-de-quelques-features-de-dask",
    "title": "Introduction à dask grâce aux données DVF",
    "section": "Démonstration de quelques features de Dask",
    "text": "Démonstration de quelques features de Dask\n\nPrésentation du Dask.DataFrame\nNous allons utiliser les données immobilières DVF pour montrer quelques éléments clefs de Dask.\n\n# Import dvf files \nimport pandas as pd\nimport dask.dataframe as dd\n\nd_urls = {\n    \"2019\" : 'https://www.data.gouv.fr/fr/datasets/r/3004168d-bec4-44d9-a781-ef16f41856a2',\n    \"2020\" : \"https://www.data.gouv.fr/fr/datasets/r/90a98de0-f562-4328-aa16-fe0dd1dca60f\",\n    \"2021\": \"https://www.data.gouv.fr/fr/datasets/r/817204ac-2202-4b4a-98e7-4184d154d98c\"\n}\n\n\ndef import_dvf_one_year(year, dict_url = d_urls):\n    df = pd.read_csv(dict_url[year], sep = \"|\", decimal=\",\")\n    df[\"year\"] = year\n    return df\n\ndef import_dvf_all_years(dict_url = d_urls):\n    dfs = [import_dvf_one_year(y, dict_url) for y in dict_url.keys()]\n    df = pd.concat(dfs).reset_index()\n    df = df.drop([\"level_0\", \"level_1\"], axis=1)\n    return df\n\nDans un premier temps, on va utiliser pandas pour\nimporter une année de données (millésime 2019), ces dernières tenant en mémoire\nsur un ordinateur normalement doté en RAM2:\n\ndvf = import_dvf_one_year(\"2019\")\ndvf.shape\ndvf.head()\n\n/tmp/ipykernel_3840/455432909.py:13: DtypeWarning:\n\nColumns (18,23,24,26,28,41) have mixed types. Specify dtype option on import or set low_memory=False.\n\n\n\n\n\n\n\n\n\n\nIdentifiant de document\nReference document\n1 Articles CGI\n2 Articles CGI\n3 Articles CGI\n4 Articles CGI\n5 Articles CGI\nNo disposition\nDate mutation\nNature mutation\n...\nNombre de lots\nCode type local\nType local\nIdentifiant local\nSurface reelle bati\nNombre pieces principales\nNature culture\nNature culture speciale\nSurface terrain\nyear\n\n\n\n\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n04/01/2019\nVente\n...\n1\n2.0\nAppartement\nNaN\n20.0\n1.0\nNaN\nNaN\nNaN\n2019\n\n\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n04/01/2019\nVente\n...\n1\n3.0\nDépendance\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\n2019\n\n\n2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n04/01/2019\nVente\n...\n2\n2.0\nAppartement\nNaN\n62.0\n3.0\nNaN\nNaN\nNaN\n2019\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n08/01/2019\nVente\n...\n0\n1.0\nMaison\nNaN\n90.0\n4.0\nS\nNaN\n940.0\n2019\n\n\n4\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n07/01/2019\nVente\n...\n0\n1.0\nMaison\nNaN\n101.0\n5.0\nS\nNaN\n490.0\n2019\n\n\n\n\n5 rows × 44 columns\n\n\n\nIci on travaille sur un DataFrame d’environ 3.5 millions de lignes et 44 variables.\nL’objet dvf est un pandas.DataFrame\nqui tient en mémoire sur le SSP Cloud ou sur les serveurs utilisés\npour construire ce site web.\n\n\n Exercice 1\nOn aurait pu lire directement les csv dans un dask.DataFrame avec le read_csv de dask. Comme exercice, vous pouvez essayer de le faire\npour une année (analogue de la fonction import_dvf_one_year) puis sur toutes les données (analogue de la fonction import_dvf_all_years).\n\n\nOn peut créer une structure Dask directement à partir\nd’un DataFrame pandas avec la méthode from_pandas.\n\ndvf_dd = dd.from_pandas(dvf, npartitions=10) \ndvf_dd\n\nDask DataFrame Structure:\n\n\n\n\n\n\n\nIdentifiant de document\nReference document\n1 Articles CGI\n2 Articles CGI\n3 Articles CGI\n4 Articles CGI\n5 Articles CGI\nNo disposition\nDate mutation\nNature mutation\nValeur fonciere\nNo voie\nB/T/Q\nType de voie\nCode voie\nVoie\nCode postal\nCommune\nCode departement\nCode commune\nPrefixe de section\nSection\nNo plan\nNo Volume\n1er lot\nSurface Carrez du 1er lot\n2eme lot\nSurface Carrez du 2eme lot\n3eme lot\nSurface Carrez du 3eme lot\n4eme lot\nSurface Carrez du 4eme lot\n5eme lot\nSurface Carrez du 5eme lot\nNombre de lots\nCode type local\nType local\nIdentifiant local\nSurface reelle bati\nNombre pieces principales\nNature culture\nNature culture speciale\nSurface terrain\nyear\n\n\nnpartitions=10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nint64\nstring\nstring\nfloat64\nfloat64\nstring\nstring\nstring\nstring\nfloat64\nstring\nstring\nint64\nfloat64\nstring\nint64\nstring\nstring\nfloat64\nstring\nfloat64\nstring\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nint64\nfloat64\nstring\nfloat64\nfloat64\nfloat64\nstring\nstring\nfloat64\nstring\n\n\n362713\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3264417\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3627129\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n\n\n\nDask Name: to_pyarrow_string, 2 graph layers\n\n\nPour souligner la différence avec un pandas.DataFrame,\nl’affichage diffère. Seule la structure du dask.DataFrame\nest affichée et non son contenu car les données\ndask ne sont pas chargées en mémoire.\n\n\n Warning\nAttention, Dask ne peut créer un Dask.DataFrame à partir d’un pandas.DataFrame multi-indexé.\nDans ce cas il a fallu faire un reset_index() pour avoir un unique index.\n\n\nOn a ainsi la structure de notre dask.DataFrame, soit environ 3.5 millions de lignes, avec 44 colonnes en 10 partitions, soit environ 350 000 observations par partition.\nIl faut savoir que Dask produit des Array, Bag et DataFrames, qui fonctionnent comme dans Numpy et Pandas (il est possible de créer d’autres structures ad hoc, cf plus loin).\nDask, comme Spark et en fait comme la plupart des frameworks permettant de\ntraiter des données plus volumineuses que la RAM disponible,\nrepose sur le principe du partitionnement et de la parallélisation\ndes opérations. Les données ne sont jamais importées dans leur\nensemble mais par bloc. Un plan des opérations à effectuer est\nensuite appliquer sur chaque bloc (nous reviendrons\nsur ce principe), indépendamment. La particularité de Dask,\npar rapport à Spark,\nest que chaque bloc est un pandas.DataFrame, ce qui\nrend très facile l’application de manipulations de données\ntraditionnelles sur des sources volumineuses:\n\n\n\n\n\nLe site de Dask cite une règle qui est la suivante :\n\n“Have 5 to 10 times as much RAM as the size of your dataset”,\n@mckinney2017apache, 10 things I hate about pandas\n\nSur disque, en sauvegardant en CSV, on\nobtient une base de 1.4GB. Si l’on suit la règle du pouce donnée plus haut, on va avoir besoin d’une RAM entre 7-14GB pour traiter la donnée, en fonction de nos traitements qui seront plus ou moins intensifs. Autrement dit, si on a moins de 8GB de RAM, il devient intéressant de faire appel à dask, sinon il vaut mieux privilégier pandas (sauf si on fait des\ntraitements très intensifs en calculs).\nIl existe un autre objet dask, les Array pour reprendre la logique de numpy. De la même manière qu’un dask.DataFrame est en quelque sorte un ensemble de pandas.DataFrame, un dask.Array est un ensemble de numpy.Array qui sont plus importants en taille que la RAM. On pourra utiliser les opérations courantes numpy avec dask de la même manière que le dask DataFrame réplique la logique du pandas DataFrame.\n\n\n Hint\nLe choix du nombre de partition (10) est arbitraire ici. Bien qu’on puisse\ntrouver des règles du pouce pour fixer un nombre optimal de\npartitions, cela dépend de beaucoup de facteurs et, en pratique,\nrien ne remplace l’essai-erreur. Par exemple, la documentation Dask recommande des blocs d’environ\n100MB\nce qui peut convenir pour des ordinateurs à la RAM limitée mais n’a pas\nforcément de sens pour des machines ayant 16GB de RAM.\nUn nombre important de partition va permettre de faire des opérations\nsur des petits blocs de données, ce qui permettra de gagner en vitesse\nd’exécution. Le prix à payer est beaucoup d’input/output car\nDask va passer du temps à lire beaucoup de blocs de données et écrire\ndes bases intermédiaires.\n\n\nOn peut accéder aux index que couvrent les partitions de la manière suivante :\n\ndvf_dd.divisions\n\n(0,\n 362713,\n 725426,\n 1088139,\n 1450852,\n 1813565,\n 2176278,\n 2538991,\n 2901704,\n 3264417,\n 3627129)\n\n\nAutrement dit, la première partition couvrira les lignes 0 à 362713. La deuxième les lignes 362714 à 725426, etc.\nEt on peut directement accéder à une partition grâce aux crochets []:\n\ndvf_dd.partitions[0]\n\nDask DataFrame Structure:\n\n\n\n\n\n\n\nIdentifiant de document\nReference document\n1 Articles CGI\n2 Articles CGI\n3 Articles CGI\n4 Articles CGI\n5 Articles CGI\nNo disposition\nDate mutation\nNature mutation\nValeur fonciere\nNo voie\nB/T/Q\nType de voie\nCode voie\nVoie\nCode postal\nCommune\nCode departement\nCode commune\nPrefixe de section\nSection\nNo plan\nNo Volume\n1er lot\nSurface Carrez du 1er lot\n2eme lot\nSurface Carrez du 2eme lot\n3eme lot\nSurface Carrez du 3eme lot\n4eme lot\nSurface Carrez du 4eme lot\n5eme lot\nSurface Carrez du 5eme lot\nNombre de lots\nCode type local\nType local\nIdentifiant local\nSurface reelle bati\nNombre pieces principales\nNature culture\nNature culture speciale\nSurface terrain\nyear\n\n\nnpartitions=1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nint64\nstring\nstring\nfloat64\nfloat64\nstring\nstring\nstring\nstring\nfloat64\nstring\nstring\nint64\nfloat64\nstring\nint64\nstring\nstring\nfloat64\nstring\nfloat64\nstring\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nint64\nfloat64\nstring\nfloat64\nfloat64\nfloat64\nstring\nstring\nfloat64\nstring\n\n\n362713\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n\n\n\nDask Name: blocks, 3 graph layers\n\n\n\n\nLa “lazy evaluation”\nDask fait de la “lazy evaluation”. Cela signifie que le résultat n’est calculé que si on le demande explicitement. Dans le cas, contraire, ce que l’on appelle un dask task graph est produit (on verra plus bas comment voir ce graph).\nPour demander explicitement le résultat d’un calcul, il faut utiliser la\nméthode compute.\nA noter que certaines méthodes vont déclencher un compute directement, comme par exemple len ou head.\nPar exemple, pour afficher le contenu des 100 premières lignes :\n\ndvf_dd.loc[0:100,:].compute()\n\n\n\n\n\n\n\n\nIdentifiant de document\nReference document\n1 Articles CGI\n2 Articles CGI\n3 Articles CGI\n4 Articles CGI\n5 Articles CGI\nNo disposition\nDate mutation\nNature mutation\n...\nNombre de lots\nCode type local\nType local\nIdentifiant local\nSurface reelle bati\nNombre pieces principales\nNature culture\nNature culture speciale\nSurface terrain\nyear\n\n\n\n\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n04/01/2019\nVente\n...\n1\n2.0\nAppartement\nNaN\n20.0\n1.0\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n2019\n\n\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n04/01/2019\nVente\n...\n1\n3.0\nDépendance\nNaN\n0.0\n0.0\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n2019\n\n\n2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n04/01/2019\nVente\n...\n2\n2.0\nAppartement\nNaN\n62.0\n3.0\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n2019\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n08/01/2019\nVente\n...\n0\n1.0\nMaison\nNaN\n90.0\n4.0\nS\n&lt;NA&gt;\n940.0\n2019\n\n\n4\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n07/01/2019\nVente\n...\n0\n1.0\nMaison\nNaN\n101.0\n5.0\nS\n&lt;NA&gt;\n490.0\n2019\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n96\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n04/01/2019\nVente\n...\n0\n1.0\nMaison\nNaN\n48.0\n2.0\nS\n&lt;NA&gt;\n935.0\n2019\n\n\n97\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n07/01/2019\nVente\n...\n0\nNaN\n&lt;NA&gt;\nNaN\nNaN\nNaN\nP\n&lt;NA&gt;\n3264.0\n2019\n\n\n98\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n18/01/2019\nVente\n...\n0\nNaN\n&lt;NA&gt;\nNaN\nNaN\nNaN\nP\n&lt;NA&gt;\n2870.0\n2019\n\n\n99\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n07/01/2019\nVente\n...\n0\nNaN\n&lt;NA&gt;\nNaN\nNaN\nNaN\nT\n&lt;NA&gt;\n1423.0\n2019\n\n\n100\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n03/01/2019\nVente\n...\n0\nNaN\n&lt;NA&gt;\nNaN\nNaN\nNaN\nP\n&lt;NA&gt;\n93.0\n2019\n\n\n\n\n101 rows × 44 columns\n\n\n\nCe qui est pratique avec dask.dataframe c’est que de nombreuses méthodes sont semblables à celles de pandas. Par exemple, si l’on souhaite connaitre les types de locaux présents dans la base en 2019:\n\ndvf_dd.loc[:,\"Type local\"].value_counts().compute()\n\nType local\nMaison                                      712958\nAppartement                                 656819\nDépendance                                  496414\nLocal industriel. commercial ou assimilé    143265\nName: count, dtype: int64[pyarrow]\n\n\nA titre de comparaison, comparons les temps de calculs entre pandas et dask ici:\n\nimport time\nstart_time = time.time()\ndvf_dd.loc[:,\"Type local\"].value_counts().compute()\nprint(f\"{time.time() - start_time} seconds\")\n\n3.3733575344085693 seconds\n\n\n\nstart_time = time.time()\ndvf.loc[:,\"Type local\"].value_counts()\nprint(f\"{time.time() - start_time} seconds\")\n\n0.12499499320983887 seconds\n\n\nOn se rend compte que le pandas.DataFrame a un temps de calcul plus court, mais c’est parce que dask va nous servir avant tout à lire des bases dont le traitement excède notre RAM. Donc, cette comparaison n’existera tout simplement pas car le pandas.DataFrame n’aura pas été chargé en RAM. On voit dans cet exemple que lorsque le traitement du DataFrame tient en RAM, l’utilisation de Dask est inutile.\nLes méthodes dans Dask peuvent être chainées, comme dans pandas, par exemple, on pourra écrire:\n\nmean_by_year = dvf_dd.loc[~dvf_dd[\"Surface terrain\"].isna(),[\"Surface terrain\", \"year\"]].groupby(\"year\").mean()\n\n\nmean_by_year.compute()\n\n\n\n\n\n\n\n\nSurface terrain\n\n\nyear\n\n\n\n\n\n2019\n3064.685954\n\n\n\n\n\n\n\nLe principe de la lazy evaluation est donc d’annoncer à Dask\nqu’on va effectuer une série d’opération qui ne vont se réaliser\nque lorsqu’on fera un appel à compute. Dask, quant à lui,\nse chargera d’optimiser les traitements.\nComme le plan d’action peut devenir difficile à suivre si on\ndésire effectuer beaucoup d’opérations enchaînées, on peut\nvouloir visualiser le graph de computation de dask.\nAvec celui-ci, on voit toutes les étapes que jusqu’ici dask n’a pas executé\net qu’il va devoir exécuter pour calculer le résultat (compute()).\n\nmean_by_year.dask\n\nEn l’occurence on voit l’enchaînement des étapes\nfrom_pandas(), getitem, isna, inv et loc-series qui résultent de nos filtres sur le DataFrame. Ensuite,\non voit les étapes de groupby et, enfin, pour calculer la moyenne il convient de faire la somme et la division. Toutes ces étapes vont être effectuées quand on appelle compute() et pas avant (lazy evaluation).\nAfin de voir la structure du dask.DataFrame on peut utiliser la méthode visualize()\n\ndvf_dd.visualize() # attention graphviz est requis\n\n\n\n\n\n\n\n\n\n\n Note\ngraphviz est requis pour ce graphique. S’il n’est pas installé dans votre environnement, faire :\n!pip install graphviz\n\n\nPour construire de véritables pipelines de données,\nles principes du pipe de pandas évoqué dans cette partie du cours et celui des pipelines scikit, évoqué dans un chapitre dédié\nont été importés dans dask.\n\n\nProblèmes de lecture dus à des types problématiques\nLa méthode read_csv de dask va inférer les types du DataFrame à partir d’échantillon, et va les implémenter sur tout le DataFrame seulement au moment d’une étape compute.\nIl peut donc y avoir des erreurs de types dûs à un échantillon ne prenant pas en compte certains cas particuliers, causant des erreurs dans la lecture du fichier.\nDans ce cas, et comme de manière générale avec pandas, il peut être recommandé de faire appel au paramètre dtype de read_csv - qui est un dict - (la doc de dask nous dit aussi que l’on peut augmenter la taille de l’échantllon sample)."
  },
  {
    "objectID": "content/manipulation/07_dask.html#utiliser-dask-avec-le-format-parquet",
    "href": "content/manipulation/07_dask.html#utiliser-dask-avec-le-format-parquet",
    "title": "Introduction à dask grâce aux données DVF",
    "section": "Utiliser Dask avec le format parquet",
    "text": "Utiliser Dask avec le format parquet\nLe format parquet tend à devenir le format\nde référence dans le monde de la data science.\nUne présentation extensive de celui-ci est disponible\ndans le chapitre dédié.\ndask permet de lire le format parquet, et plus précisément d’utiliser des fonctionnalités spécifiques à ce format. La lecture et l’écriture en parquet reposent par défaut sur pyarrow. On peut aussi utiliser fastparquet et préciser dans la lecture/écriture ce que l’on souhaite des deux.\n\ndvf_net = dvf.loc[:,[ 'Date mutation', 'Nature mutation', 'Valeur fonciere', 'Commune', \n       'Code commune', 'Type local', 'Identifiant local', 'Surface reelle bati',\n       'Nombre pieces principales', 'Nature culture',\n       'Nature culture speciale', 'Surface terrain', 'year']]\n\nOn va utiliser l’engine par défaut pour\nl’écriture de parquet qui est pyarrow (faire pip install pyarrow si vous ne l’avez pas déjà installé). to_parquet qui est une méthode pandas a été également étendue aux objets dask:\n\ndvf_net.to_parquet(\"dvf/\", partition_cols=\"year\")\n\nLorsqu’il est partitionné, le format parquet amène à une structure\nde fichiers similaire à celle-ci:\npath\n└── to\n    └── table\n        ├── gender=male\n        │   ├── ...\n        │   │\n        │   ├── country=US\n        │   │   └── data.parquet\n        │   ├── country=CN\n        │   │   └── data.parquet\n        │   └── ...\n        └── gender=female\n            ├── ...\n            │\n            ├── country=US\n            │   └── data.parquet\n            ├── country=CN\n            │   └── data.parquet\n            └── ...\nOn peut alors facilement traiter un sous-échantillon des données,\npar exemple l’année 2019:\n\ndvf_2019 = dd.read_parquet(\"dvf/year=2019/\", columns=[\"Date mutation\", \"Valeur fonciere\"]) # On peut sélectionner directement les deux colonnes\n\nLorsqu’il faudra passer à l’échelle, on changera le chemin en \"dvf/\npour utiliser l’ensemble des données."
  },
  {
    "objectID": "content/manipulation/07_dask.html#a-quoi-sert-persist",
    "href": "content/manipulation/07_dask.html#a-quoi-sert-persist",
    "title": "Introduction à dask grâce aux données DVF",
    "section": "A quoi sert persist ?",
    "text": "A quoi sert persist ?\nPar défaut, compute exécute l’ensemble du plan et ne conserve\nen mémoire que le résultat de celui-ci. Les données intermédiaires\nne sont pas conservées. Si on désire réutiliser une partie de celui-ci,\npar exemple les premières étapes, on devra donc ré-effectuer\nles calculs.\nIl est possible de garder une partie des données en mémoire avec persist(). Les données sont sauvegardées dans des objets appelés Futures. Cela peut être intéressant si un bloc particulier de données est utilisé dans plusieurs compute ou si l’on a besoin de voir ce qu’il y a à l’intérieur souvent.\n\ndvf_dd_mem = dvf_dd.persist()\n\n\nstart_time = time.time()\ndvf_dd_mem.head()\nprint(f\"{time.time() - start_time} seconds\")\n\n\nstart_time = time.time()\ndvf_dd.head()\nprint(f\"{time.time() - start_time} seconds\")\n\n0.33945131301879883 seconds\n\n\nOn a bien un temps plus important avec le dask.DataFrame initial, comparé avec celui sur lequel on a utilisé persist. L’opération qu’on réalise ici étant peu complexe, la différence n’est pas substantielle. Elle serait beaucoup plus marquée avec un jeu de données plus volumineux ou des étapes intensives en calcul."
  },
  {
    "objectID": "content/manipulation/07_dask.html#aller-plus-loin-utiliser-le-decorator-dask.delayed-pour-paralléliser-du-code",
    "href": "content/manipulation/07_dask.html#aller-plus-loin-utiliser-le-decorator-dask.delayed-pour-paralléliser-du-code",
    "title": "Introduction à dask grâce aux données DVF",
    "section": "Aller plus loin: Utiliser le decorator dask.delayed pour paralléliser du code",
    "text": "Aller plus loin: Utiliser le decorator dask.delayed pour paralléliser du code\nIl est possible de paralléliser des fonctions par exemple en utilisant le decorator dask.delayed. Cela permet de rendre les fonctions lazy. Cela signifie que lorsqu’on appelle la fonction, un delayed object est construit. Pour avoir le résultat, il faut faire un compute. Pour aller plus loin: https://tutorial.dask.org/03_dask.delayed.html.\nPrenons par exemple des fonctions permettant de calculer\ndes aires et des périmètres. Comme il s’agit d’une opération\ntrès peu complexe, on ajoute un délai de calcul avec time.sleep\npour que le timer ne nous suggère pas que l’opération est\ninstantanée.\n\ndef aire_carre(longueur):\n    time.sleep(1)\n    return longueur**2\n\ndef perimetre_carre(longueur):\n    time.sleep(1)\n    return 4*longueur\n\ndef ajout_aire_perim(a, b):\n    return a + b\n\nSans timer, c’est-à-dire de manière classique,\non ferait nos appels de fonctions de la\nmanière suivante :\n\nstart_time = time.time()\ncar1 = aire_carre(7)\ncar2 = perimetre_carre(9)\ncar3 = ajout_aire_perim(car1, car2)\ncar3\nprint(time.time() - start_time)\n\n2.001528739929199\n\n\nAvec le décorateur dask.delayed, on définit\nnos fonctions de la manière suivante :\n\nimport dask\n\n@dask.delayed\ndef aire_carre(longueur):\n    time.sleep(1)\n    return longueur**2\n\n@dask.delayed\ndef perimetre_carre(longueur):\n    time.sleep(1)\n    return 4*longueur\n\n@dask.delayed\ndef ajout_aire_perim(a, b):\n    return a + b\n\nL’appel de fonctions est identique\n\ncar1 = aire_carre(7)\ncar2 = perimetre_carre(9)\ncar3 = ajout_aire_perim(car1, car2)\n\nCependant, en fait rien n’a été calculé, si l’on souhaite le résultat, il faut appeler compute:\n\nstart_time = time.time()\ncar3.compute()\nprint(time.time() - start_time)\n\n1.002249002456665\n\n\nIci l’intérêt est assez limité, mais on voit que l’on réduit quand même de 2 à 1 seconde le temps de calcul. Mais l’idée derrière est que l’on a transformé car3 en un objet Delayed. Cela a généré un task graph permettant de paralléliser certaines opérations.\nIci il est important de noter que les fonctions que l’on parallélise doivent mettre un certain temps, sinon il n’y aura pas de gain de performance (si on retire le time.sleep il n’y a pas de gain de performance car le fait de paralléliser rajoute en fait du temps vu que chaque fonction a un temps de calcul trop faible pour que la parallélisation soit intéressante).\n\ncar3.visualize() # on peut visualiser le task graph et voir ce qui est fait en parallèle \n\n\n\n\n\n\n\n\nIl y a des exercices intéressants dans la doc de Dask sur les objets Delayed, notamment sur la parallélisation de séquence de traitement de données. Ils donnent l’exemple d’un ensemble de csv ayant le même format dont on veut résumer un indicateur final. On peut appliquer le decorator à une fonction permettant de lire le csv, puis utiliser une boucle for pour lire chaque fichier et appliquer les traitements. Ensuite, il faudra appeler compute sur l’objet final que l’on souhaite.\nPour aller plus loin sur l’utilisation de Dask sur un cluster voir https://tutorial.dask.org/04_distributed.html."
  },
  {
    "objectID": "content/manipulation/07_dask.html#remerciements",
    "href": "content/manipulation/07_dask.html#remerciements",
    "title": "Introduction à dask grâce aux données DVF",
    "section": "Remerciements",
    "text": "Remerciements\nCe chapitre a été rédigé avec Raphaële Adjerad."
  },
  {
    "objectID": "content/manipulation/07_dask.html#footnotes",
    "href": "content/manipulation/07_dask.html#footnotes",
    "title": "Introduction à dask grâce aux données DVF",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCe site web est testé sur les serveurs d’intégration\ncontinue mis à disposition gratuitement par Github. Ces\nderniers sont des machines à la mémoire limitée. Il s’agit\nd’un bon exemple d’intérêt de dask: avec pandas, on ne\npeut tester les exemples sur les trois millésimes disponibles\ncar la volumétrie accède la RAM disponible.↩︎\nCe site web est testé sur les serveurs d’intégration\ncontinue mis à disposition gratuitement par Github. Ces\nderniers sont des machines à la mémoire limitée. Il s’agit\nd’un bon exemple d’intérêt de dask: avec pandas, on ne\npeut tester les exemples sur les trois millésimes disponibles\ncar la volumétrie accède la RAM disponible.↩︎"
  },
  {
    "objectID": "content/manipulation/04c_API_TP.html",
    "href": "content/manipulation/04c_API_TP.html",
    "title": "Récupérer des données avec des API depuis Python",
    "section": "",
    "text": "La partie utilisant l’API DVF n’est plus à jour, elle sera mise à jour prochainement."
  },
  {
    "objectID": "content/manipulation/04c_API_TP.html#introduction-quest-ce-quune-api",
    "href": "content/manipulation/04c_API_TP.html#introduction-quest-ce-quune-api",
    "title": "Récupérer des données avec des API depuis Python",
    "section": "Introduction : Qu’est-ce qu’une API ?",
    "text": "Introduction : Qu’est-ce qu’une API ?\n\nDéfinition\nPour expliquer le principe d’une API, je vais reprendre le début de\nla fiche dédiée dans la documentation collaborative\nutilitR que je recommande de lire :\n\nUne Application Programming Interface (ou API) est une interface de programmation qui permet d’utiliser une application existante pour restituer des données. Le terme d’API peut être paraître intimidant, mais il s’agit simplement d’une façon de mettre à disposition des données : plutôt que de laisser l’utilisateur consulter directement des bases de données (souvent volumineuses et complexes), l’API lui propose de formuler une requête qui est traitée par le serveur hébergeant la base de données, puis de recevoir des données en réponse à sa requête.\nD’un point de vue informatique, une API est une porte d’entrée clairement identifiée par laquelle un logiciel offre des services à d’autres logiciels (ou utilisateurs). L’objectif d’une API est de fournir un point d’accès à une fonctionnalité qui soit facile à utiliser et qui masque les détails de la mise en oeuvre. Par exemple, l’API Sirene permet de récupérer la raison sociale d’une entreprise à partir de son identifiant Siren en interrogeant le référentiel disponible sur Internet directement depuis un script R, sans avoir à connaître tous les détails du répertoire Sirene.\nÀ l’Insee comme ailleurs, la connexion entre les bases de données pour les nouveaux projets tend à se réaliser par des API. L’accès à des données par des API devient ainsi de plus en plus commun et est amené à devenir une compétence de base de tout utilisateur de données.\nutilitR\n\n\n\nAvantages des API\nA nouveau, citons la documentation utilitR :\nLes API présentent de multiples avantages :\n\n\nLes API rendent les programmes plus reproductibles. En effet, grâce aux API, il est possible de mettre à jour facilement les données utilisées par un programme si celles-ci évoluent. Cette flexibilité accrue pour l’utilisateur évite au producteur de données d’avoir à réaliser de multiples extractions, et réduit le problème de la coexistence de versions différentes des données.\nGrâce aux API, l’utilisateur peut extraire facilement une petite partie d’une base de données plus conséquente.\nLes API permettent de mettre à disposition des données tout en limitant le nombre de personnes ayant accès aux bases de données elles-mêmes.\nGrâce aux API, il est possible de proposer des services sur mesure pour les utilisateurs (par exemple, un accès spécifique pour les gros utilisateurs).\n\nutilitR\n\nL’utilisation accrue d’API dans le cadre de stratégies open-data est l’un\ndes piliers des 15 feuilles de route ministérielles\nen matière d’ouverture, de circulation et de valorisation des données publiques.\n\n\nUtilisation des API\nCitons encore une fois\nla documentation utilitR :\n\nUne API peut souvent être utilisée de deux façons : par une interface Web, et par l’intermédiaire d’un logiciel (R, Python…). Par ailleurs, les API peuvent être proposées avec un niveau de liberté variable pour l’utilisateur :\n\nsoit en libre accès (l’utilisation n’est pas contrôlée et l’utilisateur peut utiliser le service comme bon lui semble) ;\nsoit via la génération d’un compte et d’un jeton d’accès qui permettent de sécuriser l’utilisation de l’API et de limiter le nombre de requêtes.\n\nutilitR\n\nDe nombreuses API nécessitent une authentification, c’est-à-dire un\ncompte utilisateur afin de pouvoir accéder aux données.\nDans un premier temps,\nnous regarderons exclusivement les API ouvertes sans restriction d’accès.\nCertains exercices et exemples permettront néanmoins d’essayer des API\navec restrictions d’accès."
  },
  {
    "objectID": "content/manipulation/04c_API_TP.html#requêter-une-api",
    "href": "content/manipulation/04c_API_TP.html#requêter-une-api",
    "title": "Récupérer des données avec des API depuis Python",
    "section": "Requêter une API",
    "text": "Requêter une API\n\nPrincipe général\n\nL’utilisation de l’interface Web est utile dans une démarche exploratoire mais trouve rapidement ses limites, notamment lorsqu’on consulte régulièrement l’API. L’utilisateur va rapidement se rendre compte qu’il est beaucoup plus commode d’utiliser une API via un logiciel de traitement pour automatiser la consultation ou pour réaliser du téléchargement de masse. De plus, l’interface Web n’existe pas systématiquement pour toutes les API.\nLe mode principal de consultation d’une API consiste à adresser une requête à cette API via un logiciel adapté (R, Python, Java…). Comme pour l’utilisation d’une fonction, l’appel d’une API comprend des paramètres qui sont détaillées dans la documentation de l’API.\nutilitR\n\nVoici les éléments importants à avoir en tête sur les requêtes (j’emprunte encore\nà utilitR) :\n\nLe point d’entrée d’un service offert par une API se présente sous la forme d’une URL (adresse web).\nChaque service proposé par une API a sa propre URL. Par exemple, dans le cas de l’OpenFood Facts,\nl’URL à utiliser pour obtenir des informations sur un produit particulier (l’identifiant 737628064502)\nest https://world.openfoodfacts.org/api/v0/product/737628064502.json\nCette URL doit être complétée avec différents paramètres qui précisent la requête (par exemple l’identifiant Siren). Ces paramètres viennent s’ajouter à l’URL, souvent à la suite de ?. Chaque service proposé par une API a ses propres paramètres, détaillés dans la documentation.\nLorsque l’utilisateur soumet sa requête, l’API lui renvoie une réponse structurée contenant l’ensemble des informations demandées. Le résultat envoyé par une API est majoritairement aux formats JSON ou XML (deux formats dans lesquels les informations sont hiérarchisées de manière emboitée). Plus rarement, certains services proposent une information sous forme plate (de type csv).\n\nDu fait de la dimension hiérarchique des formats JSON ou XML,\nle résultat n’est pas toujours facile à récupérer mais\nPython propose d’excellents outils pour cela (meilleurs que ceux de R).\nCertains packages, notamment json, facilitent l’extraction de champs d’une sortie d’API.\nDans certains cas, des packages spécifiques à une API ont été créés pour simplifier l’écriture d’une requête ou la récupération du résultat. Par exemple, le package\npynsee\npropose des options qui seront retranscrites automatiquement dans l’URL de\nrequête pour faciliter le travail sur les données Insee.\n\n\nIllustration avec une API de l’Ademe pour obtenir des diagnostics energétiques\nLe diagnostic de performance énergétique (DPE)\nrenseigne sur la performance énergétique d’un logement ou d’un bâtiment,\nen évaluant sa consommation d’énergie et son impact en terme d’émissions de gaz à effet de serre.\nLes données des performances énergétiques des bâtiments sont\nmises à disposition par l’Ademe.\nComme ces données sont relativement\nvolumineuses, une API peut être utile lorsqu’on ne s’intéresse\nqu’à un sous-champ des données.\nUne documentation et un espace de test de l’API sont disponibles\nsur le site API GOUV1.\nSupposons qu’on désire récupérer une centaine de valeurs pour la commune\nde Villieu-Loyes-Mollon dans l’Ain (code Insee 01450).\nL’API comporte plusieurs points d’entrée. Globalement, la racine\ncommune est :\n\nhttps://koumoul.com/data-fair/api/v1/datasets/dpe-france\n\nEnsuite, en fonction de l’API désirée, on va ajouter des éléments\nà cette racine. En l’occurrence, on va utiliser\nl’API field qui permet de récupérer des lignes en fonction d’un\nou plusieurs critères (pour nous, la localisation géographique):\nL’exemple donné dans la documentation technique est\n\nGET https://koumoul.com/data-fair/api/v1/datasets/dpe-france/values/{field}\n\nce qui en Python se traduira par l’utilisation de la méthode get du\npackage Request\nsur un url dont la structure est la suivante :\n\nil commencera par https://koumoul.com/data-fair/api/v1/datasets/dpe-france/values/ ;\nil sera ensuite suivi par des paramètres de recherche. Le champ {field}\ncommence ainsi généralement par un ? qui permet ensuite de spécifier des paramètres\nsous la forme nom_parameter=value\n\nA la lecture de la documentation, les premiers paramètres qu’on désire :\n\nLe nombre de pages, ce qui nous permet d’obtenir un certain nombre d’échos. On\nva seulement récupérer 10 pages ce qui correspond à une centaine d’échos. On va\nnéanmoins préciser qu’on veut 100 échos\nLe format de sortie. On va privilégier le JSON qui est un format standard dans le\nmonde des API. Python offre beaucoup de flexibilité grâce à l’un de\nses objets de base, à savoir le dictionnaire (type dict), pour manipuler de tels\nfichiers\nLe code commune des données qu’on désire obtenir. Comme on l’a évoqué,\non va récupérer les données dont le code commune est 01450. D’après la doc,\nil convient de passer le code commune sous le format:\ncode_insee_commune_actualise:{code_commune}. Pour éviter tout risque de\nmauvais formatage, on va utiliser %3A pour signifier :, %2A pour signifier * et\n%22 pour signifier \".\nD’autres paramètres annexes, suggérés par la documentation\n\nCela nous donne ainsi un URL dont la structure est la suivante :\n\ncode_commune = \"01450\"\nsize = 100\napi_root = \"https://koumoul.com/data-fair/api/v1/datasets/dpe-france/lines\"\nurl_api = f\"{api_root}?page=1&after=10&format=json&q_mode=simple&qs=code_insee_commune_actualise\" + \"%3A%22\" + f\"{code_commune}\" + \"%22\" + f\"&size={size}&select=\" + \"%2A&sampling=neighbors\"\n\nSi vous introduisez cet URL dans votre navigateur, vous devriez aboutir\nsur un JSON non formaté2. En Python,\non peut utiliser requests pour récupérer les données3 :\n\nimport requests\nimport pandas as pd\n\nreq = requests.get(url_api)\nwb = req.json()\n\nPrenons par exemple les 1000 premiers caractères du résultat, pour se donner\nune idée du résultat et se convaincre que notre filtre au niveau\ncommunal est bien passé :\nprint(req.content[:1000])\nb’{“total”: 121,“next”: “https://koumoul.com/data-fair/api/v1/datasets/dpe-france/lines?after=102721&format=json&q_mode=simple&qs=code_insee_commune_actualise%3A%2201450%22&size=100&select=*&sampling=neighbors”,“results”: [\\n {“classe_consommation_energie”: “D”,“tr001_modele_dpe_type_libelle”: “Vente”,“annee_construction”: 1947,“_geopoint”: “45.925922,5.229964”,“latitude”: 45.925922,“surface_thermique_lot”: 117.16,“_i”: 487,“tr002_type_batiment_description”: “Maison Individuelle”,“geo_adresse”: “Rue de la Brugni8re 01800 Villieu-Loyes-Mollon”,“_rand”: 23215,“code_insee_commune_actualise”: “01450”,“estimation_ges”: 53,“geo_score”: 0.4,“classe_estimation_ges”: “E”,“nom_methode_dpe”: “M9thode Facture”,“tv016_departement_code”: “01”,“consommation_energie”: 178,“date_etablissement_dpe”: “2013-06-13”,“longitude”: 5.229964,“_score”: null,’\nIci, il n’est même pas nécessaire en première approche\nd’utiliser le package json, l’information\nétant déjà tabulée dans l’écho renvoyé (on a la même information pour tous les pays):\nOn peut donc se contenter de Pandas pour transformer nos données en\nDataFrame et Geopandas pour convertir en données\ngéographiques :\n\nimport pandas as pandas\nimport geopandas as gpd\n\ndef get_dpe_from_url(url):\n\n    req = requests.get(url)\n    wb = req.json()\n    df = pd.json_normalize(wb[\"results\"])\n\n    dpe = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude), crs = 4326)\n    dpe = dpe.dropna(subset = ['longitude', 'latitude'])\n\n    return dpe\n\ndpe = get_dpe_from_url(url_api)\ndpe.head(2)\n\n/opt/mamba/lib/python3.9/site-packages/geopandas/_compat.py:123: UserWarning:\n\nThe Shapely GEOS version (3.12.0-CAPI-1.18.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n\n/tmp/ipykernel_3709/2008334648.py:2: UserWarning:\n\nShapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n\nimport os\nos.environ['USE_PYGEOS'] = '0'\nimport geopandas\n\nIn a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n\n\n\n\n\n\n\n\n\n\nclasse_consommation_energie\ntr001_modele_dpe_type_libelle\nannee_construction\n_geopoint\nlatitude\nsurface_thermique_lot\n_i\ntr002_type_batiment_description\ngeo_adresse\n_rand\n...\nclasse_estimation_ges\nnom_methode_dpe\ntv016_departement_code\nconsommation_energie\ndate_etablissement_dpe\nlongitude\n_score\n_id\nversion_methode_dpe\ngeometry\n\n\n\n\n0\nD\nVente\n1947\n45.925922,5.229964\n45.925922\n117.16\n487\nMaison Individuelle\nRue de la Brugnière 01800 Villieu-Loyes-Mollon\n23215\n...\nE\nMéthode Facture\n01\n178.00\n2013-06-13\n5.229964\nNone\n04JZNel3WCJYcfsHpCcHv\nNaN\nPOINT (5.22996 45.92592)\n\n\n2\nD\nNeuf\n2006\n45.923421,5.223777\n45.923421\n90.53\n689\nMaison Individuelle\nChemin du Pont-vieux 01800 Villieu-Loyes-Mollon\n401672\n...\nC\nFACTURE - DPE\n01\n227.99\n2013-06-11\n5.223777\nNone\nrkdV2lJn2wxaidVBaHBFY\nV2012\nPOINT (5.22378 45.92342)\n\n\n\n\n2 rows × 23 columns\n\n\n\nEssayons de représenter sur une carte ces DPE avec les\nannées de construction des logements.\nAvec Folium, on obtient la carte interactive suivante :\n\nimport seaborn as sns\nimport folium\n\npalette = sns.color_palette(\"coolwarm\", 8)\n\ndef interactive_map_dpe(dpe):\n\n    # convert in number\n    dpe['color'] = [ord(dpe.iloc[i]['classe_consommation_energie'].lower()) - 96 for i in range(len(dpe))]\n    dpe = dpe.loc[dpe['color']&lt;=7]\n    dpe['color'] = [palette.as_hex()[x] for x in dpe['color']]\n\n\n    center = dpe[['latitude', 'longitude']].mean().values.tolist()\n    sw = dpe[['latitude', 'longitude']].min().values.tolist()\n    ne = dpe[['latitude', 'longitude']].max().values.tolist()\n\n    m = folium.Map(location = center, tiles='OpenStreetMap')\n\n    # I can add marker one by one on the map\n    for i in range(0,len(dpe)):\n        folium.Marker([dpe.iloc[i]['latitude'], dpe.iloc[i]['longitude']],\n                    popup=f\"Année de construction: {dpe.iloc[i]['annee_construction']}, &lt;br&gt;DPE: {dpe.iloc[i]['classe_consommation_energie']}\",\n                    icon=folium.Icon(color=\"black\", icon=\"home\", icon_color = dpe.iloc[i]['color'])).add_to(m)\n\n    m.fit_bounds([sw, ne])\n\n    return m\n\nm = interactive_map_dpe(dpe)\n\n/opt/mamba/lib/python3.9/site-packages/geopandas/geodataframe.py:1443: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n# Afficher la carte\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nUn catalogue incomplet d’API existantes\nDe plus en plus de sites mettent des API à disposition des développeurs et autres curieux.\nPour en citer quelques-unes très connues :\n\nTwitter  : https://dev.twitter.com/rest/public\nFacebook  : https://developers.facebook.com/\nInstagram  : https://www.instagram.com/developer/\nSpotify  : https://developer.spotify.com/web-api/\n\nCependant, il est intéressant de ne pas se restreindre à celles-ci dont les\ndonnées ne sont pas toujours les plus intéressantes. Beaucoup\nde producteurs de données, privés comme publics, mettent à disposition\nleurs données sous forme d’API.\n\nAPI gouv : beaucoup d’API officielles de l’Etat français\net accès à de la documentation\nInsee : https://api.insee.fr/catalogue/ et pynsee\nPôle Emploi : https://www.emploi-store-dev.fr/portail-developpeur-cms/home.html\nSNCF : https://data.sncf.com/api\nBanque Mondiale : https://datahelpdesk.worldbank.org/knowledgebase/topics/125589"
  },
  {
    "objectID": "content/manipulation/04c_API_TP.html#lapi-dvf-accéder-à-des-données-de-transactions-immobilières-simplement",
    "href": "content/manipulation/04c_API_TP.html#lapi-dvf-accéder-à-des-données-de-transactions-immobilières-simplement",
    "title": "Récupérer des données avec des API depuis Python",
    "section": "L’API DVF : accéder à des données de transactions immobilières simplement",
    "text": "L’API DVF : accéder à des données de transactions immobilières simplement\n⚠️ Cette partie nécessite une mise à jour pour privilégier l’API DVF du Cerema.\nLe site DVF (demandes de valeurs foncières) permet de visualiser toutes les données relatives aux mutations à titre onéreux (ventes de maisons, appartements, garages…) réalisées durant les 5 dernières années.\nUn site de visualisation est disponible sur https://app.dvf.etalab.gouv.fr/.\nCe site est très complet quand il s’agit de connaître le prix moyen au mètre\ncarré d’un quartier ou de comparer des régions entre elles.\nL’API DVF permet d’aller plus loin afin de récupérer les résultats dans\nun logiciel de traitement de données. Elle a été réalisée par\nChristian Quest et le code\nsource est disponible sur Github .\nLes critères de recherche sont les suivants :\n- code_commune = code INSEE de la commune (ex: 94068)\n- section = section cadastrale (ex: 94068000CQ)\n- numero_plan = identifiant de la parcelle, (ex: 94068000CQ0110)\n- lat + lon + dist (optionnel): pour une recherche géographique, dist est par défaut un rayon de 500m\n- code_postal\nLes filtres de sélection complémentaires :\n- nature_mutation (Vente, etc)\n- type_local (Maison, Appartement, Local, Dépendance)\nLes requêtes sont de la forme : http://api.cquest.org/dvf?code_commune=29168.\n\n\n Exercice 1 : Exploiter l'API DVF\n\nRechercher toutes les transactions existantes dans DVF à Plogoff (code commune 29168, en Bretagne).\nAfficher les clés du JSON et en déduire le nombre de transactions répertoriées.\nN’afficher que les transactions portant sur des maisons.\nUtiliser l’API geo pour\nrécupérer le découpage communal de la ville de Plogoff.\nReprésenter l’histogramme des prix de vente.\n\nN’hésitez pas à aller plus loin en jouant sur des variables de\ngroupes par exemple.\n\n\nLe résultat de la question 2 devrait\nressembler au DataFrame suivant :\nL’histogramme des prix de vente (question 4) aura l’aspect suivant :\nOn va faire une carte des ventes en affichant le prix de l’achat.\nLa cartographie réactive sera présentée dans les chapitres\nconsacrés à la visualisation de données.\nSupposons que le DataFrame des ventes s’appelle ventes. Il faut d’abord le\nconvertir\nen objet geopandas.\nAvant de faire une carte, on va convertir\nles limites de la commune de Plogoff en geoJSON pour faciliter\nsa représentation avec folium\n(voir la doc geopandas à ce propos):\nPour représenter graphiquement, on peut utiliser le code suivant (essayez de\nle comprendre et pas uniquement de l’exécuter).\n\n# Afficher la carte\nm"
  },
  {
    "objectID": "content/manipulation/04c_API_TP.html#géocoder-des-données-grâce-aux-api-officielles",
    "href": "content/manipulation/04c_API_TP.html#géocoder-des-données-grâce-aux-api-officielles",
    "title": "Récupérer des données avec des API depuis Python",
    "section": "Géocoder des données grâce aux API officielles",
    "text": "Géocoder des données grâce aux API officielles\nPour pouvoir faire cet exercice\n\n!pip install xlrd\n\nJusqu’à présent, nous avons travaillé sur des données où la dimension\ngéographique était déjà présente ou relativement facile à intégrer.\nCe cas idéal ne se rencontre pas nécessairement dans la pratique.\nOn dispose parfois de localisations plus ou moins précises et plus ou\nmoins bien formattées pour déterminer la localisation de certains\nlieux.\nDepuis quelques années, un service officiel de géocodage a été mis en place.\nCelui-ci est gratuit et permet de manière efficace de coder des adresses\nà partir d’une API. Cette API, connue sous le\nnom de la Base d’Adresses Nationale (BAN) a bénéficié de la mise en commun de données de plusieurs\nacteurs (collectivités locales, Poste) et de compétences d’acteurs\ncomme Etalab. La documentation de celle-ci est disponible à l’adresse\nhttps://api.gouv.fr/les-api/base-adresse-nationale.\nPour illustrer la manière de géocoder des données avec Python, nous\nallons partir de la base\ndes résultats des auto-écoles à l’examen du permis sur l’année 2018.\nCes données nécessitent un petit peu de travail pour être propres à une\nanalyse statistique.\nAprès avoir renommé les colonnes, nous n’allons conserver que\nles informations relatives au permis B (permis voiture classique) et\nles auto-écoles ayant présenté au moins 20 personnes à l’examen.\n\nimport pandas as pd\nimport xlrd\nimport geopandas as gpd\n\ndf = pd.read_excel(\"https://www.data.gouv.fr/fr/datasets/r/d4b6b072-8a7d-4e04-a029-8cdbdbaf36a5\", header = [0,1])\n\n# Le Excel a des noms de colonne emboitées, \n# on nettoie\nindex_0 = [\"\" if df.columns[i][0].startswith(\"Unnamed\") else df.columns[i][0] for i in range(len(df.columns))]\nindex_1 = [df.columns[i][1] for i in range(len(df.columns))]\nkeep_index = [True if el in ('', \"B\") else False for el in index_0] \ncols = [index_0[i] + \" \" + index_1[i].replace(\"+\", \"_\") for i in range(len(df.columns))]\ndf.columns = cols\ndf = df.loc[:, keep_index]\ndf.columns = df.columns.str.replace(\"(^ |°)\", \"\", regex = True).str.replace(\" \", \"_\")\n\n# On garde le sous-échantillon d'intérêt\ndf = df.dropna(subset = ['B_NB'])\ndf = df.loc[~df[\"B_NB\"].astype(str).str.contains(\"(\\%|\\.)\"),:]\ndf['B_NB'] = df['B_NB'].astype(int)\ndf['B_TR'] = df['B_TR'].str.replace(\",\", \".\").str.replace(\"%\",\"\").astype(float)\ndf = df.loc[df[\"B_NB\"]&gt;20]\n\n/tmp/ipykernel_3709/3216706257.py:19: UserWarning:\n\nThis pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n\n\n\nSur cet échantillon, le taux de réussite moyen était, en 2018, de 58.02%\nNos informations géographiques prennent la forme suivante :\n\ndf.loc[:,['Adresse','CP','Ville']].head(5)\n\n\n\n\n\n\n\n\nAdresse\nCP\nVille\n\n\n\n\n0\n56 RUE CHARLES ROBIN\n01000\nBOURG EN BRESSE\n\n\n2\n7, avenue Revermont\n01250\nCeyzeriat\n\n\n3\n72 PLACE DE LA MAIRIE\n01000\nSAINT-DENIS LES BOURG\n\n\n4\n6 RUE DU LYCEE\n01000\nBOURG EN BRESSE\n\n\n5\n9 place Edgard Quinet\n01000\nBOURG EN BRESSE\n\n\n\n\n\n\n\nAutrement dit, nous disposons d’une adresse, d’un code postal et d’un nom\nde ville. Ces informations peuvent servir à faire une recherche\nsur la localisation d’une auto-école puis, éventuellement, de se restreindre\nà un sous-échantillon.\n\nUtiliser l’API BAN\nLa documentation officielle de l’API\npropose un certain nombre d’exemples de manière de géolocaliser des données.\nDans notre situation, deux points d’entrée paraissent intéressants:\n\nL’API /search/ qui représente un point d’entrée avec des URL de la forme\nhttps://api-adresse.data.gouv.fr/search/?q=\\&lt;adresse\\&gt;&postcode=\\&lt;codepostal\\&gt;&limit=1\nL’API /search/csv qui prend un CSV en entrée et retourne ce même CSV avec\nles observations géocodées. La requête prend la forme suivante, en apparence\nmoins simple à mettre en oeuvre :\ncurl -X POST -F data=@search.csv -F columns=adresse -F columns=postcode https://api-adresse.data.gouv.fr/search/csv/\n\nLa tentation serait forte d’utiliser la première méthode avec une boucle sur les\nlignes de notre DataFrame pour géocoder l’ensemble de notre jeu de données.\nCela serait néanmoins une mauvaise idée car les communications entre notre\nsession Python et les serveurs de l’API seraient beaucoup trop nombreuses\npour offrir des performances satisfaisantes.\nPour vous en convaincre, vous pouvez exécuter le code suivant sur un petit\néchantillon de données (par exemple 100 comme ici) et remarquer que le temps\nd’exécution est assez important\n\nimport time\n\ndfgeoloc = df.loc[:, ['Adresse','CP','Ville']].apply(lambda s: s.str.lower().str.replace(\",\",\" \"))\ndfgeoloc['url'] = (dfgeoloc['Adresse'] + \"+\" + dfgeoloc['Ville'].str.replace(\"-\",'+')).str.replace(\" \",\"+\")\ndfgeoloc['url'] = 'https://api-adresse.data.gouv.fr/search/?q=' + dfgeoloc['url'] + \"&postcode=\" + df['CP'] + \"&limit=1\"\ndfgeoloc = dfgeoloc.dropna()\n\nstart_time = time.time()\n\ndef get_geoloc(i):\n    print(i)\n    return gpd.GeoDataFrame.from_features(requests.get(dfgeoloc['url'].iloc[i]).json()['features'])\n\nlocal = [get_geoloc(i) for i in range(len(dfgeoloc.head(10)))]\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\nComme l’indique la documentation, si on désire industrialiser notre processus\nde géocodage, on va privilégier l’API CSV.\nPour obtenir une requête CURL cohérente avec le format désiré par l’API\non va à nouveau utiliser Requests mais cette fois avec des paramètres\nsupplémentaires:\n\ndata va nous permettre de passer des paramètres à CURL (équivalents aux -F\nde la requête CURL) :\n\ncolumns: Les colonnes utilisées pour localiser une donnée. En l’occurrence,\non utilise l’adresse et la ville (car les codes postaux n’étant pas uniques,\nun même nom de voirie peut se trouver dans plusieurs villes partageant le même\ncode postal) ;\npostcode: Le code postal de la ville. Idéalement nous aurions utilisé\nle code Insee mais nous ne l’avons pas dans nos données ;\nresult_columns: on restreint les données échangées avec l’API aux\ncolonnes qui nous intéressent. Cela permet d’accélérer les processus (on\néchange moins de données) et de réduire l’impact carbone de notre activité\n(moins de transferts = moins d’énergie dépensée). En l’occurrence, on ne ressort\nque les données géolocalisées et un score de confiance en la géolocalisation ;\n\nfiles: permet d’envoyer un fichier via CURL.\n\nLes données sont récupérées avec request.post. Comme il s’agit d’une\nchaîne de caractère, nous pouvons directement la lire avec Pandas en\nutilisant io.StringIO pour éviter d’écrire des données intermédiaires.\nLe nombre d’échos semblant être limité, il\nest proposé de procéder par morceaux\n(ici, le jeu de données est découpé en 5 morceaux).\n\nimport requests\nimport io   \nimport numpy as np\nimport time\n\nparams = {\n    'columns': ['Adresse', 'Ville'],\n    'postcode': 'CP',\n    'result_columns': ['result_score', 'latitude', 'longitude'],\n}\n\ndf[['Adresse','CP','Ville']] = df.loc[:, ['Adresse','CP','Ville']].apply(lambda s: s.str.lower().str.replace(\",\",\" \"))\n\ndef geoloc_chunk(x):\n    dfgeoloc = x.loc[:, ['Adresse','CP','Ville']]\n    dfgeoloc.to_csv(\"datageocodage.csv\", index=False)\n    response = requests.post('https://api-adresse.data.gouv.fr/search/csv/', data=params, files={'data': ('datageocodage.csv', open('datageocodage.csv', 'rb'))})\n    geoloc = pd.read_csv(io.StringIO(response.text), dtype = {'CP': 'str'})\n    return geoloc\n    \nstart_time = time.time()\ngeodata = [geoloc_chunk(dd) for dd in np.array_split(df, 10)]\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\nCette méthode est beaucoup plus rapide et permet ainsi, une fois retourné à nos\ndonnées initiales, d’avoir un jeu de données géolocalisé.\n\n# Retour aux données initiales\ngeodata = pd.concat(geodata, ignore_index = True)\ndf_xy = df.merge(geodata, on = ['Adresse','CP','Ville'])\ndf_xy = df_xy.dropna(subset = ['latitude','longitude'])\n\n# Mise en forme pour le tooltip\ndf_xy['text'] = (\n    df_xy['Raison_Sociale'] + '&lt;br&gt;' +\n    df_xy['Adresse'] + '&lt;br&gt;' +\n    df_xy['Ville'] + '&lt;br&gt;Nombre de candidats:' + df_xy['B_NB'].astype(str)\n)\ndf_xy.filter(\n    ['Raison_Sociale','Adresse','CP','Ville','latitude','longitude'],\n    axis = \"columns\"\n).sample(10)\n\n\n\n\n\n\n\n\nRaison_Sociale\nAdresse\nCP\nVille\nlatitude\nlongitude\n\n\n\n\n3907\nBEGON BIS\n176 bis rue de cabochon\n41000\nblois\n47.577073\n1.303151\n\n\n5689\nAUTO-ECOLE DANJOU\n24 rue de landrecies\n59360\nle cateau\n50.104758\n3.545127\n\n\n2314\nECF ROUDAUT\n22 rue de la porte\n29200\nbrest\n48.384001\n-4.499404\n\n\n977\nAE Avenir Conduite\n6 avenue des rosiers\n13109\nsimiane collongue\n43.430206\n5.428678\n\n\n6622\nJOFFRE AE\n132 avenue joffre\n66000\nperpignan\n42.711406\n2.890075\n\n\n11111\nGTA DRIVE AUTO ECOLE\n43 rue jean-jaurès\n97200\nfort de france\n14.609408\n-61.070442\n\n\n6295\nALOHA PERMIS\n16 rue d'alsace - résidence sologne\n62700\nbruay-la-buissiere\n50.480136\n2.547684\n\n\n2646\nGRENADE\n13 c allées alsace-lorraine\n31330\ngrenade sur garonne\n43.769966\n1.293552\n\n\n1038\nECOLE DE CONDUITE MARYLINE CHERR\n3 rue emile fassin\n13200\narles\n43.674361\n4.629041\n\n\n441\nREFLEX LA TRINITE AUTO ECOLE\n2 boulevard stalingrad\n06340\nla trinite\n43.740554\n7.311126\n\n\n\n\n\n\n\nIl ne reste plus qu’à utiliser Geopandas\net nous serons en mesure de faire une carte des localisations des auto-écoles :\n\n# Transforme en geopandas pour les cartes\nimport geopandas as gpd\ndfgeo = gpd.GeoDataFrame(\n    df_xy,\n    geometry = gpd.points_from_xy(df_xy.longitude, df_xy.latitude)\n)\n\nNous allons représenter les stations dans l’Essonne avec un zoom initialement\nsur les villes de Massy et Palaiseau. Le code est le suivant :\n\nimport folium\n\n# Représenter toutes les autoécoles de l'Essonne\ndf_91 = df_xy.loc[df_xy[\"Dept\"] == \"091\"]\n\n# Centrer la vue initiale sur Massy-Palaiseau\ndf_pal = df_xy.loc[df_xy['Ville'].isin([\"massy\", \"palaiseau\"])]\ncenter = df_pal[['latitude', 'longitude']].mean().values.tolist()\nsw = df_pal[['latitude', 'longitude']].min().values.tolist()\nne = df_pal[['latitude', 'longitude']].max().values.tolist()\n\nm = folium.Map(location = center, tiles='OpenStreetMap')\n\n# I can add marker one by one on the map\nfor i in range(0,len(df_91)):\n    folium.Marker([df_91.iloc[i]['latitude'], df_91.iloc[i]['longitude']],\n                  popup=df_91.iloc[i]['text'],\n                  icon=folium.Icon(icon='car', prefix='fa')).add_to(m)\n\nm.fit_bounds([sw, ne])\n\nCe qui permet d’obtenir la carte:\n\n# Afficher la carte\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nVous pouvez aller plus loin avec l’exercice suivant.\n\n\n Exercice 2 : Quelles sont les auto-écoles les plus proches de chez moi ?\nOn va supposer que vous cherchez, dans un rayon donné autour d’un centre ville,\nles auto-écoles disponibles.\n\n\nFonction nécessaire pour cet exercice\n\nCet exercice nécessite une fonction pour créer un cercle\nautour d’un point\n(source ici).\nLa voici :\nfrom functools import partial\nimport pyproj\nfrom shapely.ops import transform\nfrom shapely.geometry import Point\n\nproj_wgs84 = pyproj.Proj('+proj=longlat +datum=WGS84')\n\n\ndef geodesic_point_buffer(lat, lon, km):\n    # Azimuthal equidistant projection\n    aeqd_proj = '+proj=aeqd +lat_0={lat} +lon_0={lon} +x_0=0 +y_0=0'\n    project = partial(\n        pyproj.transform,\n        pyproj.Proj(aeqd_proj.format(lat=lat, lon=lon)),\n        proj_wgs84)\n    buf = Point(0, 0).buffer(km * 1000)  # distance in metres\n    return transform(project, buf).exterior.coords[:]\n\n\nPour commencer, utiliser l’API Geo\npour la ville de Palaiseau.\nAppliquer la fonction geodesic_point_buffer au centre ville de Palaiseau\nNe conserver que les auto-écoles dans ce cercle et les ordonner\n\nSi vous avez la réponse à la question 3, n’hésitez pas à la soumettre sur Github afin que je complète la correction 😉 !\n\n\n\n\nERROR 1: PROJ: proj_create_from_database: Open of /opt/mamba/share/proj failed\n\n\n\n\n/opt/mamba/lib/python3.9/site-packages/shapely/ops.py:276: FutureWarning:\n\nThis function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n\n\n\nPour se convaincre, de notre cercle constitué lors de\nla question 2, on peut représenter une carte.\nOn a bien un cercle centré autour de Palaiseau :\n\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "content/manipulation/04c_API_TP.html#exercices-supplémentaires",
    "href": "content/manipulation/04c_API_TP.html#exercices-supplémentaires",
    "title": "Récupérer des données avec des API depuis Python",
    "section": "Exercices supplémentaires",
    "text": "Exercices supplémentaires\n\nDécouvrir l’API d’OpenFoodFacts\nPour vous aidez, vous pouvez regarder une exemple de structure du JSON ici :\nhttps://world.openfoodfacts.org/api/v0/product/3274080005003.json en particulier la catégorie nutriments.\n\n\n Exercice 3 : Retrouver des produits dans l'openfood facts 🍕\nVoici une liste de code-barres:\n3274080005003,  5449000000996, 8002270014901, 3228857000906, 3017620421006, 8712100325953\nUtiliser l’API d’openfoodfacts\n(l’API, pas depuis le CSV !)\npour retrouver les produits correspondants\net leurs caractéristiques nutritionnelles.\nLe panier paraît-il équilibré ? 🍫\nRécupérer l’URL d’une des images et l’afficher dans votre navigateur.\n\n\nVoici par exemple la photo du produit ayant le code-barre 5449000000996. Vous le reconnaissez ?"
  },
  {
    "objectID": "content/manipulation/04c_API_TP.html#footnotes",
    "href": "content/manipulation/04c_API_TP.html#footnotes",
    "title": "Récupérer des données avec des API depuis Python",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLa documentation est également disponible ici↩︎\nLe JSON est un format très apprécié dans le domaine du big data\ncar il permet d’empiler des données\nqui ne sont pas complètes. Il\ns’agit d’un des formats privilégiés du paradigme No-SQL pour lequel\ncet excellent cours propose plus de détails.↩︎\nSuivant les API, nous avons soit besoin de rien de plus si nous parvenons directement à obtenir un json, soit devoir utiliser un parser comme BeautifulSoup dans le cas contraire. Ici, le JSON peut être formaté relativement aisément.↩︎"
  },
  {
    "objectID": "content/manipulation/03_geopandas_TP.html",
    "href": "content/manipulation/03_geopandas_TP.html",
    "title": "Pratique de geopandas avec les données vélib",
    "section": "",
    "text": "Dans ce TP,\nnous allons apprendre à importer et\nmanipuler des données spatiales avec\nPython.\nCe langage propose\ndes fonctionnalités très intéressantes pour ce type de\ndonnées complexes qui le rendent capable de se comporter\ncomme un logiciel de SIG1.\nGrâce à la librairie Geopandas, une extension\nde Pandas aux données spatiales, les\ndonnées géographiques pourront être manipulées\ncomme n’importe quel type de données avec Python.\nLa complexité induite par la dimension spatiale ne sera pas ressentie.\nIllustration du principe des données spatiales (documentation de `sf`, l'équivalent de `Geopandas` en `R`)\n\n\n\n![](https://user-images.githubusercontent.com/520851/50280460-e35c1880-044c-11e9-9ed7-cc46754e49db.jpg){width=\"70%\"}\nCe chapitre illustre à partir d’exemples pratiques certains principes centraux de l’analyse de données :\nSi vous êtes intéressés par R,\nune version très proche de ce TP est disponible dans ce cours de R.\nNote\nLe package cartiflette est expérimental\net n’est disponible que sur\nGithub, pas sur PyPi.\nIl est amené à évoluer rapidement et cette page sera mise à jour\nquand de nouvelles fonctionalités (notamment l’utilisation d’API)\nseront disponibles pour encore simplifier la récupération de\ncontours géographiques.\nPour installer cartiflette, il est nécessaire d’utiliser les commandes suivantes\ndepuis un Jupyter Notebook (si vous utilisez la ligne de commande directement,\nvous pouvez retirer les ! en début de ligne):\nCes commandes permettent de récupérer l’ensemble du code\nsource depuis Github"
  },
  {
    "objectID": "content/manipulation/03_geopandas_TP.html#préliminaires",
    "href": "content/manipulation/03_geopandas_TP.html#préliminaires",
    "title": "Pratique de geopandas avec les données vélib",
    "section": "Préliminaires",
    "text": "Préliminaires\nAvant de se lancer dans le TD, il est nécessaire d’installer quelques\nlibrairies qui ne sont pas disponibles par défaut, dans l’environnement Python\nde base de la data science. Pour installer celles-ci depuis une\ncellule de notebook Jupyter, le code suivant est à exécuter :\n\n!pip install pandas fiona shapely pyproj rtree # à faire obligatoirement en premier pour utiliser rtree ou pygeos pour les jointures spatiales\n!pip install contextily\n!pip install geopandas\n!pip install pygeos\n!pip install topojson\n\nAprès installations,\nles packages à importer pour progresser\ndans ce chapitre sont les suivants :\n\nimport geopandas as gpd\nimport contextily as ctx\nimport matplotlib.pyplot as plt\n\nLes instructions d’installation du package cartiflette\nsont quant à elles détaillées dans le chapitre\nprécédent.\n\n!pip install requests py7zr geopandas openpyxl tqdm s3fs PyYAML xlrd\n!pip install git+https://github.com/inseefrlab/cartiflette@80b8a5a28371feb6df31d55bcc2617948a5f9b1a\n\n\nfrom cartiflette.s3 import download_vectorfile_url_all"
  },
  {
    "objectID": "content/manipulation/03_geopandas_TP.html#lire-et-enrichir-des-données-spatiales",
    "href": "content/manipulation/03_geopandas_TP.html#lire-et-enrichir-des-données-spatiales",
    "title": "Pratique de geopandas avec les données vélib",
    "section": "Lire et enrichir des données spatiales",
    "text": "Lire et enrichir des données spatiales\nDans cette partie,\nnous utiliserons\nles fonds de carte de l’IGN dont\nla mise à disposition est facilitée\npar le projet cartiflette2.\n\n\n Exercice 1: découverte des objets géographiques\nEn premier lieu, on récupère des données géographiques grâce\nau package cartiflette.\n\nUtiliser\nle code ci-dessous pour\ntélécharger les données communales (produit Admin Express de l’IGN)\ndes départements de la petite couronne (75, 92, 93 et 94)\nde manière simplifiée grâce au package\ncartiflette:\n\n\ncommunes_borders = download_vectorfile_url_all(\n    crs = 4326,\n    values = [\"75\", \"92\", \"93\", \"94\"],\n    borders=\"COMMUNE\",\n    vectorfile_format=\"topojson\",\n    filter_by=\"DEPARTEMENT\",\n    source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n    year=2022)\n\n\nRegarder les premières lignes des données. Identifier la différence avec\nun dataframe standard.\n\n\n\n\n\n\n\n\n\n\nid\nID\nNOM\nNOM_M\nINSEE_COM\nSTATUT\nPOPULATION\nINSEE_CAN\nINSEE_ARR\nINSEE_DEP\nINSEE_REG\nSIREN_EPCI\nsource\ngeometry\n\n\n\n\n0\nCOMMUNE_0000000009736048\nNaN\nParis\nPARIS\n75056\nCapitale d'état\n2165423\nNR\n1\n75\n11\n200054781\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.36421 48.81640, 2.36333 48.81615, ...\n\n\n0\nCOMMUNE_0000000009736037\nNaN\nLevallois-Perret\nLEVALLOIS-PERRET\n92044\nCommune simple\n66082\n16\n2\n92\n11\n200054781/200057982\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.28739 48.90364, 2.28846 48.90302, ...\n\n\n1\nCOMMUNE_0000000009736055\nNaN\nBois-Colombes\nBOIS-COLOMBES\n92009\nCommune simple\n28841\n11\n2\n92\n11\n200054781/200057990\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.26639 48.90629, 2.26645 48.90615, ...\n\n\n2\nCOMMUNE_0000000009736538\nNaN\nMalakoff\nMALAKOFF\n92046\nCommune simple\n30950\n18\n1\n92\n11\n200054781/200057966\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.27818 48.81425, 2.28066 48.81469, ...\n\n\n3\nCOMMUNE_0000000009736038\nNaN\nClichy\nCLICHY\n92024\nCommune simple\n63089\n09\n2\n92\n11\n200054781/200057990\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.30377 48.89415, 2.30258 48.89487, ...\n\n\n\n\n\n\n\n\nAfficher le crs de communes_borders. Ce dernier contrôle la\ntransformation de l’espace tridimensionnel terrestre en une surface plane.\nUtiliser to_crs pour transformer les données en Lambert 93, le\nsystème officiel (code EPSG 2154).\nAfficher les communes des Hauts de Seine (département 92) et utiliser la méthode\nplot\nNe conserver que Paris et réprésenter les frontières sur une carte : quel est le problème pour\nune analyse de Paris intramuros?\n\nOn remarque rapidement le problème.\nOn ne dispose ainsi pas des limites des arrondissements parisiens, ce\nqui appauvrit grandement la carte de Paris.\n\nCette fois, utiliser l’argument borders=\"COMMUNE_ARRONDISSEMENT\" pour obtenir\nun fonds de carte consolidé des communes avec les arrondissements dans les grandes villes.\nConvertir en Lambert 93."
  },
  {
    "objectID": "content/manipulation/03_geopandas_TP.html#le-système-de-projection",
    "href": "content/manipulation/03_geopandas_TP.html#le-système-de-projection",
    "title": "Pratique de geopandas avec les données vélib",
    "section": "Le système de projection",
    "text": "Le système de projection\nUn concept central dans les logiciels de SIG est la notion de\nprojection. L’exercice précédent imposait parfois certaines projections\nsans expliquer l’importance de ces choix. Python, comme\ntout SIG, permet une gestion cohérente des projections.\nObservez les variations significatives\nde proportions pour certains pays selon les projections\nchoisies:\n\nhtml`&lt;div&gt;${container_projection}&lt;/div&gt;`\n\n\n\n\n\n\n\ncontainer_projection = html`&lt;div class=\"container\"&gt;\n  &lt;div class=\"row\"&gt;\n    &lt;div class=\"projection\"&gt;\n      &lt;div class=\"projection-label\"&gt;Choisir une projection&lt;/div&gt;\n      ${viewof projection}\n    &lt;/div&gt;\n  &lt;/div&gt;\n  &lt;div class=\"row\"&gt;\n    &lt;div class=\"projectedMap\"&gt;\n      ${projectedMap}\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/div&gt;`\n\n\n\n\n\n\n\nviewof projection = projectionInput({\n  name: \"\",\n  value: \"Mercator\"\n})\n\n\n\n\n\n\n\nimport {projectionInput} from \"@fil/d3-projections\"\nimport {map} from \"@linogaliana/base-map\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprojectedMap = map(projection,\n                   {\n                     //svg: true,\n                     value: projection.options,\n                     width: width_projected_map,\n                     //height: 300,\n                     //rotate: [0, -90],\n                     //inertia: true,\n                     show_equator: true,\n                     background: \"#f1f0eb\"\n                     \n                     //show_structure: true\n                   })\n\n\n\n\n\n\n\nwidth_projected_map = screen.width/2\n\n\n\n\n\n\n\n\n Exercice 2 : Les projections, représentations et approximations\nVoici un code utilisant encore\ncartiflette\npour récupérer les frontières françaises (découpées par région):\n\nfrance = download_vectorfile_url_all(\n      values = \"metropole\",\n      crs = 4326,\n      borders = \"REGION\",\n      vectorfile_format=\"topojson\",\n      filter_by=\"FRANCE_ENTIERE\",\n      source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n      year=2022)\n\n\nS’amuser à représenter les limites de la France avec plusieurs projections:\n\n\nMercator WGS84 (EPSG: 4326)\nProjection healpix (+proj=healpix +lon_0=0 +a=1)\nProjection prévue pour Tahiti (EPSG: 3304)\nProjection Albers prévue pour Etats-Unis (EPSG: 5070)\n\n\nCalculer la superficie en \\(km^2\\)\ndes régions françaises dans les deux systèmes de projection suivants :\nWorld Mercator WGS84 (EPSG: 3395) et Lambert 93 (EPSG: 2154). Calculer la différence en \\(km^2\\)\npour chaque région.\n\n\n\nAvec la question 1 illustrant quelques cas pathologiques,\non comprend que les projections ont un effet déformant\nqui se voit bien lorsqu’on les représente côte à côte sous\nforme de cartes :\n\n\n\n\n\n\n(a) Mercator WGS84 (EPSG: 4326)\n\n\n\n\n\n(b) Projection healpix (+proj=healpix +lon_0=0 +a=1)\n\n\n\n\n\n\n\n(c) Projection prévue pour Tahiti (EPSG: 3304)\n\n\n\n\n\n(d) Projection Albers prévue pour Etats-Unis (EPSG: 5070)\n\n\n\nFigure 1: Comparaison des projections\n\n\nCependant le problème n’est pas que visuel, il est également\nnumérique. Les calculs géométriques amènent à des différences\nassez notables selon le système de référence utilisé.\nOn peut représenter ces approximations sur une carte3 pour se faire\nune idée des régions où l’erreur de mesure est la plus importante.\n\n\n/opt/mamba/lib/python3.9/site-packages/geopandas/plotting.py:730: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n\n\n\n\n\n\n\nCe type d’erreur de mesure est normal à l’échelle du territoire français.\nLes projections héritères du Mercator déforment les distances,\nsurtout lorqu’on se rapproche de l’équateur ou des pôles.\n\n\n\n\n\n(a) Exemple de reprojection de pays depuis le site thetruesize.com\n\n\n\n\n\n(b) “Don’t trust the Mercator projection” sur Reddit\n\n\nFigure 2: La projection Mercator, une vision déformante\n\n\nPour aller plus loin, la carte interactive\nsuivante, construite par Nicolas Lambert, issue de\nce notebook Observable, illustre l’effet\ndéformant de la projection Mercator, et de quelques-unes autres,\nsur notre perception de la taille des pays.\n\n\nVoir la carte interactive\n\n\nhtml`&lt;div class=\"grid-container\"&gt;\n  &lt;div class=\"viewof-projection\"&gt;${viewof projectionBertin}&lt;/div&gt;\n  &lt;div class=\"viewof-mycountry\"&gt;${viewof mycountry}&lt;/div&gt;\n  &lt;div class=\"map-bertin\"&gt;${mapBertin}&lt;/div&gt;\n&lt;/div&gt;`\n\n\n\n\n\n\n\n\nimport {map as mapBertin, viewof projection as projectionBertin, viewof mycountry} from \"@neocartocnrs/impact-of-projections-on-areas\"\n\n\n\n\n\n\nIl n’est donc pas suprenant que nos déformations soient exacerbées aux\nextrèmes du territoire métropolitain.\nSi les approximations sont légères sur de petits territoires,\nles erreurs peuvent être\nnon négligeables à l’échelle de la France.\nIl faut donc systématiquement\nrepasser les données dans le système de projection Lambert 93 (le\nsystème officiel pour la métropole) avant d’effectuer des calculs géométriques."
  },
  {
    "objectID": "content/manipulation/03_geopandas_TP.html#utiliser-des-données-géographiques-comme-des-couches-graphiques",
    "href": "content/manipulation/03_geopandas_TP.html#utiliser-des-données-géographiques-comme-des-couches-graphiques",
    "title": "Pratique de geopandas avec les données vélib",
    "section": "Utiliser des données géographiques comme des couches graphiques",
    "text": "Utiliser des données géographiques comme des couches graphiques\nSouvent, le découpage communal ne sert qu’en fond de cartes, pour donner des\nrepères. En complément de celui-ci, on peut désirer exploiter\nun autre jeu de données.\nOn va partir des données de localisation des\nstations velib,\ndisponibles sur le site d’open data de la ville de Paris et\nrequêtables directement en utilisant un URL\n\nurl = \"https://opendata.paris.fr/explore/dataset/velib-emplacement-des-stations/download/?format=geojson&timezone=Europe/Berlin&lang=fr\"\n\nDans le prochain exercice, nous proposons de créer rapidement une\ncarte comprenant trois couches :\n\nLes localisations de stations sous forme de points ;\nLes bordures des communes et arrondissements pour contextualiser ;\nLes bordures des départements en traits plus larges pour contextualiser également.\n\nNous irons plus loin dans le travail cartographique dans le prochain\nchapitre. Mais être en mesure de positionner rapidement\nses données sur une carte est\ntoujours utile dans un travail exploratoire.\nEn amont de l’exercice,\nutiliser la fonction suivante du package cartiflette pour récupérer\nle fonds de carte des départements de la petite couronne:\n\nidf = download_vectorfile_url_all(\n      values = \"11\",\n      crs = 4326,\n      borders = \"DEPARTEMENT\",\n      vectorfile_format=\"topojson\",\n      filter_by=\"REGION\",\n      source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n      year=2022)\n\npetite_couronne_departements = idf.loc[idf['INSEE_DEP'].isin([\"75\",\"92\",\"93\",\"94\"])].to_crs(2154)\n\n\n\n Exercice 3: importer et explorer les données velib\nOn commence par récupérer les données nécessaires à la production\nde cette carte.\n\nEn utilisant l’URL précédent, importer les données velib sous le nom station\nVérifier la projection géographique de station (attribut crs). Si celle-ci est différente des données communales, reprojeter ces\ndernières dans le même système de projection que les stations de vélib\nNe conserver que les 50 principales stations (variable capacity)\n\nOn peut maintenant construire la carte de manière séquentielle avec la méthode plot en s’aidant de cette documentation\n\nEn premier lieu, grâce à boundary.plot,\nreprésenter la couche de base des limites des communes et arrondissements:\n\nUtiliser les options edgecolor = \"black\" et linewidth = 0.5\nNommer cet objet base\n\nAjouter la couche des départements avec les options edgecolor = \"blue\" et linewidth = 0.7\nAjouter les positions des stations\net ajuster la taille en fonction de la variable capacity. L’esthétique des points obtenus peut être contrôlé grâce aux options color = \"red\" et alpha = 0.4.\nRetirer les axes et ajouter un titre avec les options ci-dessous:\n\nbase.set_axis_off()\nbase.set_title(\"Les 50 principales stations de Vélib\")\n\n\nLa couche de base obtenue à l’issue de la question 4.\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nPuis en y ajoutant les limites départementales (question 5).\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nPuis les stations (question 6).\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nLa carte finale, après mise en forme:\n\n\n&lt;Axes: title={'center': 'Les 50 principales stations de Vélib'}&gt;"
  },
  {
    "objectID": "content/manipulation/03_geopandas_TP.html#jointures-spatiales",
    "href": "content/manipulation/03_geopandas_TP.html#jointures-spatiales",
    "title": "Pratique de geopandas avec les données vélib",
    "section": "Jointures spatiales",
    "text": "Jointures spatiales\nLes jointures attributaires fonctionnent comme avec un Pandas classique.\nPour conserver un objet spatial in fine, il faut faire attention à utiliser en premier (base de gauche) l’objet Geopandas.\nEn revanche, l’un des intérêts des objets Geopandas est qu’on peut également faire une jointure sur la dimension spatiale grâce à plusieurs fonctions.\nLa documentation à laquelle se référer est ici.\nUne version pédagogique pour R se trouve dans la documentation utilitR.\n\n\n Exercice 4: Associer les stations aux communes et arrondissements auxquels elles appartiennent\nDans cet exercice, on va supposer que :\n\nles localisations des stations velib\nsont stockées dans un dataframe nommé stations\nles données administratives\nsont dans un dataframe nommé petite_couronne.\n\n\nFaire une jointure spatiale pour enrichir les données de stations en y ajoutant des informations de petite_couronne. Appeler cet objet stations_info.\nCréer les objets stations_19e et arrondissement_19e pour stocker, respectivement,\nles stations appartenant au 19e et les limites de l’arrondissement.\nReprésenter la carte des stations du 19e arrondissement avec le code suivant :\n\nbase = petite_couronne.loc[petite_couronne['INSEE_DEP']==\"75\"].boundary.plot(edgecolor = \"k\", linewidth=0.5)\narrondissement_19e.boundary.plot(ax = base, edgecolor = \"red\", linewidth=0.9)\nstations_19.plot(ax = base, color = \"red\", alpha = 0.4)\nbase.set_axis_off()\nbase.set_title(\"Les stations Vélib du 19e arrondissement\")\nbase\n\nCompter le nombre de stations velib et le nombre de places velib par arrondissement ou commune. Représenter sur une carte chacune des informations\nReprésenter les mêmes informations mais en densité (diviser par la surface de l’arrondissement ou commune en km2)\n\n\n\n\n\n&lt;Axes: title={'center': 'Les stations Vélib du 19e arrondissement'}&gt;\n\n\n\n\n\n\n\n\n\nCarte obtenue à la question 4 :\n\n\n/opt/mamba/lib/python3.9/site-packages/geopandas/plotting.py:730: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nAvec la carte de la question 4, basée sur des aplats de couleurs (choropleth map), le lecteur est victime d’une illusion classique. Les arrondissements les plus visibles sur la carte sont les plus grands. D’ailleurs c’est assez logique qu’ils soient également mieux pourvus en velib. Même si l’offre de velib est probablement plus reliée à la densité de population et d’équipements, on peut penser que l’effet taille joue et qu’ainsi on est victime d’une illusion avec la carte précédente.\nSi on représente plutôt la capacité sous forme de densité, pour tenir compte de la taille différente des arrondissements, les conclusions sont inversées et correspondent mieux aux attentes d’un modèle centre-périphérie. Les arrondissements centraux sont mieux pourvus, cela se voit encore mieux avec des ronds proportionnels plutôt qu’une carte chorolèpthe.\n\n\n/opt/mamba/lib/python3.9/site-packages/geopandas/plotting.py:730: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "content/manipulation/03_geopandas_TP.html#exercice-supplémentaire",
    "href": "content/manipulation/03_geopandas_TP.html#exercice-supplémentaire",
    "title": "Pratique de geopandas avec les données vélib",
    "section": "Exercice supplémentaire",
    "text": "Exercice supplémentaire\nLes exercices précédents ont permis de se familiariser au traitement de données\nspatiales. Néanmoins il arrive de devoir jongler plus avec la\ndimension géométrique par exemple pour changer d’échelle ou introduire\ndes fusions/dissolutions de géométries.\nImaginons que chaque utilisateur de velib se déplace exclusivement\nvers la station la plus proche (à supposer qu’il n’y a jamais pénurie\nou surcapacité). Quelle est la carte de la couverture des vélibs ?\nPour répondre à ce type de question, on utilise fréquemment la\nla tesselation de Voronoï,\nune opération classique pour transformer des points en polygones. L’exercice suivant\npermet de se familiariser avec cette approche4.\nExercice à venir"
  },
  {
    "objectID": "content/manipulation/03_geopandas_TP.html#footnotes",
    "href": "content/manipulation/03_geopandas_TP.html#footnotes",
    "title": "Pratique de geopandas avec les données vélib",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nD’ailleurs, le logiciel de cartographie spécialisé QGIS, s’appuie sur Python\npour les manipulations de données nécessaires avant de réaliser une carte.↩︎\nLa librairie Python est encore expérimentale mais\nles prochaines semaines devraient permettre de combler ce manque.\nUne documentation interactive illustrant le code nécessaire pour reproduire\ntelle ou telle carte est disponible sur linogaliana.github.io/cartiflette-website.↩︎\nCette carte n’est pas trop soignée, c’est normal nous verrons comment\nfaire de belles cartes ultérieurement.↩︎\nDans ce document de travail sur données de téléphonie mobile, on montre néanmoins que cette approche n’est pas sans biais\nsur des phénomènes où l’hypothèse de proximité spatiale est\ntrop simplificatrice.↩︎"
  },
  {
    "objectID": "content/manipulation/02b_pandas_TP.html",
    "href": "content/manipulation/02b_pandas_TP.html",
    "title": "Pratique de pandas : un exemple complet",
    "section": "",
    "text": "Les exemples de ce TP sont visualisables sous forme de Jupyter Notebooks:\nDans cette série d’exercices Pandas,\nnous allons découvrir comment manipuler plusieurs\njeux de données avec Python.\nSi vous êtes intéressés par R,\nune version très proche de ce TP est\ndisponible dans ce cours.\nDans ce tutoriel, nous allons utiliser deux sources de données :\nLa librairie pynsee n’est pas installée par défaut avec Python. Avant de pouvoir l’utiliser,\nil est nécessaire de l’installer :\n!pip install xlrd\n!pip install pynsee\nToutes les dépendances indispensables étant installées, il suffit\nmaintenant d’importer les librairies qui seront utilisées\npendant ces exercices :\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pynsee\nimport pynsee.download"
  },
  {
    "objectID": "content/manipulation/02b_pandas_TP.html#importer-les-données",
    "href": "content/manipulation/02b_pandas_TP.html#importer-les-données",
    "title": "Pratique de pandas : un exemple complet",
    "section": "Importer les données",
    "text": "Importer les données\n\nImport d’un csv de l’Ademe\nL’URL d’accès aux données peut être conservé dans une variable ad hoc :\n\nurl = \"https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert\"\n\nL’objectif du premier exercice est de se familiariser à l’import et l’affichage de données\navec Pandas.\n\n\n Exercice 1: Importer un CSV et explorer la structure de données\n\nImporter les données de l’Ademe à l’aide du package Pandas et de la commande consacrée pour l’import de csv. Nommer le DataFrame obtenu emissions1.\nUtiliser les méthodes adéquates afin d’afficher pour les 10 premières valeurs, les 15 dernières et un échantillon aléatoire de 10 valeurs grâce aux méthodes adéquates du package Pandas.\nTirer 5 pourcents de l’échantillon sans remise.\nNe conserver que les 10 premières lignes et tirer aléatoirement dans celles-ci pour obtenir un DataFrame de 100 données.\nFaire 100 tirages à partir des 6 premières lignes avec une probabilité de 1/2 pour la première observation et une probabilité uniforme pour les autres.\n\n\n\nEn cas de blocage à la question 1\n\nLire la documentation de read_csv (très bien faite) ou chercher des exemples\nen ligne pour découvrir cette fonction."
  },
  {
    "objectID": "content/manipulation/02b_pandas_TP.html#premières-manipulations-de-données",
    "href": "content/manipulation/02b_pandas_TP.html#premières-manipulations-de-données",
    "title": "Pratique de pandas : un exemple complet",
    "section": "Premières manipulations de données",
    "text": "Premières manipulations de données\nLe chapitre précédent évoquait quelques manipulations traditionnelles\nde données. Les principales sont rappelées ici :\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRéordonner le DataFrame\n\n\n\n\nLa cheatsheet suivante est très pratique puisqu’elle illustre ces différentes\nfonctions. Il est recommandé de régulièrement\nla consulter :\n\n\n\nCheasheet Pandas\n\n\nL’objectif du prochain exercice est de se familiariser aux principales manipulations de données\nsur un sous-ensemble de la table des émissions de gaz carbonique.\n\n\n Exercice 2: Découverte des verbes de Pandas pour manipuler des données\nEn premier lieu, on propose de se familiariser avec les opérations sur\nles colonnes.\n\nCréer un dataframe emissions_copy ne conservant que les colonnes\nINSEE commune, Commune, Autres transports et Autres transports international\n\n\n\nIndice pour cette question\n\n\n\n\n\n\n\n\nComme les noms de variables sont peu pratiques, les renommer de la\nmanière suivante :\n\nINSEE commune \\(\\to\\) code_insee\nAutres transports \\(\\to\\) transports\nAutres transports international \\(\\to\\) transports_international\n\n\n\n\nIndice pour cette question\n\n\n\n\n\n\n\n\nOn propose, pour simplifier, de remplacer les valeurs manquantes (NA)\npar la valeur 0. Utiliser la\nméthode fillna\npour transformer les valeurs manquantes en 0.\nCréer les variables suivantes :\n\ndep: le département. Celui-ci peut être créé grâce aux deux premiers caractères de code_insee en appliquant la méthode str ;\ntransports_total: les émissions du secteur transports (somme des deux variables)\n\n\n\n\nIndice pour cette question\n\n\n\n\n\n\n\n\nOrdonner les données du plus gros pollueur au plus petit\npuis ordonner les données\ndu plus gros pollueur au plus petit par département (du 01 au 95).\n\n\n\nIndice pour cette question\n\n\n\n\n\n\n\n\nNe conserver que les communes appartenant aux départements 13 ou 31.\nOrdonner ces communes du plus gros pollueur au plus petit.\n\n\n\nIndice pour cette question\n\n\n\n\n\n\n\n\nCalculer les émissions totales par départements\n\n\n\nIndice pour cette question\n\n\n“Grouper par” = groupby\n“émissions totales” = agg({***: \"sum\"})\n\n\n\n\nA la question 5, quand on ordonne les communes exclusivement à partir de la variable\ntransports_total, on obtient ainsi:\n\n\n\n\n\n\n\n\n\ncode_insee\nCommune\ntransports\ntransports_international\ndep\ntransports_total\n\n\n\n\n31108\n77291\nLE MESNIL-AMELOT\n133834.090767\n3.303394e+06\n77\n3.437228e+06\n\n\n31099\n77282\nMAUREGARD\n133699.072712\n3.303394e+06\n77\n3.437093e+06\n\n\n31111\n77294\nMITRY-MORY\n89815.529858\n2.202275e+06\n77\n2.292090e+06\n\n\n\n\n\n\n\nA la question 6, on obtient ce classement :\n\n\n\n\n\n\n\n\n\ncode_insee\nCommune\ntransports\ntransports_international\ndep\ntransports_total\n\n\n\n\n4438\n13096\nSAINTES-MARIES-DE-LA-MER\n271182.758578\n0.000000\n13\n271182.758578\n\n\n4397\n13054\nMARIGNANE\n245375.418650\n527360.799265\n13\n772736.217915\n\n\n11684\n31069\nBLAGNAC\n210157.688544\n403717.366279\n31\n613875.054823\n\n\n\n\n\n\n\n\nImport des données de l’Insee\nEn ce qui concerne nos informations communales, on va utiliser l’une des\nsources de l’Insee les plus utilisées : les données Filosofi.\nAfin de faciliter la récupération de celles-ci, nous allons\nutiliser le package communautaire pynsee :\n\n\n Note\nLe package pynsee comporte deux principaux points d’entrée:\n\nLes API de l’Insee, ce qui sera illustré dans le chapitre consacré.\nQuelques jeux de données directement issus du site web de\nl’Insee (insee.fr)\n\nDans ce chapitre, nous allons exclusivement utiliser cette deuxième\napproche. Cela se fera par le module pynsee.download.\nLa liste des données disponibles depuis ce package est ici.\nLa fonction download_file attend un identifiant unique\npour savoir quelle base de données aller chercher et\nrestructurer depuis le\nsite insee.fr.\n\n\nConnaître la liste des bases disponibles\n\nPour connaître la liste des bases disponibles, vous\npouvez utiliser la fonction meta = pynsee.get_file_list()\naprès avoir fait import pynsee.\nCelle-ci renvoie un DataFrame dans lequel on peut\nrechercher, par exemple grâce à une recherche\nde mots-clefs :\n\nmeta = pynsee.get_file_list()\nmeta.loc[meta['label'].str.contains(r\"Filosofi.*2016\")]\n\npynsee.download's metadata rely on volunteering contributors and their manual updates\nget_file_list does not provide data from official Insee's metadata API\nConsequently, please report any issue\n\n\n\n\n\n\n\n\n\nid\nname\nlabel\ncollection\nlink\ntype\nzip\nbig_zip\ndata_file\ntab\n...\nlabel_col\ndate_ref\nmeta_file\nseparator\ntype_col\nlong_col\nval_col\nencoding\nlast_row\nmissing_value\n\n\n\n\n79\nFILOSOFI_COM_2016\nFILOSOFI_COM\nDonnées Filosofi niveau communal – 2016\nFILOSOFI\nhttps://www.insee.fr/fr/statistiques/fichier/4...\nxls\nTrue\nFalse\nbase-cc-filosofi-2016.xls\nCOM\n...\nNaN\n2016-01-01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n80\nFILOSOFI_EPCI_2016\nFILOSOFI_EPCI\nDonnées Filosofi niveau EPCI – 2016\nFILOSOFI\nhttps://www.insee.fr/fr/statistiques/fichier/4...\nxls\nTrue\nFalse\nbase-cc-filosofi-2016.xls\nEPCI\n...\nNaN\n2016-01-01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n81\nFILOSOFI_ARR_2016\nFILOSOFI_ARR\nDonnées Filosofi niveau arondissement – 2016\nFILOSOFI\nhttps://www.insee.fr/fr/statistiques/fichier/4...\nxls\nTrue\nFalse\nbase-cc-filosofi-2016.xls\nARR\n...\nNaN\n2016-01-01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n82\nFILOSOFI_DEP_2016\nFILOSOFI_DEP\nDonnées Filosofi niveau départemental – 2016\nFILOSOFI\nhttps://www.insee.fr/fr/statistiques/fichier/4...\nxls\nTrue\nFalse\nbase-cc-filosofi-2016.xls\nDEP\n...\nNaN\n2016-01-01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n83\nFILOSOFI_REG_2016\nFILOSOFI_REG\nDonnées Filosofi niveau régional – 2016\nFILOSOFI\nhttps://www.insee.fr/fr/statistiques/fichier/4...\nxls\nTrue\nFalse\nbase-cc-filosofi-2016.xls\nREG\n...\nNaN\n2016-01-01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n84\nFILOSOFI_METRO_2016\nFILOSOFI_METRO\nDonnées Filosofi niveau France métropolitaine ...\nFILOSOFI\nhttps://www.insee.fr/fr/statistiques/fichier/4...\nxls\nTrue\nFalse\nbase-cc-filosofi-2016.xls\nMETRO\n...\nNaN\n2016-01-01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n85\nFILOSOFI_AU2010_2016\nFILOSOFI_AU2010\nDonnées Filosofi niveau aire urbaine – 2016\nFILOSOFI\nhttps://www.insee.fr/fr/statistiques/fichier/4...\nxls\nTrue\nFalse\nbase-cc-filosofi-2016.xls\nAU2010\n...\nNaN\n2016-01-01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n86\nFILOSOFI_UU2010_2016\nFILOSOFI_UU2010\nDonnées Filosofi niveau unité urbaine – 2016\nFILOSOFI\nhttps://www.insee.fr/fr/statistiques/fichier/4...\nxls\nTrue\nFalse\nbase-cc-filosofi-2016.xls\nUU2010\n...\nNaN\n2016-01-01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n87\nFILOSOFI_ZE2010_2016\nFILOSOFI_ZE2010\nDonnées Filosofi niveau zone d’emploi – 2016\nFILOSOFI\nhttps://www.insee.fr/fr/statistiques/fichier/4...\nxls\nTrue\nFalse\nbase-cc-filosofi-2016.xls\nZE2010\n...\nNaN\n2016-01-01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n9 rows × 24 columns\n\n\n\nIci, meta['label'].str.contains(r\"Filosofi.*2016\") signifie:\n“pandas trouve moi tous les labels où sont contenus les termes Filosofi et 2016.”\n(.* signifiant “peu m’importe le nombre de mots ou caractères entre”)\n\n\n\nOn va utiliser les données Filosofi (données de revenus) au niveau communal de 2016.\nCe n’est pas la même année que les données d’émission de CO2, ce n’est donc pas parfaitement rigoureux,\nmais cela permettra tout de même d’illustrer\nles principales fonctionnalités de Pandas\nLe point d’entrée principal de la fonction pynsee est la fonction download_file.\nLe code pour télécharger les données est le suivant :\n\nfrom pynsee.download import download_file\nfilosofi = download_file(\"FILOSOFI_COM_2016\")\n\nLe DataFrame en question a l’aspect suivant :\n\n\n\n\n\n\n\n\n\nCODGEO\nLIBGEO\nNBMENFISC16\nNBPERSMENFISC16\nMED16\nPIMP16\nTP6016\nTP60AGE116\nTP60AGE216\nTP60AGE316\n...\nPPEN16\nPPAT16\nPPSOC16\nPPFAM16\nPPMINI16\nPPLOGT16\nPIMPOT16\nD116\nD916\nRD16\n\n\n\n\n16997\n46253\nSaint-Chamarand\n89\n196\n19185.23809523809\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n24793\n63386\nSaint-Pierre-Roche\n204\n457\n20378.571428571428\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n329\n01384\nSaint-Rambert-en-Bugey\n946\n2035\n17868.46153846154\n39\n20\nNaN\nNaN\nNaN\n...\n29.1\n7\n8.2\n3.3\n2.7\n2.2\n-12.6\n9348.09523809524\n28880\n3.0893994192858227\n\n\n23460\n61423\nSaint-Martin-d'Écublei\n255\n686.5\n21940.666666666668\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n5187\n15257\nVezels-Roussy\n59\n122\n17124.2\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 29 columns\n\n\n\nPandas a géré automatiquement les types de variables. Il le fait relativement bien, mais une vérification est toujours utile pour les variables qui ont un statut spécifique.\nPour les variables qui ne sont pas en type float alors qu’elles devraient l’être, on modifie leur type.\n\nfilosofi.loc[:, filosofi.columns[2:]] = (\n  filosofi.loc[:, filosofi.columns[2:]]\n  .apply(pd.to_numeric, errors='coerce')\n)\n\nUn simple coup d’oeil sur les données\ndonne une idée assez précise de la manière dont les données sont organisées.\nOn remarque que certaines variables de filosofi semblent avoir beaucoup de valeurs manquantes (secret statistique)\nalors que d’autres semblent complètes.\nSi on désire exploiter filosofi, il faut faire attention à la variable choisie.\nNotre objectif à terme va être de relier l’information contenue entre ces\ndeux jeux de données. En effet, sinon, nous risquons d’être frustré : nous allons\nvouloir en savoir plus sur les émissions de gaz carbonique mais seront très\nlimités dans les possibilités d’analyse sans ajout d’une information annexe\nissue de filosofi."
  },
  {
    "objectID": "content/manipulation/02b_pandas_TP.html#les-indices",
    "href": "content/manipulation/02b_pandas_TP.html#les-indices",
    "title": "Pratique de pandas : un exemple complet",
    "section": "Les indices",
    "text": "Les indices\nLes indices sont des éléments spéciaux d’un DataFrame puisqu’ils permettent d’identifier certaines observations.\nIl est tout à fait possible d’utiliser plusieurs indices, par exemple si on a des niveaux imbriqués.\nPour le moment, on va prendre comme acquis que les codes communes (dits aussi codes Insee) permettent\nd’identifier de manière unique une commune. Un exercice ultérieur permettra de s’en assurer.\nPandas propose un système d’indice qui permet d’ordonner les variables mais également de gagner\nen efficacité sur certains traitements, comme des recherches d’observations. Le prochain\nexercice illustre cette fonctionnalité.\n\n\n Exercice 3 : Les indices\nOn suppose ici qu’on peut se fier aux codes communes. En effet, on a un même ordre de grandeur de communes dans les deux bases.\n\nprint(emissions[['INSEE commune', 'Commune']].nunique())\nprint(filosofi[['CODGEO', 'LIBGEO']].nunique())\n\nINSEE commune    35798\nCommune          33338\ndtype: int64\nCODGEO    34932\nLIBGEO    32676\ndtype: int64\n\n\n\nFixer comme indice la variable de code commune dans les deux bases.\nRegarder le changement que cela induit sur le display du DataFrame\nLes deux premiers chiffres des codes communes sont le numéro de département.\nCréer une variable de département dep dans emissions et filosofi\nCalculer les émissions totales par secteur pour chaque département.\nMettre en log ces résultats dans un objet emissions_log.\nGarder 5 départements et produire un barplot grâce à la méthode plot (la\nfigure n’a pas besoin d’être vraiment propre, c’est seulement pour illustrer\ncette méthode)\nRepartir de emissions.\nCalculer les émissions totales par département et sortir la liste\ndes 10 principaux émetteurs de CO2 et des 5 départements les moins émetteurs.\nEssayer de comprendre pourquoi ce sont ces départements qui apparaissent en tête\ndu classement. Pour cela, il peut être utile de regarder les caractéristiques de ces\ndépartements dans filosofi\n\n\n\nEn pratique, l’utilisation des indices en Pandas peut être piégeuse, notamment lorsqu’on\nassocie des sources de données.\nIl est plutôt recommandé de ne pas les utiliser ou de les utiliser avec parcimonie,\ncela pourra éviter de mauvaises surprises."
  },
  {
    "objectID": "content/manipulation/02b_pandas_TP.html#restructurer-les-données",
    "href": "content/manipulation/02b_pandas_TP.html#restructurer-les-données",
    "title": "Pratique de pandas : un exemple complet",
    "section": "Restructurer les données",
    "text": "Restructurer les données\nQuand on a plusieurs informations pour un même individu ou groupe, on\nretrouve généralement deux types de structure de données :\n\nformat wide : les données comportent des observations répétées, pour un même individu (ou groupe), dans des colonnes différentes\nformat long : les données comportent des observations répétées, pour un même individu, dans des lignes différentes avec une colonne permettant de distinguer les niveaux d’observations\n\nUn exemple de la distinction entre les deux peut être pris à l’ouvrage de référence d’Hadley Wickham, R for Data Science:\n\n\n\nDonnées long et wide\n\n\nL’aide mémoire suivante aidera à se rappeler les fonctions à appliquer si besoin :\n\n\n\n\n\nLe fait de passer d’un format wide au format long (ou vice-versa)\npeut être extrêmement pratique car certaines fonctions sont plus adéquates sur une forme de données ou sur l’autre.\nEn règle générale, avec Python comme avec R, les formats long sont souvent préférables.\nLes formats wide sont plutôt pensés pour des tableurs comme Excel ou on dispose d’un nombre réduit\nde lignes à partir duquel faire des tableaux croisés dynamiques.\nLe prochain exercice propose donc une telle restructuration de données.\nLes données de l’ADEME, et celles de l’Insee également, sont au format\nwide.\nLe prochain exercice illustre l’intérêt de faire la conversion long \\(\\to\\) wide\navant de faire un graphique.\n\n\n Exercice 5: Restructurer les données : wide to long\n\nCréer une copie des données de l’ADEME en faisant df_wide = emissions_wide.copy()\nRestructurer les données au format long pour avoir des données d’émissions par secteur en gardant comme niveau d’analyse la commune (attention aux autres variables identifiantes).\nFaire la somme par secteur et représenter graphiquement\nGarder, pour chaque département, le secteur le plus polluant"
  },
  {
    "objectID": "content/manipulation/02b_pandas_TP.html#combiner-les-données",
    "href": "content/manipulation/02b_pandas_TP.html#combiner-les-données",
    "title": "Pratique de pandas : un exemple complet",
    "section": "Combiner les données",
    "text": "Combiner les données\n\nTravail préliminaire\nJusqu’à présent lorsque nous avons produit des statistiques descriptives,\ncelles-ci étaient univariées, c’est-à-dire que nous produisions de l’information\nsur une variable mais nous ne la mettions pas en lien avec une autre. Pourtant,\non est rapidement amené à désirer expliquer certaines statistiques agrégées à partir\nde caractéristiques issues d’une autre source de données. Cela implique\ndonc d’associer des jeux de données,\nautrement dit de mettre en lien deux jeux de données\nprésentant le même niveau d’information.\nOn appelle ceci faire un merge ou un join. De manière illustrée,\nceci revient à effectuer ce type d’opération :\n\n\n\n\n\nAvant de faire ceci, il est néanmoins nécessaire de s’assurer que les variables\ncommunes entre les bases de données présentent le bon niveau d’information.\n\n\n Exercice 6: vérification des clés de jointure\nOn commence par vérifier les dimensions des DataFrames et la structure de certaines variables clés.\nEn l’occurrence, les variables fondamentales pour lier nos données sont les variables communales.\nIci, on a deux variables géographiques: un code commune et un nom de commune.\n\nVérifier les dimensions des DataFrames.\nIdentifier dans filosofi les noms de communes qui correspondent à plusieurs codes communes et sélectionner leurs codes. En d’autres termes, identifier les LIBGEO tels qu’il existe des doublons de CODGEO et les stocker dans un vecteur x (conseil: faire attention à l’index de x).\n\nOn se focalise temporairement sur les observations où le libellé comporte plus de deux codes communes différents\n\nQuestion 3. Regarder dans filosofi ces observations.\nQuestion 4. Pour mieux y voir, réordonner la base obtenue par order alphabétique.\nQuestion 5. Déterminer la taille moyenne (variable nombre de personnes: NBPERSMENFISC16) et quelques statistiques descriptives de ces données.\nComparer aux mêmes statistiques sur les données où libellés et codes communes coïncident.\nQuestion 6. Vérifier les grandes villes (plus de 100 000 personnes),\nla proportion de villes pour lesquelles un même nom est associé à différents codes commune.\nQuestion 7. Vérifier dans filosofi les villes dont le libellé est égal à Montreuil.\nVérifier également celles qui contiennent le terme ‘Saint-Denis’.\n\n\n\nCe petit exercice permet de se rassurer car les libellés dupliqués\nsont en fait des noms de commune identiques mais qui ne sont pas dans le même département.\nIl ne s’agit donc pas d’observations dupliquées.\nOn peut donc se fier aux codes communes, qui eux sont uniques.\n\n\nAssocier des données\nUne information que l’on cherche à obtenir s’obtient de moins en moins à partir d’une unique base de données. Il devient commun de devoir combiner des données issues de sources différentes.\nNous allons ici nous focaliser sur le cas le plus favorable qui est la situation où une information permet d’apparier de manière exacte deux bases de données (autrement nous serions dans une situation, beaucoup plus complexe, d’appariement flou). La situation typique est l’appariement entre deux sources de données selon un identifiant individuel ou un identifiant de code commune, ce qui est notre cas.\nIl est recommandé de lire ce guide assez complet sur la question des jointures avec R qui donne des recommandations également utiles en Python.\nDans le langage courant du statisticien,\non utilise de manière indifférente les termes merge ou join. Le deuxième terme provient de la syntaxe SQL.\nQuand on fait du Pandas, on utilise plutôt la commande merge.\n\n\n\n\n\n\n\n Exercice 7: Calculer l'empreinte carbone par habitant\nEn premier lieu, on va calculer l’empreinte carbone de chaque commune.\n\nCréer une variable emissions qui correspond aux émissions totales d’une commune\nFaire une jointure à gauche entre les données d’émissions et les données de cadrage2.\nCalculer l’empreinte carbone (émissions totales / population).\n\nA ce stade nous pourrions avoir envie d’aller vers la modélisation pour essayer d’expliquer\nles déterminants de l’empreinte carbone à partir de variables communales.\nUne approche inférentielle nécessite néanmoins pour être pertinente de\nvérifier en amont des statistiques descriptives.\n\nSortir un histogramme en niveau puis en log de l’empreinte carbone communale.\n\nAvec une meilleure compréhension de nos données, nous nous rapprochons\nde la statistique inférentielle. Néanmoins, nous avons jusqu’à présent\nconstruit des statistiques univariées mais n’avons pas cherché à comprendre\nles résultats en regardant le lien avec d’autres variables.\nCela nous amène vers la statistique bivariée, notamment l’analyse des corrélations.\nCe travail est important puisque toute modélisation ultérieure consistera à\nraffiner l’analyse des corrélations pour tenir compte des corrélations croisées\nentre multiples facteurs. On propose ici de faire cette analyse\nde manière minimale.\n\nRegarder la corrélation entre les variables de cadrage et l’empreinte carbone. Certaines variables semblent-elles pouvoir potentiellement influer sur l’empreinte carbone ?\n\n\n\nA l’issue de la question 5, le graphique des corrélations est le suivant :\n\n\n&lt;Axes: ylabel='index'&gt;"
  },
  {
    "objectID": "content/manipulation/02b_pandas_TP.html#exercices-bonus",
    "href": "content/manipulation/02b_pandas_TP.html#exercices-bonus",
    "title": "Pratique de pandas : un exemple complet",
    "section": "Exercices bonus",
    "text": "Exercices bonus\nLes plus rapides d’entre vous sont invités à aller un peu plus loin en s’entraînant avec des exercices bonus qui proviennent du site de Xavier Dupré. 3 notebooks en lien avec numpy et pandas vous y sont proposés :\n\nCalcul Matriciel, Optimisation : énoncé / corrigé\nDataFrame et Graphes : énoncé / corrigé\nPandas et itérateurs : énoncé / corrigé"
  },
  {
    "objectID": "content/manipulation/02b_pandas_TP.html#footnotes",
    "href": "content/manipulation/02b_pandas_TP.html#footnotes",
    "title": "Pratique de pandas : un exemple complet",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPar manque d’imagination, on est souvent tenté d’appeler notre\ndataframe principal df ou data. C’est souvent une mauvaise idée puisque\nce nom n’est pas très informatif quand on relit le code quelques semaines\nplus tard. L’autodocumentation, approche qui consiste à avoir un code\nqui se comprend de lui-même, est une bonne pratique et il est donc recommandé\nde donner un nom simple mais efficace pour connaître la nature du dataset en question.↩︎\nIdéalement, il serait nécessaire de s’assurer que cette jointure n’introduit\npas de biais. En effet, comme nos années de référence ne sont pas forcément identiques,\nil peut y avoir un mismatch entre nos deux sources. Le TP étant déjà long, nous n’allons pas dans cette voie.\nLes lecteurs intéressés pourront effectuer une telle analyse en exercice supplémentaire.↩︎"
  },
  {
    "objectID": "content/manipulation/01_numpy.html",
    "href": "content/manipulation/01_numpy.html",
    "title": "Numpy, la brique de base de la data science",
    "section": "",
    "text": "Pour essayer les exemples présents dans ce tutoriel :\nIl est recommandé de régulièrement se référer à\nla cheatsheet numpy et à la\ndoc officielle en cas de doute\nsur une fonction.\nDans ce chapitre, on ne dérogera pas à la convention qui s’est imposée\nd’importer Numpy de la\nmanière suivante :\nimport numpy as np\nNous allons également fixer la racine du générateur aléatoire de nombres\nafin d’avoir des résultats reproductibles :\nnp.random.seed(12345)\nNote\nLes auteurs\nde numpy préconisent désormais\nde privilégier l’utilisation de\ngénérateurs via la fonction default_rng() plutôt que la simple utilisation de numpy.random.\nAfin d’être en phase avec les codes qu’on peut trouver partout sur internet, nous\nconservons encore np.random.seed mais cela peut être amené à évoluer.\nSi les scripts suivants sont exécutés dans un Notebook Jupyter,\nil est recommandé d’utiliser les paramètres suivants\npour contrôler le rendu:"
  },
  {
    "objectID": "content/manipulation/01_numpy.html#le-concept-darray",
    "href": "content/manipulation/01_numpy.html#le-concept-darray",
    "title": "Numpy, la brique de base de la data science",
    "section": "Le concept d’array",
    "text": "Le concept d’array\nLe concept central de NumPy (Numerical Python) est\nl’array qui est un tableau de données multidimensionnel.\nL’array numpy peut être unidimensionnel et s’apparenter à un\nvecteur (1d-array),\nbidimensionnel et ainsi s’apparenter à une matrice (2d-array) ou,\nde manière plus générale,\nprendre la forme d’un objet\nmultidimensionnel (Nd-array).\nLes tableaux simples (uni ou bi-dimensionnels) sont faciles à se représenter et seront particulièrement\nutilisés dans le paradigme des DataFrames mais\nla possibilité d’avoir des objets multidimensionnels permettra d’exploiter des\nstructures très complexes.\nUn DataFrame sera construit à partir d’une collection\nd’array uni-dimensionnels (les variables de la table), ce qui permettra d’effectuer des opérations cohérentes\n(et optimisées) avec le type de la variable.\nPar rapport à une liste,\n\nun array ne peut contenir qu’un type de données (integer, string, etc.),\ncontrairement à une liste.\nles opérations implémentées par numpy seront plus efficaces et demanderont moins\nde mémoire\n\nLes données géographiques constitueront une construction un peu plus complexe qu’un DataFrame traditionnel.\nLa dimension géographique prend la forme d’un tableau plus profond, au moins bidimensionnel\n(coordonnées d’un point)."
  },
  {
    "objectID": "content/manipulation/01_numpy.html#créer-un-array",
    "href": "content/manipulation/01_numpy.html#créer-un-array",
    "title": "Numpy, la brique de base de la data science",
    "section": "Créer un array",
    "text": "Créer un array\nOn peut créer un array de plusieurs manières. Pour créer un array à partir d’une liste,\nil suffit d’utiliser la méthode array:\n\nnp.array([1,2,5])\n\narray([1, 2, 5])\n\n\nIl est possible d’ajouter un argument dtype pour contraindre le type du array:\n\nnp.array([[\"a\",\"z\",\"e\"],[\"r\",\"t\"],[\"y\"]], dtype=\"object\")\n\narray([list(['a', 'z', 'e']), list(['r', 't']), list(['y'])], dtype=object)\n\n\nIl existe aussi des méthodes pratiques pour créer des array:\n\nséquences logiques : np.arange (suite) ou np.linspace (interpolation linéaire entre deux bornes)\nséquences ordonnées : array rempli de zéros, de 1 ou d’un nombre désiré : np.zeros, np.ones ou np.full\nséquences aléatoires : fonctions de génération de nombres aléatoires : np.rand.uniform, np.rand.normal, etc.\ntableau sous forme de matrice identité: np.eye\n\nCeci donne ainsi, pour les séquences logiques:\n\nnp.arange(0,10)\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\nnp.arange(0,10,3)\n\narray([0, 3, 6, 9])\n\n\n\nnp.linspace(0, 1, 5)\n\narray([0.  , 0.25, 0.5 , 0.75, 1.  ])\n\n\nPour un array initialisé à 0:\n\nnp.zeros(10, dtype=int)\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\nou initialisé à 1:\n\nnp.ones((3, 5), dtype=float)\n\narray([[1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.]])\n\n\nou encore initialisé à 3.14:\n\n\narray([[3.14, 3.14, 3.14, 3.14, 3.14],\n       [3.14, 3.14, 3.14, 3.14, 3.14],\n       [3.14, 3.14, 3.14, 3.14, 3.14]])\n\n\nEnfin, pour créer la matrice \\(I_3\\):\n\nnp.eye(3)\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\n\n\n Exercice 1\nGénérer:\n\n\\(X\\) une variable aléatoire, 1000 répétitions d’une loi \\(U(0,1)\\)\n\\(Y\\) une variable aléatoire, 1000 répétitions d’une loi normale de moyenne nulle et de variance égale à 2\nVérifier la variance de \\(Y\\) avec np.var"
  },
  {
    "objectID": "content/manipulation/01_numpy.html#indexation-et-slicing",
    "href": "content/manipulation/01_numpy.html#indexation-et-slicing",
    "title": "Numpy, la brique de base de la data science",
    "section": "Indexation et slicing",
    "text": "Indexation et slicing\n\nLogique dans le cas d’un array unidimensionnel\nLa structure la plus simple est l’array unidimensionnel:\n\nx = np.arange(10)\nprint(x)\n\n[0 1 2 3 4 5 6 7 8 9]\n\n\nL’indexation est dans ce cas similaire à celle d’une liste:\n\nle premier élément est 0\nle énième élément est accessible à la position \\(n-1\\)\n\nLa logique d’accès aux éléments est ainsi la suivante :\nx[start:stop:step]\nAvec un array unidimensionnel, l’opération de slicing (garder une coupe du array) est très simple.\nPar exemple, pour garder les K premiers éléments d’un array, on fera:\nx[:(K-1)]\nEn l’occurrence, on sélectionne le K\\(^{eme}\\) élément en utilisant\nx[K-1]\nPour sélectionner uniquement un élément, on fera ainsi:\n\nx = np.arange(10)\nx[2]\n\n2\n\n\nLes syntaxes qui permettent de sélectionner des indices particuliers d’une liste fonctionnent également\navec les arrays.\n\n\n Exercice 2\nPrenez x = np.arange(10) et…\n\nSélectionner les éléments 0,3,5 de x\nSélectionner les éléments pairs\nSélectionner tous les éléments sauf le premier\nSélectionner les 5 premiers éléments\n\n\n\n\n\nSur la performance\nUn élément déterminant dans la performance de numpy par rapport aux listes,\nlorsqu’il est question de\nslicing est qu’un array ne renvoie pas une\ncopie de l’élément en question (copie qui coûte de la mémoire et du temps)\nmais simplement une vue de celui-ci.\nLorsqu’il est nécessaire d’effectuer une copie,\npar exemple pour ne pas altérer l’array sous-jacent, on peut\nutiliser la méthode copy:\nx_sub_copy = x[:2, :2].copy()\n\n\n\nFiltres logiques\nIl est également possible, et plus pratique, de sélectionner des données à partir de conditions logiques\n(opération qu’on appelle un boolean mask).\nCette fonctionalité servira principalement à\neffectuer des opérations de filtre sur les données.\nPour des opérations de comparaison simples, les comparateurs logiques peuvent être suffisants.\nCes comparaisons fonctionnent aussi sur les tableaux multidimensionnels grâce au\nbroadcasting sur lequel nous reviendrons :\n\nx = np.arange(10)\nx2 = np.array([[-1,1,-2],[-3,2,0]])\nprint(x)\nprint(x2)\n\n[0 1 2 3 4 5 6 7 8 9]\n[[-1  1 -2]\n [-3  2  0]]\n\n\n\nx==2\nx2&lt;0\n\narray([[ True, False,  True],\n       [ True, False, False]])\n\n\nPour sélectionner les observations relatives à la condition logique,\nil suffit d’utiliser la logique de slicing de numpy qui fonctionne avec les conditions logiques\n\n\n Exercice 3\nSoit\nx = np.random.normal(size=10000)\n\nNe conserver que les valeurs dont la valeur absolue est supérieure à 1.96\nCompter le nombre de valeurs supérieures à 1.96 en valeur absolue et leur proportion dans l’ensemble\nSommer les valeurs absolues de toutes les observations supérieures (en valeur absolue) à 1.96\net rapportez les à la somme des valeurs de x (en valeur absolue)\n\n\n\nLorsque c’est possible, il est recommandé d’utiliser les fonctions logiques de numpy (optimisées et\nqui gèrent bien la dimension).\nParmi elles, on peut retrouver:\n\ncount_nonzero\nisnan\nany ; all ; notamment avec l’argument axis\nnp.array_equal pour vérifier, élément par élément, l’égalité\n\nSoit\n\nx = np.random.normal(0, size=(3, 4))\n\nun array multidimensionnel et\n\ny = np.array([np.nan, 0, 1])\n\nun array unidimensionnel présentant une valeur manquante.\n\n\n Exercice 4\n\nUtiliser count_nonzero sur y\nUtiliser isnan sur y et compter le nombre de valeurs non NaN\nVérifier que x comporte au moins une valeur positive dans son ensemble, en parcourant les lignes puis les colonnes.\n\nNote : Jetez un oeil à ce que correspond le paramètre axis dans numpy en vous documentant sur internet. Par exemple ici."
  },
  {
    "objectID": "content/manipulation/01_numpy.html#manipuler-un-array",
    "href": "content/manipulation/01_numpy.html#manipuler-un-array",
    "title": "Numpy, la brique de base de la data science",
    "section": "Manipuler un array",
    "text": "Manipuler un array\nDans cette section, on utilisera un array multidimensionnel:\n\nx = np.random.normal(0, size=(3, 4))\n\n\nStatistiques sur un array\nPour les statistiques descriptives classiques,\nNumpy propose un certain nombre de fonctions déjà implémentées,\nqui peuvent être combinées avec l’argument axis\n\n\n Exercice 5\n\nFaire la somme de tous les éléments d’un array, des éléments en ligne et des éléments en colonne. Vérifier\nla cohérence\nEcrire une fonction statdesc pour renvoyer les valeurs suivantes : moyenne, médiane, écart-type, minimum et maximum.\nL’appliquer sur x en jouant avec l’argument axis\n\n\n\n\n\nFonctions de manipulation\nVoici quelques fonctions pour modifier un array,\n\n\n\n\n\n\n\nOpération\nImplémentation\n\n\n\n\nAplatir un array\nx.flatten() (méthode)\n\n\nTransposer un array\nx.T (méthode) ou np.transpose(x) (fonction)\n\n\nAjouter des éléments à la fin\nnp.append(x, [1,2])\n\n\nAjouter des éléments à un endroit donné (aux positions 1 et 2)\nnp.insert(x, [1,2], 3)\n\n\nSupprimer des éléments (aux positions 0 et 3)\nnp.delete(x, [0,3])\n\n\n\nPour combiner des array, on peut utiliser, selon les cas,\nles fonctions np.concatenate, np.vstack ou la méthode .r_ (concaténation rowwise).\nnp.hstack ou la méthode .column_stack ou .c_ (concaténation column-wise)\n\nx = np.random.normal(size = 10)\n\nPour ordonner un array, on utilise np.sort\n\nx = np.array([7, 2, 3, 1, 6, 5, 4])\n\nnp.sort(x)\n\narray([1, 2, 3, 4, 5, 6, 7])\n\n\nSi on désire faire un ré-ordonnement partiel pour trouver les k valeurs les plus petites d’un array sans les ordonner, on utilise partition:\n\nnp.partition(x, 3)\n\narray([2, 1, 3, 4, 6, 5, 7])"
  },
  {
    "objectID": "content/manipulation/01_numpy.html#broadcasting",
    "href": "content/manipulation/01_numpy.html#broadcasting",
    "title": "Numpy, la brique de base de la data science",
    "section": "Broadcasting",
    "text": "Broadcasting\nLe broadcasting désigne un ensemble de règles permettant\nd’appliquer des opérations sur des tableaux de dimensions différentes. En pratique,\ncela consiste généralement à appliquer une seule opération à l’ensemble des membres d’un tableau numpy.\nLa différence peut être comprise à partir de l’exemple suivant. Le broadcasting permet\nde transformer le scalaire 5 en array de dimension 3:\n\na = np.array([0, 1, 2])\n\nb = np.array([5, 5, 5])\n\na + b\na + 5\n\narray([5, 6, 7])\n\n\nLe broadcasting peut être très pratique pour effectuer de manière efficace des opérations sur des données à\nla structure complexe. Pour plus de détails, se rendre\nici ou ici."
  },
  {
    "objectID": "content/manipulation/01_numpy.html#une-application-programmer-ses-propres-k-nearest-neighbors",
    "href": "content/manipulation/01_numpy.html#une-application-programmer-ses-propres-k-nearest-neighbors",
    "title": "Numpy, la brique de base de la data science",
    "section": "Une application: programmer ses propres k-nearest neighbors",
    "text": "Une application: programmer ses propres k-nearest neighbors\n\n\n\n Exercice (un peu plus corsé)\n\nCréer X un tableau à deux dimensions (i.e. une matrice) comportant 10 lignes\net 2 colonnes. Les nombres dans le tableau sont aléatoires.\nImporter le module matplotlib.pyplot sous le nom plt. Utiliser\nplt.scatter pour représenter les données sous forme de nuage de points.\nConstuire une matrice 10x10 stockant, à l’élément \\((i,j)\\), la distance euclidienne entre les points \\(X[i,]\\) et \\(X[j,]\\). Pour cela, il va falloir jouer avec les dimensions en créant des tableaux emboîtés à partir par des appels à np.newaxis :\n\n\nEn premier lieu, utiliser X1 = X[:, np.newaxis, :] pour transformer la matrice en tableau emboîté. Vérifier les dimensions\nCréer X2 de dimension (1, 10, 2) à partir de la même logique\nEn déduire, pour chaque point, la distance avec les autres points pour chaque coordonnées. Elever celle-ci au carré\nA ce stade, vous devriez avoir un tableau de dimension (10, 10, 2). La réduction à une matrice s’obtient en sommant sur le dernier axe. Regarder dans l’aide de np.sum comme effectuer une somme sur le dernier axe.\nEnfin, appliquer la racine carrée pour obtenir une distance euclidienne en bonne et due forme.\n\n\nVérifier que les termes diagonaux sont bien nuls (distance d’un point à lui-même…)\nIl s’agit maintenant de classer, pour chaque point, les points dont les valeurs sont les plus similaires. Utiliser np.argsort pour obtenir, pour chaque ligne, le classement des points les plus proches\nOn va s’intéresser aux k-plus proches voisins. Pour le moment, fixons k=2. Utiliser argpartition pour réordonner chaque ligne de manière à avoir les 2 plus proches voisins de chaque point d’abord et le reste de la ligne ensuite\nUtiliser le morceau de code ci-dessous\n\n\n\n\nUn indice pour représenter graphiquement les plus proches voisins\nplt.scatter(X[:, 0], X[:, 1], s=100)\n\n# draw lines from each point to its two nearest neighbors\nK = 2\n\nfor i in range(X.shape[0]):\n    for j in nearest_partition[i, :K+1]:\n        # plot a line from X[i] to X[j]\n        # use some zip magic to make it happen:\n        plt.plot(*zip(X[j], X[i]), color='black')\n\n\nPour la question 2, vous devriez obtenir un graphique ayant cet aspect :\n\n\n\n\n\nLe résultat de la question 7 est le suivant :\n\n\n\n\n\nAi-je inventé cet exercice corsé ? Pas du tout, il vient de l’ouvrage Python Data Science Handbook. Mais, si je vous l’avais indiqué immédiatement, auriez-vous cherché à répondre aux questions ?\nPar ailleurs, il ne serait pas une bonne idée de généraliser cet algorithme à de grosses données. La complexité de notre approche est \\(O(N^2)\\). L’algorithme implémenté par Scikit-Learn est\nen \\(O[NlogN]\\).\nDe plus, le calcul de distances matricielles en utilisant la puissance des cartes graphiques serait plus rapide. A cet égard, la librairie faiss offre des performances beaucoup plus satisfaisantes que celles que permettraient numpy sur ce problème précis."
  },
  {
    "objectID": "content/manipulation/01_numpy.html#exercices-supplémentaires",
    "href": "content/manipulation/01_numpy.html#exercices-supplémentaires",
    "title": "Numpy, la brique de base de la data science",
    "section": "Exercices supplémentaires",
    "text": "Exercices supplémentaires\n\n\n Comprendre le principe de l'algorithme PageRank\nGoogle est devenu célèbre grâce à son algorithme PageRank. Celui-ci permet, à partir\nde liens entre sites web, de donner un score d’importance à un site web qui va\nêtre utilisé pour évaluer sa centralité dans un réseau.\nL’objectif de cet exercice est d’utiliser Numpy pour mettre en oeuvre un tel\nalgorithme à partir d’une matrice d’adjacence qui relie les sites entre eux.\n\nCréer la matrice suivante avec numpy. L’appeler M:\n\n\\[\n\\begin{bmatrix}\n0 & 0 & 0 & 0 & 1 \\\\\n0.5 & 0 & 0 & 0 & 0 \\\\\n0.5 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0.5 & 0 & 0 \\\\\n0 & 0 & 0.5 & 1 & 0\n\\end{bmatrix}\n\\]\n\nPour représenter visuellement ce web minimaliste,\nconvertir en objet networkx (une librairie spécialisée\ndans l’analyse de réseau) et utiliser la fonction draw\nde ce package.\n\nIl s’agit de la transposée de la matrice d’adjacence\nqui permet de relier les sites entre eux. Par exemple,\nle site 1 (première colonne) est référencé par\nles sites 2 et 3. Celui-ci ne référence que le site 5.\n\nA partir de la page wikipedia anglaise de PageRank, tester\nsur votre matrice.\n\n\n\n\n\n\n\n\n\n\n\n\nLe site 1 est assez central car il est référencé 2 fois. Le site\n5 est lui également central puisqu’il est référencé par le site 1.\n\n\narray([[0.25419178],\n       [0.13803151],\n       [0.13803151],\n       [0.20599017],\n       [0.26375504]])\n\n\nD’autres idées :\n\nSimulations de variables aléatoires ;\nTCL ;"
  },
  {
    "objectID": "content/getting-started/07_rappels_classes.html#quest-ce-que-la-programmation-orientée-objet",
    "href": "content/getting-started/07_rappels_classes.html#quest-ce-que-la-programmation-orientée-objet",
    "title": "Les classes en Python",
    "section": "Qu’est-ce que la programmation orientée objet ?",
    "text": "Qu’est-ce que la programmation orientée objet ?\nLe langage Python se base sur des objets et définit pour eux des actions.\nSelon le type d’objet, les actions seront différentes.\nOn parle à ce propos de langage orienté objet ce qui signifie\nque la syntaxe du langage Python permet de définir de manière conceptuelle\ndes objets et appliquer des traitements cohérents avec leur structure interne.\nPar exemple,\npour manipuler des données textuelles ou numériques, on aura\nbesoin d’appliquer des méthodes différentes. Prenons l’exemple\nde l’opération +. Pour des données numériques, il s’agit\nde l’addition. Pour des données textuelles, l’addition n’a pas de sens\nmais on peut envisager d’appliquer cette opération pour faire de la\nconcaténation.\nChaque type d’objet se verra donc appliquer des actions\nadaptées. Cela offre une grande flexibilité au langage Python car on\npeut définir une méthode générique (par exemple l’addition) et l’adapter\nà différents types d’objets.\nLe fait que Python soit un langage orienté objet a une influence sur la\nsyntaxe. On retrouvera régulière la syntaxe objet.method qui est au coeur\nde Python. Par exemple pd.DataFrame.mean se traduit par\nappliquer la méthode mean a un objet de type pd.DataFrame.\n\nQuand utilise-t-on cela dans le domaine de la data science ?\nLes réseaux de neurones programmés avec Keras ou PyTorch fonctionnent de\ncette manière. On part d’une structure de base et modifie les attributs (par\nexemple le nombre de couches) ou les méthodes."
  },
  {
    "objectID": "content/getting-started/07_rappels_classes.html#la-définition-dun-objet",
    "href": "content/getting-started/07_rappels_classes.html#la-définition-dun-objet",
    "title": "Les classes en Python",
    "section": "La définition d’un objet",
    "text": "La définition d’un objet\nPour définir un objet, il faut lui donner des caractéristiques et des actions, ce qu’il est, ce qu’il peut faire.\nAvec une liste, on peut ajouter des éléments par exemple avec l’action .append(). On peut créer autant d’objets “liste” qu’on le souhaite.\nUne classe regroupe des fonctions et des attributs qui définissent un objet.\nUn objet est une instance d’une classe, c’est-à-dire un exemplaire issu de la classe. L’objet avec un comportement et un état, tous deux définis par la classe. On peut créer autant d’objets que l’on désire avec une classe donnée.\nIci nous allons essayer de créer une classe chat, avec des attributs pour caractériser le chat et des actions, pour voir ce qu’il peut faire avec un objet de la classe chat."
  },
  {
    "objectID": "content/getting-started/07_rappels_classes.html#exemple-la-classe-chat",
    "href": "content/getting-started/07_rappels_classes.html#exemple-la-classe-chat",
    "title": "Les classes en Python",
    "section": "Exemple : la Classe chat()",
    "text": "Exemple : la Classe chat()\n\nLes attributs de la classe chat\n\nClasse chat version 1 - premiers attributs\nOn veut pouvoir créer un objet chat() qui nous permettra à terme de créer une colonie de chats (on sait\njamais ca peut servir …).\nPour commencer, on va définir un chat avec des attributs de base : une couleur et un nom.\n\nclass chat: # Définition de notre classe chat\n    \"\"\"Classe définissant un chat caractérisé par :\n    - son nom\n    - sa couleur \"\"\"\n    \n    def __init__(self): # Notre méthode constructeur - \n        # self c'est notre objet qu'on est en train de créer\n        \"\"\"Pour l'instant, on ne va définir que deux attributs - nom et couleur \"\"\"\n        self.couleur = \"Noir\"   \n        self.nom = \"Aucun nom\"\n\n\nmon_chat = chat()\n\nprint(type(mon_chat), mon_chat.couleur ,\",\", mon_chat.nom) \n\n&lt;class '__main__.chat'&gt; Noir , Aucun nom\n\n\nOn nous dit bien que Mon chat est défini à partir de la classe chat,\nc’est ce que nous apprend la fonction type.\nPour l’instant il n’a pas de nom\n\n\nClasse chat version 2 - autres attributs\nAvec un nom et une couleur, on ne va pas loin. On peut continuer à définir des attributs pour la classe chat\nde la même façon que précédemment.\n\nclass chat: # Définition de notre classe chat\n    \"\"\"Classe définissant un chat caractérisé par :\n    - sa couleur\n    - son âge\n    - son caractère\n    - son poids\n    - son maitre\n    - son nom \"\"\"\n\n    \n    def __init__(self): # Notre méthode constructeur - \n        #self c'est notre objet qu'on est en train de créer\n        self.couleur = \"Noir\"    \n        self.age = 10\n        self.caractere = \"Joueur\"\n        self.poids = 3\n        self.maitre = \"Jeanne\"\n        self.nom = \"Aucun nom\"\n\n\nhelp(chat) \n# si on veut savoir ce que fait la classe \"chat\" on appelle l'aide\n\nHelp on class chat in module __main__:\n\nclass chat(builtins.object)\n |  Classe définissant un chat caractérisé par :\n |  - sa couleur\n |  - son âge\n |  - son caractère\n |  - son poids\n |  - son maitre\n |  - son nom\n |  \n |  Methods defined here:\n |  \n |  __init__(self)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n\n\n\nmon_chat = chat()\nprint(\"L'âge du chat est\", mon_chat.age,\"ans\") \n# on avait défini l'attribut age de la classe chat comme étant égal à 10\n#, si on demande l'attribut age de notre Martin on obtient 10\n\nL'âge du chat est 10 ans\n\n\nPar défaut, les attributs de la classe Chat seront toujours les mêmes à chaque création de chat à partir\nde la classe Chat.\nMais une fois qu’une instance de classe est créée (ici mon chat est une instance de classe) on peut décider\nde changer la valeur de ses attributs.\n\n\nUn nouveau poids\n\nprint(mon_chat.poids)\n# si on veut changer le poids de mon chat, parce qu'il a un peu grossi après les fêtes\nmon_chat.poids = 3.5\nprint(mon_chat.poids) # maintenant le poids est 3.5\n\n3\n3.5\n\n\n\n\nUn nouveau nom\n\n# on veut aussi lui donner un nom \nmon_chat.nom = \"Martin\"\nmon_chat.nom\n\n'Martin'\n\n\n\n\nUne autre instance de la classe Chat\nOn peut aussi créer d’autres objets chat à partir de la classe chat :\n\n# on appelle la classe\nl_autre_chat = chat()\n# on change les attributs qui nous intéressent\nl_autre_chat.nom = \"Ginette\"\nl_autre_chat.maitre = \"Roger\"\n# les attributs inchangés donnent la même chose \n# que ceux définis par défaut pour la classe\nprint(l_autre_chat.couleur)\n\nNoir\n\n\n\n\n\nLes méthodes de la classe chat\nLes attributs sont des variables propres à notre objet, qui servent à le caractériser.\nLes méthodes sont plutôt des actions, comme nous l’avons vu dans la partie précédente, agissant sur l’objet.\nPar exemple, la méthode append de la classe list permet d’ajouter un élément dans l’objet list manipulé.\n\nClasse chat version 3 - première méthode\nOn peut définir une première méthode : nourrir\n\nclass chat: # Définition de notre classe chat\n    \"\"\"Classe définissant un chat caractérisé par :\n    - sa couleur\n    - son âge\n    - son caractère\n    - son poids\n    - son maitre\n    - son nom \n    \n    L'objet chat a une méthode : nourrir \"\"\"\n\n    \n    def __init__(self): # Notre méthode constructeur - \n        #self c'est notre objet qu'on est en train de créer\n        self.couleur = \"Noir\"    \n        self.age = 10\n        self.caractere = \"Joueur\"\n        self.poids = 3\n        self.maitre = \"Jeanne\"\n        self.nom = \"Aucun nom\"\n        \n        \"\"\"Par défaut, notre ventre est vide\"\"\"\n        self.ventre = \"\"\n        \n    def nourrir(self, nourriture):\n        \"\"\"Méthode permettant de donner à manger au chat.\n        Si le ventre n'est pas vide, on met une virgule avant de rajouter\n        la nourriture\"\"\"       \n        if self.ventre != \"\":\n            self.ventre += \",\"\n        self.ventre += nourriture\n\n\nmon_chat = chat()\nmon_chat.nom = \"Martin\"\nmon_chat.ventre # On n'a rien donné à Martin, son ventre est vide\n\n''\n\n\n\n# on appelle la méthode \"nourrir\" de la classe chat, \n# on lui donne un élément, ici des croquettes\nmon_chat.nourrir('Croquettes')\nprint(\"Le contenu du ventre de martin : \",mon_chat.ventre)\n\nLe contenu du ventre de martin :  Croquettes\n\n\n\nmon_chat.nourrir('Saumon')\nprint(\"Le contenu du ventre de martin : \",mon_chat.ventre)\n\nLe contenu du ventre de martin :  Croquettes,Saumon\n\n\n\n\nClasse chat version 4 - autre méthode\nAvec un chat, on peut imaginer plein de méthodes. Ici on va définir une action “nourrir” et une autre action\n“litiere”, qui consiste à vider l’estomac du chat.\n\nclass chat: # Définition de notre classe Personne\n    \"\"\"Classe définissant un chat caractérisé par :\n    - sa couleur\n    - son âge\n    - son caractère\n    - son poids\n    - son maitre\n    - son nom \n    \n    L'objet chat a deux méthodes : nourrir et litiere \"\"\"\n\n    \n    def __init__(self): # Notre méthode constructeur - \n        #self c'est notre objet qu'on est en train de créer\n        self.nom = \"\"\n        self.couleur = \"Roux\"    \n        self.age = 10\n        self.caractere = \"Joueur\"\n        self.poids = 3\n        self.maitre = \"Jeanne\"\n        \"\"\"Par défaut, notre ventre est vide\"\"\"\n        self.ventre = \"\"\n        \n    def nourrir(self, nourriture):\n        \"\"\"Méthode permettant de donner à manger au chat.\n        Si le ventre n'est pas vide, on met une virgule avant de rajouter\n        la nourriture\"\"\"       \n        if self.ventre != \"\":\n            self.ventre += \",\"\n        self.ventre += nourriture\n\n    def litiere(self) : \n        \"\"\" Méthode permettant au chat d'aller à sa litière : \n        en conséquence son ventre est vide \"\"\"       \n        self.ventre = \"\"\n        print(self.nom,\"a le ventre vide\")\n\n\n# on définit Martin le chat\nmon_chat = chat()\nmon_chat.nom = \"Martin\"\n# on le nourrit avec des croquettes\nmon_chat.nourrir('croquettes')\nprint(\"Le contenu du ventre de martin\", mon_chat.ventre)\n\n\n# Il va dans sa litiere\nmon_chat.litiere()\n\nLe contenu du ventre de martin croquettes\nMartin a le ventre vide\n\n\n\nhelp(mon_chat.nourrir)\nhelp(mon_chat.litiere)\n\nHelp on method nourrir in module __main__:\n\nnourrir(nourriture) method of __main__.chat instance\n    Méthode permettant de donner à manger au chat.\n    Si le ventre n'est pas vide, on met une virgule avant de rajouter\n    la nourriture\n\nHelp on method litiere in module __main__:\n\nlitiere() method of __main__.chat instance\n    Méthode permettant au chat d'aller à sa litière : \n    en conséquence son ventre est vide\n\n\n\n\n\nLes méthodes spéciales (facultatif)\nSi on reprend notre classe chat, il y a en réalité des méthodes spéciales que nous n’avons pas définies mais\nqui sont implicites.\nPython comprend seul ce que doivent faire ces méthodes. Il a une idée préconcue de ce qu’elles doivent\neffectuer comme opération. Si vous ne redéfinissez par une méthode spéciale pour qu’elle fasse ce que vous\nsouhaitez, ca peut donner des r\u0013esultats inattendus.\nElles servent à plusieurs choses :\n\nà initialiser l’objet instancié : __init__\nà modifier son affichage : __repr__\n\n\n\n# pour avoir la valeur de l'attribut \"nom\"\n\nprint(mon_chat.__getattribute__(\"nom\"))\n# on aurait aussi pu faire plus simple :\nprint(mon_chat.nom)\n\nMartin\nMartin\n\n\n# si l'attribut n'existe pas : on a une erreur\n# Python recherche l'attribut et, s'il ne le trouve pas dans l'objet et si une méthode __getattr__ est spécifiée, \n# il va l'appeler en lui passant en paramètre le nom de l'attribut recherché, sous la forme d'une chaîne de caractères.\n\nprint(mon_chat.origine)\n## Error in py_call_impl(callable, dots$args, dots$keywords): AttributeError: 'chat' object has no attribute 'origine'\n## \n## Detailed traceback: \n##   File \"&lt;string&gt;\", line 1, in &lt;module&gt;\nMais on peut modifier les méthodes spéciales de notre classe chat pour éviter d’avoir des erreurs d’attributs. On va aussi en profiter pour modifier la représentation de l’instance chat qui pour l’instant donne &lt;_main_.chat object at 0x0000000005AB4C50&gt;\n\n\nClasse chat version 5 - méthode spéciale\n\nclass chat: # Définition de notre classe Personne\n    \"\"\"Classe définissant un chat caractérisé par :\n    - sa couleur\n    - son âge\n    - son caractère\n    - son poids\n    - son maitre\n    - son nom \n    \n    L'objet chat a deux méthodes : nourrir et litiere \"\"\"\n\n    \n    def __init__(self): # Notre méthode constructeur - \n        #self c'est notre objet qu'on est en train de créer\n        self.nom = \"\"\n        self.couleur = \"Roux\"    \n        self.age = 10\n        self.caractere = \"Joueur\"\n        self.poids = 3\n        self.maitre = \"Jeanne\"\n        \"\"\"Par défaut, notre ventre est vide\"\"\"\n        self.ventre = \"\"\n        \n    def nourrir(self, nourriture):\n        \"\"\"Méthode permettant de donner à manger au chat.\n        Si le ventre n'est pas vide, on met une virgule avant de rajouter\n        la nourriture\"\"\"       \n        if self.ventre != \"\":\n            self.ventre += \",\"\n        self.ventre += nourriture\n\n    def litiere(self) : \n        \"\"\" Méthode permettant au chat d'aller à sa litière : \n        en conséquence son ventre est vide \"\"\"       \n        self.ventre = \"\"\n        print(self.nom,\"a le ventre vide\")\n    \n    def __getattribute__(self, key):\n            return print(key,\"n'est pas un attribut de la classe chat\")   \n            \n    def __repr__(self):\n            return \"Je suis une instance de la classe chat\"\n\n\n# j'ai gardé l'exemple chat défini selon la classe version 4\n# Martin, le chat\n# on a vu précédemment qu'il n'avait pas d'attribut origine\n# et que cela levait une erreur AttributeError\nprint(mon_chat.nom)\n\n\n# on va définir un nouveau chat avec la version 5\n# on appelle à nouveau un attribut qui n'existe pas \"origine\"\n# on a bien le message défini par la méthode spéciale _gettattribute\n\nmon_chat_nouvelle_version = chat()\nmon_chat_nouvelle_version.origine\n\n# Maintenant on a aussi une définition de l'objet plus clair\nprint(mon_chat)\nprint(mon_chat_nouvelle_version)\n\nMartin\norigine n'est pas un attribut de la classe chat\n&lt;__main__.chat object at 0x7f61fc41b7c0&gt;\nJe suis une instance de la classe chat\n\n\n\n\n\n\nConclusion sur les classes : ce qu’on retient\n\nLes méthodes se définissent comme des fonctions, sauf qu’elles se trouvent dans le corps de la classe.\nOn définit les attributs d’une instance dans le constructeur de sa classe, en suivant cette syntaxe : self.nom_attribut = valeur.\nfacultatif : Les méthodes d’instance prennent en premier paramètre “self”, l’instance de l’objet manipulé.\nfacultatif : On construit une instance de classe en appelant son constructeur, une méthode d’instance appelée init."
  },
  {
    "objectID": "content/getting-started/05_rappels_types.html",
    "href": "content/getting-started/05_rappels_types.html",
    "title": "Quelques rappels sur les principes de base de Python",
    "section": "",
    "text": "pandas et numpy sont\nessentiels pour manipuler les données.\nNéanmoins, il est nécessaire de ne pas faire l’impasse sur les fondements\ndu langage Python. Une bonne compréhension des éléments structurants le\nlangage entraîne une plus grande productivité et liberté.\nCe chapitre est inspiré du matériel qui était proposé\npar Xavier Dupré,\nle précédent professeur de ce cours.\nVoir aussi Essential Cheat Sheets for Machine Learning and Deep Learning Engineers."
  },
  {
    "objectID": "content/getting-started/05_rappels_types.html#les-quelques-règles-de-python",
    "href": "content/getting-started/05_rappels_types.html#les-quelques-règles-de-python",
    "title": "Quelques rappels sur les principes de base de Python",
    "section": "Les quelques règles de Python",
    "text": "Les quelques règles de Python\nPython est un peu susceptible et protocolaire, plus formaliste que ne l’est R.\nIl y a ainsi quelques règles à respecter :\nRègle 1: L’indentation est primordiale : un code mal indenté provoque une erreur.\nL’indentation indique à l’interpréteur où se trouvent les\nséparations entre des blocs d’instructions. Un peu comme des points dans un\ntexte.\nSi les lignes ne sont pas bien alignées, l’interpréteur ne sait plus à quel\nbloc associer la ligne. Par exemple, le corps d’une fonction doit être indenté\nd’un niveau ; les éléments dans une clause logique (if, else, etc.) également.\nRègle 2: On commence à compter à 0, comme dans beaucoup de langages\n(C++, java…). Python diffère dans ce domaine de R où on commence\nà compter à 1.\nLe premier élément d’une liste est ainsi, en Python, le 0-ème.\nRègle 3: Comme dans une langue naturelle, les marques de\nponctuation sont importantes :\n\nPour une liste : []\nPour un dictionnaire : {}\nPour un tuple : ()\nPour séparer des éléments : ,\nPour commenter un bout de code : #\nPour aller à la ligne dans un bloc d’instructions : \\\nLes majuscules et minuscules sont importantes\nPar contre l’usage des ' ou des \" est indifférent.\nIl faut juste avoir les mêmes début et fin.\nPour documenter une fonction ou une classe ““” mon texte de documentation “““"
  },
  {
    "objectID": "content/getting-started/05_rappels_types.html#les-outputs-de-python-lopération-le-print-et-le-return",
    "href": "content/getting-started/05_rappels_types.html#les-outputs-de-python-lopération-le-print-et-le-return",
    "title": "Quelques rappels sur les principes de base de Python",
    "section": "Les outputs de Python : l’opération, le print et le return",
    "text": "Les outputs de Python : l’opération, le print et le return\nQuand Python réalise des opérations, il faut lui préciser ce qu’il doit en faire :\n\nest-ce qu’il doit juste faire l’opération,\nafficher le résultat de l’opération,\ncréer un objet avec le résultat de l’opération ?\n\n\n\n Note\nDans l’environnement Jupyter Notebook, le dernier élement d’une cellule\nest automatiquement affiché (print), qu’on lui demande ou non de le faire.\nCe comportement est particulièrement pratique pour afficher des figures\ngénérées via matplotlib ou seaborn.\nCe comportement\nn’est pas le cas dans un éditeur classique comme VisualStudio,\nSpyder ou PyCharm. Pour afficher un résultat dans la console,\nil faut utiliser\nprint ou la commande consacrée (par exemple plt.show()\npour afficher la dernière figure générée par matplotlib)\n\n\n\nLe print\n\n# on calcule : dans le cas d'une opération par exemple une somme\n2+3 # Python calcule le résultat mais n'affiche rien dans la sortie\n\n# le print : on affiche\n\nprint(2+3) # Python calcule et on lui demande juste de l'afficher\n# le résultat est en dessous du code\n\n5\n\n\n\n# le print dans une fonction \n\ndef addition_v1(a,b) : \n    print(a+b)\n\nresultat_print = addition_v1(2,0) \nprint(type(resultat_print))\n\n# dans la sortie on a l'affichage du résultat, car la sortie de la fonction est un print \n# en plus on lui demande quel est le type du résultat. Un print ne renvoie aucun type, ce n'est ni un numérique,\n# ni une chaine de charactères, le résultat d'un print n'est pas un format utilisable\n\n2\n&lt;class 'NoneType'&gt;\n\n\nLe résultat de l’addition est affiché\ncar la fonction addition_v1 effectue un print\nPar contre, l’objet créé n’a pas de type, il n’est pas un chiffre,\nce n’est qu’un affichage.\n\n\nLe return\nPour créer un objet avec le résultat de la fonction, il faut utiliser return\n\n# le return dans une fonction\ndef addition_v2(a,b) : \n    return a+b\n\nresultat_return = addition_v2(2,5) # \nprint(type(resultat_return))\n## là on a bien un résultat qui est du type \"entier\"\n\n&lt;class 'int'&gt;\n\n\nLe résultat de addition_v2 n’est pas affiché comme dans addition_v1\nPar contre, la fonction addition_v2 permet d’avoir un objet de type int,\nun entier donc."
  },
  {
    "objectID": "content/getting-started/05_rappels_types.html#les-types-de-base-variables-listes-dictionnaires",
    "href": "content/getting-started/05_rappels_types.html#les-types-de-base-variables-listes-dictionnaires",
    "title": "Quelques rappels sur les principes de base de Python",
    "section": "Les types de base : variables, listes, dictionnaires…",
    "text": "Les types de base : variables, listes, dictionnaires…\nPython permet de manipuler différents types de base. Nous en\nverrons des extensions dans la suite du cours (np.array par exemple)\nqui, d’une manière ou d’une autre, s’appuient sur ces types de base.\nOn distingue deux types de variables : les immuables (immutables)\nqui ne peuvent être\nmodifiés et les modifiables (mutables)\n\nLes variables immuables\nLes variables immuables ne peuvent être modifiées\n\nNone : ce type est une convention de programmation pour dire que la valeur n’est pas calculée\nbool : un booléen\nint : un entier\nfloat : un réel\nstr : une chaine de caractères\ntuple : un vecteur\n\n\ni = 3         # entier = type numérique (type int)\nr = 3.3       # réel   = type numérique (type float)\ns = \"exemple\" # chaîne de caractères = type str \nn = None      # None signifie que la variable existe mais qu'elle ne contient rien\n              # elle est souvent utilisée pour signifier qu'il n'y a pas de résultat\na = (1,2)     # tuple\n\nprint(i,r,s,n,a)         \n\n3 3.3 exemple None (1, 2)\n\n\nSi on essaie de changer le premier élément de la chaine de caractères s on va avoir un peu de mal.\nPar exemple si on voulait mettre une majuscule à “exemple”,\non aurait envie d’écrire que le premier élément de la chaine s est “E” majuscule\nMais Python ne va pas nous laisser faire, il nous dit que les objets “chaine de caractère” ne peuvent être modifiés\n\ns[0] = \"E\"  # déclenche une exception\n\nTypeError: 'str' object does not support item assignment\n\n\nTout ce qu’on peut faire avec une variable immuable,\nc’est la réaffecter à une autre valeur : elle ne peut pas être modifiée.\nPour s’en convaincre, utilisons la fonction id() qui donne un identifiant à chaque objet.\n\nprint(s)\nid(s)\n\nexemple\n\n\n140566062835376\n\n\n\ns = \"autre_mot\"\nid(s)\n\n140566028547184\n\n\nOn voit bien que s a changé d’identifiant : il peut avoir le même nom, ce n’est plus le même objet\n\n\nLes types modifiable : listes et dictionnaires\nHeureusement, il existe des variables modifiables comme les listes et les dictionnaires.\n\nLes listes - elles s’écrivent entre [ ]\nLes listes sont des élements très utiles, notamment quand vous souhaitez faire des boucles.\nPour faire appel aux élements d’une liste, on donne leur position dans la liste : le 1er est le 0, le 2ème est le 1 …\n\nma_liste = [1,2,3,4]\n\n\nprint(\"La longueur de ma liste est de\", len(ma_liste))\nprint(\"Le premier élément de ma liste est :\", ma_liste[0])\nprint(\"Le dernier élément de ma liste est :\", ma_liste[3])\nprint(\"Le dernier élément de ma liste est :\", ma_liste[-1])\n\nLa longueur de ma liste est de 4\nLe premier élément de ma liste est : 1\nLe dernier élément de ma liste est : 4\nLe dernier élément de ma liste est : 4\n\n\nPour effectuer des boucles sur les listes, la méthode la plus lisible\nest d’utiliser les list comprehension. Cette approche consiste\nà itérer les éléments d’une liste à la volée.\nPar exemple, si on reprend cet exemple,\nun code qui repose sur les list comprehension sera le suivant :\n\nfruits = [\"apple\", \"banana\", \"cherry\", \"kiwi\", \"mango\"]\nnewlist = [x for x in fruits if \"a\" in x]\nprint(newlist)\n\n['apple', 'banana', 'mango']\n\n\nLe même code, ne reposant pas sur les compréhensions de liste, sera beaucoup\nmoins concis et ainsi inutilement verbeux:\n\nfruits = [\"apple\", \"banana\", \"cherry\", \"kiwi\", \"mango\"]\nnewlist = []\n\nfor x in fruits:\n  if \"a\" in x:\n    newlist.append(x)\n\nprint(newlist) \n\n['apple', 'banana', 'mango']\n\n\n\n\nLes dictionnaires - ils s’écrivent entre accolades {}\nUn dictionnaire associe à une clé un autre élément, appelé une valeur : un chiffre, un nom, une liste, un autre dictionnaire etc.\nLe format d’un dictionnaire est le suivant : {Clé : valeur}. Il s’agit\nd’un objet très pratique pour la recherche, beaucoup plus que les listes\nqui ne permettent pas de stocker de l’information diverse de manière\nhiérarchisée.\n\n\nDictionnaire avec des valeurs int\nOn peut par exemple associer à un nom, un nombre\n\nmon_dictionnaire_notes = { 'Nicolas' : 18 , 'Pimprenelle' : 15} \n# un dictionnaire qui à chaque nom associe un nombre\n# à Nicolas, on associe 18\n\nprint(mon_dictionnaire_notes) \n\n{'Nicolas': 18, 'Pimprenelle': 15}\n\n\n\n\nDictionnaire avec des valeurs qui sont des listes\nPour chaque clé d’un dictionnaire, il ne faut pas forcément garder la même forme de valeur\nDans l’exemple, la valeur de la clé “Nicolas” est une liste, alors que celle de “Philou” est une liste de liste\n\nmon_dictionnaire_loisirs =  \\\n{ 'Nicolas' : ['Rugby','Pastis','Belote'] , \n  'Pimprenelle' : ['Gin Rami','Tisane','Tara Jarmon','Barcelone','Mickey Mouse'],\n  'Philou' : [['Maths','Jeux'],['Guillaume','Jeanne','Thimothée','Adrien']]}\n\nPour accéder à un élément du dictionnaire, on fait appel à la clé et non plus à la position, comme c’était le cas dans les listes.\nC’est beaucoup plus pratique pour rechercher de l’information:\n\nprint(mon_dictionnaire_loisirs['Nicolas']) # on affiche une liste\n\n['Rugby', 'Pastis', 'Belote']\n\n\n\nprint(mon_dictionnaire_loisirs['Philou']) # on affiche une liste de listes\n\n[['Maths', 'Jeux'], ['Guillaume', 'Jeanne', 'Thimothée', 'Adrien']]\n\n\nSi on ne veut avoir que la première liste des loisirs de Philou, on demande le premier élément de la liste\n\nprint(mon_dictionnaire_loisirs['Philou'][0]) # on affiche alors juste la première liste\n\n['Maths', 'Jeux']\n\n\nOn peut aussi avoir des valeurs qui sont des int et des list\n\nmon_dictionnaire_patchwork_good = \\\n{ 'Nicolas' : ['Rugby','Pastis','Belote'] ,\n  'Pimprenelle' : 18 }\n\n\n\n\nA retenir\n\nL’indentation du code est importante (4 espaces et pas une tabulation)\nUne liste est entre [] et on peut appeler les positions par leur place\nUn dictionnaire, clé x valeur, s’écrit entre {} et on appelle un élément en fonction de la clé"
  },
  {
    "objectID": "content/getting-started/05_rappels_types.html#questions-pratiques",
    "href": "content/getting-started/05_rappels_types.html#questions-pratiques",
    "title": "Quelques rappels sur les principes de base de Python",
    "section": "Questions pratiques :",
    "text": "Questions pratiques :\n\n\n\n Exercice\n\nQuelle est la position de 7 dans la liste suivante\n\n\nliste_nombres = [1,2,7,5,3]\n\n\nCombien de clés a ce dictionnaire ?\n\n\ndictionnaire_evangile = {\"Marc\" : \"Lion\", \"Matthieu\" : [\"Ange\",\"Homme ailé\"] , \n                          \"Jean\" : \"Aigle\" , \"Luc\" : \"Taureau\"}\n\n\nQue faut-il écrire pour obtenir “Ange” en résultat à partir du dictionnaire_evangile ?"
  },
  {
    "objectID": "content/getting-started/05_rappels_types.html#effectuer-des-opérations-sur-les-objets-de-base-python",
    "href": "content/getting-started/05_rappels_types.html#effectuer-des-opérations-sur-les-objets-de-base-python",
    "title": "Quelques rappels sur les principes de base de Python",
    "section": "Effectuer des opérations sur les objets de base Python",
    "text": "Effectuer des opérations sur les objets de base Python\nMaintenant qu’on a vu quels objets existent en Python,\nnous allons\nvoir comment nous en servir.\nPour comprendre comment modifier un objet, il convient\nde distinguer deux concepts, développés plus amplement\ndans le chapitre dédié: les attributs et les méthodes :\n\nLes attributs décrivent la structure interne d’un objet. Par exemple,\nla taille d’un objet, sa langue, etc.\nNous n’allons pas trop développer ce concept ici. Le chapitre dédié au sujet\npermettra de plus développer ce concept.\nLes méthodes correspondent à des actions qui s’appliqueront à l’objet et s’adaptent à sa structure.\nLa même méthode (par exemple append) fonctionnera ainsi de manière différente selon le type d’objet.\n\n\nPremiers exemples de méthodes\nAvec les éléments définis dans la partie 1\n(les listes, les dictionnaires) on peut faire appel à des méthodes qui sont directement liées à ces objets.\n\nUne méthode pour les listes\nPour ajouter un élément (item) dans une liste : on va utiliser la méthode .append()\n\nma_liste = [\"Nicolas\",\"Michel\",\"Bernard\"]\n\nma_liste.append(\"Philippe\")\n\nprint(ma_liste)\n\n['Nicolas', 'Michel', 'Bernard', 'Philippe']\n\n\n\n\nUne méthode pour les dictionnaires\nPour connaitre l’ensemble des clés d’un dictionnaire, on appelle la méthode .keys()\n\nmon_dictionnaire = {\"Marc\" : \"Lion\", \"Matthieu\" : [\"Ange\",\"Homme ailé\"] , \n                          \"Jean\" : \"Aigle\" , \"Luc\" : \"Taureau\"}\n\nprint(mon_dictionnaire.keys())\n\ndict_keys(['Marc', 'Matthieu', 'Jean', 'Luc'])\n\n\n\n\n\nConnaitre les méthodes d’un objet\nPour savoir quelles sont les méthodes d’un objet vous pouvez :\n\ntaper help(mon_objet) ou mon_objet? dans la console Python\ntaper mon_objet. + touche tabulation dans la console Python ou dans le Notebook.\nPython permet la complétion, c’est-à-dire que vous pouvez faire appaître la liste\ndes méthodes possibles."
  },
  {
    "objectID": "content/getting-started/05_rappels_types.html#les-opérations-et-méthodes-classiques-des-listes",
    "href": "content/getting-started/05_rappels_types.html#les-opérations-et-méthodes-classiques-des-listes",
    "title": "Quelques rappels sur les principes de base de Python",
    "section": "Les opérations et méthodes classiques des listes",
    "text": "Les opérations et méthodes classiques des listes\n\nCréer une liste\nPour créer un objet de la classe list, il suffit de le déclarer. Ici on affecte à x une liste\n\nx = [4, 5] # création d’une liste composée de deux entiers\nx = [\"un\", 1, \"deux\", 2] # création d’une liste composée de 2 chaînes de caractères\n# et de deux entiers, l’ordre d’écriture est important\nx = [3] # création d’une liste d’un élément, sans la virgule,\nx = [ ] # crée une liste vide\nx = list () # crée une liste vide\n\n\n\nUn premier test sur les listes\nSi on veut tester la présence d’un élément dans une liste, on l’écrit de la manière suivante :\n\n# Exemple \n\nx = \"Marcel\"\n\nl = [\"Marcel\",\"Edith\",\"Maurice\",\"Jean\"]\n\nprint(x in l)\n\n#vrai si x est un des éléments de l\n\nTrue\n\n\n\n\n+: une méthode pour concaténer deux listes\nOn utilise le symbole +\n\nt = [\"Antoine\",\"David\"]\nprint(l + t) #concaténation de l et t\n\n['Marcel', 'Edith', 'Maurice', 'Jean', 'Antoine', 'David']\n\n\n\n\nPour trouver certains éléments d’une liste\nPour chercher des élements dans une liste, on utilise la position dans la liste.\n\nl[1] # donne l'élément qui est en 2ème position de la liste\n\n'Edith'\n\n\n\nl[1:3] # donne les éléments de la 2ème position de la liste à la 4ème exclue\n\n['Edith', 'Maurice']\n\n\n\n\nQuelques fonctions des listes\nLes listes embarquent ainsi nativement un certain nombre de méthodes\nqui sont pratiques. Cependant, pour avoir certaines informations\nsur une liste, il faut parfois plutôt passer par\ndes fonctions natives comme les suivantes :\n\nlongueur = len(l) # nombre d’éléments de l\nminimum = min(l) # plus petit élément de l, ici par ordre alphabétique\nmaximum = max(l) # plus grand élément de l, ici par ordre alphabétique\nprint(longueur,minimum,maximum)\n\n4 Edith Maurice\n\n\n\ndel l[0 : 2] # supprime les éléments entre la position 0 et 2 exclue\nprint(l)\n\n['Maurice', 'Jean']\n\n\n\n\nLes méthodes des listes\nOn les trouve dans l’aide de la liste.\nOn distingue les méthodes et les méthodes spéciales : visuellement,\nles méthodes spéciales sont celles qui précédées et suivis de deux caractères de soulignement,\nles autres sont des méthodes classiques.\n\nhelp(l)\n\nHelp on list object:\n\nclass list(object)\n |  list(iterable=(), /)\n |  \n |  Built-in mutable sequence.\n |  \n |  If no argument is given, the constructor creates a new empty list.\n |  The argument must be an iterable if specified.\n |  \n |  Methods defined here:\n |  \n |  __add__(self, value, /)\n |      Return self+value.\n |  \n |  __contains__(self, key, /)\n |      Return key in self.\n |  \n |  __delitem__(self, key, /)\n |      Delete self[key].\n |  \n |  __eq__(self, value, /)\n |      Return self==value.\n |  \n |  __ge__(self, value, /)\n |      Return self&gt;=value.\n |  \n |  __getattribute__(self, name, /)\n |      Return getattr(self, name).\n |  \n |  __getitem__(...)\n |      x.__getitem__(y) &lt;==&gt; x[y]\n |  \n |  __gt__(self, value, /)\n |      Return self&gt;value.\n |  \n |  __iadd__(self, value, /)\n |      Implement self+=value.\n |  \n |  __imul__(self, value, /)\n |      Implement self*=value.\n |  \n |  __init__(self, /, *args, **kwargs)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __iter__(self, /)\n |      Implement iter(self).\n |  \n |  __le__(self, value, /)\n |      Return self&lt;=value.\n |  \n |  __len__(self, /)\n |      Return len(self).\n |  \n |  __lt__(self, value, /)\n |      Return self&lt;value.\n |  \n |  __mul__(self, value, /)\n |      Return self*value.\n |  \n |  __ne__(self, value, /)\n |      Return self!=value.\n |  \n |  __repr__(self, /)\n |      Return repr(self).\n |  \n |  __reversed__(self, /)\n |      Return a reverse iterator over the list.\n |  \n |  __rmul__(self, value, /)\n |      Return value*self.\n |  \n |  __setitem__(self, key, value, /)\n |      Set self[key] to value.\n |  \n |  __sizeof__(self, /)\n |      Return the size of the list in memory, in bytes.\n |  \n |  append(self, object, /)\n |      Append object to the end of the list.\n |  \n |  clear(self, /)\n |      Remove all items from list.\n |  \n |  copy(self, /)\n |      Return a shallow copy of the list.\n |  \n |  count(self, value, /)\n |      Return number of occurrences of value.\n |  \n |  extend(self, iterable, /)\n |      Extend list by appending elements from the iterable.\n |  \n |  index(self, value, start=0, stop=9223372036854775807, /)\n |      Return first index of value.\n |      \n |      Raises ValueError if the value is not present.\n |  \n |  insert(self, index, object, /)\n |      Insert object before index.\n |  \n |  pop(self, index=-1, /)\n |      Remove and return item at index (default last).\n |      \n |      Raises IndexError if list is empty or index is out of range.\n |  \n |  remove(self, value, /)\n |      Remove first occurrence of value.\n |      \n |      Raises ValueError if the value is not present.\n |  \n |  reverse(self, /)\n |      Reverse *IN PLACE*.\n |  \n |  sort(self, /, *, key=None, reverse=False)\n |      Sort the list in ascending order and return None.\n |      \n |      The sort is in-place (i.e. the list itself is modified) and stable (i.e. the\n |      order of two equal elements is maintained).\n |      \n |      If a key function is given, apply it once to each list item and sort them,\n |      ascending or descending, according to their function values.\n |      \n |      The reverse flag can be set to sort in descending order.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods defined here:\n |  \n |  __class_getitem__(...) from builtins.type\n |      See PEP 585\n |  \n |  ----------------------------------------------------------------------\n |  Static methods defined here:\n |  \n |  __new__(*args, **kwargs) from builtins.type\n |      Create and return a new object.  See help(type) for accurate signature.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __hash__ = None\n\n\n\n\n\nA retenir et questions\nA retenir :\n\nChaque objet Python a des attributs et des méthodes\nVous pouvez créer des classes avec des attributs et des méthodes\nLes méthodes des listes et des dictionnaires qui sont les plus utilisées :\n\nlist.count()\nlist.sort()\nlist.append()\ndict.keys()\ndict.items()\ndict.values()\n\n\n\n\n Exercice 2\n\nDéfinir la liste allant de 1 à 10, puis effectuez les actions suivantes :\n\n\ntriez et affichez la liste\najoutez l’élément 11 à la liste et affichez la liste\nrenversez et affichez la liste\naffichez l’élément d’indice 7\nenlevez l’élément 9 et affichez la liste\naffichez la sous-liste du 2e au 3e éléments inclus ;\naffichez la sous-liste du début au 2e élément inclus ;\naffichez la sous-liste du 3e élément à la fin de la liste ;\n\n\nConstruire le dictionnaire des 6 premiers mois de l’année avec comme valeurs le nombre de jours respectif.\n\n\nRenvoyer la liste des mois\nRenvoyer la liste des jours\nAjoutez la clé du mois de Juillet"
  },
  {
    "objectID": "content/getting-started/05_rappels_types.html#passer-des-listes-dictionnaires-à-pandas",
    "href": "content/getting-started/05_rappels_types.html#passer-des-listes-dictionnaires-à-pandas",
    "title": "Quelques rappels sur les principes de base de Python",
    "section": "Passer des listes, dictionnaires à pandas",
    "text": "Passer des listes, dictionnaires à pandas\nSupposons que la variable ‘data’ est une liste qui contient nos données.\nUne observation correspond à un dictionnaire qui contient le nom, le type, l’ambiance et la note d’un restaurant.\nIl est aisé de transformer cette liste en dataframe grâce à la fonction ‘DataFrame’.\n\nimport pandas \n\ndata = [{\"nom\": \"Little Pub\", \"type\" : \"Bar\", \"ambiance\": 9, \"note\": 7},\n     {\"nom\": \"Le Corse\", \"type\" : \"Sandwicherie\", \"ambiance\": 2, \"note\": 8},\n     {\"nom\": \"Café Caumartin\", \"type\" : \"Bar\", \"ambiance\": 1}]\n\ndf = pandas.DataFrame(data)\n\nprint(data)\ndf\n\n[{'nom': 'Little Pub', 'type': 'Bar', 'ambiance': 9, 'note': 7}, {'nom': 'Le Corse', 'type': 'Sandwicherie', 'ambiance': 2, 'note': 8}, {'nom': 'Café Caumartin', 'type': 'Bar', 'ambiance': 1}]\n\n\n\n\n\n\n\n\n\nnom\ntype\nambiance\nnote\n\n\n\n\n0\nLittle Pub\nBar\n9\n7.0\n\n\n1\nLe Corse\nSandwicherie\n2\n8.0\n\n\n2\nCafé Caumartin\nBar\n1\nNaN"
  },
  {
    "objectID": "content/getting-started/03_data_analysis.html",
    "href": "content/getting-started/03_data_analysis.html",
    "title": "Comment aborder un jeu de données",
    "section": "",
    "text": "Pour bien débuter des travaux sur une base de données,\nil est nécessaire de se poser quelques questions de bon sens\net de suivre une démarche scientifique dont un certain\nnombre de gestes sont assez simple.\nDans un projet sur des jeux de données, on peut schématiquement\nséparer les étapes en quatre grandes parties :\n\nLa récupération et structuration des données ;\nL’analyse de celle-ci, notamment la production de statistiques descriptives indispensables pour orienter les exploitations ultérieures ;\nLa modélisation ;\nLa valorisation finale des étapes précédentes et la communication de résultats ou la mise en oeuvre d’une chaine de production.\n\nCe cours explore ces différentes étapes de manière progressive grâce à\nl’écosystème Python qui est très complet. Chaque chapitre du cours\npeut être vu comme une manière de progresser dans ce fil conducteur.\nDans ce chapitre, nous allons plutôt mettre en avant quelques réflexions\nà avoir avant de se lancer dans chaque étape.\n\n\n\n\nLa phase de constitution de son jeu de données sous-tend tout le projet qui suit.\nLa première question à se poser est\n“de quelles données ai-je besoin pour répondre à ma problématique ?”.\nCette problématique pourra éventuellement\nêtre affinée en fonction des besoins mais les travaux sont généralement\nde meilleure qualité lorsque la problématique amène à la réflexion sur les données\ndisponibles plutôt que l’inverse.\nEnsuite, “qui produit et met à disposition ces données” ?\nLes sources disponibles sur internet sont-elles fiables ?\nPar exemple, les sites d’open data gouvernementaux sont par exemple assez fiables mais autorisent parfois l’archivage de données restructurées par des tiers et non des producteurs officiels. A l’inverse, sur Kaggle ou sur Github la source de certains jeux de données n’est pas tracée ce qui rend compliquée la confiance sur la qualité de la donnée\nUne fois identifié une ou plusieurs sources de données,\nest-ce que je peux les compléter avec d’autres données ?\n(dans ce cas, faire attention à avoir des niveaux de granularité adéquats).\n\n\n\nVient ensuite la phase de mise en forme et nettoyage des jeux de données récupérés.\nCette étape est primordiale et est généralement celle qui mobilise le plus\nde temps. Pendant quelques années, on parlait de data cleaning. Cependant,\ncela a pu, implicitement, laisser penser qu’il s’agissait d’une tâche\nsubalterne. On commence à lui préférer le concept de feature engineering\nqui souligne bien qu’il s’agit d’une compétence qui nécessite beaucoup\nde compétences.\nUn jeu de données propre est un jeu de données dont la structure est\nadéquate et n’entraînera pas d’erreur, visible ou non,\nlors de la phase d’analyse. Voici quelques éléments structurants\nd’un jeu de données propre :\n\nles informations manquantes sont bien comprises et traitées. numpy et\npandas proposent un certain formalisme sur le sujet qu’il est utile\nd’adopter en remplaçant par NaN les observations manquantes. Cela\nimplique de faire attention à la manière dont certains producteurs\ncodent les valeurs manquantes : certains ont la facheuse tendance à\nêtre imaginatifs sur les codes pour valeurs manquantes : “-999”, “XXX”, “NA”\nles variables servant d’identifiants sont bien les mêmes d’une table à l’autre (notamment dans le cas de jointure) : même format, même modalités\npour des variables textuelles, qui peuvent etre mal saisies, avoir corrigé les éventuelles fautes (ex “Rolland Garros” -&gt; “Roland Garros”)\ncréer des variables qui synthétisent l’information dont vous avez besoin\nsupprimer les éléments inutiles (colonne ou ligne vide)\nrenommer les colonnes avec des noms compréhensibles\n\n\n\n\n\nUne fois les jeux de données nettoyés, vous pouvez plus sereinement\nétudier l’information présente dans les données.\nCette phase et celle du nettoyage ne sont pas séquentielles,\nen réalité vous devrez régulièrement passer de votre nettoyage à quelques statistiques descriptives qui vous montreront un problème, retourner au nettoyage etc.\nLes questions à se poser pour “challenger” le jeu de données :\n\nEst-ce que mon échantillon est bien représentatif de ce qui m’intéresse ? N’avoir que 2000 communes sur les 35000 n’est pas nécessairement un problème mais il est bon de s’être posé la question.\nEst-ce que les ordres de grandeur sont bons ? Pour cela, confronter vos premieres stats desc à vos recherches internet. Par exemple trouver que les maisons vendues en France en 2020 font en moyenne 400 m² n’est pas un ordre de grandeur réaliste.\nEst-ce que je comprends toutes les variables de mon jeu de données ? Est-ce qu’elles se “comportent” de la bonne façon ? A ce stade, il est parfois utile de se faire un dictionnaire de variables (qui explique comment elles sont construites ou calculées). On peut également mener des études de corrélation entre nos variables.\nEst-ce que j’ai des outliers, i.e. des valeurs aberrantes pour certains individus ? Dans ce cas, il faut décider quel traitement on leur apporte (les supprimer, appliquer une transformation logarithmique, les laisser tel quel) et surtout bien le justifier.\nEst-ce que j’ai des premiers grands messages sortis de mon jeu de données ? Est-ce que j’ai des résultats surprenants ? Si oui, les ai-je creusé suffisamment pour voir si les résultats tiennent toujours ou si c’est à cause d’un souci dans la construction du jeu de données (mal nettoyées, mauvaise variable…)\n\n\n\n\nA cette étape, l’analyse descriptive doit voir avoir donné quelques premières pistes pour savoir dans quelle direction vous voulez mener votre modèle.\nUne erreur de débutant est de se lancer directement dans la modélisation parce\nqu’il s’agirait d’une compétence plus poussée. Cela amène généralement\nà des analyses de pauvre qualité : la modélisation tend généralement à confirmer\nles intuitions issues de l’analyse descriptive. Sans cette dernière,\nl’interprétation des résultats d’un modèle peu s’avérer inutilement complexe.\nVous devrez plonger dans vos autres cours (Econométrie 1, Series Temporelles, Sondages, Analyse des données etc.) pour trouver le modèle le plus adapté à votre question.\nLa méthode sera guidée par l’objectif.\n\nEst-ce que vous voulez expliquer ou prédire ? https://hal-cnam.archives-ouvertes.fr/hal-02507348/document\nEst-ce que vous voulez classer un élément dans une catégorie (classification ou clustering) ou prédire une valeur numérique (régression) ?\n\nEn fonction des modèles que vous aurez déjà vu en cours et des questions que vous souhaiterez résoudre sur votre jeu de données, le choix du modèle sera souvent assez direct.\nVous pouvez également vous référez à la démarche proposée par Xavier Dupré\nhttp://www.xavierdupre.fr/app/ensae_teaching_cs/helpsphinx3/debutermlprojet.html#l-debutermlprojet\nPour aller plus loin (mais de manière simplifiée) sur les algorithmes de Machine Learning :\nhttps://datakeen.co/8-machine-learning-algorithms-explained-in-human-language/\n\n\nLa mise à disposition de code sur Github ou Gitlab est une incitation\ntrès forte pour produire du code de qualité. Il est ainsi recommandé de\nsystématiquement utiliser ces plateformes pour la mise à disposition de\ncode. Cependant, il ne s’agit que d’une petite partie des gains à\nl’utiliser.\nLe cours que je donne avec Romain Avouac en troisième année d’ENSAE\n(ensae-reproductibilite.github.io/website/) évoque\nl’un des principaux gains à utiliser ces plateformes, à savoir\nla possibilité de mettre à disposition automatiquement différents livrables\npour valoriser son travail auprès de différents publics.\nSelon le public visé, la communication ne sera pas identique. Le code peut\nintéresser les personnes désirant avoir des détails sur la méthodologie mise\nen oeuvre en pratique mais il peut s’agir d’un format rebutant pour d’autres\npublics. Une visualisation de données dynamiques parlera à des publics\nmoins experts de la donnée mais est plus dure à mettre en oeuvre\nqu’un graphique standard.\n\n\n\nLes Notebooks Jupyter ont eu beaucoup de succès dans le monde de\nla data science pour valoriser des travaux. Pourtant il ne s’agit\npas forcément toujours du meilleur format. En effet, beaucoup\nde notebooks tentent à empiler des pavés de code et du texte, ce\nqui les rend difficilement lisibles.\nSur un projet conséquent, il vaut mieux reporter le plus de code\npossible dans des scripts bien structurés et avoir un notebook\nqui appelle ces scripts pour produire des outputs. Ou alors ne\npas utiliser un notebook et privilégier un autre format (un\ntableau de bord, un site web, une appli réactive…).\nDans le cours de dernière année de\nl’ENSAE, Mise en production de projets data science, Romain\nAvouac et moi revenons sur les moyens de communication et de partage de code alternatifs au notebook.\n\n\n\n\n\n\n\n\nLes données sont une représentation synthétique de la réalité et les\nconclusions de certaines analyses peuvent avoir un vrai impact sur\nla vie des citoyens. Les chiffres erronés de\nReinhart et Rogoff ont ainsi pu servir de justification théorique à des\npolitiques d’austérité qui ont pu avoir des conséquences violentes\npour certains citoyens de\npays en crise1. En Grande-Bretagne, le recensement des personnes\ncontaminées par le Covid en 2020, et donc de leurs proches pour le\nsuivi de l’épidémie,\na été incomplet à cause de\ntroncatures dues à l’utilisation d’un format non adapté de stockage\ndes données (tableur Excel)2.\nDernier exemple avec le credit scoring mis en oeuvre aux Etats-Unis.\nLa citation ci-dessous, issue de l’article de Hurley and Adebayo (2016),\nillustre très bien les conséquences et les aspects problématiques\nd’un système de construction automatisée d’un score de crédit :\n\nConsumers have limited ability to identify and contest unfair credit\ndecisions, and little chance to understand what steps they\nshould take to improve their credit. Recent studies have also\nquestioned the accuracy of the data used by these tools, in some\ncases identifying serious flaws that have a substantial bearing\non lending decisions. Big-data tools may also risk creating a\nsystem of “creditworthinessby association” in which consumers’\nfamilial, religious, social, and other affiliations determine their\neligibility for an affordable loan.\nHurley and Adebayo (2016)\n\n\n\n\nLa transparence sur les intérêts et limites d’une méthode mise en oeuvre\nest donc importante.\nCette exigence de la recherche, parfois oubliée à cause de la course\naux résultats novateurs, mérite également d’être appliquée\nen entreprise ou administration.\nMême sans intention manifeste de la part de la personne qui analyse des données,\nune mauvaise interprétation est toujours possible. Tout en valorisant un\nrésultat, il est possible d’alerter sur certaines limites. Il est important,\ndans ses recherches comme dans les discussions avec d’autres interlocuteurs,\nde faire attention au biais de confirmation qui consiste\nà ne retenir que l’information qui correspond à nos conceptions a priori et\nà ne pas considérer celles qui pourraient aller à l’encontre de celles-ci:\n\n\n\n\n\nCertaines représentations de données sont à exclure car des biais cognitifs\npeuvent amener à des interprétations erronées3. Dans le domaine de la\nvisualisation de données, les camemberts (pie chart) ou les diagrammes\nradar sont par exemple\nà exclure car l’oeil humain perçoit mal ces formes circulaires. Pour une raison\nsimilaire, les cartes avec aplat de couleur (cartes\nchoroplèthes) sont trompeuses.\nLes posts de blog pour datawrapper\nde Lisa Charlotte Muth ou ceux d’Eric Mauvière sont d’excellentes ressources\npour apprendre les bonnes et mauvaises pratiques de\nvisualisation (voir la partie visualisation de ce cours\npour plus de détails).\n\n\n\nLe cadre réglementaire de protection des données a évolué ces dernières\nannées avec le RGPD. Cette réglementation a permis de mieux faire\nsaisir le fait que la collecte de données se justifie au nom\nde finalités plus ou moins bien identifiées. Prendre conscience que\nla confidentialité des données se justifie pour éviter la dissémination\nnon contrôlée d’informations sur une personne est important.\nDes données particulièrement sensibles, notamment les données de santé,\npeuvent être plus contraignantes à traiter que des données peu sensibles.\nEn Europe, par exemple, les agents du service statistique public\n(Insee ou services statistiques ministériels) sont tenus au secret professionnel\n(article L121-6 du Code général de la fonction publique),\nqui leur interdit la communication des informations confidentielles\ndont ils sont dépositaires au titre de leurs missions ou fonctions,\nsous peine des sanctions prévues par l’article 226-13 du Code pénal\n(jusqu’à un an d’emprisonnement et 15 000 € d’amende).\nLe secret statistique, défini dans une loi de 1951,\nrenforce cette obligation dans le cas de données détenues pour des usages statistiques.\nIl interdit strictement la communication de données individuelles\nou susceptibles d’identifier les personnes,\nissues de traitements à finalités statistiques,\nque ces traitements proviennent d’enquêtes ou de bases de données.\nLe secret statistique exclut par principe de diffuser des données\nqui permettraient l’identification des personnes concernées,\npersonnes physiques comme personnes morales.\nCette obligation limite la finesse des informations disponibles en diffusion\nCe cadre contraignant s’explique par l’héritage de la Seconde Guerre Mondiale\net le désir de ne plus revivre une situation où la collecte d’information\nsert une action publique basée sur la discrimination entre catégories\nde la population.\n\n\n\nUn article récent de Nature,\nqui reprend les travaux d’une équipe d’épidémiologistes (Gabelica, Bojčić, and Puljak 2022)\névoque le problème de l’accès aux données pour des chercheurs désirant reproduire\nune étude. Même dans les articles scientifiques où il est mentionné que les\ndonnées peuvent être mises à disposition d’autres chercheurs, le partage\nde celles-ci est rare :\n\nGraphique issu de l’article de Nature\nCe constat, quelque peu inquiétant, est confirmé par une étude récente\nde Samuel and Mietchen (2023) qui a tenté d’exécuter un peu moins de\n30 000 notebooks associés à des études scientifiques. Seuls 3%\ndes notebooks reproduisent les résultats espérés.\nAfin de partager les moyens de reproduire des publications sans diffuser des\ndonnées potentiellement confidentielles, les jeux de données synthétiques\nsont de plus en plus utilisés. Par le biais de modèles de deep learning,\nil est ainsi possible de générer des jeux de données synthétiques complexes\nqui permettent de reproduire les principales caractéristiques d’un jeu de données\ntout en évitant, si le modèle a été bien calibré, de diffuser une information\nindividuelle.\nDans l’administration française, les codes sources sont\nconsidérés comme des documents administratifs et peuvent\ndonc être mis à disposition de tout citoyen sur demande à la\nCommission d’accès aux documents administratifs (CADA):\n\n« Sont considérés comme documents administratifs, au sens des titres Ier, III et IV du présent livre, quels que soient leur date, leur lieu de conservation, leur forme et leur support, les documents produits ou reçus, dans le cadre de leur mission de service public, par l’État, les collectivités territoriales ainsi que par les autres personnes de droit public ou les personnes de droit privé chargées d’une telle mission. Constituent de tels documents notamment les dossiers, rapports, études, comptes rendus, procès-verbaux, statistiques, instructions, circulaires, notes et réponses ministérielles, correspondances, avis, prévisions, codes sources et décisions. »\nAvis 20230314 - Séance du 30/03/2023 de la Commission d’accès aux documents administratifs\n\nEn revanche, les poids des modèles utilisés par l’administration, notamment ceux\ndes modèles de machine learning ne sont pas réglementés de la même\nmanière (Avis 20230314 de la CADA).\nEn effet, comme il existe toujours\nun risque de rétro-ingénierie amenant à une révélation partielle\ndes données\nd’entraînement lors d’un partage de modèle, les modèles\nentraînés sur des données\nsensibles (comme les décisions de justice étudiées\npar (l’avis 20230314 de la CADA))\nn’ont pas vocation à être partagés.\n\n\n\nLe numérique constitue une part croissante des\némissions de gaz à effet de serre.\nReprésentant aujourd’hui 4 % des émissions mondiales\nde CO2, cette part devrait encore croître (Arcep 2019).\nLe monde de la data science est également\nconcerné.\nL’utilisation de données de plus en\nplus massives, notamment la constitution\nde corpus monumentaux de textes,\nrécupérés par scraping, est une première source\nde dépense d’énergie. De même, la récupération\nen continu de nouvelles traces numériques\nnécessite d’avoir des serveurs fonctionnels\nen continu. A cette première source de\ndépense d’énergie, s’ajoute l’entraînement\ndes modèles qui peut prendre des jours,\ny compris sur des architectures très\npuissantes. Strubell, Ganesh, and McCallum (2019)\nestime que l’entraînement d’un modèle à\nl’état de l’art dans le domaine du\nNLP nécessite autant d’énergie que ce que\nconsommeraient cinq voitures, en moyenne,\nau cours de l’ensemble de leur\ncycle de vie.\nL’utilisation accrue de l’intégration\ncontinue, qui permet de mettre en oeuvre de manière\nautomatisée l’exécution de certains scripts ou\nla production de livrables en continu,\namène également à une dépense d’énergie importante.\nIl convient donc d’essayer de limiter l’intégration\ncontinue à la production d’output vraiment nouveaux.\n\n\n\nPar exemple, cet ouvrage utilise de manière intensive\ncette approche. Néanmoins, pour essayer de limiter\nles effets pervers de la production en continu d’un\nouvrage extensif, seuls les chapitres modifiés\nsont produits lors des prévisualisations mises en\noeuvre à chaque pull request sur le dépôt\nGithub.\n\n\nLes data scientists doivent être conscients\ndes implications de leur usage intensif de\nressources et essayer de minimiser leur\nimpact. Par exemple, plutôt que ré-estimer\nun modèle de NLP,\nla méthode de l’apprentissage par transfert,\nqui permet de transférer les poids d’apprentissage\nd’un modèle à une nouvelle source, permet\nde réduire les besoins computationnels.\nDe même, il peut être utile, pour prendre\nconscience de l’effet d’un code trop long,\nde convertir le temps de calcul en\némissions de gaz à effet de serre.\nLe package codecarbon\npropose cette solution en adaptant l’estimation\nen fonction du mix énergétique du pays\nen question. Mesurer étant le\nprérequis pour prendre conscience puis comprendre,\nce type d’initiatives peut amener à responsabiliser\nles data scientists et ainsi permettre un\nmeilleur partage des ressources.\n\n\n\n\n\n\nArcep. 2019. “L’empreinte Carbone Du Numérique.” Rapport de l’Arcep.\n\n\nGabelica, Mirko, Ružica Bojčić, and Livia Puljak. 2022. “Many Researchers Were Not Compliant with Their Published Data Sharing Statement: Mixed-Methods Study.” Journal of Clinical Epidemiology.\n\n\nHurley, Mikella, and Julius Adebayo. 2016. “Credit Scoring in the Era of Big Data.” Yale JL & Tech. 18: 148.\n\n\nSamuel, Sheeba, and Daniel Mietchen. 2023. “Computational Reproducibility of Jupyter Notebooks from Biomedical Publications.” https://arxiv.org/abs/2308.07333.\n\n\nStrubell, Emma, Ananya Ganesh, and Andrew McCallum. 2019. “Energy and Policy Considerations for Deep Learning in NLP.” https://arxiv.org/abs/1906.02243."
  },
  {
    "objectID": "content/getting-started/03_data_analysis.html#lors-de-la-récupération-des-données",
    "href": "content/getting-started/03_data_analysis.html#lors-de-la-récupération-des-données",
    "title": "Comment aborder un jeu de données",
    "section": "",
    "text": "La phase de constitution de son jeu de données sous-tend tout le projet qui suit.\nLa première question à se poser est\n“de quelles données ai-je besoin pour répondre à ma problématique ?”.\nCette problématique pourra éventuellement\nêtre affinée en fonction des besoins mais les travaux sont généralement\nde meilleure qualité lorsque la problématique amène à la réflexion sur les données\ndisponibles plutôt que l’inverse.\nEnsuite, “qui produit et met à disposition ces données” ?\nLes sources disponibles sur internet sont-elles fiables ?\nPar exemple, les sites d’open data gouvernementaux sont par exemple assez fiables mais autorisent parfois l’archivage de données restructurées par des tiers et non des producteurs officiels. A l’inverse, sur Kaggle ou sur Github la source de certains jeux de données n’est pas tracée ce qui rend compliquée la confiance sur la qualité de la donnée\nUne fois identifié une ou plusieurs sources de données,\nest-ce que je peux les compléter avec d’autres données ?\n(dans ce cas, faire attention à avoir des niveaux de granularité adéquats).\n\n\n\nVient ensuite la phase de mise en forme et nettoyage des jeux de données récupérés.\nCette étape est primordiale et est généralement celle qui mobilise le plus\nde temps. Pendant quelques années, on parlait de data cleaning. Cependant,\ncela a pu, implicitement, laisser penser qu’il s’agissait d’une tâche\nsubalterne. On commence à lui préférer le concept de feature engineering\nqui souligne bien qu’il s’agit d’une compétence qui nécessite beaucoup\nde compétences.\nUn jeu de données propre est un jeu de données dont la structure est\nadéquate et n’entraînera pas d’erreur, visible ou non,\nlors de la phase d’analyse. Voici quelques éléments structurants\nd’un jeu de données propre :\n\nles informations manquantes sont bien comprises et traitées. numpy et\npandas proposent un certain formalisme sur le sujet qu’il est utile\nd’adopter en remplaçant par NaN les observations manquantes. Cela\nimplique de faire attention à la manière dont certains producteurs\ncodent les valeurs manquantes : certains ont la facheuse tendance à\nêtre imaginatifs sur les codes pour valeurs manquantes : “-999”, “XXX”, “NA”\nles variables servant d’identifiants sont bien les mêmes d’une table à l’autre (notamment dans le cas de jointure) : même format, même modalités\npour des variables textuelles, qui peuvent etre mal saisies, avoir corrigé les éventuelles fautes (ex “Rolland Garros” -&gt; “Roland Garros”)\ncréer des variables qui synthétisent l’information dont vous avez besoin\nsupprimer les éléments inutiles (colonne ou ligne vide)\nrenommer les colonnes avec des noms compréhensibles"
  },
  {
    "objectID": "content/getting-started/03_data_analysis.html#lors-de-lanalyse-descriptive",
    "href": "content/getting-started/03_data_analysis.html#lors-de-lanalyse-descriptive",
    "title": "Comment aborder un jeu de données",
    "section": "",
    "text": "Une fois les jeux de données nettoyés, vous pouvez plus sereinement\nétudier l’information présente dans les données.\nCette phase et celle du nettoyage ne sont pas séquentielles,\nen réalité vous devrez régulièrement passer de votre nettoyage à quelques statistiques descriptives qui vous montreront un problème, retourner au nettoyage etc.\nLes questions à se poser pour “challenger” le jeu de données :\n\nEst-ce que mon échantillon est bien représentatif de ce qui m’intéresse ? N’avoir que 2000 communes sur les 35000 n’est pas nécessairement un problème mais il est bon de s’être posé la question.\nEst-ce que les ordres de grandeur sont bons ? Pour cela, confronter vos premieres stats desc à vos recherches internet. Par exemple trouver que les maisons vendues en France en 2020 font en moyenne 400 m² n’est pas un ordre de grandeur réaliste.\nEst-ce que je comprends toutes les variables de mon jeu de données ? Est-ce qu’elles se “comportent” de la bonne façon ? A ce stade, il est parfois utile de se faire un dictionnaire de variables (qui explique comment elles sont construites ou calculées). On peut également mener des études de corrélation entre nos variables.\nEst-ce que j’ai des outliers, i.e. des valeurs aberrantes pour certains individus ? Dans ce cas, il faut décider quel traitement on leur apporte (les supprimer, appliquer une transformation logarithmique, les laisser tel quel) et surtout bien le justifier.\nEst-ce que j’ai des premiers grands messages sortis de mon jeu de données ? Est-ce que j’ai des résultats surprenants ? Si oui, les ai-je creusé suffisamment pour voir si les résultats tiennent toujours ou si c’est à cause d’un souci dans la construction du jeu de données (mal nettoyées, mauvaise variable…)"
  },
  {
    "objectID": "content/getting-started/03_data_analysis.html#lors-de-la-modélisation",
    "href": "content/getting-started/03_data_analysis.html#lors-de-la-modélisation",
    "title": "Comment aborder un jeu de données",
    "section": "",
    "text": "A cette étape, l’analyse descriptive doit voir avoir donné quelques premières pistes pour savoir dans quelle direction vous voulez mener votre modèle.\nUne erreur de débutant est de se lancer directement dans la modélisation parce\nqu’il s’agirait d’une compétence plus poussée. Cela amène généralement\nà des analyses de pauvre qualité : la modélisation tend généralement à confirmer\nles intuitions issues de l’analyse descriptive. Sans cette dernière,\nl’interprétation des résultats d’un modèle peu s’avérer inutilement complexe.\nVous devrez plonger dans vos autres cours (Econométrie 1, Series Temporelles, Sondages, Analyse des données etc.) pour trouver le modèle le plus adapté à votre question.\nLa méthode sera guidée par l’objectif.\n\nEst-ce que vous voulez expliquer ou prédire ? https://hal-cnam.archives-ouvertes.fr/hal-02507348/document\nEst-ce que vous voulez classer un élément dans une catégorie (classification ou clustering) ou prédire une valeur numérique (régression) ?\n\nEn fonction des modèles que vous aurez déjà vu en cours et des questions que vous souhaiterez résoudre sur votre jeu de données, le choix du modèle sera souvent assez direct.\nVous pouvez également vous référez à la démarche proposée par Xavier Dupré\nhttp://www.xavierdupre.fr/app/ensae_teaching_cs/helpsphinx3/debutermlprojet.html#l-debutermlprojet\nPour aller plus loin (mais de manière simplifiée) sur les algorithmes de Machine Learning :\nhttps://datakeen.co/8-machine-learning-algorithms-explained-in-human-language/\n\n\nLa mise à disposition de code sur Github ou Gitlab est une incitation\ntrès forte pour produire du code de qualité. Il est ainsi recommandé de\nsystématiquement utiliser ces plateformes pour la mise à disposition de\ncode. Cependant, il ne s’agit que d’une petite partie des gains à\nl’utiliser.\nLe cours que je donne avec Romain Avouac en troisième année d’ENSAE\n(ensae-reproductibilite.github.io/website/) évoque\nl’un des principaux gains à utiliser ces plateformes, à savoir\nla possibilité de mettre à disposition automatiquement différents livrables\npour valoriser son travail auprès de différents publics.\nSelon le public visé, la communication ne sera pas identique. Le code peut\nintéresser les personnes désirant avoir des détails sur la méthodologie mise\nen oeuvre en pratique mais il peut s’agir d’un format rebutant pour d’autres\npublics. Une visualisation de données dynamiques parlera à des publics\nmoins experts de la donnée mais est plus dure à mettre en oeuvre\nqu’un graphique standard.\n\n\n\nLes Notebooks Jupyter ont eu beaucoup de succès dans le monde de\nla data science pour valoriser des travaux. Pourtant il ne s’agit\npas forcément toujours du meilleur format. En effet, beaucoup\nde notebooks tentent à empiler des pavés de code et du texte, ce\nqui les rend difficilement lisibles.\nSur un projet conséquent, il vaut mieux reporter le plus de code\npossible dans des scripts bien structurés et avoir un notebook\nqui appelle ces scripts pour produire des outputs. Ou alors ne\npas utiliser un notebook et privilégier un autre format (un\ntableau de bord, un site web, une appli réactive…).\nDans le cours de dernière année de\nl’ENSAE, Mise en production de projets data science, Romain\nAvouac et moi revenons sur les moyens de communication et de partage de code alternatifs au notebook."
  },
  {
    "objectID": "content/getting-started/03_data_analysis.html#ethique-et-responsabilité-du-data-scientist",
    "href": "content/getting-started/03_data_analysis.html#ethique-et-responsabilité-du-data-scientist",
    "title": "Comment aborder un jeu de données",
    "section": "",
    "text": "Les données sont une représentation synthétique de la réalité et les\nconclusions de certaines analyses peuvent avoir un vrai impact sur\nla vie des citoyens. Les chiffres erronés de\nReinhart et Rogoff ont ainsi pu servir de justification théorique à des\npolitiques d’austérité qui ont pu avoir des conséquences violentes\npour certains citoyens de\npays en crise1. En Grande-Bretagne, le recensement des personnes\ncontaminées par le Covid en 2020, et donc de leurs proches pour le\nsuivi de l’épidémie,\na été incomplet à cause de\ntroncatures dues à l’utilisation d’un format non adapté de stockage\ndes données (tableur Excel)2.\nDernier exemple avec le credit scoring mis en oeuvre aux Etats-Unis.\nLa citation ci-dessous, issue de l’article de Hurley and Adebayo (2016),\nillustre très bien les conséquences et les aspects problématiques\nd’un système de construction automatisée d’un score de crédit :\n\nConsumers have limited ability to identify and contest unfair credit\ndecisions, and little chance to understand what steps they\nshould take to improve their credit. Recent studies have also\nquestioned the accuracy of the data used by these tools, in some\ncases identifying serious flaws that have a substantial bearing\non lending decisions. Big-data tools may also risk creating a\nsystem of “creditworthinessby association” in which consumers’\nfamilial, religious, social, and other affiliations determine their\neligibility for an affordable loan.\nHurley and Adebayo (2016)\n\n\n\n\nLa transparence sur les intérêts et limites d’une méthode mise en oeuvre\nest donc importante.\nCette exigence de la recherche, parfois oubliée à cause de la course\naux résultats novateurs, mérite également d’être appliquée\nen entreprise ou administration.\nMême sans intention manifeste de la part de la personne qui analyse des données,\nune mauvaise interprétation est toujours possible. Tout en valorisant un\nrésultat, il est possible d’alerter sur certaines limites. Il est important,\ndans ses recherches comme dans les discussions avec d’autres interlocuteurs,\nde faire attention au biais de confirmation qui consiste\nà ne retenir que l’information qui correspond à nos conceptions a priori et\nà ne pas considérer celles qui pourraient aller à l’encontre de celles-ci:\n\n\n\n\n\nCertaines représentations de données sont à exclure car des biais cognitifs\npeuvent amener à des interprétations erronées3. Dans le domaine de la\nvisualisation de données, les camemberts (pie chart) ou les diagrammes\nradar sont par exemple\nà exclure car l’oeil humain perçoit mal ces formes circulaires. Pour une raison\nsimilaire, les cartes avec aplat de couleur (cartes\nchoroplèthes) sont trompeuses.\nLes posts de blog pour datawrapper\nde Lisa Charlotte Muth ou ceux d’Eric Mauvière sont d’excellentes ressources\npour apprendre les bonnes et mauvaises pratiques de\nvisualisation (voir la partie visualisation de ce cours\npour plus de détails).\n\n\n\nLe cadre réglementaire de protection des données a évolué ces dernières\nannées avec le RGPD. Cette réglementation a permis de mieux faire\nsaisir le fait que la collecte de données se justifie au nom\nde finalités plus ou moins bien identifiées. Prendre conscience que\nla confidentialité des données se justifie pour éviter la dissémination\nnon contrôlée d’informations sur une personne est important.\nDes données particulièrement sensibles, notamment les données de santé,\npeuvent être plus contraignantes à traiter que des données peu sensibles.\nEn Europe, par exemple, les agents du service statistique public\n(Insee ou services statistiques ministériels) sont tenus au secret professionnel\n(article L121-6 du Code général de la fonction publique),\nqui leur interdit la communication des informations confidentielles\ndont ils sont dépositaires au titre de leurs missions ou fonctions,\nsous peine des sanctions prévues par l’article 226-13 du Code pénal\n(jusqu’à un an d’emprisonnement et 15 000 € d’amende).\nLe secret statistique, défini dans une loi de 1951,\nrenforce cette obligation dans le cas de données détenues pour des usages statistiques.\nIl interdit strictement la communication de données individuelles\nou susceptibles d’identifier les personnes,\nissues de traitements à finalités statistiques,\nque ces traitements proviennent d’enquêtes ou de bases de données.\nLe secret statistique exclut par principe de diffuser des données\nqui permettraient l’identification des personnes concernées,\npersonnes physiques comme personnes morales.\nCette obligation limite la finesse des informations disponibles en diffusion\nCe cadre contraignant s’explique par l’héritage de la Seconde Guerre Mondiale\net le désir de ne plus revivre une situation où la collecte d’information\nsert une action publique basée sur la discrimination entre catégories\nde la population.\n\n\n\nUn article récent de Nature,\nqui reprend les travaux d’une équipe d’épidémiologistes (Gabelica, Bojčić, and Puljak 2022)\névoque le problème de l’accès aux données pour des chercheurs désirant reproduire\nune étude. Même dans les articles scientifiques où il est mentionné que les\ndonnées peuvent être mises à disposition d’autres chercheurs, le partage\nde celles-ci est rare :\n\nGraphique issu de l’article de Nature\nCe constat, quelque peu inquiétant, est confirmé par une étude récente\nde Samuel and Mietchen (2023) qui a tenté d’exécuter un peu moins de\n30 000 notebooks associés à des études scientifiques. Seuls 3%\ndes notebooks reproduisent les résultats espérés.\nAfin de partager les moyens de reproduire des publications sans diffuser des\ndonnées potentiellement confidentielles, les jeux de données synthétiques\nsont de plus en plus utilisés. Par le biais de modèles de deep learning,\nil est ainsi possible de générer des jeux de données synthétiques complexes\nqui permettent de reproduire les principales caractéristiques d’un jeu de données\ntout en évitant, si le modèle a été bien calibré, de diffuser une information\nindividuelle.\nDans l’administration française, les codes sources sont\nconsidérés comme des documents administratifs et peuvent\ndonc être mis à disposition de tout citoyen sur demande à la\nCommission d’accès aux documents administratifs (CADA):\n\n« Sont considérés comme documents administratifs, au sens des titres Ier, III et IV du présent livre, quels que soient leur date, leur lieu de conservation, leur forme et leur support, les documents produits ou reçus, dans le cadre de leur mission de service public, par l’État, les collectivités territoriales ainsi que par les autres personnes de droit public ou les personnes de droit privé chargées d’une telle mission. Constituent de tels documents notamment les dossiers, rapports, études, comptes rendus, procès-verbaux, statistiques, instructions, circulaires, notes et réponses ministérielles, correspondances, avis, prévisions, codes sources et décisions. »\nAvis 20230314 - Séance du 30/03/2023 de la Commission d’accès aux documents administratifs\n\nEn revanche, les poids des modèles utilisés par l’administration, notamment ceux\ndes modèles de machine learning ne sont pas réglementés de la même\nmanière (Avis 20230314 de la CADA).\nEn effet, comme il existe toujours\nun risque de rétro-ingénierie amenant à une révélation partielle\ndes données\nd’entraînement lors d’un partage de modèle, les modèles\nentraînés sur des données\nsensibles (comme les décisions de justice étudiées\npar (l’avis 20230314 de la CADA))\nn’ont pas vocation à être partagés.\n\n\n\nLe numérique constitue une part croissante des\némissions de gaz à effet de serre.\nReprésentant aujourd’hui 4 % des émissions mondiales\nde CO2, cette part devrait encore croître (Arcep 2019).\nLe monde de la data science est également\nconcerné.\nL’utilisation de données de plus en\nplus massives, notamment la constitution\nde corpus monumentaux de textes,\nrécupérés par scraping, est une première source\nde dépense d’énergie. De même, la récupération\nen continu de nouvelles traces numériques\nnécessite d’avoir des serveurs fonctionnels\nen continu. A cette première source de\ndépense d’énergie, s’ajoute l’entraînement\ndes modèles qui peut prendre des jours,\ny compris sur des architectures très\npuissantes. Strubell, Ganesh, and McCallum (2019)\nestime que l’entraînement d’un modèle à\nl’état de l’art dans le domaine du\nNLP nécessite autant d’énergie que ce que\nconsommeraient cinq voitures, en moyenne,\nau cours de l’ensemble de leur\ncycle de vie.\nL’utilisation accrue de l’intégration\ncontinue, qui permet de mettre en oeuvre de manière\nautomatisée l’exécution de certains scripts ou\nla production de livrables en continu,\namène également à une dépense d’énergie importante.\nIl convient donc d’essayer de limiter l’intégration\ncontinue à la production d’output vraiment nouveaux.\n\n\n\nPar exemple, cet ouvrage utilise de manière intensive\ncette approche. Néanmoins, pour essayer de limiter\nles effets pervers de la production en continu d’un\nouvrage extensif, seuls les chapitres modifiés\nsont produits lors des prévisualisations mises en\noeuvre à chaque pull request sur le dépôt\nGithub.\n\n\nLes data scientists doivent être conscients\ndes implications de leur usage intensif de\nressources et essayer de minimiser leur\nimpact. Par exemple, plutôt que ré-estimer\nun modèle de NLP,\nla méthode de l’apprentissage par transfert,\nqui permet de transférer les poids d’apprentissage\nd’un modèle à une nouvelle source, permet\nde réduire les besoins computationnels.\nDe même, il peut être utile, pour prendre\nconscience de l’effet d’un code trop long,\nde convertir le temps de calcul en\némissions de gaz à effet de serre.\nLe package codecarbon\npropose cette solution en adaptant l’estimation\nen fonction du mix énergétique du pays\nen question. Mesurer étant le\nprérequis pour prendre conscience puis comprendre,\nce type d’initiatives peut amener à responsabiliser\nles data scientists et ainsi permettre un\nmeilleur partage des ressources."
  },
  {
    "objectID": "content/getting-started/03_data_analysis.html#références",
    "href": "content/getting-started/03_data_analysis.html#références",
    "title": "Comment aborder un jeu de données",
    "section": "",
    "text": "Arcep. 2019. “L’empreinte Carbone Du Numérique.” Rapport de l’Arcep.\n\n\nGabelica, Mirko, Ružica Bojčić, and Livia Puljak. 2022. “Many Researchers Were Not Compliant with Their Published Data Sharing Statement: Mixed-Methods Study.” Journal of Clinical Epidemiology.\n\n\nHurley, Mikella, and Julius Adebayo. 2016. “Credit Scoring in the Era of Big Data.” Yale JL & Tech. 18: 148.\n\n\nSamuel, Sheeba, and Daniel Mietchen. 2023. “Computational Reproducibility of Jupyter Notebooks from Biomedical Publications.” https://arxiv.org/abs/2308.07333.\n\n\nStrubell, Emma, Ananya Ganesh, and Andrew McCallum. 2019. “Energy and Policy Considerations for Deep Learning in NLP.” https://arxiv.org/abs/1906.02243."
  },
  {
    "objectID": "content/getting-started/03_data_analysis.html#footnotes",
    "href": "content/getting-started/03_data_analysis.html#footnotes",
    "title": "Comment aborder un jeu de données",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLe livre de Reinhart et Rogoff, This time is different, s’appuyait\nsur un Excel constitué à la main. Un doctorant s’est aperçu d’erreurs\ndans celui-ci et a remarqué que lorsqu’on\nsubstituait les chiffres officiels, les résultats n’étaient plus valides.↩︎\nOn suppose ici que le message erroné est transmis sans volonté de\nmanipulation. La manipulation manifeste est un problème encore plus grave.↩︎\nOn suppose ici que le message erroné est transmis sans volonté de\nmanipulation. La manipulation manifeste est un problème encore plus grave.↩︎"
  },
  {
    "objectID": "content/getting-started/01_installation.html",
    "href": "content/getting-started/01_installation.html",
    "title": "Configuration de Python",
    "section": "",
    "text": "Les exercices sont présentés sous la\nforme de notebook jupyter. Ils peuvent être exécutés\ndans plusieurs environnement, au gré des préférences et des connaissances\nde chacun :\nConcernant la première méthode, qui est celle recommandée,\nchaque\nchapitre présente les badges suivants qui permettent d’ouvrir\nla page web en question dans l’environnement de prédilection.\nPar exemple, pour ouvrir le chapitre relatif à\nnumpy dans l’un des environnements temporaires proposés,\nles badges suivants sont proposés:\nQuel que soit l’environnement d’exécution des scripts, l’un des objectifs\nde ce cours est d’adopter un environnement favorable à la reproductibilité\ndes traitements. Ils devraient donc fonctionner, dès lors que l’environnement\nest bien configuré, d’une manière similaire quel que soit\nla machine qui exécute le code.\nComme la reproductibilité est une notion centrale dans une démarche\nscientifique mais également importante dans le monde\nde l’entreprise ou de l’administration, en supplément des notions relatives\nà Python, ce cours montrera comment utiliser Git avec Python et\névoquera un\ncertain nombre de critères de qualité du code qui sont devenus\ndes standards dans la communauté open-source, dans l’industrie et dans\nl’administration. Ces compétences ne sont pas\npropres à Python et seront\nutiles pour tout projet ultérieur. Un cours dédié à cette question\nest proposé par Romain Avouac et moi en dernière année de l’ENSAE. Son\ncontenu est disponible sur https://ensae-reproductibilite.github.io/website/.\nLe projet final devra impérativement\nêtre associé à un dépôt\nsur Github (nous reviendrons dessus) et répondre à\nces critères de qualité, qui serviront toute la vie.\nCe cours vise à acculturer à la conduite de projets de data-science avec\nPython. L’environnement foisonnant de la data-science nécessite un\ncertain nombre d’éléments supplémentaires à Python. La suite\nde ce chapitre permettra de décrire les configurations à mettre\nen oeuvre pour être en mesure d’exploiter la richesse de l’écosystème Python."
  },
  {
    "objectID": "content/getting-started/01_installation.html#local",
    "href": "content/getting-started/01_installation.html#local",
    "title": "Configuration de Python",
    "section": "Installer un environnement adapté à la data-science sur son ordinateur personnel",
    "text": "Installer un environnement adapté à la data-science sur son ordinateur personnel\nCette partie présente plusieurs éléments de configuration d’un environnement\nen local. Cependant, cette approche est de moins en moins fréquente. En effet,\nplusieurs facteurs conjoints ont amené à privilégier des\nserveurs plutôt que des installations locales (évolutions dans les technologies cloud,\nbesoins accrus de ressources, besoins de plus de contrôle sur la confidentialité\ndes données en limitant leur prolifération…). Au sein des administrations et\ndes entreprises, les approches cloud, où l’utilisateur se voit mis à disposition\nune interface graphique alors que les calculs sont déportés sur un serveur\ndistant, est de plus en plus fréquent.\n\nInstaller Python en local\nPour installer Python, il est recommandé d’utiliser\nla distribution Anaconda\nqui permet d’installer une distribution minimale de Python ainsi qu’éventuellement\nun environnement plus complet :\n\nSous Windows, il suffit de télécharger l’exécutable puis\nl’exécuter (cf. la doc officielle ;\nSous Mac, se reporter à la doc officielle ;\nSous Linux, suivre les instructions de la doc officielle selon sa distribution\n\nPasser par Anaconda permet:\n\nd’installer Python ;\nd’installer par défaut une multitude de packages utiles ;\nde pouvoir utiliser un gestionnaire de package nommé conda.\n\nAnaconda permet de créer des environnements isolés et facilite l’installation\nde certaines librairies qui nécessitent l’usage de langages externes (par exemple\ndu C++).\n\n\nInstaller un environnement de développement\nLes notebooks Jupyter (extension .ipynb)\nsont très utilisés en data science. Ils sont en\nparticulier très adaptés à la réalisation d’analyses exploratoires.\nLes notebooks permettent de mêler du code, du texte et des sorties\ngraphiques ou des tableaux. L’intérêt principal des notebooks est qu’ils\npermettent d’exécuter du code très facilement dans un environnement\nPython donné (le kernel Jupyter). Ils sont particulièrement pratiques\npour ajouter du code ou du texte à un document déjà existant, d’où le\nterme de notebook.\nNéanmoins, passée l’étape d’exploration, il est recommandé de plutôt recourir à des\nscripts au format .py. L’utilisation du format .py est l’un des premiers\ngestes pour favoriser la reproductibilité des analyses.\nCes scripts peuvent être édités à l’aide d’éditeurs de texte adaptés au code, comme\nVisual Studio\n(mon préféré),\nSublime Text,\nou PyCharm (privilégier Pycharm Community Edition)\nentre autres.\nCes éditeurs\noffrent des fonctionalités supplémentaires pratiques :\n\nnombreux plugins pour une pleine utilisation de l’écosystème Python: éditeur de Markdown,\ninterface Git, etc.\nfonctionalités classiques d’un IDE dont manque Jupyter: autocomplétion, diagnostic du code, etc.\nintégration avec les environnements Conda\n\n\n\nInstallation de Git\nLe principe de Git ainsi que son usage avec Python sont présentés dans\nune partie dédiée. Cette partie se concentre ainsi sur la question\nde la configuration de Git.\nGit est un langage dont la fonction est de tracer l’historique de modification\nd’un fichier. Pour disposer de ce langage, il est nécessaire d’installer\nle logiciel Git Bash. Grâce à lui, Git sera disponible et des outils\nexternes, notamment les interfaces de développement comme\nVisual Studio, pourront l’utiliser."
  },
  {
    "objectID": "content/getting-started/01_installation.html#exécution-dans-un-environnement-temporaire-sur-un-serveur-distant",
    "href": "content/getting-started/01_installation.html#exécution-dans-un-environnement-temporaire-sur-un-serveur-distant",
    "title": "Configuration de Python",
    "section": "Exécution dans un environnement temporaire sur un serveur distant",
    "text": "Exécution dans un environnement temporaire sur un serveur distant\nComme évoqué précédemment, les technologies dominantes dans\nle domaine du traitement des données ont amené à une évolution des pratiques\ndepuis quelques années.\nLa multiplication de données volumineuses qui dépassent les capacités en RAM\nvoire en stockage des machines personnelles,\nles progrès dans les technologies de stockage type cloud,\nl’adhésion de la communauté aux outils de versioning\n(le plus connu étant Git) sont autant de facteurs\nayant amené à repenser la manière de traiter des données.\nLes infrastructures à l’état de l’art permettent ainsi de découpler stockage\ndes données, stockage du code et exécution des traitements sur les données.\nL’exécution des traitements s’effectue ainsi sur des machines à la durée de vie\ncourte qui stockent temporairement données et code ensembles pour tester\nles traitements.\nAvec les dépôts sur Github ou Gitlab,\non dissocie environnement de stockage des codes et\nd’exécution de ceux-ci. Un système de stockage S3, présenté dans un\nchapitre ultérieur, permet en supplément de dissocier l’environnement\nde stockage des données de ces deux premiers environnements.\nSur le\ndépôt Github de ce cours , on peut\nnaviguer dans les fichiers\n(et voir tout l’historique de modification de ceux-ci). Mais,\ncomment exécuter les scripts sans passer par un poste local ?\nDepuis quelques années, des services en ligne permettant de\nlancer une instance Jupyter à distance (analogue à celle que vous pouvez\nlancer en local en utilisant Anaconda) ont émergé. Parmi celles-ci :\n\nLe SSP Cloud , plateforme développée par l’Insee qui fournit des environnements bac à sable basés sur des technologie de conteneurisation\nGoogle colaboratory\n\n;\nGithub Visual Studio Editor  ;\nBinder  ;\n\nIl est également possible d’exécuter des codes sur les services d’intégration continue de\nGitlab (service Gitlab CI)\nou de Github (via Github Actions). Il s’agit d’une approche\nbash, c’est-à-dire que les scripts sont exécutés par une console à chaque interaction avec le dépôt\ndistant Gitlab/Github, sans session ouverte pour les éditer.\nCette approche est très appropriée\npour assurer la reproductibilité d’une chaîne de traitement (on peut aller\njusqu’au\ndéploiement de visualisations automatiques1) mais n’est pas très pratique pour\nle griffonnage.\n2 A cet égard, il est recommandé de consulter le cours de dernière année\nde l’ENSAE déjà cité: https://ensae-reproductibilite.github.io/website/\nKaggle \npropose des compétitions de code mais\ndonne également la possibilité d’exécuter des notebooks,\ncomme les solutions précédentes.\nIl existe une API Kaggle pour\naccéder à des données Kaggle hors du système Kaggle\n\n\n Warning\nLes performances de ces solutions peuvent être variables.\nLes serveurs publics mis à disposition\nne sont pas forcément des foudres de guerre. Avec ceux-ci,\non vérifie plutôt la reproductibilité des scripts avec des jeux d’exemples.\nIl est bien sûr interdit de mettre des données confidentielles dessus: ces\ndernières doivent rester dans des infrastructures où elles sont autorisées.\nQuand on est dans une entreprise ou administration,\nqui dispose de serveurs propres,\non peut aller plus loin en utilisant ces outils\npour automatiser l’ensemble de la chaîne de traitement.\nAttention: il n’y a pas de garantie de perennité de service\n(notamment avec Binder où\n10 minutes d’inactivité mènent à l’extinction du service). Il s’agit plus d’un service pour griffoner\ndans le même environnement que celui du dépôt Git que de solutions durables.\nLes sessions sur l’environnement SSPCloud sont plus durables mais il convient\nde garder à l’esprit qu’elles sont également temporaires.\n\n\n\nSSP Cloud \nOnyxia, l’autre petit nom du SSP Cloud,\nest une plateforme libre service mutualisée de traitement\nde données statistiques et de datascience.\nCe cloud met à disposition aux statisticiens et aux data scientists\nde l’État un catalogue de services et un environnement de travail simple, rapide et collaboratif, permettant de lancer facilement ces outils et d’y connecter ses données et son code.\nAu-delà des ressources techniques, cette plateforme\nreprésente une opportunité pour les statisticiens publics et les\nétudiants de découvrir\net d’adopter de nouvelles méthodes de travail.\nElle est aussi utilisé à des fins de formations et d’auto-formations.\nDans cet environnement, Jupyter et Visual Studio sont tous deux\ndisponibles.\n\n\nGoogle colaboratory \nGoogle met à disposition une plateforme de calculs basée sur le format Jupyter Notebook.\nUn grand avantage de cette solution est la mise à disposition gratuite de\nGPUs de qualité raisonnable,\noutil quasi-indispensable dans les projets basés sur des méthodes de deep learning.\nIl est possible de connecter les notebooks ouverts à Google Drive ou à\nGithub. L’icone\n\nfournit un raccourci pour lancer le notebook dans un environnement dédié.\n\n\nGithub Visual Studio Editor \nMicrosoft qui possède à la fois Github et Visual Studio a récemment\nlancé une offre Github dev qui permet d’ouvrir et lancer un notebook\nJupyter depuis un navigateur web.\nEn plus des fonctionalités attendues du logiciel Visual Studio\nCette interface permet également de gérer les issues et pull request\nd’un dépôt Github.\n\n\nLa technologie en arrière-plan : Docker \nDocker est l’outil open-source de référence\nen matière de création d’environnements isolés et auto-suffisants (les conteneurs).\nEn pratique, une application codée en Python ne repose que rarement seulement sur\ndu code produit par son développeur, elle fait généralement intervenir des dépendances :\nd’autres librairies Python, ainsi que des librairies liées au système d’exploitation\nsur laquelle elle est développée. Docker va permettre d’empaqueter l’application ainsi\nque toutes ses dépendances et rendre son exécution portable, c’est à dire indépendante\ndu système sur laquelle elle est éxécutée.\nDocker  est utilisé dans\nle cadre de cours afin d’assurer la reproductibilité des exemples.\nPlus de détails sont disponibles dans le cours de dernière année d’ENSAE\ndédié à la mise en production de projets data science\n(https://ensae-reproductibilite.github.io/website/).\nIl est possible d’utiliser les images Docker sur lesquelles reposent\nl’environnement de reproductibilité du cours. Celles-ci sont mises à\ndisposition sur DockerHub, le principal réseau de mise à disposition\nd’images Docker. Il existe une image minimale\nqui intègre Python et Quarto.\nPour utiliser l’image Visual Studio:\ndocker pull linogaliana/python-datascientist-vstudio\ndocker run --rm -p 8787:8787 -e PASSWORD=test linogaliana/python-datascientist-vstudio\nEn se rendant depuis un navigateur sur localhost:8887/, et en rentrant\nle mot de passe test (défini plus haut), on peut ainsi accéder\nà l’interface désirée (attention il s’agit d’un environnement temporaire, pas\npérenne)."
  },
  {
    "objectID": "content/getting-started/01_installation.html#installer-des-packages-supplémentaires",
    "href": "content/getting-started/01_installation.html#installer-des-packages-supplémentaires",
    "title": "Configuration de Python",
    "section": "Installer des packages supplémentaires",
    "text": "Installer des packages supplémentaires\nUn module est un script qui a vocation à définir des objets utilisés\npostérieurement par un interpréteur. C’est un script .py autosuffisant,\ndéfinissant des objets et des relations entre eux et le monde extérieur\n(d’autres modules). Un package est un ensemble cohérent de modules. Par exemple\nscikit-learn propose de nombreux modules utiles pour le machine learning.\nPython, sans ajout de briques supplémentaires,\ntrouvera rapidement ses limites.\nMême dans les scripts les plus simples, on a généralement besoin de packages qui\névitent de réinventer la roue.\nLes packages sont les éléments qui font la richesse des\nlangages open-source.\nIls sont l’équivalent des packages R ou Stata.\nLe monde de développeurs Python est très prolifique :\ndes mises à jour sont très souvent disponibles,\nles bibliothèques de packages sont très nombreuses. Un data scientist\nprendra l’habitude de jongler avec des dizaines de packages dont il connaîtra\nquelques fonctions et où, surtout, il saura aller chercher de l’information.\nLe rythme des mises à jour et des ajouts de fonctionalités\ns’est accéléré ces dernières années. Les grandes compagnies du\nnumérique ont elles-mêmes opensourcées des librairies\ndevenues centrales dans l’écosystème de la data-science\n(TensorFlow par Google, PyTorch par Facebook…)\nLes forums, notamment StackOverflow\nregorgent de bons conseils.\nLes deux meilleurs conseils qu’on puisse donner :\n\nregarder la documentation officielle d’un package. Les bons packages sont\ngénéralement très bien documentés et beaucoup d’erreurs peuvent être évitées\nen apprenant à chercher dans la documentation ;\nen cas d’erreur : copiez-collez l’erreur sur votre moteur de recherche préféré. Quelqu’un aura déjà posé la question, sans doute sur stackoverflow. Néanmoins, ne copiez-collez\npas la réponse sans comprendre la solution.\n\n\nLes gestionnaires de packages\nLes packages d’un langage open-source sont mis à disposition sur\ndes dépôts. Le CTAN est ainsi le dépôt LaTeX le plus connu, le\nCRAN celui du langage R.\nEn Python, il existe deux gestionnaires de packages qu’on utilise\nassociés à deux dépôts différents :\n\npip associé au dépôt PyPi\nconda associé au dépôt Anaconda\n\nAnaconda a permis, il y a quelques années, de faciliter grandement\nl’installation de librairies dépendants d’autres langages\nque Python (notamment des librairies C pour améliorer\nla performance des calculs). Ces dernières sont\ncompliquées à installer, notamment sur Windows.\nLe fait de proposer des librairies pré-compilées sur une grande\nvariété de systèmes d’exploitation a été une avancée\nd’anaconda. PyPi a adopté ce même principe avec les\nwheels ce qui finalement, rend les installations\navec pip à nouveau intéressantes (sauf pour certaines\nlibrairies en Windows).\nAnaconda a deux défauts par rapport à pip :\n\nl’installation de packages via pip est plus rapide que via\nconda. conda est en effet plus précautionneux sur l’interaction\nentre les différentes versions des packages installés.\nmamba a récemment\nété développé pour accélérer l’installation de packages dans un\nenvironnement conda3\nles versions disponibles sur PyPi sont plus récentes\nque celles sur le canal par défaut d’Anaconda. En effet,\npour un développeur de packages, il est possible de publier\nun package de manière automatique sur PyPi\nL’utilisation\ndu canal alternatif qu’est la conda forge permet de disposer de versions plus récentes des packages et limite l’écart avec les versions\ndisponibles sur PyPi.\n\n\n\n Warning\nLes conditions d’utilisation du canal par défaut d’Anaconda sont\nassez restrictives. L’utilisation d’Anaconda dans un cadre commercial est ainsi, depuis 2020,\nsoumis à l’achat de licences commerciales d’Anaconda pour réduire le problème de\npassager clandestin.\nIl est ainsi recommandé, notamment lorsqu’on travaille dans le\nsecteur privé où du code Python peut être utilisé,\nde ne pas ignorer ces conditions pour ne pas se mettre en faute juridiquement.\nLa conda forge n’est pas soumise à ces conditions et est ainsi préférable\ndans les entreprises.\n\n\n\n\nComment installer des packages\nAvec Anaconda, il faut passer par la ligne de commande et taper\nconda install &lt;nom_module&gt;\nPar exemple conda install geopandas. Depuis une cellule de notebook\nJupyter, on ajoute un point d’exclamation pour indiquer à Jupyter\nque la commande doit être interprétée comme une commande shell\net non une commande Python\n!conda install &lt;nom_module&gt; -y\nL’option -y permet d’éviter que conda nous demande confirmation\nsur l’installation du package. Pour mettre à jour un package, on fera\nconda upgrade plutôt que conda install\nAvec pip, on va cette fois taper\npip install &lt;nom_module&gt;\npip permet également d’installer des librairies directement depuis\nGithub à condition que Anaconda et Git sachent\ncommuniquer (ce qui implique en général que Git soit dans le PATH\ndu système d’exploitation). Par exemple, pour installer le package\npynsee\npip install git+https://github.com/InseeFrLab/Py-Insee-Data.git#egg=pynsee\nLa partie dédiée aux environnement virtuels du cours de dernière année de\nl’ENSAE présente plus d’éléments sur les différences moins évidentes\nentre pip et conda."
  },
  {
    "objectID": "content/getting-started/01_installation.html#footnotes",
    "href": "content/getting-started/01_installation.html#footnotes",
    "title": "Configuration de Python",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLes gains de performance peuvent être assez impressionnants.\nLa création de l’environnement nécessaire à la construction automatisée\nde ce site web a ainsi été divisée par 12 en utilisant mamba plutôt\nque conda pour installer des packages dans un environnement.↩︎\nLes gains de performance peuvent être assez impressionnants.\nLa création de l’environnement nécessaire à la construction automatisée\nde ce site web a ainsi été divisée par 12 en utilisant mamba plutôt\nque conda pour installer des packages dans un environnement.↩︎\nLes gains de performance peuvent être assez impressionnants.\nLa création de l’environnement nécessaire à la construction automatisée\nde ce site web a ainsi été divisée par 12 en utilisant mamba plutôt\nque conda pour installer des packages dans un environnement.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Thèmes en vrac",
    "section": "",
    "text": "Python pour la data science \n\n\nLino Galiana\n\nStar this website on Github\n\nCe site web rend public le contenu du cours de deuxième année (Master 1) de l’ENSAE :\nPython pour la data science\n\nTout est présent sur ce site web ! Des Notebooks Jupyter peuvent être récupérés pour s’exercer. L’ensemble des codes sources est stocké sur Github\n\n\n\n\n # Image manquante ?\n\n\n\n\nPour découvrir Python  de manière thématique\n\n\n\n\n\n\n\n\n\n\nQuelques éléments pour comprendre les enjeux du NLP\n\n\n\nNLP\n\n\nTutoriel\n\n\n\nLes corpus textuels étant des objets de très grande dimension\noù le ratio signal/bruit est faible, il est nécessaire de mettre\nen oeuvre une série d’étapes de nettoyage de…\n\n\n\nLino Galiana\n\n\nJul 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNettoyer un texte: des exercices pour découvrir l’approche bag-of-words\n\n\n\nNLP\n\n\nExercice\n\n\n\nCe chapitre continue de présenter l’approche de nettoyage de données\ndu NLP en s’appuyant sur le corpus de trois auteurs\nanglo-saxons : Mary Shelley, Edgar Allan Poe…\n\n\n\nLino Galiana\n\n\nOct 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLatent Dirichlet Allocation (LDA)\n\n\n\nTutoriel\n\n\nNLP\n\n\n\nLa Latent Dirichlet Allocation (LDA)\nest un modèle probabiliste génératif qui permet\nde décrire des…\n\n\n\nLino Galiana\n\n\nOct 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMéthodes de vectorisation : comptages et word embeddings\n\n\n\nTutoriel\n\n\nNLP\n\n\n\nPour pouvoir utiliser des données textuelles dans des algorithmes\nde machine learning, il faut les vectoriser, c’est à dire transformer\nle texte en données numériques.…\n\n\n\nLino Galiana\n\n\nOct 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercices supplémentaires\n\n\n\nExercice\n\n\nNLP\n\n\n\nDes exercices supplémentaires pour pratiquer les concepts du NLP\n\n\n\nLino Galiana\n\n\nApr 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPartie 4 : Natural Language Processing (NLP)\n\n\n\nIntroduction\n\n\nNLP\n\n\n\nL’un des grands avantages comparatifs de Python par rapport aux\nlangages concurrents (R notamment) est dans\nla richesse des librairies de Traitement du Langage Naturel…\n\n\n\nLino Galiana\n\n\nOct 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCorrections\n\n\nNotebooks corrigés des différents chapitres du cours\n\n\n\nLino Galiana\n\n\nSep 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluation\n\n\nRésumé des attentes pour les projets de fin d’année\n\n\n\nLino Galiana\n\n\nSep 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfiguration de Python\n\n\n\nTutoriel\n\n\nRappels\n\n\n\nL’environnement que propose Python pour la data science\nest très riche. Afin de bénéficier du meilleur environnement\npour tirer parti du langage, ce chapitre…\n\n\n\nLino Galiana\n\n\nJul 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL’environnement Python pour la data science\n\n\n\nTutoriel\n\n\nRappels\n\n\n\nPython propose un écosystème très riche pour la\ndata science. Ce chapitre fait un tour\nd’horizon de celui-ci en présentant les principaux\npackages qui seront présentés…\n\n\n\nLino Galiana\n\n\nJul 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComment aborder un jeu de données\n\n\n\nTutoriel\n\n\nRappels\n\n\n\nQuelques éléments pour adopter une démarche\nscientifique et éthique face à un\njeu de données.\n\n\n\nLino Galiana\n\n\nJul 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBonne pratique de Python\n\n\n\nTutoriel\n\n\nRappels\n\n\n\nLes normes communautaires du monde de\nl’open-source ont permis une\nharmonisation de la structure des projets\nPython et des scripts. Ce chapitre\névoque quelques-unes de…\n\n\n\nLino Galiana\n\n\nJul 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuelques rappels sur les principes de base de Python\n\n\n\nTutoriel\n\n\nRappels\n\n\n\nRappels d’éléments essentiels en Python: les règles de syntaxes, les classes,\nles méthodes, etc.\n\n\n\nLino Galiana\n\n\nJul 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModules, tests, boucles, fonctions\n\n\n\nTutoriel\n\n\nRappels\n\n\n\nLes fonctions permettent de généraliser des\ninstructions. Il s’agit ainsi d’un outil privilégié\npour automatiser des tâches répétitives ou réduire\nla complexité d’une chaîne…\n\n\n\nLino Galiana\n\n\nJul 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLes classes en Python\n\n\n\nTutoriel\n\n\nRappels\n\n\n\nLa programmation orientée objet (POO) est\nl’un des atouts de Python. Elle permet\nd’adapter des…\n\n\n\nLino Galiana\n\n\nJul 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\nCette introduction propose quelques éléments de\nrévision des concepts de base en Python et\nprésente l’écosystème Python que nous allons\ndécouvrir tout au long de ce…\n\n\n\nLino Galiana\n\n\nJun 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUn cadavre exquis pour découvrir Git\n\n\n\nExercice\n\n\nGit\n\n\n\nCe chapitre propose une mise en application de quelques principes\ncentraux du langage Git vus précédemment.\n\n\n\nLino Galiana\n\n\nSep 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGit: un outil nécessaire pour les data scientists\n\n\n\nGit\n\n\n\nUne partie annexe au cours pour découvrir Git,\nun outil\ndevenu indispensable pour les data scientists\nafin de mener des projets impliquant\ndu code Python.\n\n\n\nLino Galiana\n\n\nJul 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGit : un élément essentiel au quotidien\n\n\n\nTutoriel\n\n\nGit\n\n\n\nGit est un système de contrôle de version qui facilite la\nsauvegarde, la gestion des évolutions et le partage\nd’un projet informatique. Il s’agit d’un élément…\n\n\n\nLino Galiana\n\n\nSep 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumpy, la brique de base de la data science\n\n\n\nTutoriel\n\n\nManipulation\n\n\n\nNumpy constitue la brique de base de l’écosystème de la data science en\nPython. Toutes les librairies de manipulation de données, de modélisation\net de visualisation…\n\n\n\nLino Galiana\n\n\nJul 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction à Pandas\n\n\n\nTutoriel\n\n\nManipulation\n\n\n\nPandas est l’élément central de l’écosystème Python pour la data science.\nLe succès récent de Python dans l’analyse de données tient beaucoup à Pandas qui a permis…\n\n\n\nLino Galiana\n\n\nJul 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPratique de pandas : un exemple complet\n\n\n\nManipulation\n\n\nExercice\n\n\n\nAprès avoir présenté la logique de Pandas dans le chapitre précédent,\nce chapitre vise à illustrer les fonctionalités du package\nà partir de données d’émissions de gaz à…\n\n\n\nLino Galiana\n\n\nJul 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPratique de geopandas avec les données vélib\n\n\n\nManipulation\n\n\nExercice\n\n\n\nCe chapitre illustre les fonctionalités de GeoPandas à partir des\ndécomptes de vélo fournis par la ville de Paris\nen…\n\n\n\nLino Galiana\n\n\nJul 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDonnées spatiales : découverte de geopandas\n\n\n\nTutoriel\n\n\nManipulation\n\n\n\nLes données géolocalisées se sont multipliées depuis quelques années, qu’il\ns’agisse de données open-data ou de traces numériques géolocalisées de\ntype big-data. Pour les…\n\n\n\nLino Galiana\n\n\nJul 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeb scraping avec Python\n\n\n\nExercice\n\n\nManipulation\n\n\n\nPython permet de facilement récupérer une page web pour en extraire des\ndonnées à restructurer. Le web scraping, que les Canadiens nomment\n“moissonnage du web”, est…\n\n\n\nLino Galiana\n\n\nSep 8, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaîtriser les expressions régulières\n\n\n\nTutoriel\n\n\nManipulation\n\n\n\nLes expressions régulières fournissent un cadre très pratique pour manipuler\nde manière flexible des données textuelles. Elles sont très utiles\nnotamment pour les tâches de…\n\n\n\nLino Galiana\n\n\nJul 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRécupérer des données avec des API depuis Python\n\n\n\nExercice\n\n\nManipulation\n\n\n\nLes API (Application Programming Interface) sont un mode d’accès aux\ndonnées en expansion. Grâce aux API, l’automatisation de scripts\nest facilitée puisqu’il n’est…\n\n\n\nLino Galiana\n\n\nJul 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction à dask grâce aux données DVF\n\n\n\nTutoriel\n\n\nManipulation\n\n\n\n\n\n\n\nLino Galiana\n\n\nAug 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPartie 1: manipuler des données\n\n\n\nManipulation\n\n\nIntroduction\n\n\n\nPython s’est imposé comme une alternative très crédible à R dans\nla manipulation de données. L’écosystème Pandas a permis de démocratiser\nl’utilisation des DataFrames…\n\n\n\nLino Galiana\n\n\nJul 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPréparation des données pour construire un modèle\n\n\n\nModélisation\n\n\nExercice\n\n\n\nAfin d’avoir des données cohérentes avec les hypothèses de modélisation,\nil est absolument fondamental de prendre le temps de\npréparer les données à fournir à un modèle. La…\n\n\n\nLino Galiana\n\n\nJul 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluer la qualité d’un modèle\n\n\n\nModélisation\n\n\nExercice\n\n\n\nFaire preuve de méthode pour évaluer la qualité d’un modèle\npermet de proposer des prédictions plus robustes, ayant\nde meilleures performances sur un nouveau jeu de…\n\n\n\nLino Galiana\n\n\nJul 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification: premier modèle avec les SVM\n\n\n\nModélisation\n\n\nExercice\n\n\n\nLa classification permet d’attribuer une classe d’appartenance (label\ndans la terminologie du machine learning)\ndiscrète à des données à partir de certaines variables…\n\n\n\nLino Galiana\n\n\nJul 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRégression : une introduction\n\n\n\nModélisation\n\n\nExercice\n\n\n\nLa régression linéaire est la première modélisation statistique\nqu’on découvre dans un cursus quantitatif. Il s’agit en effet d’une\nméthode très intuitive et très riche. Le…\n\n\n\nLino Galiana\n\n\nJul 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSélection de variables : une introduction\n\n\n\nModélisation\n\n\nExercice\n\n\n\nL’accès à des bases de données de plus en plus riches permet\ndes modélisations de plus en plus raffinées. Cependant,\nles modèles parcimonieux sont généralement…\n\n\n\nLino Galiana\n\n\nJul 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustering\n\n\n\nModélisation\n\n\nExercice\n\n\n\nLe clustering consiste à répartir des observations dans des groupes,\ngénéralement non observés,\nen fonction de caractéristiques observables. Il s’agit d’une\napplication…\n\n\n\nLino Galiana\n\n\nJul 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPremier pas vers l’industrialisation avec les pipelines scikit\n\n\n\nModélisation\n\n\nTutoriel\n\n\n\nLes pipelines scikit permettent d’intégrer de manière très flexible\nun ensemble d’opérations de pre-processing et d’entraînement de modèles\ndans une chaîne d’opérations.…\n\n\n\nLino Galiana\n\n\nOct 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPartie 3: modéliser\n\n\n\nIntroduction\n\n\nModélisation\n\n\n\nLa facilité à modéliser des processus très diverses a grandement\nparticipé au succès de Python. La librairie scikit offre une\ngrande variété de modèles et permet ainsi…\n\n\n\nLino Galiana\n\n\nOct 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntégration continue avec Python\n\n\nUn chapitre plus avancé sur l’intégration continue\n\n\n\nLino Galiana\n\n\nJul 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGénération d’images avec Python, DALL-E et StableDiffusion\n\n\n\nTutoriel\n\n\nAvancé\n\n\n\nLa hype autour du\nmodèle de génération d’image Dall-E a amené\nune grande attention sur les modèles\nautogénératifs de contenu. Dall-E est, à l’heure\nactuelle, le modèle…\n\n\n\nLino Galiana\n\n\nAug 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApprofondissement ElasticSearch pour des recherches de proximité géographique\n\n\n\nTutoriel\n\n\nAvancé\n\n\n\nUn chapitre plus approfondi sur ElasticSearch\n\n\n\nLino Galiana\n\n\nSep 3, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction à ElasticSearch pour la recherche textuelle\n\n\n\nTutoriel\n\n\nAvancé\n\n\n\nElasticSearch est un moteur de recherche extrêmement rapide et flexible.\nCette technologie s’est imposée dans le domaine du traitement des\ndonnées textuelles. L’API…\n\n\n\nLino Galiana\n\n\nSep 3, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPartie 5: Introduction aux outils et méthodes à l’état de l’art\n\n\n\nIntroduction\n\n\nAvancé\n\n\n\nAprès avoir abordé les différents champs de la\ndata science, nous pouvons maintenant\nintroduire à quelques outils et méthodes plus avancés\nqui correspondent à des aspects…\n\n\n\nLino Galiana\n\n\nOct 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLes nouveaux modes d’accès aux données : le format parquet et les données sur le cloud\n\n\n\nTutoriel\n\n\nAvancé\n\n\n\n\n\n\n\nLino Galiana\n\n\nAug 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPartie 2: visualiser les données\n\n\n\nIntroduction\n\n\nVisualisation\n\n\n\nCette partie présente les outils pour visualiser des\ndonnées avec Python, qu’il s’agisse de graphiques\nfigés (matplotlib, seaborn, geoplot…) ou de\nvisualisation…\n\n\n\nLino Galiana\n\n\nJul 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDe belles cartes avec python : mise en pratique\n\n\n\nVisualisation\n\n\nExercice\n\n\n\nLa cartographie est un excellent moyen de diffuser\nune connaissance, y compris à des publics peu\nfamiliers de la statistique. Ce chapitre permet\nde découvrir la manière dont…\n\n\n\nLino Galiana\n\n\nJul 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDe beaux graphiques avec python : mise en pratique\n\n\n\nVisualisation\n\n\nExercice\n\n\n\nUne partie essentielle du travail du\ndata scientist est d’être en mesure\nde synthétiser une information dans des\nreprésentations graphiques percutantes. Ce\nchapitre permet…\n\n\n\nLino Galiana\n\n\nJul 14, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n Back to topCitationBibTeX citation:@book{galiana2023,\n  author = {Galiana, Lino},\n  title = {Python Pour La Data Science},\n  date = {2023},\n  url = {https://pythonds.linogaliana.fr/},\n  doi = {10.5281/zenodo.8229676},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGaliana, Lino. 2023. Python Pour La Data Science. https://doi.org/10.5281/zenodo.8229676."
  },
  {
    "objectID": "content/getting-started/index.html",
    "href": "content/getting-started/index.html",
    "title": "Introduction",
    "section": "",
    "text": "Ce cours rassemble l’ensemble du contenu du cours\nPython  pour la data science que je donne\nà l’ENSAE\ndepuis 2018.\nCe cours était auparavant donné par Xavier Dupré.\nQuelques éléments supplémentaires sont disponibles dans\nles slides d’introduction.\nDes éléments plus avancés sont présents dans un autre cours consacré\nà la mise en production de projets data science\nque je donne avec Romain Avouac\nà l’ENSAE (ensae-reproductibilite.github.io/website)\nPython est un langage qui a déjà plus de trente ans\nmais qui a connu, au cours de la décennie 2010, une\nnouvelle jeunesse du fait de l’engouement pour\nla data science.\nPython, plus que tout autre\nlangage informatique, réunit des communautés aussi\ndiverses que des statisticiens, des développeurs,\ndes gestionnaires\nd’applications ou d’infrastructures informatiques,\ndes lycées - Python est au programme du bac français\ndepuis quelques années - ou des chercheurs\ndans des champs à la fois théoriques et appliqués. Contrairement\nà beaucoup de langages informatiques qui fédèrent\nune communauté assez homogène, Python est parvenu à réunir\nlargement grâce à quelques principes centraux : la lisibilité\ndu langage, la simplicité à utiliser des modules,\nla simplicité à l’associer à des langages plus performants\npour certaines tâches données, l’énorme volume de documentation\ndisponible en ligne…\nÊtre le deuxième meilleur langage pour réaliser telle ou telle\ntâche\npeut ainsi être une source de succès lorsque la concurrence ne dispose\npas d’un éventail aussi large d’avantages.\nLe succès de Python, de par sa nature de\nlangage couteau-suisse, est indissociable\nde l’émergence du profil du data scientist, individu\ncapable de s’intégrer à différents niveaux dans la valorisation\nde données.\nDavenport and Patil (2012a), dans la Harvard Business Review,\nont ainsi pu parler du “boulot le plus sexy du 21e siècle”\net ont pu, dix ans plus tard, faire un panorama complet de l’évolution\ndes compétences attendues d’un data scientist dans\nla même revue (Davenport and Patil 2012b).\nLa richesse de Python permet de l’utiliser dans toutes les phases\ndu traitement de la donnée, de sa récupération et structuration à partir de\nsources diverses à sa valorisation.\nPar le prisme de la data science, nous verrons que Python est\nun très bon candidat pour assister les data scientists dans tous\nles aspects du travail de données.\nCe cours introduit différents outils qui permettent de mettre en relation\ndes données et des théories grâce à Python. Néanmoins, ce cours\nva au-delà d’une simple introduction au langage et propose\ndes éléments plus approfondis, notamment sur les dernières\ninnovations permises par la data science dans les méthodes de travail.\n\n\nLe succès de scikit-learn et\nde Tensorflow dans la communauté\nde la Data-Science ont beaucoup contribué à l’adoption de Python. Cependant,\nrésumer Python à ces quelques librairies serait réducteur tant il s’agit\nd’un véritable couteau-suisse pour les data scientists,\nles social scientists ou les économistes.\nL’intérêt de Python pour un data scientist ou data economist\nva au-delà du champ du Machine Learning.\nComme pour R, l’intérêt de Python est son rôle central dans un\nécosystème plus large autour d’outils puissants, flexibles et open-source.\nPython concurrence très bien R dans son domaine de prédilection, à\nsavoir l’analyse statistique sur des bases de données structurées.\nComme dans R, les dataframes sont un concept central de Python.\nPython est néanmoins bien plus complet dans certains domaines.\nOutre le Machine Learning,\nPython est mieux adapté aux données volumineuses que\nR. Python est également meilleur que R pour faire\ndu webscraping ou accéder à des données par le biais d’API.\nDans le domaine de l’économétrie, Python offre\nl’avantage de la simplicité avec un nombre restreint de packages (scikit et\nstatsmodels) permettant d’avoir des modèles très généraux\n(les generalized estimating equations)\nalors qu’il faut\nchoisir parmi une grande variété de packages en R pour obtenir les\nmodèles équivalents. Dans le domaine du Deep Learning, Python écrase\nla concurrence.\nAu contraire, dans certains domaines, R reste meilleur, même si les\névolutions très récentes de certains outils peuvent amener à réviser\nce constat. Historiquement,\nR était très bien intégré au langage de publication Markdown ce qui,\npermet la construction de documents reproductibles très raffinés.\nL’émergence récente de Quarto, héritier de R Markdown développé par\nla société Posit permet aux utilisateur de Python de bénéficier\négalement de la richesse de cette approche pour leur langage de prédilection.\nCe site web, à l’arborescence relativement complexe, est ainsi\nconstruit grâce à cet outil qui permet à la fois de tester les blocs\nde code présentés mais aussi de produire de manière automatisée les\ntableaux et graphiques présentés. S’il fallait trouver un point faible\nà Python par rapport à R dans le domaine de la data science\nc’est sur la production de graphiques. matplotlib et seaborn, qui sont\nprésentés dans la partie visualisation, sont d’excellents outils. Néanmoins,\nggplot2, l’équivalent en R est plus facile de prise en main et\npropose une syntaxe extrêmement flexible, qu’il est difficile de ne pas\napprécier. Cependant, l’écosystème de la\nvisualisation de données est en pleine révolution avec le succès\nd’Observable qui\nrapproche l’écosystème JavaScript des développeurs web\nde la communauté des analystes de données.\nUn des avantages comparatifs de Python par rapport à d’autres\nlangages (notamment R et Julia) est sa dynamique,\nce que montre l’explosion du nombre de questions\nsur Stack Overflow.\nCependant, il ne s’agit pas bêtement d’enterrer R.\nAu contraire, outre leur logique très proche,\nles deux langages sont dans une phase de convergence avec des initiatives comme\nreticulate,\nquarto ou\nsnakemake qui\npermettent, de manière différente, de créer des chaînes de traitement\nmélangeant R et Python.\nUne autre raison pour laquelle cette guéguerre R/Python n’a pas\nde sens est que les bonnes\npratiques peuvent être transposées de manière presque transparente d’un\nlangage à l’autre. Il s’agit d’un point qui est développé plus amplement\ndans le cours plus avancé que je donne avec Romain Avouac en dernière année\nd’ENSAE : ensae-reproductibilite.github.io/website.\nA terme, les data scientists et chercheurs en sciences sociales ou\néconomie utiliseront\nde manière presque indifférente, et en alternance, Python et R. Ce cours\nprésentera ainsi régulièrement des analogies avec R pour aider les\npersonnes découvrant Python, mais connaissant déjà bien R, à\nmieux comprendre certains messages.\n\n\n\nLe but de ce cours est de rendre autonome sur\nl’utilisation de Python\ndans un contexte de travail de data scientist ou de\nsocial scientist (économie, sociologie, géographie…).\nAutrement dit,\nil présuppose qu’on désire faire un usage intense\nde données dans un cadre statistique rigoureux.\nLa data science est un ensemble de techniques\nvisant à donner du sens à des sources de données\ndiverses. Selon les organisations,\nles data scientists peuvent ainsi être à\nl’interface de projets nécessitant un\nlarge spectre de compétences\n(analyse\nde données textuelles, représentation\ngraphique interactive…),\navoir des interactions avec des profils\ntrès différents (experts métiers,\ndéveloppeurs, data architect,\ndata engineer…) voire adopter\nun peu tous ces rôles.\nLes innovations\nrécentes de la data science ne se réduisent\nnéanmoins\npas qu’à des découvertes méthodologiques.\nLa data science propose un ensemble de\ntechniques et de méthodes de travail\npour réduire les coûts de passage\nd’un protype à une chaine\nde production pérenne.\nCe cours introduit à quelques notions\nsur le sujet, notamment les\npipelines scikit, pour adopter\ndès l’apprentissage du langage\nquelques bons réflexes (ensae-reproductibilite.github.io/website).\n\n\n\nCe cours ne revient que de manière secondaire\nsur les fondements statistiques ou algorithmiques\nderrière certaines des techniques évoquées.\nNe pas connaître ces notions n’empêche néanmoins pas de comprendre\nle contenu de ce site web. En effet, la facilité d’usage de Python\névite de devoir programmer soi-même un modèle, ce qui rend\npossible l’application\nde modèles dont on n’est pas expert. La connaissance des modèles sera\nplutôt nécessaire dans l’interprétation des résultats.\nCependant, la facilité avec laquelle il est possible de construire des modèles complexes\navec Python peut laisser apparaître que connaître les spécifités de chaque\nmodèle est inutile. Il\ns’agirait d’une grave erreur : même si l’implémentation de modèles est aisée, il\nest nécessaire de bien comprendre la structure des données et leur adéquation\navec les hypothèses d’un modèle.\n\n\n\nCe cours donne une place centrale à\nla notion de reproductibilité. Cette exigence se traduit de diverses\nmanières dans cet enseignement, en particulier en insistant sur un\noutil indispensable pour favoriser le partage de codes informatiques,\nà savoir Git.\nL’ensemble du contenu du site web est reproductible dans des environnements\ninformatiques divers. Il est bien sûr possible de copier-coller les morceaux\nde code présents dans ce site. Cette méthode montrant rapidement ses limites,\nle site présente un certain nombre de boutons disponibles pour\nouvrir la page sous un format Jupyter Notebook sur divers\npages web:\n\nSur l’ensemble du site web,\nil est possible de cliquer\nsur la petite icone \npour être redirigé vers le dépôt Github associé à ce cours.\nUn certain nombre de boutons permettent de transformer chaque\npage web en Jupyter Notebooks s’il est nécessaire de\nvisualiser ou exécuter du code Python.\n\nVoici, par exemple, ces boutons pour le tutoriel numpy :\n\n\n\n\n\n\n\n\n\n\n\nPour les agents de la fonction publique, ou\nles élèves des écoles partenaires, il est recommandé\nde privilégier le bouton SSPCloud qui est\nune infrastructure cloud moderne, puissante et flexible\ndéveloppée par l’Insee et accessible à l’url\nhttps://datalab.sspcloud.fr1.\nL’ensemble du contenu de ce site s’appuie sur des données\nouvertes, qu’il s’agisse de données françaises (principalement\nissues de la plateforme\ncentralisatrice data.gouv ou du site\nweb de l’Insee) ou de données\naméricaines. Les résultats sont donc reproductibles pour quelqu’un\ndisposant d’un environnement identique.\n\n\n\nCe cours présente\ndes tutoriels et des exercices complets.\nChaque page est structurée sous la forme\nd’un problème concret et présente la\ndémarche générique pour résoudre ce problème général.\nVous pouvez naviguer dans l’architecture du site via la table des matières\nou par les liens vers le contenu antérieur ou postérieur à la fin de chaque\npage. Certaines parties, notamment celle consacrée à la modélisation,\nproposent des exemples fil-rouge pour illustrer la démarche de manière\nplus extensive.\n\n\n\nLes élèves de l’ENSAE valident le cours grâce à\nun projet approfondi.\nLes éléments relatifs à l’évaluation du cours, ainsi qu’une\nliste des projets déjà effectués, sont disponibles dans la\nSection Evaluation.\n\n\n\n\n\nDavenport, Thomas H, and DJ Patil. 2012a. “Data Scientist, the Sexiest Job of the 21st Century.” Harvard Business Review 90 (5): 70–76. https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century.\n\n\n———. 2012b. “Is Data Scientist Still the Sexiest Job of the 21st Century?” Harvard Business Review 90 (5): 70–76. https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century."
  },
  {
    "objectID": "content/getting-started/index.html#pourquoi-faire-du-python-pour-lanalyse-de-données",
    "href": "content/getting-started/index.html#pourquoi-faire-du-python-pour-lanalyse-de-données",
    "title": "Introduction",
    "section": "",
    "text": "Le succès de scikit-learn et\nde Tensorflow dans la communauté\nde la Data-Science ont beaucoup contribué à l’adoption de Python. Cependant,\nrésumer Python à ces quelques librairies serait réducteur tant il s’agit\nd’un véritable couteau-suisse pour les data scientists,\nles social scientists ou les économistes.\nL’intérêt de Python pour un data scientist ou data economist\nva au-delà du champ du Machine Learning.\nComme pour R, l’intérêt de Python est son rôle central dans un\nécosystème plus large autour d’outils puissants, flexibles et open-source.\nPython concurrence très bien R dans son domaine de prédilection, à\nsavoir l’analyse statistique sur des bases de données structurées.\nComme dans R, les dataframes sont un concept central de Python.\nPython est néanmoins bien plus complet dans certains domaines.\nOutre le Machine Learning,\nPython est mieux adapté aux données volumineuses que\nR. Python est également meilleur que R pour faire\ndu webscraping ou accéder à des données par le biais d’API.\nDans le domaine de l’économétrie, Python offre\nl’avantage de la simplicité avec un nombre restreint de packages (scikit et\nstatsmodels) permettant d’avoir des modèles très généraux\n(les generalized estimating equations)\nalors qu’il faut\nchoisir parmi une grande variété de packages en R pour obtenir les\nmodèles équivalents. Dans le domaine du Deep Learning, Python écrase\nla concurrence.\nAu contraire, dans certains domaines, R reste meilleur, même si les\névolutions très récentes de certains outils peuvent amener à réviser\nce constat. Historiquement,\nR était très bien intégré au langage de publication Markdown ce qui,\npermet la construction de documents reproductibles très raffinés.\nL’émergence récente de Quarto, héritier de R Markdown développé par\nla société Posit permet aux utilisateur de Python de bénéficier\négalement de la richesse de cette approche pour leur langage de prédilection.\nCe site web, à l’arborescence relativement complexe, est ainsi\nconstruit grâce à cet outil qui permet à la fois de tester les blocs\nde code présentés mais aussi de produire de manière automatisée les\ntableaux et graphiques présentés. S’il fallait trouver un point faible\nà Python par rapport à R dans le domaine de la data science\nc’est sur la production de graphiques. matplotlib et seaborn, qui sont\nprésentés dans la partie visualisation, sont d’excellents outils. Néanmoins,\nggplot2, l’équivalent en R est plus facile de prise en main et\npropose une syntaxe extrêmement flexible, qu’il est difficile de ne pas\napprécier. Cependant, l’écosystème de la\nvisualisation de données est en pleine révolution avec le succès\nd’Observable qui\nrapproche l’écosystème JavaScript des développeurs web\nde la communauté des analystes de données.\nUn des avantages comparatifs de Python par rapport à d’autres\nlangages (notamment R et Julia) est sa dynamique,\nce que montre l’explosion du nombre de questions\nsur Stack Overflow.\nCependant, il ne s’agit pas bêtement d’enterrer R.\nAu contraire, outre leur logique très proche,\nles deux langages sont dans une phase de convergence avec des initiatives comme\nreticulate,\nquarto ou\nsnakemake qui\npermettent, de manière différente, de créer des chaînes de traitement\nmélangeant R et Python.\nUne autre raison pour laquelle cette guéguerre R/Python n’a pas\nde sens est que les bonnes\npratiques peuvent être transposées de manière presque transparente d’un\nlangage à l’autre. Il s’agit d’un point qui est développé plus amplement\ndans le cours plus avancé que je donne avec Romain Avouac en dernière année\nd’ENSAE : ensae-reproductibilite.github.io/website.\nA terme, les data scientists et chercheurs en sciences sociales ou\néconomie utiliseront\nde manière presque indifférente, et en alternance, Python et R. Ce cours\nprésentera ainsi régulièrement des analogies avec R pour aider les\npersonnes découvrant Python, mais connaissant déjà bien R, à\nmieux comprendre certains messages."
  },
  {
    "objectID": "content/getting-started/index.html#objectif-du-cours",
    "href": "content/getting-started/index.html#objectif-du-cours",
    "title": "Introduction",
    "section": "",
    "text": "Le but de ce cours est de rendre autonome sur\nl’utilisation de Python\ndans un contexte de travail de data scientist ou de\nsocial scientist (économie, sociologie, géographie…).\nAutrement dit,\nil présuppose qu’on désire faire un usage intense\nde données dans un cadre statistique rigoureux.\nLa data science est un ensemble de techniques\nvisant à donner du sens à des sources de données\ndiverses. Selon les organisations,\nles data scientists peuvent ainsi être à\nl’interface de projets nécessitant un\nlarge spectre de compétences\n(analyse\nde données textuelles, représentation\ngraphique interactive…),\navoir des interactions avec des profils\ntrès différents (experts métiers,\ndéveloppeurs, data architect,\ndata engineer…) voire adopter\nun peu tous ces rôles.\nLes innovations\nrécentes de la data science ne se réduisent\nnéanmoins\npas qu’à des découvertes méthodologiques.\nLa data science propose un ensemble de\ntechniques et de méthodes de travail\npour réduire les coûts de passage\nd’un protype à une chaine\nde production pérenne.\nCe cours introduit à quelques notions\nsur le sujet, notamment les\npipelines scikit, pour adopter\ndès l’apprentissage du langage\nquelques bons réflexes (ensae-reproductibilite.github.io/website)."
  },
  {
    "objectID": "content/getting-started/index.html#public-cible",
    "href": "content/getting-started/index.html#public-cible",
    "title": "Introduction",
    "section": "",
    "text": "Ce cours ne revient que de manière secondaire\nsur les fondements statistiques ou algorithmiques\nderrière certaines des techniques évoquées.\nNe pas connaître ces notions n’empêche néanmoins pas de comprendre\nle contenu de ce site web. En effet, la facilité d’usage de Python\névite de devoir programmer soi-même un modèle, ce qui rend\npossible l’application\nde modèles dont on n’est pas expert. La connaissance des modèles sera\nplutôt nécessaire dans l’interprétation des résultats.\nCependant, la facilité avec laquelle il est possible de construire des modèles complexes\navec Python peut laisser apparaître que connaître les spécifités de chaque\nmodèle est inutile. Il\ns’agirait d’une grave erreur : même si l’implémentation de modèles est aisée, il\nest nécessaire de bien comprendre la structure des données et leur adéquation\navec les hypothèses d’un modèle."
  },
  {
    "objectID": "content/getting-started/index.html#reproductibilité",
    "href": "content/getting-started/index.html#reproductibilité",
    "title": "Introduction",
    "section": "",
    "text": "Ce cours donne une place centrale à\nla notion de reproductibilité. Cette exigence se traduit de diverses\nmanières dans cet enseignement, en particulier en insistant sur un\noutil indispensable pour favoriser le partage de codes informatiques,\nà savoir Git.\nL’ensemble du contenu du site web est reproductible dans des environnements\ninformatiques divers. Il est bien sûr possible de copier-coller les morceaux\nde code présents dans ce site. Cette méthode montrant rapidement ses limites,\nle site présente un certain nombre de boutons disponibles pour\nouvrir la page sous un format Jupyter Notebook sur divers\npages web:\n\nSur l’ensemble du site web,\nil est possible de cliquer\nsur la petite icone \npour être redirigé vers le dépôt Github associé à ce cours.\nUn certain nombre de boutons permettent de transformer chaque\npage web en Jupyter Notebooks s’il est nécessaire de\nvisualiser ou exécuter du code Python.\n\nVoici, par exemple, ces boutons pour le tutoriel numpy :\n\n\n\n\n\n\n\n\n\n\n\nPour les agents de la fonction publique, ou\nles élèves des écoles partenaires, il est recommandé\nde privilégier le bouton SSPCloud qui est\nune infrastructure cloud moderne, puissante et flexible\ndéveloppée par l’Insee et accessible à l’url\nhttps://datalab.sspcloud.fr1.\nL’ensemble du contenu de ce site s’appuie sur des données\nouvertes, qu’il s’agisse de données françaises (principalement\nissues de la plateforme\ncentralisatrice data.gouv ou du site\nweb de l’Insee) ou de données\naméricaines. Les résultats sont donc reproductibles pour quelqu’un\ndisposant d’un environnement identique."
  },
  {
    "objectID": "content/getting-started/index.html#architecture-du-site-web",
    "href": "content/getting-started/index.html#architecture-du-site-web",
    "title": "Introduction",
    "section": "",
    "text": "Ce cours présente\ndes tutoriels et des exercices complets.\nChaque page est structurée sous la forme\nd’un problème concret et présente la\ndémarche générique pour résoudre ce problème général.\nVous pouvez naviguer dans l’architecture du site via la table des matières\nou par les liens vers le contenu antérieur ou postérieur à la fin de chaque\npage. Certaines parties, notamment celle consacrée à la modélisation,\nproposent des exemples fil-rouge pour illustrer la démarche de manière\nplus extensive."
  },
  {
    "objectID": "content/getting-started/index.html#evaluation",
    "href": "content/getting-started/index.html#evaluation",
    "title": "Introduction",
    "section": "",
    "text": "Les élèves de l’ENSAE valident le cours grâce à\nun projet approfondi.\nLes éléments relatifs à l’évaluation du cours, ainsi qu’une\nliste des projets déjà effectués, sont disponibles dans la\nSection Evaluation."
  },
  {
    "objectID": "content/getting-started/index.html#références",
    "href": "content/getting-started/index.html#références",
    "title": "Introduction",
    "section": "",
    "text": "Davenport, Thomas H, and DJ Patil. 2012a. “Data Scientist, the Sexiest Job of the 21st Century.” Harvard Business Review 90 (5): 70–76. https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century.\n\n\n———. 2012b. “Is Data Scientist Still the Sexiest Job of the 21st Century?” Harvard Business Review 90 (5): 70–76. https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century."
  },
  {
    "objectID": "content/getting-started/index.html#footnotes",
    "href": "content/getting-started/index.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPour les utilisateurs de cette infrastructure, les notebooks\nsont également listés, parmi de nombreuses autres\nressources de qualité, sur la\npage Formation↩︎"
  },
  {
    "objectID": "content/getting-started/02_DS_environment.html",
    "href": "content/getting-started/02_DS_environment.html",
    "title": "L’environnement Python pour la data science",
    "section": "",
    "text": "La richesse des langages open-source est la possibilité\nd’utiliser des packages\ndéveloppés par des spécialistes. Python est particulièrement\nbien doté dans le domaine. Pour caricaturer, on lit parfois\nque Python est le deuxième meilleur langage pour toutes les\ntâches, ce qui en fait le meilleur langage.\nEn effet, la malléabilité de Python fait qu’on peut\nl’aborder de manière très différentes\nselon que l’on est plutôt SysAdmin, développeur web ou\ndata scientist. C’est ce dernier profil qui va ici nous\nintéresser.\nLe data scientist devant disposer de nombreuses cordes\nà son arc. Cela se reflète sur l’écosystème de la data science\nqui est assez éclaté. Cependant, ce foisonnement\nn’est pas propre à Python puisque R propose encore plus de\npackages que Python où un certain nombre de framework\nnormalisés limitent l’éclatement de l’écosystème. De plus,\nle foisonnement de l’environnement du data scientist\nest une véritable opportunité puisqu’elle permet\naux packages de se spécialiser dans un\ndomaine, où ils sont plus efficaces, et aux concepteurs\nde package d’oser mettre en oeuvre de nouvelles méthodes,\nindispensables pour que le langage suive les évolutions\nrapides de la recherche ou de la technologie."
  },
  {
    "objectID": "content/getting-started/02_DS_environment.html#les-packages-python-essentiels-pour-le-cours-et-la-vie-des-data-scientists",
    "href": "content/getting-started/02_DS_environment.html#les-packages-python-essentiels-pour-le-cours-et-la-vie-des-data-scientists",
    "title": "L’environnement Python pour la data science",
    "section": "Les packages Python essentiels pour le cours et la vie des data scientists",
    "text": "Les packages Python essentiels pour le cours et la vie des data scientists\n\n\n\n\n\nCe\npost,\ndont l’image ci-dessus est tirée, résume la plupart des packages utiles\npour un data scientist ou un économiste/sociologue. Nous nous bornerons\nici à évoquer ceux utilisés quotidiennement.\n\nnumpy\nnumpy gère tout ce qui est calcul matriciel.\nLe langage Python est un des langages les plus lents qui soient1.\nTous les calculs rapides ne sont pas écrits en Python mais en C++, voire Fortran.\nC’est le cas du package numpy. Celui-ci est incontournable\ndès qu’on veut être rapide. Le package\nscipy est une extension où l’on peut trouver\ndes fonctions statistiques, d’optimisation.\nLa Cheat Sheet de numpy est pratique:\nhttps://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf\nComme numpy est la brique de base de l’analyse de données, un chapitre\nde ce cours lui est consacré.\n\n\npandas\nAvant tout, un bon data scientist doit être capable de\ns’approprier et manipuler des données rapidement. Pour cette raison,\npandas est incontournable.\nIl gère la plupart des formats de données. Pour être efficace,\nil est lui aussi implémenté en C++.\nLe package est rapide si on utilise les méthodes pré-implémentées sur\ndes données d’une taille raisonnable (par rapport à la RAM disponible). Il faut\nnéanmoins s’en méfier avec des données volumineuses.\nEn règle générale, un jeu de données nécessite\ntrois fois plus d’espace en mémoire que les\ndonnées n’en prennent sur le disque.\nLa Cheat Sheet de pandas :\nhttps://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_Pandas_Cheat_Sheet_2.pdf\npandas étant un élément incontournable, deux chapitres y sont consacrés.\n\n\nmatplotlib et seaborn\nmatplotlib existe depuis une vingtaine d’années pour doter Python de\nfonctionalités graphiques. Il s’agit d’un package très flexible, offrant\nde nombreuses fonctionalités. Néanmoins, ces dernières années,\nseaborn a émergé pour simplifier la création de certains graphiques\nstandards de l’analyse de données (histogrammes, diagramme en barre, etc. ).\nLe succès de seaborn n’éclipse néanmoins pas matplotlib puisque ce\ndernier est souvent nécessaire pour finaliser la customisation d’un\ngraphique produit par seaborn2\n\n\nscikit-learn\nscikit-learn est le module de machine learning le plus populaire pour\ntrois raisons:\n\nil s’appuie sur une API extrêmement consistante (méthodes fit, transform\net predict, respectivement pour apprendre des données, appliquer des transformations et prédire sur de nouvelles données) ;\nil permet de construire\ndes analyses reproductibles en construisant des pipelines de données ;\nsa documentation est un modèle à suivre.\n\nL’INRIA, institution française, est l’un des éléments moteurs dans\nla création et la maintenance de scikit-learn\n\n\nTensorFlow, PyTorch et Keras\nLes librairies essentielles pour implémenter et utiliser des modèles\nde deep learning en Python ont été développées par des acteurs du\nnumérique.\nTensorFlow est la librairie la plus mature, mais pas nécessairement la plus facile à prendre en main. D’ailleurs, Google semble l’abandonner en usage interne pour lui\npréférer JAX.\nKeras propose une interface high-level,\ndonc plus facile d’utilisation,\nmais qui n’en reste pas moins suffisante pour une grande variété d’usages.\nLa documentation de Keras est très bien faite.\nPyTorch est un framework plus récent mais très complet,\ndont la syntaxe plaira aux amateurs de programmation orienté-objet.\nDéveloppé par Facebook,\nil est très utilisé dans certains domaines de recherche, comme le NLP.\nIl s’agit du framework dont la dynamique récente a été la plus\nascensionnelle.\n\n\nstatsmodels\nstatsmodels plaira plus aux statisticiens, il implémente des modèles\néconométriques similaires à scikit-learn.\nPar rapport à scikit-learn,\nstatsmodels est plus orienté économétrie. La présentation des\nrésultats est très proche de ce qu’on trouve en R.\n\n\nrequests et beautifulsoup\nrequests est l’une des librairies de base de Python, dédiée\nà gérer la connexion avec internet. Les amateurs d’API\nseront des utilisateurs fréquents de celle-ci. Les\npersonnes plus spécialistes de web scraping l’utiliseront avec\nbeautifulsoup qui offre une syntaxe extrêmement puissante\npour récupérer automatiquement du contenu de pages web.\n\n\nnltk et spaCy\nDans le domaine du traitement automisé du langage, plus connu\nsous son acronyme anglais NLP, les deux packages phares sont\nnltk et spaCy.\nnltk est le package historique. Il existe depuis les années\n1990 et propose de nombreuses ressources utiles pour l’analyse\ntextuelle. Néanmoins, ces dernières années, spaCy est venu\nmoderniser l’approche en proposant une approche permettant\nde mieux intégrer les différentes phases du traitement de données\ntextuelles, une excellente documentation et un meilleur support\ndes langues non anglo-saxonnes, comme le Français.\nMais Python est également un outil privilégié pour communiquer:\n\nUne bonne intégration de Python à Markdown (grâce notamment à … R Markdown) qui facilite la construction de documents HTML ou PDF (via Latex)\nSphynx et JupyterBook proposent des modèles de documentation\ntrès complets\nbokeh ou streamlit comme alternative à shiny (R)\nDjango et Flask permettent de construire des applications web en Python\nLes librairies dynamiques, notamment\nfolium ou\nplotly, sont très appréciées pour construire des\nvisualisations dynamiques qui sont pratiques dans une analyse exploratoire\nmais également lorsqu’il faut valoriser ses travaux auprès de\npublics non experts de la donnée.\n\nL’un des nouveaux arrivants dans cet écosystème déjà riche\nest FastAPI). Avec ce package,\nil est très facile de transformer un code Python en API ce qui facilite\nla mise à disposition de données mais aussi de productions par Python (comme\nla mise à disposition d’une API pour permettre à des personnes de tester\nles résultats d’un modèle de machine learning).\nCe n’est qu’une petite partie de l’écosystème Python, d’une richesse rare."
  },
  {
    "objectID": "content/getting-started/02_DS_environment.html#environnement-autour-de-python",
    "href": "content/getting-started/02_DS_environment.html#environnement-autour-de-python",
    "title": "L’environnement Python pour la data science",
    "section": "Environnement autour de Python",
    "text": "Environnement autour de Python\nPython est un langage très riche, grâce à sa logique open-source. Mais l’un\ndes principaux intérêts réside dans le riche écosystème avec lequel Python\ns’intègre. On peut donner quelques éléments, dans un inventaire à la Prévert non exaustif.\nEn premier lieu, des éléments reliés au traitement des données :\n\nSpark,\nle framework dominant dans le domaine du traitement des big-data, très bien\ninterfacé avec Python (grâce à l’API pyspark), qui facilite le traitement des données volumineuses. Son utilisation nécessite cependant d’avoir accès à une\ninfrastructure de calculs distribuée.\nCython permet d’intégrer facilement du code C, très\nefficace avec Python (équivalent de Rcpp pour R).\nJulia est un langage récent, qui propose une syntaxe familière aux utilisateurs de languages scientifiques (Python, R, MATLAB), tout en permettant des performances proches du C grâce à une compilation à la volée.\n\nEnfin, des éléments permettant un déploiement de résultats ou d’applications\nen continu :\n* Les images Docker de Jupyterhub facilitent l’usage de l’intégration continue\npour construire des modules, les tester et déployer des site web.\n* Les services type Binder, Google Colab et Kaggle proposent des kernels\nPython"
  },
  {
    "objectID": "content/getting-started/02_DS_environment.html#rester-au-courant-des-évolutions",
    "href": "content/getting-started/02_DS_environment.html#rester-au-courant-des-évolutions",
    "title": "L’environnement Python pour la data science",
    "section": "Rester au courant des évolutions",
    "text": "Rester au courant des évolutions\nL’écosystème riche et foisonnant de Python a comme contrepartie\nqu’il faut rester attentif à ses évolutions pour ne pas\nvoir son capital humain vieillir et ainsi devenir has-been.\nAlors qu’avec des langages\nmonolithiques comme\nSAS ou Stata on pouvait se permettre de ne faire de vieille technique\nmais seulement consulter la documentation officielle, avec Python\nou R c’est impossible. Ce cours lui-même est en évolution continue, ce\nqui est assez exigeant :sweating:, pour épouser les évolutions\nde l’écosystème.\nTwitter est une excellente source d’information pour être rapidement\nau courant des évolutions du monde de la data science. Les agrégateurs\nde contenu comme medium ou towardsdatascience proposent des posts\nde qualité hétérogène mais il peut être utile de recevoir par mail\nle feed des nouveaux posts : au bout d’un certain temps, cela peut\npermettre de dégager les nouvelles tendances. Le site\nrealpython propose généralement de très bon posts, complets et\npédagogiques.\nEn ce qui concerne les ouvrages papiers, certains sont de très bonne qualité.\nCependant, il convient de faire attention à la date de mise à jour de ceux-ci :\nla vitesse d’évolution de certains éléments de l’écosystème peut les\npérimer très rapidement."
  },
  {
    "objectID": "content/getting-started/02_DS_environment.html#footnotes",
    "href": "content/getting-started/02_DS_environment.html#footnotes",
    "title": "L’environnement Python pour la data science",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPython est un langage interprété, comme R. Cela le rend très\nintelligible, y compris par un non-expert. C’est une des raisons de son\nsuccès. Le créateur de Python, Guido Van Rossum,\nen a fait un des principes philosophiques\nà l’origine de Python: un code est plus souvent lu qu’écrit.\nLa contrepartie est qu’il s’agit d’une surcouche à des langages\nplus bas-niveau, notamment C. Ces derniers proposent beaucoup moins de\nsurcouches. En réalité, les fonctions Python font appel, plus ou moins\ndirectement, à du C. Une manière d’optimiser le code est ainsi d’arriver,\navec le moins de surcouches possible, à la fonction C sous-jacente,\nbeaucoup plus rapide.↩︎\nLa situation est différente en R où ggplot2 a quasiment éclipsé\nl’outil de graphique de base de R.↩︎"
  },
  {
    "objectID": "content/getting-started/04_python_practice.html",
    "href": "content/getting-started/04_python_practice.html",
    "title": "Bonne pratique de Python",
    "section": "",
    "text": "Une référence utile à lire est le\nHitchhiker’s Guide to Python"
  },
  {
    "objectID": "content/getting-started/04_python_practice.html#structure-dun-projet-en-python",
    "href": "content/getting-started/04_python_practice.html#structure-dun-projet-en-python",
    "title": "Bonne pratique de Python",
    "section": "Structure d’un projet en python",
    "text": "Structure d’un projet en python\nLa structure basique d’un projet développé en Python est la suivante, qu’on peut retrouver dans\nce dépôt:\nREADME.md\nLICENSE\nsetup.py\nrequirements.txt\nmonmodule/__init__.py\nmonmodule/core.py\nmonmodule/helpers.py\ndocs/conf.py\ndocs/index.rst\ntests/context.py\ntests/test_basic.py\ntests/test_advanced.py\nQuelques explications et parallèles avec les packages R1 :\n\nLe code Python est stocké dans un module nommé monmodule. C’est le coeur du code dans le projet. Contrairement\nà R, il est possible d’avoir une arborescence avec plusieurs modules dans un seul package. Un bon exemple\nde package dont le fonctionnement adopte une arborescence à plusieurs niveaux est scikit\nLe fichier setup.py sert à construire le package monmodule pour en faire un code utilisable. Il n’est pas\nobligatoire quand le projet n’a pas vocation à être sur PyPi mais il est assez facile à créer en suivant ce\ntemplate. C’est l’équivalent\ndu fichier Description dans un package R\n(exemple)\nLe fichier requirements.txt permet de contrôler les dépendances du projet. Il s’agit des\ndépendances nécessaires pour faire tourner les fonctions (par exemple numpy), les tester et\nconstruire automatiquement la documentation (par exemple sphinx). Dans un package R, le fichier qui contrôle\nl’environnement est le NAMESPACE.\nLe dossier docs stocke la documentation du package. Le mieux est de le générer à partir de\nsphinx et non de l’éditer\nmanuellement. (cf. plus tard).\nLes éléments qui s’en rapprochent dans un package R sont les vignettes.\nLes tests génériques des fonctions. Ce n’est pas obligatoire mais c’est recommandé: ça évite de découvrir deux jours\navant un rendu de projet que la fonction ne produit pas le résultat espéré.\nLe README.md permet de créer une présentation du package qui s’affiche automatiquement sur\ngithub/gitlab et le fichier LICENSE vise à protéger la propriété intellectuelle. Un certain nombre de licences\nstandards existent et peuvent être utilisées comme template grâce au site https://choosealicense.com/\n\n2 La structure nécessaire des projets nécessaire pour pouvoir construire un package R est plus contrainte.\nLes packages devtools, usethis et testthat ont grandement facilité l’élaboration d’un package R. A cet égard,\nil est recommandé de lire l’incontournable livre d’Hadley Wickham"
  },
  {
    "objectID": "content/getting-started/04_python_practice.html#style-de-programmation-et-de-documentation",
    "href": "content/getting-started/04_python_practice.html#style-de-programmation-et-de-documentation",
    "title": "Bonne pratique de Python",
    "section": "Style de programmation et de documentation",
    "text": "Style de programmation et de documentation\n\nThe code is read much more often than it is written.\nGuido Van Rossum [créateur de Python]\n\nPython est un langage très lisible. Avec un peu d’effort sur le nom des objets, sur la gestion\ndes dépendances et sur la structure du programme, on peut\ntrès bien comprendre un script sans avoir besoin de l’exécuter. La communauté Python a abouti à un certain\nnombre de normes, dites PEP (Python Enhancement Proposal), qui constituent un standard\ndans l’écosystème Python. Les deux normes les plus connues sont\nla norme PEP8 (code) et la norme PEP257 (documentation).\nLa plupart de ces recommandations ne sont pas propres à Python, on les retrouve aussi dans R\n(cf. ici).\nOn retrouve de nombreux conseils dans cet ouvrage qu’il est\nrecommandé de suivre. La suite se concentrera sur des éléments complémentaires.\n\nImport des modules\nLes éléments suivants concernent plutôt les scripts finaux, qui appellent de multiples fonctions, que des\nscripts qui définissent des fonctions.\nUn module est un ensemble de fonctions stockées dans un fichier .py. Lorsqu’on écrit dans un script\nimport modu\nPython commence par chercher le fichier modu.py dans le dossier de travail. Il n’est donc pas une bonne\nidée d’appeler un fichier du nom d’un module standard de python, par exemple math.py ou os.py. Si le fichier\nmodu.py n’est pas trouvé dans le dossier de travail, Python va chercher dans le chemin et s’il ne le trouve pas\nretournera une erreur.\nUne fois que modu.py est trouvé, il sera exécuté dans un environnement isolé (relié de manière cohérente\naux dépendances renseignées) et le résultat rendu disponible à l’interpréteur Python pour un usage\ndans la session via le namespace (espace où Python associe les noms donnés aux objets).\nEn premier lieu, ne jamais utiliser la syntaxe suivante :\n# A NE PAS UTILISER\nfrom modu import *\nx = sqrt(4)  # Is sqrt part of modu? A builtin? Defined above?\nL’utilisation de la syntaxe import * créé une ambiguité sur les fonctions disponibles dans l’environnement. Le code\nest ainsi moins clair, moins compartimenté et ainsi moins robuste. La syntaxe à privilégier est la suivante :\nimport modu\nx = modu.sqrt(4)  # Is sqrt part of modu? A builtin? Defined above?"
  },
  {
    "objectID": "content/getting-started/04_python_practice.html#structuration-du-code",
    "href": "content/getting-started/04_python_practice.html#structuration-du-code",
    "title": "Bonne pratique de Python",
    "section": "Structuration du code",
    "text": "Structuration du code\nIl est commun de trouver sur internet des codes très longs, généralement dans un fichier __init__.py\n(méthode pour passer d’un module à un package, qui est un ensemble plus structuré de fonctions).\nContrairement à la légende, avoir des scripts longs est peu désirable et est même mauvais ;\ncela rend le code difficilement à s’approprier et à faire évoluer. Mieux vaut avoir des scripts relativement courts\n(sans l’être à l’excès…) qui font éventuellement appels à des fonctions définies dans d’autres scripts.\nPour la même raison, la multiplication de conditions logiques if…else if…else est généralement très mauvais\nsigne (on parle de code spaghetti) ; mieux vaut\nutiliser des méthodes génériques dans ce type de circonstances."
  },
  {
    "objectID": "content/getting-started/04_python_practice.html#écrire-des-fonctions",
    "href": "content/getting-started/04_python_practice.html#écrire-des-fonctions",
    "title": "Bonne pratique de Python",
    "section": "Écrire des fonctions",
    "text": "Écrire des fonctions\nLes fonctions sont un objet central en Python.\nLa fonction idéale est une fonction qui agit de manière compartimentée :\nelle prend un certain nombre d’inputs et est reliée au monde extérieur uniquement par les dépendances,\nelle effectue des opérations sans interaction avec le monde extérieur et retourne un résultat.\nCette définition assez consensuelle masque un certain nombre d’enjeux :\n\nUne bonne gestion des dépendances nécessite d’avoir appliqué les recommandations évoquées précédemment\nIsoler du monde extérieur nécessite de ne pas faire appel à un objet extérieur à l’environnement de la fonction.\nAutrement dit, aucun objet hors de la portée (scope) de la fonction ne doit être altéré ou utilisé.\n\nPar exemple, le script suivant est mauvais au sens où il utilise un objet y hors du scope de la fonction add\ndef add(x):\n    return x + y\nIl faudrait revoir la fonction pour y ajouter un élément y:\ndef add(x, y):\n    return x + y\nPycharm offre des outils de diagnostics très pratiques pour détecter et corriger ce type d’erreur.\n\n⚠️ aux arguments optionnels\nLa fonction la plus lisible (mais la plus contraignante) est celle\nqui utilise exclusivement des arguments positionnels avec des noms explicites.\nDans le cadre d’une utilisation avancée des fonctions (par exemple un gros modèle de microsimulation), il est\ndifficile d’anticiper tous les objets qui seront nécessaires à l’utilisateur. Dans ce cas, on retrouve généralement\ndans la définition d’une fonction le mot-clé **kwargs (équivalent du ... en R) qui capture les\narguments supplémentaires et les stocke sous forme de dictionnaire. Il s’agit d’une technique avancée de\nprogrammation qui est à utiliser avec parcimonie."
  },
  {
    "objectID": "content/getting-started/04_python_practice.html#documenter-les-fonctions",
    "href": "content/getting-started/04_python_practice.html#documenter-les-fonctions",
    "title": "Bonne pratique de Python",
    "section": "Documenter les fonctions",
    "text": "Documenter les fonctions\nLa documentation d’une fonction s’appelle le docstring. Elle prend la forme suivante :\ndef square_and_rooter(x):\n    \"\"\"Return the square root of self times self.\"\"\"\n    ...\nAvec PyCharm, lorsqu’on utilise trois guillemets sous la définition d’une fonction, un template minimal à\ncompleter est automatiquement généré. Les normes à suivre pour que la docstrings soit reconnue par le package\nsphinx sont présentées dans la PEP257. Néanmoins,\nelles ont été enrichies par le style de docstrings NumPy qui est plus riche et permet ainsi des documentations\nplus explicites\n(voir ici et\nici).\nSuivre ces canons formels permet une lecture simplifiée du code source de la documentation. Mais cela a surtout\nl’avantage, lors de la génération d’un package, de permettre une mise en forme automatique des fichiers\nhelp d’une fonction à partir de la docstrings. L’outil canonique pour ce type de construction automatique est\nsphinx (dont l’équivalent R est Roxygen)"
  },
  {
    "objectID": "content/getting-started/04_python_practice.html#les-tests",
    "href": "content/getting-started/04_python_practice.html#les-tests",
    "title": "Bonne pratique de Python",
    "section": "Les tests",
    "text": "Les tests\nTester ses fonctions peut apparaître formaliste mais c’est, en fait, souvent d’un grand secours car cela permet de\ndétecter et corriger des bugs précoces (ou au moins d’être conscient de leur existence).\nAu-delà de la correction de bug, cela permet de vérifier que\nla fonction produit bien un résultat espéré dans une expérience contrôlée.\nEn fait, il existe deux types de tests:\n\ntests unitaires : on teste seulement une fonctionalité ou propriété\ntests d’intégration : on teste l’intégration de la fonction dans un ensemble plus large de fonctionalités\n\nIci, on va plutôt se focaliser sur la notion de test unitaire, la notion de\ntest d’intégration nécessitant d’avoir une chaîne plus complète de fonctions (mais il ne faut\npas la négliger).\nOn peut partir du principe suivant :\n\ntoute fonctionnalité non testée comporte un bug\n\nLe fichier tests/context.py sert à définir le contexte dans lequel le test de la fonction s’exécute, de manière\nisolée. On peut adopter le modèle suivant, en changeant import monmodule par le nom de module adéquat\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nimport monmodule\nChaque fichier du dossier de test\n(par exemple test_basic.py et test_advanced.py) incorpore ensuite la ligne suivante,\nen début de script\nfrom .context import sample\nPour automatiser les tests, on peut utiliser le package unittest\n(doc ici). L’idée est que dans un cadre contrôlé\n(on connaît l’input et en tant que concepteur de la fonction on connaît l’output ou, a minima\nles propriétés de l’output) on peut tester la sortie d’une fonction.\nLa structure canonique de test est la suivante3\nimport unittest\n\ndef fun(x):\n    return x + 1\n\nclass MyTest(unittest.TestCase):\n    def test(self):\n        self.assertEqual(fun(3), 4)\n4 Le code équivalent avec R serait testthat::expect_equal(fun(3),4)\nParler de codecov"
  },
  {
    "objectID": "content/getting-started/04_python_practice.html#partager",
    "href": "content/getting-started/04_python_practice.html#partager",
    "title": "Bonne pratique de Python",
    "section": "Partager",
    "text": "Partager\nCe point est ici évoqué en dernier mais, en fait, il est essentiel et mérite d’être une réflexion prioritaire.\nTout travail n’a pas vocation à être public\nou à dépasser le cadre d’une équipe. Cependant, les mêmes exigences qui s’appliquent lorsqu’un code est public méritent\nde s’appliquer avec un projet personnel. Avant de partager un code avec d’autres, on le partage avec le “futur moi”.\nReprendre un code écrit il y a plusieurs semaines est coûteux et mérite d’anticiper en adoptant des bonnes pratiques qui\nrendront quasi-indolore la ré-appropriation du code.\nL’intégration d’un projet avec git fiabilise grandement le processus d’écriture du code mais aussi, grâce aux\noutils d’intégration continue, la production de contenu (par exemple des visualisations html ou des rapports\nfinaux écrits avec markdown). Il est recommandé d’immédiatement connecter un projet à git, même avec un\ndépôt qui aura vocation à être personnel. Les instructions d’utilisation de git sont détaillées ici."
  },
  {
    "objectID": "content/getting-started/04_python_practice.html#footnotes",
    "href": "content/getting-started/04_python_practice.html#footnotes",
    "title": "Bonne pratique de Python",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n1↩︎\n1:↩︎\n2↩︎\n2:↩︎"
  },
  {
    "objectID": "content/getting-started/06_rappels_fonctions.html#utilisation-dun-module-installé",
    "href": "content/getting-started/06_rappels_fonctions.html#utilisation-dun-module-installé",
    "title": "Modules, tests, boucles, fonctions",
    "section": "Utilisation d’un module installé",
    "text": "Utilisation d’un module installé\nPython, comme R, sont des langages\nconstruits sur le principe de briques.\nCes briques sont ce qu’on appelle des packages.\nAu contraire de Stata mais comme pour R,\nil\nfaut toujours préciser les packages que vous utilisez au début du code,\nsinon Python ne reconnait pas les fonctions appelées.\n\nImport module\nOn charge un module grâce à la commande import.\nPour chaque code que vous exécutez,\nil faut charger les modules en introduction.\nUne fois qu’on a chargé le module,\non peut faire appel aux commandes qui en dépendent en les appelant\naprès avoir tapé le nom du module.\nSi vous ne précisez pas le nom du module avant celui de la fonction,\nil ne la trouvera pas forcément.\nVoici un exemple avec le module numpy\nqui est très courant et permet de faire des\ncalculs matriciels sous Python.\n\nimport numpy\nprint(numpy.arange(5))\n\n[0 1 2 3 4]\n\n\n\n\nImport module as md - donner un nom au module\nOn peut aussi donner un pseudonyme au module pour\néviter de taper un nom trop long à chaque fois\nqu’on utilise une fonction.\nClassiquement le nom raccourci de numpy est np,\ncelui de pandas est pd.\n\nimport pandas as pd\nimport numpy as np\nsmall_array = np.array([[1, 2], [3, 4]])\ndata = pd.DataFrame(small_array)\ndata.head()\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n1\n2\n\n\n1\n3\n4\n\n\n\n\n\n\n\n\n\nfrom Module Import fonction - seulement une partie du module\nSi on ne veut pas être obligé de donner\nle nom du module avant d’appeler\nla fonction,\nil y a toujours la possibilité de n’importer qu’une fonction du module.\nDans le cas de l’exemple, Python sait que la fonction arrange est celle de numpy.\nMais attention : si deux fonctions de modules différents\nont le même nom,\nc’est toujours la dernière importée qui gagne.\nOn voit souvent from _module_ import *.\nC’est-à-dire qu’on importe toutes\nles fonctions du module\nmais on n’a pas besoin de spécifier le nom du module avant les méthodes.\n\n\n Warning\nLa méthode from _module_ import * n’est pas recommandée car elle rend le code moins intelligible.\nEn effet, d’où vient la fonction floor ? De maths ou de numpy ?\nElle risque\naussi de créer des conflits de fonction, qui malgré un nom commun peuvent ne\npas attendre les mêmes arguments ou objets.\n\n\n\nfrom numpy import array\nprint(array(5))\n\n5"
  },
  {
    "objectID": "content/getting-started/06_rappels_fonctions.html#les-tests",
    "href": "content/getting-started/06_rappels_fonctions.html#les-tests",
    "title": "Modules, tests, boucles, fonctions",
    "section": "Les tests",
    "text": "Les tests\nLes tests permettent d’exécuter telle ou telle instruction\nselon la valeur d’une condition.\nPour faire un test avec un bloc d’instructions, il faut toujours :\n\nque l’expression à vérifier soit suivie de :\nque le bloc d’instruction soit forcément indenté\n\n\nTest avec contrepartie : if et else\nComme dans les autres langages,\non teste une condition. Si elle est vérifiée,\nalors une instruction suit et sinon, une autre instruction est exécutée.\nIl est conseillé de toujours indiquer une contrepartie afin d’éviter les surprises.\n\nTest d’une égalité ou inégalité\n\nx = 6\n\nif x &gt; 5 :\n    print(\"x est plus grand que 5\")\nelse : # la contrepartie si la condition if n'est pas réalisée\n    print(\"x est plus petit que 5\")\n\nx est plus grand que 5\n\n\n\n\nTest dans un intervalle\n\n# on peut avoir des intervalles directement\nx = 6\nif 5 &lt; x &lt; 10 : \n    print(\"x est entre 5 et 10\")\nelse : \n    print(\"x est plus grand que 10\")\n\nx est entre 5 et 10\n\n\n\n# tester plusieurs valeurs avec l'opérateur logique \"or\"\nx = 5\nif x == 5 or x == 10 : \n    print(\"x vaut 5 ou 10\")    \nelse : \n    print(\"x est différent de 5 et 10\")\n\nx vaut 5 ou 10\n\n\n\n\n\nTests successifs : if, elif et else\nAvec if et elif,\ndès qu’on rencontre une condition qui est réalisée,\non n’en cherche pas d’autres potentiellement vérifiées.\nPlusieurs if à la suite peuvent quant à eux être vérifiés.\nSuivant ce que vous souhaitez faire, les opérateurs ne sont pas substituables.\nNotez la différence entre ces deux bouts de code :\n\n#code 1\nx = 5\n\nif x != 10 : \n    print(\"x ne vaut pas 10\")   \nelif x &gt;= 5 : \n    print(\"x est égal ou supérieur à 5\")\n\nx ne vaut pas 10\n\n\nDans le cas de elif, on s’arrête à la première condition vérifiée et dans le cas suivant, on continue à chaque condition vérifiée\n\n#code 2\nx = 5\n\nif x != 10 : \n    print(\"x ne vaut pas 10\")   \nif x &gt;= 5 : \n    print(\"x est égal ou supérieur à 5\")\n\nx ne vaut pas 10\nx est égal ou supérieur à 5"
  },
  {
    "objectID": "content/getting-started/06_rappels_fonctions.html#boucles",
    "href": "content/getting-started/06_rappels_fonctions.html#boucles",
    "title": "Modules, tests, boucles, fonctions",
    "section": "Boucles",
    "text": "Boucles\nIl existe deux types de boucles : les boucles for et les boucles while\nLa boucle for parcourt un ensemble, tandis que la boucle while\ncontinue tant qu’une condition est vraie.\n\nBoucle for\n\nParcourir une liste croissantes d’entiers\n\n# parcourt les entiers de 0 à n-1 inclus\nfor i in range(0,3) : \n    print(i)\n\n0\n1\n2\n\n\n\n\nParcourir une liste décroissante d’entiers\n\n# parcourt les entiers de 3 à n+1 inclus\nfor i in range(3,0,-1) : \n    print(i)\n\n3\n2\n1\n\n\n\n\nParcourir une liste de chaines de caractères\nOn va faire une boucle sur les éléments d’une liste\n\n\nBoucle sur les éléments d’une liste\n\nliste_elements = ['Nicolas','Romain','Florimond']\n\n# pour avoir l'ensemble des éléments de la liste\nfor item in liste_elements : \n    print(item)\n\nNicolas\nRomain\nFlorimond\n\n\n\n\nBoucle sur les éléments d’une liste dans une autre liste\n\n# pour avoir la place des éléments de la première liste dans la seconde liste  \n\nliste_globale = ['Violette','Nicolas','Mathilde','Romain','Florimond','Helene'] \n\nfor item in liste_elements : \n    print(item,liste_globale.index(item))\n\nNicolas 1\nRomain 3\nFlorimond 4\n\n\n\n\n\nBonus : les list comprehension\nAvec les listes, il existe aussi un moyen très élégant de condenser son code pour éviter de faire apparaitre des boucles sans arrêt. Comme les boucles doivent etre indentées, le code peut rapidement devenir illisible.\nGrace aux list comprehension, vous pouvez en une ligne faire ce qu’une boucle vous permettait de faire en 3 lignes.\nPar exemple, imaginez que vous vouliez faire la liste de toutes les lettres contenues dans un mot, avec un boucle vous devrez d’abord créer une liste vide, puis ajouter à cette liste toutes les lettres en question avec un .append()\n\nliste_lettres = []\n\nfor lettre in 'ENSAE':\n    liste_lettres.append(lettre)\n\nprint(liste_lettres)\n\n['E', 'N', 'S', 'A', 'E']\n\n\navec une list comprehension, on condense la syntaxe de la manière suivante :\n\nh_letters = [ letter for letter in 'ENSAE' ]\nprint(h_letters)\n\n['E', 'N', 'S', 'A', 'E']\n\n\nAvec une list comprehension\n[ expression for item in list if conditional ]\nest équivalent à\nfor item in list:\n    if conditional:\n        expression  \n\nMise en application\nMettez sous forme de list comprehension le bout de code suivant\n\nsquares = []\n\nfor x in range(10):\n    squares.append(x**2)\nprint(squares)\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\n\n\n\n\nBoucle while\nLe bloc d’instruction d’une boucle while est exécuté tant que la condition est vérifiée.\nLe piège de ces boucles : la boucle while infinie ! Il faut toujours vérifier que votre boucle s’arrêtera un jour, il faut qu’à un moment ou à un autre, il y ait un élément qui s’incrémente ou qui soit modifié.\n\nx = 10\ny = 8\n# tant que y est plus petit que 10, je continue de lui ajouter 1\nwhile y &lt;= x : \n    print(\"y n'est pas encore plus grand que x\")\n    y += 1 # l'incrément\nelse : \n    print(\"y est plus grand que x et vaut\",y)\n\ny n'est pas encore plus grand que x\ny n'est pas encore plus grand que x\ny n'est pas encore plus grand que x\ny est plus grand que x et vaut 11\n\n\n\n\nBreak et continue\nDans les boucles for ou while on peut avoir besoin d’ignorer ou de ne pas effectuer certaines itérations. 2 instructions utiles :\n\nl’instruction break : permet de sortir de la boucle\nl’instruction continue : permet de passer à l’itération suivante sans exécuter les instructions qui suivent\n\n\n# utilisation de break\nfor x in range(5) : \n    if x == 2 : \n        break\n    else :\n        print(x)\n\n0\n1\n\n\n\n# utilisation de continue\nfor x in range(5) : \n    if x == 2 : \n        continue\n    else :\n        print(x)\n\n0\n1\n3\n4"
  },
  {
    "objectID": "content/getting-started/06_rappels_fonctions.html#créer-ses-fonctions",
    "href": "content/getting-started/06_rappels_fonctions.html#créer-ses-fonctions",
    "title": "Modules, tests, boucles, fonctions",
    "section": "Créer ses fonctions",
    "text": "Créer ses fonctions\nLes fonctions permettent de faire la même chose sans avoir à recopier le code plusieurs fois dans le même programme. Dès que vous le pouvez, faites des fonctions : le copier-coller est trop dangereux.\n\nElles peuvent prendre plusieurs paramètres (ou aucun) elles peuvent retourner plusieurs résultats (ou aucun)\nPour mettre une aide à la fonction, on écrit au début entre ““” ““” (en rouge dans l’exemple)\n\n\n# 1er exemple de fonction\n\ndef ma_fonction_increment(parametre) : \n    \"\"\"Cette fonction ajoute 1 au paramètre qu'on lui donne\"\"\"\n    x = parametre + 1 \n    return x\n\n# pour documenter la fonction, on écrit son aide\nhelp(ma_fonction_increment)\n\nHelp on function ma_fonction_increment in module __main__:\n\nma_fonction_increment(parametre)\n    Cette fonction ajoute 1 au paramètre qu'on lui donne\n\n\n\nOn peut également :\n\navoir des paramètres facultatifs, mais ils doivent toujours être placés à la fin des paramètres\nen cas de paramètre facultatif, il faut lui donner une valeur par défaut\nretourner plus d’un élément à la fin d’une fonction\navoir des paramètres de tailles différentes\n\n\ndef ma_fonction(p,q = 2) :\n    y1 = p + q\n    y2 = y1%3 #reste de la division euclidienne\n    return y1,y2\n\nx = ma_fonction(11) \n# ici, on n'a pas de 2nd paramètre\n#, par défaut, x = ma_fonction(10,2)\nprint(\"x=\", x)\n\nz = ma_fonction(10,-1)\nprint(\"z =\",z)\n\nx= (13, 1)\nz = (9, 0)\n\n\nUne fonction peut également s’appeler elle même : c’est ce qu’on appelle une fonction récursive.\nDans cet exemple, somme_recursion() est une fonction que nous avons définie de sorte à ce qu’elle s’appelle elle-même (récursif).\nOn utilise l’argument k, qui décroit (-1) chaque fois qu’on fait appel à la fonction.\nLa récursion s’arrête quand k est nul.\nDans cet exemple, on va donc appeler 6 fois la fonction récursive.\n\ndef somme_recursion(k):\n    if(k &gt; 0):\n        result = k + somme_recursion(k - 1)\n        print(k,result)\n    else:\n        result = 0\n    return result\nsomme_recursion(6)\n\n1 1\n2 3\n3 6\n4 10\n5 15\n6 21\n\n\n21\n\n\nLes fonctions sont très utiles et nous vous invitons à les utiliser dès que vous le pouvez car elles permettent d’avoir un code clair et structuré, plutôt que des bouts de code éparpillés."
  },
  {
    "objectID": "content/getting-started/06_rappels_fonctions.html#lever-des-exceptions",
    "href": "content/getting-started/06_rappels_fonctions.html#lever-des-exceptions",
    "title": "Modules, tests, boucles, fonctions",
    "section": "Lever des exceptions",
    "text": "Lever des exceptions\nPython peut rencontrer des erreurs en exécutant votre programme.\nCes erreurs peuvent être interceptées très facilement et c’est même, dans certains cas, indispensable. Par exemple, si vous voulez faire une boucle mais que vous savez que l’instruction ne marchera pas toujours : au lieu de lister les cas où une opération n’est pas possible, on peut indiquer directement quelle erreur doit être ignorée.\nCependant, il ne faut pas tout intercepter non plus : si Python envoie une erreur, c’est qu’il y a une raison. Si vous ignorez une erreur, vous risquez d’avoir des résultats très étranges dans votre programme.\n\n# éviter une division par 0, c'est une bonne idée : \n\ndef inverse(x) :\n    '''Cette fonction renvoie l inverse de l argument'''\n    y = 1/x\n    return y\n\ndiv = inverse(0)\n\nZeroDivisionError: division by zero\n\n\nL’erreur est écrite noir sur blanc : ZeroDivisionError\nDans l’idéal on aimerait éviter que notre code bloque sur ce problème. On pourrait passer par un test if et vérifier que x est différent de 0. Mais on se rend vite compte que dans certains cas, on ne peut lister tous les tests en fonction de valeurs.\nAlors on va lui précisier ce qu’il doit faire en fonction de l’erreur retournée.\nSyntaxe\nTry :\ninstruction\nexcept TypeErreur :\nautre instruction\n\n\ndef inverse(x) : \n    try :\n        y = 1/x\n    except ZeroDivisionError :\n        y = None\n    return y\n    \nprint(inverse(10))\nprint(inverse(0))\n\n0.1\nNone\n\n\nIl est recommandé de toujours préciser le type d’erreur qu’on rencontre. Si on met uniquement “except” sans préciser le type, on peut passer à côté d’erreurs pour lesquelles la solution n’est pas universelle."
  },
  {
    "objectID": "content/getting-started/06_rappels_fonctions.html#a-retenir-et-questions",
    "href": "content/getting-started/06_rappels_fonctions.html#a-retenir-et-questions",
    "title": "Modules, tests, boucles, fonctions",
    "section": "A retenir et questions",
    "text": "A retenir et questions\n\nA retenir\n\nToujours mettre “:” avant un bloc d’instructions\nIndenter avant un bloc d’instructions (avec 4 espaces et non une tabulation !)\nIndiquer les modules nécessaires à l’exécution en début de code\nDocumenter les fonctions créées\nPréciser le type d’erreur pour les exceptions et potentiellement différencier les blocs d’instructions en fonction de l’erreur\n\n\n\nQuestions\n\nQue fait ce programme ?\n\n\ndef inverse(x) : \n    try :\n        y = 1/x\n    except ZeroDivisionError :\n        y = None\n        return y\n\n\nEcrivez un programme qui peut trouver tous les nombres divisibles par 7 et non multiples de 5 entre 6523 et 8463 (inclus)\nEcrivez un programme qui prend une phrase en entrée et qui calcule le nombre de voyelles en Majuscules et de consonnes en minuscules :\n\nphrase = “Vous savez, moi je ne crois pas qu’il y ait de bonne ou de mauvaise situation. Moi, si je devais résumer ma vie aujourd’hui avec vous, je dirais que c’est d’abord des rencontres. Des gens qui m’ont tendu la main, peut-être à un moment où je ne pouvais pas, où j’étais seul chez moi. Et c’est assez curieux de se dire que les hasards, les rencontres forgent une destinée… Parce que quand on a le goût de la chose, quand on a le goût de la chose bien faite, le beau geste, parfois on ne trouve pas l’interlocuteur en face je dirais, le miroir qui vous aide à avancer. Alors ça n’est pas mon cas, comme je disais là, puisque moi au contraire, j’ai pu : et je dis merci à la vie, je lui dis merci, je chante la vie, je danse la vie… je ne suis qu’amour ! Et finalement, quand beaucoup de gens aujourd’hui me disent ‘Mais comment fais-tu pour avoir cette humanité ?’, et bien je leur réponds très simplement, je leur dis que c’est ce goût de l’amour ce goût donc qui m’a poussé aujourd’hui à entreprendre une construction mécanique, mais demain qui sait ? Peut-être simplement à me mettre au service de la communauté, à faire le don, le don de soi…”"
  },
  {
    "objectID": "content/manipulation/index.html",
    "href": "content/manipulation/index.html",
    "title": "Partie 1: manipuler des données",
    "section": "",
    "text": "Si on associe souvent les data scientists à la mise en oeuvre\nde modèles d’intelligence artificielle, il est important\nde ne pas oublier que l’entraînement et l’utilisation\nde ces modèles ne représente pas\nforcément le quotidien des data scientists.\nEn pratique,\nla récupération de sources de données hétérogènes, la structuration\net harmonisation de celles-ci en vue d’une analyse exploratoire\npréalable à la modélisation ou la visualisation\nreprésente une part importante du travail des data scientists.\nDans de nombreux environnements c’est même l’essence du travail\ndu data scientist.\nL’élaboration de modèles pertinents requiert en effet une réflexion approfondie sur les données ;\nune étape que l’on ne saurait négliger.\nCe cours,\ncomme de nombreuses ressources introductives sur\nla data science (Wickham, Çetinkaya-Rundel, and Grolemund 2023; VanderPlas 2016; McKinney 2012),\nproposera donc beaucoup d’éléments sur la manipulation de données, compétence\nessentielle pour les data scientists.\nLes logiciels de programmation\norientés autour du concept de base de données\nsont devenus les outils principaux des data scientists.\nLe fait de pouvoir appliquer un certain nombre d’opérations\nstandards sur des bases de données, quelle que soit leur nature,\npermet aux programmeurs d’être plus efficaces que s’ils devaient\nrépéter ces opérations à la main, comme dans Excel.\nTous les langages de programmation dominants dans l’écosystème\nde la data science reposent sur le principe du dataframe.\nIl s’agit même d’un objet central dans certains logiciels,\nnotamment R.\nLa logique SQL,\nun langage de déclaration d’opérations sur des données qui a déjà plus de cinquante ans,\noffre un cadre pertinent pour effectuer des opérations standardisées\nsur les colonnes (création de nouvelles colonnes, sélection de sous-ensemble de lignes…).\nNéanmoins, le dataframe ne s’est imposé que récemment en Python,\ngrâce au package Pandas créé\npar Wes McKinney.\nL’essor de la librairie Pandas (téléchargée plus de 5 millions de fois\npar jour en 2023) est pour beaucoup dans le succès de Python\ndans l’écosystème de la data science et a amené, en quelques années,\na un renouvellement complet de la manière de coder en Python, ce\nlangage si malléable, autour de l’analyse de données.\nCette partie du cours est une introduction\ngénérale à l’écosystème très riche de\nla manipulation de données avec Python.\nCes chapitres évoquent aussi bien la récupération de données\nque la restructuration et la production d’analyse\nà partir de celles-ci."
  },
  {
    "objectID": "content/manipulation/index.html#résumé-de-cette-partie",
    "href": "content/manipulation/index.html#résumé-de-cette-partie",
    "title": "Partie 1: manipuler des données",
    "section": "Résumé de cette partie",
    "text": "Résumé de cette partie\nPandas est devenu incontournable dans l’écosystème Python pour la data science.\nPandas est lui-même construit à partir du package Numpy, qu’il est utile de comprendre\npour être à l’aise avec Pandas. Numpy est une librairie bas-niveau\npour stocker et manipuler des données.\nNumpy est au coeur de l’écosystème de la data science car la plupart des librairies, même celles\nqui manient des objets destructurés,\nutilisent des objets construits à partir de Numpy1.\nL’approche Pandas, qui offre un point d’entrée harmonisé pour manipuler\ndes jeux de données de nature très différente,\na été étendue aux objets géographiques avec Geopandas.\nIl est ainsi possible de manipuler des données géographiques comme s’il\ns’agissait de données structurées classiques. Les données géographiques et\nla représentation cartographique deviennent de plus en plus commun avec\nla multiplication de données ouvertes localisées et de big-data géolocalisées.\nCependant, les données structurées, importées depuis des fichiers plats\nne représentent pas l’unique source de données. Les API et le webscraping\npermettent de télécharger ou d’extraire\ndes données de manière très flexible depuis des pages web ou des guichets\nspécialisés. Ces données, notamment\ncelles obtenues par webscraping nécessitent souvent un peu plus de travail\nde nettoyage de données, notamment des chaînes de caractère.\nL’écosystème Pandas représente donc un couteau-suisse\npour l’analyse de données. C’est pour cette raison que ce cours\ndéveloppera beaucoup de contenu dessus.\nAvant d’essayer de mettre en oeuvre une solution ad hoc, il est\nsouvent utile de se poser la question suivante : “ne pourrais-je pas le faire\navec les fonctionalités de base de Pandas ?” Se poser cette question peut\néviter des chemins ardus et faire économiser beaucoup de temps.\nNéanmoins, Pandas n’est pas\nadapté à des données ayant une volumétrie\nimportante. Pour traiter de telles\ndonnées, il est plutôt recommander\nde privilégier Polars ou Dask qui reprennent la logique de Pandas mais\noptimisent son fonctionnement, Spark si on a une infrastructure adaptée, généralement dans\ndes environnements big data, ou\nDuckDB si on est prêt à utiliser des requêtes SQL plutôt qu’une librairie haut-niveau."
  },
  {
    "objectID": "content/manipulation/index.html#exercices",
    "href": "content/manipulation/index.html#exercices",
    "title": "Partie 1: manipuler des données",
    "section": "Exercices",
    "text": "Exercices\nCette partie présente à la fois des tutoriels détaillés\net des exercices guidés.\nIl est\npossible de les consulter sur ce site ou d’utiliser l’un des\nbadges présents en début de chapitre, par exemple\nceux-ci pour ouvrir\nle chapitre d’exercices sur Pandas:"
  },
  {
    "objectID": "content/manipulation/index.html#pour-aller-plus-loin",
    "href": "content/manipulation/index.html#pour-aller-plus-loin",
    "title": "Partie 1: manipuler des données",
    "section": "Pour aller plus loin",
    "text": "Pour aller plus loin\nCe cours n’aborde pas vraiment les questions de volumétrie ou de vitesse de\ncalcul.\nPandas peut montrer ses limites dans ce domaine sur des jeux de données\nd’une volumétrie conséquente (plusieurs Gigas).\nIl est ainsi intéressant de porter attention à:\n\nLe livre Modern Pandas\npour obtenir des éléments supplémentaires sur la question de la performance\navec Pandas ;\nLa question des\nobjets sparse ;\nLes packages Dask ou Polars pour accélérer les calculs ;\nDuckDB pour effectuer de manière très efficace des requêtes SQL ;\nPySpark pour des données très volumineuses.\n\n\nRéférences\nVoici une bibliographie sélective des ouvrages\nintéressants en complément des chapitres de la partie “Manipulation” de ce cours :\n\n\nMcKinney, Wes. 2012. Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython. \" O’Reilly Media, Inc.\".\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook: Essential Tools for Working with Data. \" O’Reilly Media, Inc.\".\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O’Reilly Media, Inc.\"."
  },
  {
    "objectID": "content/manipulation/index.html#footnotes",
    "href": "content/manipulation/index.html#footnotes",
    "title": "Partie 1: manipuler des données",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCertaines librairies commencent, petit à petit, à s’émanciper\nde Numpy qui n’est pas toujours le plus adapté pour la gestion\nde certains types de données. Le framework Arrow tend à devenir\nla couche basse utilisée par de plus en plus de librairies de data science.\nCe post de blog approfondit\nde manière très pédagogique ce sujet.↩︎"
  },
  {
    "objectID": "content/manipulation/02a_pandas_tutorial.html",
    "href": "content/manipulation/02a_pandas_tutorial.html",
    "title": "Introduction à Pandas",
    "section": "",
    "text": "Pour essayer les exemples présents dans ce tutoriel :\nLe package Pandas est l’une des briques centrales de l’écosystème de\nla data science. Le DataFrame,\nobjet central dans des langages comme R\nou Stata, a longtemps était un grand absent dans l’écosystème Python.\nPourtant, grâce à Numpy, toutes les briques de base étaient présentes.\nWes McKinney, lorsqu’il a créé Pandas en s’appuyant sur Numpy\na ainsi introduit cet objet devenu banal qu’est le DataFrame.\nPandas est rapidement\ndevenu un incontournable de la data-science. L’ouvrage\nde référence de McKinney (2012) présente de manière plus\nample ce package.\nCe tutoriel vise à introduire aux concepts\nde base de ce package par l’exemple et à introduire à certaines\ndes tâches les plus fréquentes de (re)structuration\ndes données du data scientist. Il ne s’agit pas d’un ensemble\nexhaustif de commandes : Pandas est un package tentaculaire\nqui permet de réaliser la même opération de nombreuses manières.\nNous nous concentrerons ainsi sur les éléments les plus pertinents\ndans le cadre d’une introduction à la data science et laisserons\nles utilisateurs intéressés approfondir leurs connaissances\ndans les ressources foisonnantes qu’il existe sur le sujet.\nDans ce tutoriel Pandas, nous allons utiliser :\nLe chapitre suivant permettra de mettre en application des éléments présents dans ce chapitre avec\nles données ci-dessus associées à des données de contexte au niveau communal.\nNous suivrons les conventions habituelles dans l’import des packages :\n!pip install pynsee\n\nRequirement already satisfied: pynsee in /opt/mamba/lib/python3.9/site-packages (0.1.4)\nRequirement already satisfied: pandas&gt;=0.24.2 in /opt/mamba/lib/python3.9/site-packages (from pynsee) (2.1.1)\nRequirement already satisfied: tqdm&gt;=4.56.0 in /opt/mamba/lib/python3.9/site-packages (from pynsee) (4.66.1)\nRequirement already satisfied: requests&gt;=2.23 in /opt/mamba/lib/python3.9/site-packages (from pynsee) (2.31.0)\nRequirement already satisfied: appdirs&gt;=1.4.4 in /opt/mamba/lib/python3.9/site-packages (from pynsee) (1.4.4)\nRequirement already satisfied: unidecode&gt;=1.1.0 in /opt/mamba/lib/python3.9/site-packages (from pynsee) (1.3.6)\nRequirement already satisfied: shapely&gt;=2.0.0 in /opt/mamba/lib/python3.9/site-packages (from pynsee) (2.0.1)\nRequirement already satisfied: urllib3 in /opt/mamba/lib/python3.9/site-packages (from pynsee) (1.26.11)\nRequirement already satisfied: numpy&gt;=1.22.4 in /opt/mamba/lib/python3.9/site-packages (from pandas&gt;=0.24.2-&gt;pynsee) (1.26.0)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /opt/mamba/lib/python3.9/site-packages (from pandas&gt;=0.24.2-&gt;pynsee) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /opt/mamba/lib/python3.9/site-packages (from pandas&gt;=0.24.2-&gt;pynsee) (2022.5)\nRequirement already satisfied: tzdata&gt;=2022.1 in /opt/mamba/lib/python3.9/site-packages (from pandas&gt;=0.24.2-&gt;pynsee) (2023.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/mamba/lib/python3.9/site-packages (from requests&gt;=2.23-&gt;pynsee) (2.1.1)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/mamba/lib/python3.9/site-packages (from requests&gt;=2.23-&gt;pynsee) (3.3)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /opt/mamba/lib/python3.9/site-packages (from requests&gt;=2.23-&gt;pynsee) (2022.9.24)\nRequirement already satisfied: six&gt;=1.5 in /opt/mamba/lib/python3.9/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas&gt;=0.24.2-&gt;pynsee) (1.16.0)\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pynsee.download\nPour obtenir des résultats reproductibles, on peut fixer la racine du générateur\npseudo-aléatoire.\nnp.random.seed(123)\nAu cours de cette démonstration des principales fonctionalités de Pandas, et\nlors du chapitre suivant,\nje recommande de se référer régulièrement aux ressources suivantes :"
  },
  {
    "objectID": "content/manipulation/02a_pandas_tutorial.html#logique-de-pandas",
    "href": "content/manipulation/02a_pandas_tutorial.html#logique-de-pandas",
    "title": "Introduction à Pandas",
    "section": "Logique de Pandas",
    "text": "Logique de Pandas\nL’objet central dans la logique Pandas est le DataFrame.\nIl s’agit d’une structure particulière de données\nà deux dimensions, structurées en alignant des lignes et colonnes.\nContrairement à une matrice, les colonnes\npeuvent être de types différents.\nUn DataFrame est composé des éléments suivants :\n\nl’indice de la ligne ;\nle nom de la colonne ;\nla valeur de la donnée ;\n\n\n\n\nStructuration d’un DataFrame Pandas,\nempruntée à https://medium.com/epfl-extension-school/selecting-data-from-a-pandas-dataframe-53917dc39953\n\n\nLe concept de tidy data, popularisé par Hadley Wickham via ses packages R,\nest parfaitement pertinent pour décrire la structure d’un DataFrame pandas.\nLes trois règles sont les suivantes :\n\nChaque variable possède sa propre colonne ;\nChaque observation possède sa propre ligne ;\nUne valeur, matérialisant une observation d’une variable,\nse trouve sur une unique cellule.\n\n\n\n\nConcept de tidy data (emprunté à H. Wickham)\n\n\nConcernant la syntaxe, une partie des commandes Python est inspirée par la logique SQL.\nOn retrouvera ainsi une philosophie proche de celle du SQL où on fait des opérations\nde sélection de ligne ou de colonne. Voici une illustration de quelques manipulations de données\nque nous mettrons en oeuvre par la suite :\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRéordonner le DataFrame\n\n\n\n\nPandas propose énormément de fonctionnalités pré-implémentées.\nIl est vivement recommandé, avant de se lancer dans l’écriture d’une\nfonction, de se poser la question de son implémentation native dans Numpy, Pandas, etc.\nLa plupart du temps, s’il existe une solution implémentée dans une librairie, il convient\nde l’utiliser car elle sera plus efficace que celle que vous mettrez en oeuvre."
  },
  {
    "objectID": "content/manipulation/02a_pandas_tutorial.html#les-series",
    "href": "content/manipulation/02a_pandas_tutorial.html#les-series",
    "title": "Introduction à Pandas",
    "section": "Les Series",
    "text": "Les Series\nEn fait, un DataFrame est une collection d’objets appelés pandas.Series.\nCes Series sont des objets d’une dimension qui sont des extensions des\narray-unidimensionnels numpy. En particulier, pour faciliter le traitement\nde données catégorielles ou temporelles, des types de variables\nsupplémentaires sont disponibles dans pandas par rapport à\nnumpy (categorical, datetime64 et timedelta64). Ces\ntypes sont associés à des méthodes optimisées pour faciliter le traitement\nde ces données.\nIl ne faut pas négliger l’attribut dtype d’un objet\npandas.Series car cela a une influence déterminante sur les méthodes\net fonctions pouvant être utilisées (on ne fait pas les mêmes opérations\nsur une donnée temporelle et une donnée catégorielle) et le volume en\nmémoire d’une variable (le type de la variable détermine le volume\nd’information stockée pour chaque élément ; être trop précis est parfois\nnéfaste).\nIl existe plusieurs types possibles pour un pandas.Series.\nLe type object correspond aux types Python str ou mixed.\nIl existe un type particulier pour les variables dont le nombre de valeurs\nest une liste finie et relativement courte, le type category.\nIl faut bien examiner les types de son DataFrame, et convertir éventuellement\nles types lors de l’étape de data cleaning.\n\nIndexation\nLa différence essentielle entre une Series et un objet numpy est l’indexation.\nDans numpy,\nl’indexation est implicite ; elle permet d’accéder à une donnée (celle à\nl’index situé à la position i).\nAvec une Series, on peut bien sûr utiliser un indice de position mais on peut\nsurtout faire appel à des indices plus explicites.\nPar exemple,\n\ntaille = pd.Series(\n    [1.,1.5,1],\n    index = ['chat', 'chien', 'koala']\n)\n\ntaille.head()\n\nchat     1.0\nchien    1.5\nkoala    1.0\ndtype: float64\n\n\nCette indexation permet d’accéder à des valeurs de la Series\nvia une valeur de l’indice. Par\nexemple, taille['koala']:\n\ntaille['koala']\n\n1.0\n\n\nL’existence d’indice rend le subsetting particulièrement aisé, ce que vous\npouvez expérimenter dans les TP :\n\n\n\n\n\n\n\n\n\n\n\n\n\nPour transformer un objet pandas.Series en array numpy,\non utilise la méthode values. Par exemple, taille.values:\n\ntaille.values\n\narray([1. , 1.5, 1. ])\n\n\nUn avantage des Series par rapport à un array numpy est que\nles opérations sur les Series alignent\nautomatiquement les données à partir des labels.\nAvec des Series labélisées, il n’est ainsi pas nécessaire\nde se poser la question de l’ordre des lignes.\nL’exemple dans la partie suivante permettra de s’en assurer.\n\n\nValeurs manquantes\nPar défaut, les valeurs manquantes sont affichées NaN et sont de type np.nan (pour\nles valeurs temporelles, i.e. de type datatime64, les valeurs manquantes sont\nNaT).\nOn a un comportement cohérent d’agrégation lorsqu’on combine deux DataFrames (ou deux colonnes).\nPar exemple,\n\nx = pd.DataFrame(\n    {'prix': np.random.uniform(size = 5),\n     'quantite': [i+1 for i in range(5)]\n    },\n    index = ['yaourt','pates','riz','tomates','gateaux']\n)\nx\n\n\n\n\n\n\n\n\nprix\nquantite\n\n\n\n\nyaourt\n0.696469\n1\n\n\npates\n0.286139\n2\n\n\nriz\n0.226851\n3\n\n\ntomates\n0.551315\n4\n\n\ngateaux\n0.719469\n5\n\n\n\n\n\n\n\n\ny = pd.DataFrame(\n    {'prix': [np.nan, 0, 1, 2, 3],\n     'quantite': [i+1 for i in range(5)]\n    },\n    index = ['tomates','yaourt','gateaux','pates','riz']\n)\ny\n\n\n\n\n\n\n\n\nprix\nquantite\n\n\n\n\ntomates\nNaN\n1\n\n\nyaourt\n0.0\n2\n\n\ngateaux\n1.0\n3\n\n\npates\n2.0\n4\n\n\nriz\n3.0\n5\n\n\n\n\n\n\n\n\nx + y\n\n\n\n\n\n\n\n\nprix\nquantite\n\n\n\n\ngateaux\n1.719469\n8\n\n\npates\n2.286139\n6\n\n\nriz\n3.226851\n8\n\n\ntomates\nNaN\n5\n\n\nyaourt\n0.696469\n3\n\n\n\n\n\n\n\ndonne bien une valeur manquante pour la ligne tomates. Au passage, on peut remarquer que l’agrégation\na tenu compte des index.\nIl est possible de supprimer les valeurs manquantes grâce à dropna().\nCette méthode va supprimer toutes les lignes où il y a au moins une valeur manquante.\nIl est aussi possible de supprimer seulement les colonnes où il y a des valeurs manquantes\ndans un DataFrame avec dropna() avec le paramètre axis=1 (par défaut égal à 0).\nIl est également possible de remplir les valeurs manquantes grâce à la méthode fillna()."
  },
  {
    "objectID": "content/manipulation/02a_pandas_tutorial.html#le-dataframe-pandas",
    "href": "content/manipulation/02a_pandas_tutorial.html#le-dataframe-pandas",
    "title": "Introduction à Pandas",
    "section": "Le DataFrame Pandas",
    "text": "Le DataFrame Pandas\nLe DataFrame est l’objet central de la librairie pandas.\nIl s’agit d’une collection de pandas.Series (colonnes) alignées par les index.\nLes types des variables peuvent différer.\nUn DataFrame non-indexé a la structure suivante :\n\ndf = pd.DataFrame(\n    {'taille': [1.,1.5,1],\n    'poids' : [3, 5, 2.5]\n    },\n    index = ['chat', 'chien', 'koala']\n)\ndf.reset_index()\n\n\n\n\n\n\n\n\nindex\ntaille\npoids\n\n\n\n\n0\nchat\n1.0\n3.0\n\n\n1\nchien\n1.5\n5.0\n\n\n2\nkoala\n1.0\n2.5\n\n\n\n\n\n\n\nAlors que le même DataFrame indexé aura la structure suivante :\n\ndf = pd.DataFrame(\n    {'taille': [1.,1.5,1],\n    'poids' : [3, 5, 2.5]\n    },\n    index = ['chat', 'chien', 'koala']\n)\ndf.head()\n\n\n\n\n\n\n\n\ntaille\npoids\n\n\n\n\nchat\n1.0\n3.0\n\n\nchien\n1.5\n5.0\n\n\nkoala\n1.0\n2.5\n\n\n\n\n\n\n\n\nLes attributs et méthodes utiles\nPour présenter les méthodes les plus pratiques pour l’analyse de données,\non peut partir de l’exemple des consommations de CO2 communales issues\ndes données de l’Ademe. Cette base de données est exploitée plus intensément\ndans le TP.\n\n\n\n\n\n\n\n\n\n\n\n\n\nL’import de données depuis un fichier plat se fait avec la fonction read_csv:\n\ndf = pd.read_csv(\"https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert\")\ndf\n\n\n\n\n\n\n\n\nINSEE commune\nCommune\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nDéchets\nEnergie\nIndustrie hors-énergie\nRésidentiel\nRoutier\nTertiaire\n\n\n\n\n0\n01001\nL'ABERGEMENT-CLEMENCIAT\n3711.425991\nNaN\nNaN\n432.751835\n101.430476\n2.354558\n6.911213\n309.358195\n793.156501\n367.036172\n\n\n1\n01002\nL'ABERGEMENT-DE-VAREY\n475.330205\nNaN\nNaN\n140.741660\n140.675439\n2.354558\n6.911213\n104.866444\n348.997893\n112.934207\n\n\n2\n01004\nAMBERIEU-EN-BUGEY\n499.043526\n212.577908\nNaN\n10313.446515\n5314.314445\n998.332482\n2930.354461\n16616.822534\n15642.420313\n10732.376934\n\n\n3\n01005\nAMBERIEUX-EN-DOMBES\n1859.160954\nNaN\nNaN\n1144.429311\n216.217508\n94.182310\n276.448534\n663.683146\n1756.341319\n782.404357\n\n\n4\n01006\nAMBLEON\n448.966808\nNaN\nNaN\n77.033834\n48.401549\nNaN\nNaN\n43.714019\n398.786800\n51.681756\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n35793\n95676\nVILLERS-EN-ARTHIES\n1628.065094\nNaN\nNaN\n165.045396\n65.063617\n11.772789\n34.556067\n176.098160\n309.627908\n235.439109\n\n\n35794\n95678\nVILLIERS-ADAM\n698.630772\nNaN\nNaN\n1331.126598\n111.480954\n2.354558\n6.911213\n1395.529811\n18759.370071\n403.404815\n\n\n35795\n95680\nVILLIERS-LE-BEL\n107.564967\nNaN\nNaN\n8367.174532\n225.622903\n534.484607\n1568.845431\n22613.830247\n12217.122402\n13849.512001\n\n\n35796\n95682\nVILLIERS-LE-SEC\n1090.890170\nNaN\nNaN\n326.748418\n108.969749\n2.354558\n6.911213\n67.235487\n4663.232127\n85.657725\n\n\n35797\n95690\nWY-DIT-JOLI-VILLAGE\n1495.103542\nNaN\nNaN\n125.236417\n97.728612\n4.709115\n13.822427\n117.450851\n504.400972\n147.867245\n\n\n\n\n35798 rows × 12 columns\n\n\n\n\n\n Note\nDans un processus de production, où normalement on connait les types des variables du DataFrame qu’on va importer,\nil convient de préciser les types avec lesquels on souhaite importer les données\n(argument dtype, sous la forme d’un dictionnaire).\nCela est particulièrement important lorsqu’on désire utiliser une colonne\ncomme une variable textuelle mais qu’elle comporte des attributs proches d’un nombre\nqui vont inciter pandas à l’importer sous forme de variable numérique.\nPar exemple, une colonne [00001,00002,...] risque d’être importée comme une variable numérique, ignorant l’information des premiers 0 (qui peuvent pourtant la distinguer de la séquence 1, 2, etc.). Pour s’assurer que pandas importe sous forme textuelle la variable, on peut utiliser dtype = {\"code\": \"str\"}\nSinon, on peut importer le csv, et modifier les types avec astype().\nAvec astype, on peut gérer les erreurs de conversion avec le paramètre errors.\n\n\nL’affichage des DataFrames est très ergonomique. On obtiendrait le même output\navec display(df). Les premières et dernières lignes s’affichent\nautomatiquement. Autrement, on peut aussi faire:\n\nhead qui permet, comme son\nnom l’indique, de n’afficher que les premières lignes ;\ntail qui permet, comme son\nnom l’indique, de n’afficher que les dernières lignes\nsample qui permet d’afficher un échantillon aléatoire de n lignes.\nCette méthode propose de nombreuses options.\n\n\n\n Warning\nIl faut faire attention au display et aux\ncommandes qui révèlent des données (head, tail, etc.)\ndans un Notebook ou un Markdown qui exploite\ndes données confidentielles lorsqu’on utilise Git.\nEn effet, on peut se\nretrouver à partager des données, involontairement, dans l’historique\nGit. Avec un R Markdown, il suffit d’ajouter les sorties au fichier\n.gitignore (par exemple avec une balise de type *.html). Avec un\nNotebook Jupyter, la démarche est plus compliquée car les fichiers\n.ipynb intègrent dans le même document, texte, sorties et mise en forme.\nTechniquement, il est possible d’appliquer des filtres avec Git\n(voir\nici)\nmais c’est une démarche très complexe.\nCe post de l’équipe à l’origine de nbdev2\nrésume bien le problème du contrôle de version avec Git et des solutions qui\npeuvent y être apportées.\nUne solution est d’utiliser Quarto qui permet de générer les\n.ipynb en output d’un document texte, ce qui facilite le contrôle sur les\néléments présents dans le document.\n\n\n\n\nDimensions et structure du DataFrame\nLes premières méthodes utiles permettent d’afficher quelques\nattributs d’un DataFrame.\n\ndf.axes\n\n[RangeIndex(start=0, stop=35798, step=1),\n Index(['INSEE commune', 'Commune', 'Agriculture', 'Autres transports',\n        'Autres transports international', 'CO2 biomasse hors-total', 'Déchets',\n        'Energie', 'Industrie hors-énergie', 'Résidentiel', 'Routier',\n        'Tertiaire'],\n       dtype='object')]\n\n\n\ndf.columns\n\nIndex(['INSEE commune', 'Commune', 'Agriculture', 'Autres transports',\n       'Autres transports international', 'CO2 biomasse hors-total', 'Déchets',\n       'Energie', 'Industrie hors-énergie', 'Résidentiel', 'Routier',\n       'Tertiaire'],\n      dtype='object')\n\n\n\ndf.index\n\nRangeIndex(start=0, stop=35798, step=1)\n\n\nPour connaître les dimensions d’un DataFrame, on peut utiliser quelques méthodes\npratiques :\n\ndf.ndim\n\n2\n\n\n\ndf.shape\n\n(35798, 12)\n\n\n\ndf.size\n\n429576\n\n\nPour déterminer le nombre de valeurs uniques d’une variable, plutôt que chercher à écrire soi-même une fonction,\non utilise la\nméthode nunique. Par exemple,\n\ndf['Commune'].nunique()\n\n33338\n\n\npandas propose énormément de méthodes utiles.\nVoici un premier résumé, accompagné d’un comparatif avec R :\n\n\n\n\n\n\n\n\n\nOpération\npandas\ndplyr (R)\ndata.table (R)\n\n\n\n\nRécupérer le nom des colonnes\ndf.columns\ncolnames(df)\ncolnames(df)\n\n\nRécupérer les indices\ndf.index\n\nunique(df[,get(key(df))])\n\n\nRécupérer les dimensions\ndf.shape\ndim(df)\ndim(df)\n\n\nRécupérer le nombre de valeurs uniques d’une variable\ndf['myvar'].nunique()\ndf %&gt;%  summarise(distinct(myvar))\ndf[,uniqueN(myvar)]\n\n\n\n\n\nStatistiques agrégées\npandas propose une série de méthodes pour faire des statistiques\nagrégées de manière efficace.\nOn peut, par exemple, appliquer des méthodes pour compter le nombre de lignes,\nfaire une moyenne ou une somme de l’ensemble des lignes\n\ndf.count()\n\nINSEE commune                      35798\nCommune                            35798\nAgriculture                        35736\nAutres transports                   9979\nAutres transports international     2891\nCO2 biomasse hors-total            35798\nDéchets                            35792\nEnergie                            34490\nIndustrie hors-énergie             34490\nRésidentiel                        35792\nRoutier                            35778\nTertiaire                          35798\ndtype: int64\n\n\n\ndf.mean(numeric_only = True)\n\nAgriculture                        2459.975760\nAutres transports                   654.919940\nAutres transports international    7692.344960\nCO2 biomasse hors-total            1774.381550\nDéchets                             410.806329\nEnergie                             662.569846\nIndustrie hors-énergie             2423.127789\nRésidentiel                        1783.677872\nRoutier                            3535.501245\nTertiaire                          1105.165915\ndtype: float64\n\n\n\ndf.sum(numeric_only = True)\n\nAgriculture                        8.790969e+07\nAutres transports                  6.535446e+06\nAutres transports international    2.223857e+07\nCO2 biomasse hors-total            6.351931e+07\nDéchets                            1.470358e+07\nEnergie                            2.285203e+07\nIndustrie hors-énergie             8.357368e+07\nRésidentiel                        6.384140e+07\nRoutier                            1.264932e+08\nTertiaire                          3.956273e+07\ndtype: float64\n\n\n\ndf.nunique()\n\nINSEE commune                      35798\nCommune                            33338\nAgriculture                        35576\nAutres transports                   9963\nAutres transports international     2883\nCO2 biomasse hors-total            35798\nDéchets                            11016\nEnergie                             1453\nIndustrie hors-énergie              1889\nRésidentiel                        35791\nRoutier                            35749\nTertiaire                           8663\ndtype: int64\n\n\n\ndf.quantile(q = [0.1,0.25,0.5,0.75,0.9], numeric_only = True)\n\n\n\n\n\n\n\n\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nDéchets\nEnergie\nIndustrie hors-énergie\nRésidentiel\nRoutier\nTertiaire\n\n\n\n\n0.10\n382.620882\n25.034578\n4.430792\n109.152816\n14.811230\n2.354558\n6.911213\n50.180933\n199.765410\n49.289082\n\n\n0.25\n797.682631\n52.560412\n10.050967\n197.951108\n25.655166\n2.354558\n6.911213\n96.052911\n419.700460\n94.749885\n\n\n0.50\n1559.381285\n106.795928\n19.924343\n424.849988\n54.748653\n4.709115\n13.822427\n227.091193\n1070.895593\n216.297718\n\n\n0.75\n3007.883903\n237.341501\n32.983111\n1094.749825\n110.820941\n51.800270\n152.046694\n749.469293\n3098.612157\n576.155869\n\n\n0.90\n5442.727470\n528.349529\n59.999169\n3143.759029\n190.695774\n367.311008\n1154.172630\n2937.699671\n8151.047248\n1897.732565\n\n\n\n\n\n\n\n\n\n Warning\nLa version 2.0 de Pandas a introduit un changement\nde comportement dans les méthodes d’agrégation.\nIl est dorénavant nécessaire de préciser quand on désire\neffectuer des opérations si on désire ou non le faire\nexclusivement sur les colonnes numériques. C’est pour cette\nraison qu’on exlicite ici l’argument numeric_only = True.\nCe comportement\nétait par le passé implicite.\n\n\nIl faut toujours regarder les options de ces fonctions en termes de valeurs manquantes, car\nces options sont déterminantes dans le résultat obtenu.\nLes exercices de TD visent à démontrer l’intérêt de ces méthodes dans quelques cas précis.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLe tableau suivant récapitule le code équivalent pour avoir des\nstatistiques sur toutes les colonnes d’un dataframe en R.\n\n\n\n\n\n\n\n\n\nOpération\npandas\ndplyr (R)\ndata.table (R)\n\n\n\n\nNombre de valeurs non manquantes\ndf.count()\ndf %&gt;% summarise_each(funs(sum(!is.na(.))))\ndf[, lapply(.SD, function(x) sum(!is.na(x)))]\n\n\nMoyenne de toutes les variables\ndf.mean()\ndf %&gt;% summarise_each(funs(mean((., na.rm = TRUE))))\ndf[,lapply(.SD, function(x) mean(x, na.rm = TRUE))]\n\n\n\nLa méthode describe permet de sortir un tableau de statistiques\nagrégées :\n\ndf.describe()\n\n\n\n\n\n\n\n\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nDéchets\nEnergie\nIndustrie hors-énergie\nRésidentiel\nRoutier\nTertiaire\n\n\n\n\ncount\n35736.000000\n9979.000000\n2.891000e+03\n35798.000000\n35792.000000\n3.449000e+04\n3.449000e+04\n35792.000000\n35778.000000\n35798.000000\n\n\nmean\n2459.975760\n654.919940\n7.692345e+03\n1774.381550\n410.806329\n6.625698e+02\n2.423128e+03\n1783.677872\n3535.501245\n1105.165915\n\n\nstd\n2926.957701\n9232.816833\n1.137643e+05\n7871.341922\n4122.472608\n2.645571e+04\n5.670374e+04\n8915.902379\n9663.156628\n5164.182507\n\n\nmin\n0.003432\n0.000204\n3.972950e-04\n3.758088\n0.132243\n2.354558e+00\n1.052998e+00\n1.027266\n0.555092\n0.000000\n\n\n25%\n797.682631\n52.560412\n1.005097e+01\n197.951108\n25.655166\n2.354558e+00\n6.911213e+00\n96.052911\n419.700460\n94.749885\n\n\n50%\n1559.381285\n106.795928\n1.992434e+01\n424.849988\n54.748653\n4.709115e+00\n1.382243e+01\n227.091193\n1070.895593\n216.297718\n\n\n75%\n3007.883903\n237.341501\n3.298311e+01\n1094.749825\n110.820941\n5.180027e+01\n1.520467e+02\n749.469293\n3098.612157\n576.155869\n\n\nmax\n98949.317760\n513140.971691\n3.303394e+06\n576394.181208\n275500.374439\n2.535858e+06\n6.765119e+06\n410675.902028\n586054.672836\n288175.400126\n\n\n\n\n\n\n\n\n\nMéthodes relatives aux valeurs manquantes\nLes méthodes relatives aux valeurs manquantes peuvent être mobilisées\nen conjonction des méthodes de statistiques agrégées. C’est utile lorsqu’on\ndésire obtenir une idée de la part de valeurs manquantes dans un jeu de\ndonnées\n\ndf.isnull().sum()\n\nINSEE commune                          0\nCommune                                0\nAgriculture                           62\nAutres transports                  25819\nAutres transports international    32907\nCO2 biomasse hors-total                0\nDéchets                                6\nEnergie                             1308\nIndustrie hors-énergie              1308\nRésidentiel                            6\nRoutier                               20\nTertiaire                              0\ndtype: int64\n\n\nOn trouvera aussi la référence à isna() qui est la même méthode que isnull()."
  },
  {
    "objectID": "content/manipulation/02a_pandas_tutorial.html#graphiques-rapides",
    "href": "content/manipulation/02a_pandas_tutorial.html#graphiques-rapides",
    "title": "Introduction à Pandas",
    "section": "Graphiques rapides",
    "text": "Graphiques rapides\nLes méthodes par défaut de graphiques\n(approfondies dans la partie visualisation)\nsont pratiques pour\nproduire rapidement un graphique, notamment après des opérations\ncomplexes de maniement de données.\nEn effet, on peut appliquer la méthode plot() directement à une pandas.Series :\n\ndf['Déchets'].plot()\ndf['Déchets'].hist()\ndf['Déchets'].plot(kind = 'hist', logy = True)\n\n\nplt.figure()\nfig = df['Déchets'].plot()\nfig\n#plt.savefig('plot_base.png', bbox_inches='tight')\n\nplt.figure()\nfig = df['Déchets'].hist()\nfig\n#plt.savefig('plot_hist.png', bbox_inches='tight')\n\nplt.figure()\nfig = df['Déchets'].plot(kind = 'hist', logy = True)\nfig\n#plt.show()\n#plt.savefig('plot_hist_log.png', bbox_inches='tight')\n\n&lt;Axes: ylabel='Frequency'&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa sortie est un objet matplotlib. La customisation de ces\nfigures est ainsi\npossible (et même désirable car les graphiques matplotlib\nsont, par défaut, assez rudimentaires), nous en verrons quelques exemples."
  },
  {
    "objectID": "content/manipulation/02a_pandas_tutorial.html#accéder-à-des-éléments-dun-dataframe",
    "href": "content/manipulation/02a_pandas_tutorial.html#accéder-à-des-éléments-dun-dataframe",
    "title": "Introduction à Pandas",
    "section": "Accéder à des éléments d’un DataFrame",
    "text": "Accéder à des éléments d’un DataFrame\n\nSélectionner des colonnes\nEn SQL, effectuer des opérations sur les colonnes se fait avec la commande\nSELECT. Avec pandas,\npour accéder à une colonne dans son ensemble on peut\nutiliser plusieurs approches :\n\ndataframe.variable, par exemple df.Energie.\nCette méthode requiert néanmoins d’avoir des\nnoms de colonnes sans espace.\ndataframe[['variable']] pour renvoyer la variable sous\nforme de DataFrame ou dataframe['variable'] pour\nla renvoyer sous forme de Series. Par exemple, df[['Autres transports']]\nou df['Autres transports']. C’est une manière préférable de procéder.\n\n\n\nAccéder à des lignes\nPour accéder à une ou plusieurs valeurs d’un DataFrame,\nil existe deux manières conseillées de procéder, selon la\nforme des indices de lignes ou colonnes utilisés:\n\ndf.loc : utilise les labels\ndf.iloc : utilise les indices\n\n\n\n Warning\nLes bouts de code utilisant la structure df.ix\nsont à bannir car la fonction est deprecated et peut\nainsi disparaître à tout moment.\n\n\niloc va se référer à l’indexation de 0 à N où N est égal à df.shape[0] d’un\npandas.DataFrame. loc va se référer aux valeurs de l’index\nde df.\nPar exemple, avec le pandas.DataFrame df_example:\n\ndf_example = pd.DataFrame(\n    {'month': [1, 4, 7, 10], 'year': [2012, 2014, 2013, 2014], 'sale': [55, 40, 84, 31]})\ndf_example = df_example.set_index('month')\ndf_example\n\n\n\n\n\n\n\n\nyear\nsale\n\n\nmonth\n\n\n\n\n\n\n1\n2012\n55\n\n\n4\n2014\n40\n\n\n7\n2013\n84\n\n\n10\n2014\n31\n\n\n\n\n\n\n\n\ndf_example.loc[1, :] donnera la première ligne de df (ligne où l’indice month est égal à 1) ;\ndf_example.iloc[1, :] donnera la deuxième ligne (puisque l’indexation en Python commence à 0) ;\ndf_example.iloc[:, 1] donnera la deuxième colonne, suivant le même principe."
  },
  {
    "objectID": "content/manipulation/02a_pandas_tutorial.html#principales-manipulation-de-données",
    "href": "content/manipulation/02a_pandas_tutorial.html#principales-manipulation-de-données",
    "title": "Introduction à Pandas",
    "section": "Principales manipulation de données",
    "text": "Principales manipulation de données\nL’objectif du TP pandas est de se familiariser plus avec ces\ncommandes à travers l’exemple des données des émissions de C02.\nLes opérations les plus fréquentes en SQL sont résumées par le tableau suivant.\nIl est utile de les connaître (beaucoup de syntaxes de maniement de données\nreprennent ces termes) car, d’une\nmanière ou d’une autre, elles couvrent la plupart\ndes usages de manipulation des données\n\n\n\n\n\n\n\n\n\n\nOpération\nSQL\npandas\ndplyr (R)\ndata.table (R)\n\n\n\n\nSélectionner des variables par leur nom\nSELECT\ndf[['Autres transports','Energie']]\ndf %&gt;% select(Autres transports, Energie)\ndf[, c('Autres transports','Energie')]\n\n\nSélectionner des observations selon une ou plusieurs conditions;\nFILTER\ndf[df['Agriculture']&gt;2000]\ndf %&gt;% filter(Agriculture&gt;2000)\ndf[Agriculture&gt;2000]\n\n\nTrier la table selon une ou plusieurs variables\nSORT BY\ndf.sort_values(['Commune','Agriculture'])\ndf %&gt;% arrange(Commune, Agriculture)\ndf[order(Commune, Agriculture)]\n\n\nAjouter des variables qui sont fonction d’autres variables;\nSELECT *, LOG(Agriculture) AS x FROM df\ndf['x'] = np.log(df['Agriculture'])\ndf %&gt;% mutate(x = log(Agriculture))\ndf[,x := log(Agriculture)]\n\n\nEffectuer une opération par groupe\nGROUP BY\ndf.groupby('Commune').mean()\ndf %&gt;% group_by(Commune) %&gt;% summarise(m = mean)\ndf[,mean(Commune), by = Commune]\n\n\nJoindre deux bases de données (inner join)\nSELECT * FROM table1 INNER JOIN table2 ON table1.id = table2.x\ntable1.merge(table2, left_on = 'id', right_on = 'x')\ntable1 %&gt;% inner_join(table2, by = c('id'='x'))\nmerge(table1, table2, by.x = 'id', by.y = 'x')\n\n\n\n\nOpérations sur les colonnes : select, mutate, drop\nLes DataFrames pandas sont des objets mutables en langage Python,\nc’est-à-dire qu’il est possible de faire évoluer le DataFrame au grès\ndes opérations. L’opération la plus classique consiste à ajouter ou retirer\ndes variables à la table de données.\n\ndf_new = df.copy()\n\n\n\n Warning\nAttention au comportement de pandas lorsqu’on crée une duplication\nd’un DataFrame.\nPar défaut, pandas effectue une copie par référence. Dans ce\ncas, les deux objets (la copie et l’objet copié) restent reliés. Les colonnes\ncréées sur l’un vont être répercutées sur l’autre. Ce comportement permet de\nlimiter l’inflation en mémoire de Python. En faisant ça, le deuxième\nobjet prend le même espace mémoire que le premier. Le package data.table\nen R adopte le même comportement, contrairement à dplyr.\nCela peut amener à quelques surprises si ce comportement d’optimisation\nn’est pas anticipé. Si vous voulez, par sécurité, conserver intact le\npremier DataFrame, faites appel à une copie profonde (deep copy) en\nutilisant la méthode copy, comme ci-dessus.\nAttention toutefois, cela a un coût mémoire.\nAvec des données volumineuses, c’est une pratique à utiliser avec précaution.\n\n\nLa manière la plus simple d’opérer pour ajouter des colonnes est\nd’utiliser la réassignation. Par exemple, pour créer une variable\nx qui est le log de la\nvariable Agriculture:\n\ndf_new['x'] = np.log(df_new['Agriculture'])\n\nIl est possible d’appliquer cette approche sur plusieurs colonnes. Un des\nintérêts de cette approche est qu’elle permet de recycler le nom de colonnes.\n\nvars = ['Agriculture', 'Déchets', 'Energie']\n\ndf_new[[v + \"_log\" for v in vars]] = np.log(df_new[vars])\ndf_new\n\n\n\n\n\n\n\n\nINSEE commune\nCommune\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nDéchets\nEnergie\nIndustrie hors-énergie\nRésidentiel\nRoutier\nTertiaire\nx\nAgriculture_log\nDéchets_log\nEnergie_log\n\n\n\n\n0\n01001\nL'ABERGEMENT-CLEMENCIAT\n3711.425991\nNaN\nNaN\n432.751835\n101.430476\n2.354558\n6.911213\n309.358195\n793.156501\n367.036172\n8.219171\n8.219171\n4.619374\n0.856353\n\n\n1\n01002\nL'ABERGEMENT-DE-VAREY\n475.330205\nNaN\nNaN\n140.741660\n140.675439\n2.354558\n6.911213\n104.866444\n348.997893\n112.934207\n6.164010\n6.164010\n4.946455\n0.856353\n\n\n2\n01004\nAMBERIEU-EN-BUGEY\n499.043526\n212.577908\nNaN\n10313.446515\n5314.314445\n998.332482\n2930.354461\n16616.822534\n15642.420313\n10732.376934\n6.212693\n6.212693\n8.578159\n6.906086\n\n\n3\n01005\nAMBERIEUX-EN-DOMBES\n1859.160954\nNaN\nNaN\n1144.429311\n216.217508\n94.182310\n276.448534\n663.683146\n1756.341319\n782.404357\n7.527881\n7.527881\n5.376285\n4.545232\n\n\n4\n01006\nAMBLEON\n448.966808\nNaN\nNaN\n77.033834\n48.401549\nNaN\nNaN\n43.714019\n398.786800\n51.681756\n6.106949\n6.106949\n3.879532\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n35793\n95676\nVILLERS-EN-ARTHIES\n1628.065094\nNaN\nNaN\n165.045396\n65.063617\n11.772789\n34.556067\n176.098160\n309.627908\n235.439109\n7.395148\n7.395148\n4.175366\n2.465791\n\n\n35794\n95678\nVILLIERS-ADAM\n698.630772\nNaN\nNaN\n1331.126598\n111.480954\n2.354558\n6.911213\n1395.529811\n18759.370071\n403.404815\n6.549122\n6.549122\n4.713854\n0.856353\n\n\n35795\n95680\nVILLIERS-LE-BEL\n107.564967\nNaN\nNaN\n8367.174532\n225.622903\n534.484607\n1568.845431\n22613.830247\n12217.122402\n13849.512001\n4.678095\n4.678095\n5.418865\n6.281303\n\n\n35796\n95682\nVILLIERS-LE-SEC\n1090.890170\nNaN\nNaN\n326.748418\n108.969749\n2.354558\n6.911213\n67.235487\n4663.232127\n85.657725\n6.994749\n6.994749\n4.691070\n0.856353\n\n\n35797\n95690\nWY-DIT-JOLI-VILLAGE\n1495.103542\nNaN\nNaN\n125.236417\n97.728612\n4.709115\n13.822427\n117.450851\n504.400972\n147.867245\n7.309951\n7.309951\n4.582194\n1.549500\n\n\n\n\n35798 rows × 16 columns\n\n\n\nIl est également possible d’utiliser la méthode assign. Pour des opérations\nvectorisées, comme le sont les opérateurs de numpy, cela n’a pas d’intérêt.\nCela permet notamment d’enchainer les opérations sur un même DataFrame (notamment grâce au pipe que\nnous verrons plus loin).\nCette approche utilise généralement\ndes lambda functions. Par exemple le code précédent (celui concernant une\nseule variable) prendrait la forme:\n\ndf_new.assign(Energie_log = lambda x: np.log(x['Energie']))\n\n\n\n\n\n\n\n\nINSEE commune\nCommune\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nDéchets\nEnergie\nIndustrie hors-énergie\nRésidentiel\nRoutier\nTertiaire\nx\nAgriculture_log\nDéchets_log\nEnergie_log\n\n\n\n\n0\n01001\nL'ABERGEMENT-CLEMENCIAT\n3711.425991\nNaN\nNaN\n432.751835\n101.430476\n2.354558\n6.911213\n309.358195\n793.156501\n367.036172\n8.219171\n8.219171\n4.619374\n0.856353\n\n\n1\n01002\nL'ABERGEMENT-DE-VAREY\n475.330205\nNaN\nNaN\n140.741660\n140.675439\n2.354558\n6.911213\n104.866444\n348.997893\n112.934207\n6.164010\n6.164010\n4.946455\n0.856353\n\n\n2\n01004\nAMBERIEU-EN-BUGEY\n499.043526\n212.577908\nNaN\n10313.446515\n5314.314445\n998.332482\n2930.354461\n16616.822534\n15642.420313\n10732.376934\n6.212693\n6.212693\n8.578159\n6.906086\n\n\n3\n01005\nAMBERIEUX-EN-DOMBES\n1859.160954\nNaN\nNaN\n1144.429311\n216.217508\n94.182310\n276.448534\n663.683146\n1756.341319\n782.404357\n7.527881\n7.527881\n5.376285\n4.545232\n\n\n4\n01006\nAMBLEON\n448.966808\nNaN\nNaN\n77.033834\n48.401549\nNaN\nNaN\n43.714019\n398.786800\n51.681756\n6.106949\n6.106949\n3.879532\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n35793\n95676\nVILLERS-EN-ARTHIES\n1628.065094\nNaN\nNaN\n165.045396\n65.063617\n11.772789\n34.556067\n176.098160\n309.627908\n235.439109\n7.395148\n7.395148\n4.175366\n2.465791\n\n\n35794\n95678\nVILLIERS-ADAM\n698.630772\nNaN\nNaN\n1331.126598\n111.480954\n2.354558\n6.911213\n1395.529811\n18759.370071\n403.404815\n6.549122\n6.549122\n4.713854\n0.856353\n\n\n35795\n95680\nVILLIERS-LE-BEL\n107.564967\nNaN\nNaN\n8367.174532\n225.622903\n534.484607\n1568.845431\n22613.830247\n12217.122402\n13849.512001\n4.678095\n4.678095\n5.418865\n6.281303\n\n\n35796\n95682\nVILLIERS-LE-SEC\n1090.890170\nNaN\nNaN\n326.748418\n108.969749\n2.354558\n6.911213\n67.235487\n4663.232127\n85.657725\n6.994749\n6.994749\n4.691070\n0.856353\n\n\n35797\n95690\nWY-DIT-JOLI-VILLAGE\n1495.103542\nNaN\nNaN\n125.236417\n97.728612\n4.709115\n13.822427\n117.450851\n504.400972\n147.867245\n7.309951\n7.309951\n4.582194\n1.549500\n\n\n\n\n35798 rows × 16 columns\n\n\n\nDans les méthodes suivantes, il est possible de modifier le pandas.DataFrame\nen place, c’est à dire en ne le réassignant pas, avec le paramètre inplace = True.\nPar défaut, inplace est égal à False et pour modifier le pandas.DataFrame,\nil convient de le réassigner.\nOn peut facilement renommer des variables avec la méthode rename qui\nfonctionne bien avec des dictionnaires (pour renommer des colonnes il faut\npréciser le paramètre axis = 1):\n\ndf_new = df_new.rename({\"Energie\": \"eneg\", \"Agriculture\": \"agr\"}, axis=1)\n\nEnfin, pour effacer des colonnes, on utilise la méthode drop avec l’argument\ncolumns:\n\ndf_new = df_new.drop(columns = [\"eneg\", \"agr\"])\n\n\n\nRéordonner\nLa méthode sort_values permet de réordonner un DataFrame. Par exemple,\nsi on désire classer par ordre décroissant de consommation de CO2 du secteur\nrésidentiel, on fera\n\ndf = df.sort_values(\"Résidentiel\", ascending = False)\n\nAinsi, en une ligne de code, on identifie les villes où le secteur\nrésidentiel consomme le plus.\n\n\nFiltrer\nL’opération de sélection de lignes s’appelle FILTER en SQL. Elle s’utilise\nen fonction d’une condition logique (clause WHERE). On sélectionne les\ndonnées sur une condition logique. Il existe plusieurs méthodes en pandas.\nLa plus simple est d’utiliser les boolean mask, déjà vus dans le chapitre\nnumpy.\nPar exemple, pour sélectionner les communes dans les Hauts-de-Seine, on\npeut utiliser le résultat de la méthode str.startswith (qui renvoie\nTrue ou False) directement dans les crochets:\n\ndf[df['INSEE commune'].str.startswith(\"92\")].head(2)\n\n\n\n\n\n\n\n\nINSEE commune\nCommune\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nDéchets\nEnergie\nIndustrie hors-énergie\nRésidentiel\nRoutier\nTertiaire\n\n\n\n\n35494\n92012\nBOULOGNE-BILLANCOURT\nNaN\n1250.483441\n34.234669\n51730.704250\n964.828694\n8817.818741\n25882.493998\n92216.971456\n64985.280901\n60349.109482\n\n\n35501\n92025\nCOLOMBES\nNaN\n411.371588\n14.220061\n53923.847088\n698.685861\n12855.885267\n50244.664227\n87469.549463\n52070.927943\n41526.600867\n\n\n\n\n\n\n\nPour remplacer des valeurs spécifiques, on utilise la méthode where ou une\nréassignation couplée à la méthode précédente.\nPar exemple, pour assigner des valeurs manquantes aux départements du 92,\non peut faire cela\n\ndf_copy = df.copy()\ndf_copy = df_copy.where(~df['INSEE commune'].str.startswith(\"92\"))\n\net vérifier les résultats:\n\ndf_copy[df['INSEE commune'].str.startswith(\"92\")].head(2)\ndf_copy[~df['INSEE commune'].str.startswith(\"92\")].head(2)\n\n\n\n\n\n\n\n\nINSEE commune\nCommune\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nDéchets\nEnergie\nIndustrie hors-énergie\nRésidentiel\nRoutier\nTertiaire\n\n\n\n\n12167\n31555\nTOULOUSE\n1434.045233\n4482.980062\n130.792683\n576394.181208\n88863.732538\n91549.914092\n277062.573234\n410675.902028\n586054.672836\n288175.400126\n\n\n16774\n44109\nNANTES\n248.019465\n138738.544337\n250814.701179\n193478.248177\n18162.261628\n17461.400209\n77897.138554\n354259.013785\n221068.632724\n173447.582779\n\n\n\n\n\n\n\nou alors utiliser une réassignation plus classique:\n\ndf_copy = df.copy()\ndf_copy[df_copy['INSEE commune'].str.startswith(\"92\")] = np.nan\n\nIl est conseillé de filtrer avec loc en utilisant un masque.\nEn effet, contrairement à df[mask], df.loc[mask, :] permet d’indiquer clairement\nà Python que l’on souhaite appliquer le masque aux labels de l’index.\nCe n’est pas le cas avec df[mask].\nD’ailleurs, lorsqu’on utilise la syntaxe df[mask], pandas renvoie généralement un warning\n\n\nOpérations par groupe\nEn SQL, il est très simple de découper des données pour\neffectuer des opérations sur des blocs cohérents et recollecter des résultats\ndans la dimension appropriée.\nLa logique sous-jacente est celle du split-apply-combine qui est repris\npar les langages de manipulation de données, auxquels pandas\nne fait pas exception.\nL’image suivante, issue de\nce site\nreprésente bien la manière dont fonctionne l’approche\nsplit-apply-combine\n\n\n\nSplit-apply-combine\n\n\nCe tutoriel sur le sujet\nest particulièrement utile.\nPour donner quelques exemples, on peut créer une variable départementale qui\nservira de critère de groupe.\n\ndf['dep'] = df['INSEE commune'].str[:2]\n\nEn pandas, on utilise groupby pour découper les données selon un ou\nplusieurs axes. Techniquement, cette opération consiste à créer une association\nentre des labels (valeurs des variables de groupe) et des\nobservations.\nPar exemple, pour compter le nombre de communes par département en SQL, on\nutiliserait la requête suivante :\nSELECT dep, count(INSEE commune)\nFROM df\nGROUP BY dep;\nCe qui, en pandas, donne:\n\ndf.groupby('dep')[\"INSEE commune\"].count()\n\ndep\n01    410\n02    805\n03    318\n04    199\n05    168\n     ... \n91    196\n92     36\n93     40\n94     47\n95    185\nName: INSEE commune, Length: 96, dtype: int64\n\n\nLa syntaxe est quasiment transparente. On peut bien sûr effectuer des opérations\npar groupe sur plusieurs colonnes. Par exemple,\n\ndf.groupby('dep').mean(numeric_only = True)\n\n\n\n\n\n\n\n\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nDéchets\nEnergie\nIndustrie hors-énergie\nRésidentiel\nRoutier\nTertiaire\n\n\ndep\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n01\n1974.535382\n100.307344\n8.900375\n1736.353087\n671.743966\n280.485435\n1744.567552\n1346.982227\n3988.658995\n1021.089078\n\n\n02\n1585.417729\n202.878748\n17.390638\n767.072924\n223.907551\n76.316247\n932.135611\n793.615867\n1722.240298\n403.744266\n\n\n03\n6132.029417\n240.076499\n45.429978\n1779.630883\n349.746819\n326.904841\n1452.423506\n1401.650215\n3662.773062\n705.937016\n\n\n04\n1825.455590\n177.321816\nNaN\n583.198128\n253.975910\n62.808435\n313.913553\n587.116013\n1962.654370\n493.609329\n\n\n05\n1847.508592\n141.272766\nNaN\n502.012857\n132.548068\n34.971220\n102.649239\n728.734494\n2071.010178\n463.604908\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n91\n802.793163\n10114.998156\n73976.107892\n3716.906101\n1496.516194\n538.761253\n1880.810170\n6532.123033\n10578.452789\n3866.757200\n\n\n92\n8.309835\n362.964554\n13.132461\n29663.579634\n7347.163353\n6745.611611\n19627.706224\n40744.279029\n33289.456629\n23222.587595\n\n\n93\n50.461775\n1753.443710\n61188.896632\n18148.789684\n6304.173594\n2570.941598\n10830.409025\n32911.305703\n35818.236459\n21575.444794\n\n\n94\n48.072971\n5474.808839\n16559.384091\n14710.744314\n4545.099181\n1624.281505\n9940.192318\n28444.561597\n24881.531613\n16247.876321\n\n\n95\n609.172047\n682.143912\n37984.576873\n3408.871963\n1334.032970\n463.860672\n1729.692179\n6684.181989\n8325.948748\n4014.985843\n\n\n\n\n96 rows × 10 columns\n\n\n\nA noter que la variable de groupe, ici dep, devient, par défaut, l’index\ndu DataFrame de sortie. Si on avait utilisé plusieurs variables de groupe,\non obtiendrait un objet multi-indexé. Sur la gestion des multi-index, on\npourra se référer à l’ouvrage Modern Pandas dont la référence est\ndonnée en fin de cours.\nTant qu’on n’appelle pas une action sur un DataFrame par groupe, du type\nhead ou display, pandas n’effectue aucune opération. On parle de\nlazy evaluation. Par exemple, le résultat de df.groupby('dep') est\nune transformation qui n’est pas encore évaluée:\n\ndf.groupby('dep')\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x7fc2357358e0&gt;\n\n\nIl est possible d’appliquer plus d’une opération à la fois grâce à la méthode\nagg. Par exemple, pour obtenir à la fois le minimum, la médiane et le maximum\nde chaque département, on peut faire:\n\nnumeric_columns = df.select_dtypes(['number']).columns\ndf.loc[:, numeric_columns.tolist() + [\"dep\"] ].groupby('dep').agg(['min',\"median\",\"max\"], numeric_only = True)\n\n\n\n\n\n\n\n\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\n...\nIndustrie hors-énergie\nRésidentiel\nRoutier\nTertiaire\n\n\n\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\n...\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\n\n\ndep\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n01\n0.003432\n1304.519570\n14402.057335\n3.307596\n75.686090\n617.281080\n0.297256\n6.985161\n2.209492e+01\n30.571400\n...\n175185.892467\n9.607822\n351.182294\n57689.832901\n20.848982\n1598.934149\n45258.256406\n10.049230\n401.490676\n30847.366865\n\n\n02\n0.391926\n1205.725078\n13257.716591\n0.326963\n130.054615\n1126.961565\n0.517437\n15.492120\n5.799402e+01\n28.294993\n...\n220963.067245\n7.849347\n138.819865\n99038.124236\n22.936184\n700.826152\n49245.101730\n6.220952\n130.639994\n34159.345750\n\n\n03\n5.041238\n5382.194339\n24912.249269\n24.158870\n144.403590\n1433.217868\n29.958027\n42.762328\n8.269019e+01\n44.825515\n...\n154061.446374\n19.441088\n217.959697\n75793.882483\n120.667614\n1426.905646\n40957.845304\n17.705787\n191.892445\n31099.772884\n\n\n04\n30.985972\n1404.752852\n11423.535554\n33.513854\n158.780500\n362.637639\nNaN\nNaN\nNaN\n7.162928\n...\n16889.531061\n1.708652\n133.130946\n18088.189529\n30.206298\n687.390045\n31438.078325\n0.957070\n122.504902\n16478.024806\n\n\n05\n38.651727\n1520.896526\n13143.465812\n0.299734\n139.754980\n456.042002\nNaN\nNaN\nNaN\n20.931602\n...\n4271.129851\n6.871678\n211.945147\n46486.555748\n57.132270\n958.506314\n37846.651181\n4.785348\n151.695524\n23666.235898\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n91\n0.400740\n516.908303\n5965.349174\n25.785594\n177.177127\n513140.971691\n1.651873\n14.762210\n7.858782e+05\n41.661474\n...\n50288.560827\n15.886514\n2580.902085\n48464.979708\n20.260110\n3610.009634\n72288.020125\n36.368643\n1428.426303\n38296.204729\n\n\n92\n0.073468\n6.505185\n32.986132\n7.468879\n297.529178\n1250.483441\n1.104401\n11.482381\n3.423467e+01\n2173.614704\n...\n95840.512400\n4122.277198\n33667.904692\n92216.971456\n4968.382962\n23516.458236\n113716.853033\n800.588678\n18086.633085\n65043.364499\n\n\n93\n3.308495\n3.308495\n1362.351634\n24.188172\n320.755486\n45251.869710\n0.171075\n12.449476\n1.101146e+06\n899.762120\n...\n89135.302368\n4364.038661\n31428.227282\n87927.730552\n1632.496185\n22506.758771\n193039.792609\n2257.370945\n20864.923339\n71918.163984\n\n\n94\n1.781885\n1.781885\n556.939161\n6.249609\n294.204166\n103252.271268\n0.390223\n14.944807\n1.571965e+05\n928.232154\n...\n96716.055178\n2668.358896\n24372.900300\n100948.169898\n1266.101605\n19088.651049\n97625.957714\n1190.115985\n14054.223449\n58528.623477\n\n\n95\n8.779506\n445.279844\n2987.287417\n1.749091\n80.838639\n44883.982753\n0.201508\n13.149987\n1.101131e+06\n13.490977\n...\n66216.914749\n11.585833\n1434.343631\n104543.465908\n2.619451\n3417.197938\n147040.904455\n11.484835\n725.467969\n61497.821477\n\n\n\n\n96 rows × 30 columns\n\n\n\nLa première ligne est présente pour nous faciliter la récupération des noms de colonnes des variables\nnumériques\n\n\nAppliquer des fonctions\npandas est, comme on a pu le voir, un package très flexible, qui\npropose une grande variété de méthodes optimisées. Cependant, il est fréquent\nd’avoir besoin de méthodes non implémentées.\nDans ce cas, on recourt souvent aux lambda functions. Par exemple, si\non désire connaître les communes dont le nom fait plus de 40 caractères,\non peut appliquer la fonction len de manière itérative:\n\n# Noms de communes superieurs à 40 caracteres\ndf[df['Commune'].apply(lambda s: len(s)&gt;40)]\n\n\n\n\n\n\n\n\nINSEE commune\nCommune\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nDéchets\nEnergie\nIndustrie hors-énergie\nRésidentiel\nRoutier\nTertiaire\ndep\n\n\n\n\n28082\n70058\nBEAUJEU-SAINT-VALLIER-PIERREJUX-ET-QUITTEUR\n4024.909815\n736.948351\n41.943384\n1253.135313\n125.101996\n2.354558\n6.911213\n549.734302\n1288.215480\n452.693897\n70\n\n\n4984\n14621\nSAINT-MARTIN-DE-BIENFAITE-LA-CRESSONNIERE\n1213.333523\nNaN\nNaN\n677.571743\n72.072503\n63.573059\n186.602760\n298.261044\n1396.353375\n260.801452\n14\n\n\n19276\n51513\nSAINT-REMY-EN-BOUZEMONT-SAINT-GENEST-ET-ISSON\n1927.401921\nNaN\nNaN\n595.583152\n71.675773\n4.709115\n13.822427\n273.826687\n521.864748\n259.365848\n51\n\n\n5402\n16053\nBORS (CANTON DE BAIGNES-SAINTE-RADEGONDE)\n1919.249545\nNaN\nNaN\n165.443226\n16.265904\n2.354558\n6.911213\n54.561623\n719.293151\n58.859777\n16\n\n\n\n\n\n\n\nCependant, toutes les lambda functions ne se justifient pas.\nPar exemple, prenons\nle résultat d’agrégation précédent. Imaginons qu’on désire avoir les résultats\nen milliers de tonnes. Dans ce cas, le premier réflexe est d’utiliser\nla lambda function suivante :\n\nnumeric_columns = df.select_dtypes(['number']).columns\n(df\n    .loc[:, numeric_columns.tolist() + [\"dep\"] ]\n    .groupby('dep')\n    .agg(['min',\"median\",\"max\"])\n    .apply(lambda s: s/1000)\n)\n\n\n\n\n\n\n\n\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\n...\nIndustrie hors-énergie\nRésidentiel\nRoutier\nTertiaire\n\n\n\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\n...\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\n\n\ndep\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n01\n0.000003\n1.304520\n14.402057\n0.003308\n0.075686\n0.617281\n0.000297\n0.006985\n0.022095\n0.030571\n...\n175.185892\n0.009608\n0.351182\n57.689833\n0.020849\n1.598934\n45.258256\n0.010049\n0.401491\n30.847367\n\n\n02\n0.000392\n1.205725\n13.257717\n0.000327\n0.130055\n1.126962\n0.000517\n0.015492\n0.057994\n0.028295\n...\n220.963067\n0.007849\n0.138820\n99.038124\n0.022936\n0.700826\n49.245102\n0.006221\n0.130640\n34.159346\n\n\n03\n0.005041\n5.382194\n24.912249\n0.024159\n0.144404\n1.433218\n0.029958\n0.042762\n0.082690\n0.044826\n...\n154.061446\n0.019441\n0.217960\n75.793882\n0.120668\n1.426906\n40.957845\n0.017706\n0.191892\n31.099773\n\n\n04\n0.030986\n1.404753\n11.423536\n0.033514\n0.158781\n0.362638\nNaN\nNaN\nNaN\n0.007163\n...\n16.889531\n0.001709\n0.133131\n18.088190\n0.030206\n0.687390\n31.438078\n0.000957\n0.122505\n16.478025\n\n\n05\n0.038652\n1.520897\n13.143466\n0.000300\n0.139755\n0.456042\nNaN\nNaN\nNaN\n0.020932\n...\n4.271130\n0.006872\n0.211945\n46.486556\n0.057132\n0.958506\n37.846651\n0.004785\n0.151696\n23.666236\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n91\n0.000401\n0.516908\n5.965349\n0.025786\n0.177177\n513.140972\n0.001652\n0.014762\n785.878155\n0.041661\n...\n50.288561\n0.015887\n2.580902\n48.464980\n0.020260\n3.610010\n72.288020\n0.036369\n1.428426\n38.296205\n\n\n92\n0.000073\n0.006505\n0.032986\n0.007469\n0.297529\n1.250483\n0.001104\n0.011482\n0.034235\n2.173615\n...\n95.840512\n4.122277\n33.667905\n92.216971\n4.968383\n23.516458\n113.716853\n0.800589\n18.086633\n65.043364\n\n\n93\n0.003308\n0.003308\n1.362352\n0.024188\n0.320755\n45.251870\n0.000171\n0.012449\n1101.145545\n0.899762\n...\n89.135302\n4.364039\n31.428227\n87.927731\n1.632496\n22.506759\n193.039793\n2.257371\n20.864923\n71.918164\n\n\n94\n0.001782\n0.001782\n0.556939\n0.006250\n0.294204\n103.252271\n0.000390\n0.014945\n157.196520\n0.928232\n...\n96.716055\n2.668359\n24.372900\n100.948170\n1.266102\n19.088651\n97.625958\n1.190116\n14.054223\n58.528623\n\n\n95\n0.008780\n0.445280\n2.987287\n0.001749\n0.080839\n44.883983\n0.000202\n0.013150\n1101.131222\n0.013491\n...\n66.216915\n0.011586\n1.434344\n104.543466\n0.002619\n3.417198\n147.040904\n0.011485\n0.725468\n61.497821\n\n\n\n\n96 rows × 30 columns\n\n\n\nEn effet, cela effectue le résultat désiré. Cependant, il y a mieux : utiliser\nla méthode div:\n\nimport timeit\ndf_numeric = df.loc[:, numeric_columns.tolist() + [\"dep\"] ]\n%timeit df_numeric.groupby('dep').agg(['min',\"median\",\"max\"]).div(1000)\n%timeit df_numeric.groupby('dep').agg(['min',\"median\",\"max\"]).apply(lambda s: s/1000)\n\nLa méthode div est en moyenne plus rapide et a un temps d’exécution\nmoins variable. Dans ce cas, on pourrait même utiliser le principe\ndu broadcasting de numpy (cf. chapitre numpy) qui offre\ndes performances équivalentes:\n\n%timeit df_numeric.groupby('dep').agg(['min',\"median\",\"max\"])/1000\n\napply est plus rapide qu’une boucle (en interne, apply utilise Cython\npour itérer) mais reste moins rapide qu’une solution vectorisée quand\nelle existe. Ce site\npropose des solutions, par exemple les méthodes isin ou digitize, pour\néviter de manuellement créer des boucles lentes.\nEn particulier, il faut noter que apply avec le paramètre axis=1 est en générale lente.\n\n\nJoindre des données\nIl est commun de devoir combiner des données issues de sources différentes.\nNous allons ici nous focaliser sur le cas le plus favorable qui est la situation\noù une information permet d’apparier de manière exacte deux bases de données (autrement nous\nserions dans une situation, beaucoup plus complexe, d’appariement flou1).\nLa situation typique est l’appariement entre deux sources de données selon un identifiant\nindividuel. Ici, il s’agit d’un identifiant de code commune.\nIl est recommandé de lire ce guide assez complet sur la question des jointures avec R\nqui donne des recommandations également utiles pour un utilisateur de Python.\n\n\n\n\n\nOn utilise de manière indifférente les termes merge ou join.\nLe deuxième terme provient de la syntaxe SQL.\nEn Pandas, dans la plupart des cas, on peut utiliser indifféremment df.join et df.merge\n\n\n\n\n\nIl est aussi possible de réaliser un merge en utilisant la fonction pandas.concat() avec axis=1.\nSe référer à la documentation de concat pour voir les options possibles.\n\n\nRestructurer des données (reshape)\nOn présente généralement deux types de données :\n\nformat wide : les données comportent des observations répétées, pour un même individu (ou groupe), dans des colonnes différentes\nformat long : les données comportent des observations répétées, pour un même individu, dans des lignes différentes avec une colonne permettant de distinguer les niveaux d’observations\n\nUn exemple de la distinction entre les deux peut être emprunté à l’ouvrage de référence d’Hadley Wickham, R for Data Science:\n\n\n\n\n\nL’aide mémoire suivante aidera à se rappeler les fonctions à appliquer si besoin :\n\n\n\n\n\nLe fait de passer d’un format wide au format long (ou vice-versa) peut être extrêmement pratique car\ncertaines fonctions sont plus adéquates sur une forme de données ou sur l’autre.\nEn règle générale, avec Python comme avec R, les formats long sont souvent préférables.\nLe chapitre suivant, qui fait office de TP, proposera des applications de ces principes :\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLes pipe\nEn général, dans un projet, le nettoyage de données va consister en un ensemble de\nméthodes appliquées à un pandas.DataFrame.\nOn a vu que assign permettait de créer une variable dans un DataFrame.\nIl est également possible d’appliquer une fonction, appelée par exemple my_udf au\nDataFrame grâce à pipe:\ndf = (pd.read_csv(path2data)\n            .pipe(my_udf))\nL’utilisation des pipe rend le code très lisible et peut être très\npratique lorsqu’on enchaine des opérations sur le même\ndataset."
  },
  {
    "objectID": "content/manipulation/02a_pandas_tutorial.html#quelques-enjeux-de-performance",
    "href": "content/manipulation/02a_pandas_tutorial.html#quelques-enjeux-de-performance",
    "title": "Introduction à Pandas",
    "section": "Quelques enjeux de performance",
    "text": "Quelques enjeux de performance\nLa librairie Dask intègre la structure de numpy, pandas et sklearn.\nElle a vocation à traiter de données en grande dimension, ainsi elle ne sera pas\noptimale pour des données qui tiennent très bien en RAM.\nIl s’agit d’une librairie construite sur la parallélisation.\nUn chapitre dans ce cours lui est consacré.\nPour aller plus loin, se référer à la documentation de Dask."
  },
  {
    "objectID": "content/manipulation/02a_pandas_tutorial.html#références",
    "href": "content/manipulation/02a_pandas_tutorial.html#références",
    "title": "Introduction à Pandas",
    "section": "Références",
    "text": "Références\n\nLe site\npandas.pydata\nfait office de référence\nLe livre Modern Pandas de Tom Augspurger : https://tomaugspurger.github.io/modern-1-intro.html\n\n\n\nMcKinney, Wes. 2012. Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython. \" O’Reilly Media, Inc.\"."
  },
  {
    "objectID": "content/manipulation/02a_pandas_tutorial.html#footnotes",
    "href": "content/manipulation/02a_pandas_tutorial.html#footnotes",
    "title": "Introduction à Pandas",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSur l’appariement flou, se reporter aux chapitres présentant ElasticSearch.↩︎"
  },
  {
    "objectID": "content/manipulation/03_geopandas_tutorial.html",
    "href": "content/manipulation/03_geopandas_tutorial.html",
    "title": "Données spatiales : découverte de geopandas",
    "section": "",
    "text": "Dans ce tutoriel, nous allons utiliser les données suivantes :\nLa représentation des données, notamment la cartographie, est présentée plus\namplement dans la partie visualiser. Quelques méthodes\npour faire rapidement des cartes seront présentées ici, mais\nl’objet de ce chapitre porte davantage sur la manipulation des données géographiques.\nCe tutoriel s’inspire beaucoup d’un autre tutoriel que j’ai fait pour\nR disponible\ndans la documentation utilitr.\nIl peut servir de pendant à celui-ci pour l’utilisateur de R.\nQuelques installations préalables sont nécessaires :\n!pip install pandas fiona shapely pyproj rtree # à faire obligatoirement en premier pour utiliser rtree ou pygeos pour les jointures spatiales\n!pip install contextily\n!pip install geopandas\n!pip install topojson\nPour être en mesure d’exécuter ce tutoriel, les imports suivants\nseront utiles.\nimport geopandas as gpd\nimport contextily as ctx\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "content/manipulation/03_geopandas_tutorial.html#données-spatiales",
    "href": "content/manipulation/03_geopandas_tutorial.html#données-spatiales",
    "title": "Données spatiales : découverte de geopandas",
    "section": "Données spatiales",
    "text": "Données spatiales\n\nQuelle différence avec des données traditionnelles ?\nLe terme “données spatiales” désigne les données qui portent sur les caractéristiques géographiques des objets (localisation, contours, liens).\nLes caractéristiques géographiques des objets sont décrites à l’aide d’un système de coordonnées\nqui permettent une représentation dans un espace euclidien \\((x,y)\\).\nLe passage de l’espace réel (la Terre, qui est une sphère) à l’espace plan\nse fait grâce à un système de projection. Voici quelques exemples\nde données spatiales :\n\nUne table décrivant des bâtiments, avec les coordonnées géographiques de chaque bâtiment ;\nLe découpage communal du territoire, avec le contour du territoire de chaque commune ;\nLes routes terrestres, avec les coordonnées décrivant leur parcours en 3 dimensions (longitude, latitude, altitude).\n\nLes données spatiales rassemblent classiquement deux types de données :\n\ndes données géographiques (ou géométries) : objets géométriques tels que des points, des vecteurs, des polygones, ou des maillages (raster). Exemple: la forme de chaque commune, les coordonnées d’un bâtiment;\ndes données attributaires (ou attributs) : des mesures et des caractéristiques associées aux objets géométriques. Exemple: la population de chaque commune, le nombre de fenêtres et le nombre d’étages d’un bâtiment.\n\nLes données spatiales sont fréquemment traitées à l’aide d’un système d’information géographique (SIG), c’est-à-dire un système d’information capable de stocker, d’organiser et de présenter des données alphanumériques spatialement référencées par des coordonnées dans un système de référence (CRS). Python dispose de fonctionnalités lui permettant de réaliser les mêmes tâches qu’un SIG (traitement de données spatiales, représentations cartographiques).\n\n\nDe Pandas à Geopandas\nLe package Geopandas est une boîte à outils conçue pour faciliter la manipulation de données spatiales. La grande force de Geopandas est qu’il permet de manipuler des données spatiales comme s’il s’agissait de données traditionnelles, car il repose sur le standard ISO 19125 simple feature access défini conjointement par l’Open Geospatial Consortium (OGC) et l’International Organization for Standardization (ISO).\nPar rapport à un DataFrame standard, un objet Geopandas comporte\nune colonne supplémentaire: geometry. Elle stocke les coordonnées des\nobjets géographiques (ou ensemble de coordonnées s’agissant de contours). Un objet Geopandas hérite des propriétés d’un\nDataFrame Pandas mais propose des méthodes adaptées au traitement des données spatiales.\nAinsi, grâce à Geopandas, on pourra effectuer des manipulations sur les attributs des données comme avec pandas mais on pourra également faire des manipulations sur la dimension spatiale des données. En particulier,\n\nCalculer des distances et des surfaces ;\nAgréger rapidement des zonages (regrouper les communes en département par exemple) ;\nTrouver dans quelle commune se trouve un bâtiment à partir de ses coordonnées géographiques ;\nRecalculer des coordonnées dans un autre système de projection ;\nFaire une carte, rapidement et simplement.\n\n\n\n Hint\nLes manipulations de données sur un objet Geopandas sont nettement plus lentes que sur\nun DataFrame traditionnel (car Python doit gérer les informations géographiques pendant la manipulation des données).\nLorsque vous manipulez des données de grandes dimensions,\nil peut être préférable d’effectuer les opérations sur les données avant de joindre une géométrie à celles-ci.\n\n\nPar rapport à un logiciel spécialisé comme QGIS, Python permettra\nd’automatiser le traitement et la représentation des données. D’ailleurs,\nQGIS utilise lui-même Python…\n\n\nRésumé\nEn résumé, un objet GeoPandas comporte les éléments suivantes :\n\n\n\n\n\n\nLes attributs. Ce sont les valeurs associées à chaque niveau géographique.\nIl s’agit de la dimension tabulaire usuelle, dont le traitement est similaire\nà celui d’un objet Pandas classique.\nLes géométries. Ce sont les valeurs numériques interprétées pour représenter la dimension géographique. Elles permettent de représenter dans un certain\nréférentiel (le système de référence) la dimension géographique.\nLe système de référence. Il s’agit du système permettant de transformer les positions sur\nle globe (3 dimensions avec une boule asymétrique) en un plan en deux dimensions.\nIl en existe une multitude, identifiables à partir d’un code EPSG (4326, 2154…).\nLeur manipulation est facilitée par Geopandas qui s’appuie sur Shapely, de la même\nmanière que Pandas s’appuie sur Numpy ou Arrow."
  },
  {
    "objectID": "content/manipulation/03_geopandas_tutorial.html#le-système-de-projection-cartographique",
    "href": "content/manipulation/03_geopandas_tutorial.html#le-système-de-projection-cartographique",
    "title": "Données spatiales : découverte de geopandas",
    "section": "Le système de projection cartographique",
    "text": "Le système de projection cartographique\n\nPrincipe\nLes données spatiales sont\nplus riches que les données traditionnelles car elles\nincluent, habituellement, des éléments supplémentaires pour placer dans\nun espace cartésien les objets. Cette dimension supplémentaire peut être simple\n(un point comporte deux informations supplémentaire: \\(x\\) et \\(y\\)) ou\nassez complexe (polygones, lignes avec direction, etc.).\nL’analyse cartographique emprunte dès lors à la géométrie\ndes concepts\npour représenter des objets dans l’espace. Les projections\nsont au coeur de la gestion des données spatiales.\nCes dernières consistent à transformer une position dans l’espace\nterrestre à une position sur un plan. Il s’agit donc d’une opération\nde projection d’un espace tri-dimensionnel dans un espace\nà deux dimensions.\nCe post propose de riches éléments sur le\nsujet, notamment l’image suivante qui montre bien le principe d’une projection :\n\n\n\nLes différents types de projection\n\n\nCette opération n’est pas neutre. L’une des conséquences du\nthéorème remarquable de Gauss\nest que la surface de la Terre ne peut être cartographiée sans distortion.\nUne projection ne peut simultanément conserver intactes les distances et les\nangles (i.e. les positions).\nIl n’existe ainsi pas de projection universellement meilleure, ce qui ouvre\nla porte à la coexistence de nombreuses projections différentes, pensées\npour des tâches différentes.\nUn mauvais système de représentation\nfausse l’appréciation visuelle mais peut aussi entraîner des erreurs dans\nles calculs sur la dimension spatiale.\nLes systèmes de projection font l’objet de standards internationaux et sont souvent désignés par des codes dits codes EPSG. Ce site est un bon aide-mémoire. Les plus fréquents, pour les utilisateurs français, sont les suivants (plus d’infos ici) :\n\n2154 : système de projection Lambert 93. Il s’agit du système de projection officiel. La plupart des données diffusées par l’administration pour la métropole sont disponibles dans ce système de projection.\n27572 : Lambert II étendu. Il s’agit de l’ancien système de projection officiel. Les données spatiales anciennes peuvent être dans ce format.\n4326 : WGS 84 ou système de pseudo-Mercator ou encore Web Mercator. Ce n’est en réalité pas un système de projection mais un système de coordonnées (longitude / latitude) qui permet simplement un repérage angulaire sur l’ellipsoïde. Il est utilisé pour les données GPS. Il s’agit du système le plus\nusuel, notamment quand on travaille avec des fonds de carte web.\n\nComme évoqué plus haut, l’une des projections les plus connues est la\nprojection Web Mercator dite WGS84 (code EPSG 4326). Il\ns’agit d’une projection conservant intacte les angles, ce\nqui implique qu’elle altère les distances. Celle-ci a en effet été\npensée, à l’origine, pour représenter l’hémisphère Nord. Plus\non s’éloigne de celui-ci, plus les distances sont distordues. Cela\namène à des distorsions bien\nconnues (le Groenland hypertrophié, l’Afrique de taille réduite, l’Antarctique démesuré…).\nEn revanche, la projection Mercator conserve intacte les positions.\nC’est cette propriété qui explique son utilisation dans les systèmes\nGPS et ainsi dans les fonds de carte de navigation du type Google Maps.\n\n\n\nExemple de reprojection de pays depuis le site thetruesize.com\n\n\nObservez les variations significatives\nde proportions pour certains pays selon les projections\nchoisies:\n\nhtml`&lt;div&gt;${container_projection}&lt;/div&gt;`\n\n\n\n\n\n\n\ncontainer_projection = html`&lt;div class=\"container\"&gt;\n  &lt;div class=\"row\"&gt;\n    &lt;div class=\"projection\"&gt;\n      &lt;div class=\"projection-label\"&gt;Choisir une projection&lt;/div&gt;\n      ${viewof projection}\n    &lt;/div&gt;\n  &lt;/div&gt;\n  &lt;div class=\"row\"&gt;\n    &lt;div class=\"projectedMap\"&gt;\n      ${projectedMap}\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/div&gt;`\n\n\n\n\n\n\n\nviewof projection = projectionInput({\n  name: \"\",\n  value: \"Mercator\"\n})\n\n\n\n\n\n\n\nimport {projectionInput} from \"@fil/d3-projections\"\nimport {map} from \"@linogaliana/base-map\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprojectedMap = map(projection,\n                   {\n                     //svg: true,\n                     value: projection.options,\n                     width: width_projected_map,\n                     //height: 300,\n                     //rotate: [0, -90],\n                     //inertia: true,\n                     show_equator: true,\n                     background: \"#f1f0eb\"\n                     \n                     //show_structure: true\n                   })\n\n\n\n\n\n\n\nwidth_projected_map = screen.width/2\n\n\n\n\n\n\nPour aller plus loin, la carte interactive\nsuivante, construite par Nicolas Lambert, issue de\nce notebook Observable, illustre l’effet\ndéformant de la projection Mercator, et de quelques-unes autres,\nsur notre perception de la taille des pays.\n\n\nVoir la carte interactive\n\n\nhtml`&lt;div class=\"grid-container\"&gt;\n  &lt;div class=\"viewof-projection\"&gt;${viewof projectionBertin}&lt;/div&gt;\n  &lt;div class=\"viewof-mycountry\"&gt;${viewof mycountry}&lt;/div&gt;\n  &lt;div class=\"map-bertin\"&gt;${mapBertin}&lt;/div&gt;\n&lt;/div&gt;`\n\n\n\n\n\n\n\n\nimport {map as mapBertin, viewof projection as projectionBertin, viewof mycountry} from \"@neocartocnrs/impact-of-projections-on-areas\"\n\n\n\n\n\n\nIl existe en fait de nombreuses représentations possibles du monde, plus ou moins\nalambiquées. Les projections sont très nombreuses et certaines peuvent avoir une forme suprenante.\nPar exemple,\nla projection de Spillhaus\npropose de centrer la vue sur les océans et non une terre. C’est pour\ncette raison qu’on parle parfois de monde tel que vu par les poissons\nà son propos.\n\nhtml`&lt;div class=\"centered\"&gt;${spilhaus}&lt;/div&gt;`\n\n\n\n\n\n\n\nspilhaus = {\n  const width = 600;\n  const height = width;\n\n  const context = DOM.context2d(width, height);\n  const projection = d3.geoStereographic()\n    .rotate([95, 45])\n    .translate([width / 2, height / 2])\n    .scale(width / 10.1)\n    .center([30, -5])\n    .clipAngle(166);\n  const path = d3.geoPath(projection, context);\n\n  const land = topojson.feature(world, world.objects.land);\n\n  context.lineJoin = \"round\";\n  context.lineCap = \"round\";\n  context.fillStyle = \"#f2f1ed\";\n  context.fillRect(0, 0, width, height);\n\n  context.beginPath();\n  path({type: \"Sphere\"});\n  path(land);\n  context.lineWidth = 0.5;\n  context.stroke();\n  context.clip(\"evenodd\");\n\n  context.save();\n  context.beginPath();\n  path(land);\n  context.filter = \"blur(12px)\";\n  context.fillStyle = \"#006994\";\n  context.fill(\"evenodd\");\n  context.restore();\n  \n  context.beginPath();\n  path(d3.geoGraticule10());\n  context.globalAlpha = 0.2;\n  context.strokeStyle = \"#000\";\n  context.stroke();\n\n  return context.canvas;\n}\n\n\n\n\n\n\n\n//import {map as spilhausmap} with {height, width} from \"@d3/spilhaus-shoreline-map\"\nimport { world } from \"@d3/spilhaus-shoreline-map\"\n\n\n\n\n\n\n\n\n Astuce pour la France\nPour la France, dans le système WGS84 (4326) :\n\nLongitude (\\(x\\)) tourne autour de 0° (de -5.2 à +9.6 pour être plus précis)\nLa latitude (\\(y\\)) autour de 45 (entre +41.3 à +51.1)\n\nDans le système Lambert 93 (2154) :\n\nCoordonnées \\(x\\): entre 100 000 et 1 300 000\nLa latitude (\\(y\\)): entre 6 000 000 et 7 200 000\n\nPlus de détails"
  },
  {
    "objectID": "content/manipulation/03_geopandas_tutorial.html#importer-des-données-spatiales",
    "href": "content/manipulation/03_geopandas_tutorial.html#importer-des-données-spatiales",
    "title": "Données spatiales : découverte de geopandas",
    "section": "Importer des données spatiales",
    "text": "Importer des données spatiales\nLes formats les plus communs de données spatiales sont les suivants :\n\nshapefile (.shp) : format (propriétaire) le plus commun de données géographiques.\nLa table de données (attributs) est stockée dans un fichier séparé des\ndonnées spatiales. En faisant geopandas.read_file(\"monfichier.shp\"), le\npackage fait lui-même le lien entre les observations et leur représentation spatiale.\ngeopackage (.gpkg) : ce (relativement) nouveau format libre en un seul fichier également (lui recommandé par l’OGC) vise progressivement à se substituer au shapefile. Il est par exemple le format par défaut dans QGIS.\ngeojson (.json) : ce format, non préconisé par l’OGC, est largement utilisé pour le développement web\ncomme dans la librairie leaflet.js.\nLa dimension spatiale est stockée dans le même fichier que les attributs.\nCes fichiers sont généralement beaucoup plus légers que les shapefiles mais possèdent des limites s’agissant de gros jeux de données.\ntopojson (.json) : une variante du geojson qui se développe progressivement pour assister les visualisations web. Au lieu de stocker l’ensemble des points permettant de représenter une\ngéométrie, seuls les arcs sont conservés. Cela allège substantiellement le poids du fichier et\npermet, avec une librairie adaptée, de reconstruire l’ensemble des contours géographiques.\n\nCette page compare plus en détail les principes formats de données géographiques.\nL’aide de Geopandas propose des bouts de code en fonction des différentes situations dans lesquelles on se trouve.\n\nExemple : récupérer les découpages territoriaux\nL’un des fonds de carte les plus fréquents qu’on utilise est celui des\nlimites administratives des communes.\nCelui-ci peut être récupéré de plusieurs manières.\nEn premier lieu, pour récupérer\nle fond de carte officiel, produit par l’IGN, sous\nle nom d’AdminExpress[^1],\nil est possible de se rendre sur le site de l’IGN et de le télécharger.\nIl est également possible d’utiliser l’une des API de l’IGN\nmais ces dernières ne sont pas encore très documentées pour des utilisateurs\nde Python.\nLe package cartiflette, issu\nd’un projet interministériel, propose\nune récupération\nfacilitée de fonds de carte officiels de l’IGN.\nCe projet vise à faciliter la récupération des sources officielles, notamment\ncelles de l’IGN, et leur association à des jeux de données géographiques.\n\n\n Note\nLe package cartiflette est expérimental\net n’est disponible que sur\nGithub, pas sur PyPi.\nIl est amené à évoluer rapidement et cette page sera mise à jour\nquand de nouvelles fonctionalités (notamment l’utilisation d’API)\nseront disponibles pour encore simplifier la récupération de\ncontours géographiques.\nPour installer cartiflette, il est nécessaire d’utiliser les commandes suivantes\ndepuis un Jupyter Notebook (si vous utilisez la ligne de commande directement,\nvous pouvez retirer les ! en début de ligne):\n\n!pip install requests py7zr geopandas openpyxl tqdm s3fs PyYAML xlrd\n!pip install git+https://github.com/inseefrlab/cartiflette@80b8a5a28371feb6df31d55bcc2617948a5f9b1a\n\nCes commandes permettent de récupérer l’ensemble du code\nsource depuis Github\n\n\nIci, nous sommes intéressés par les contours des communes\nde la petite couronne. On pourrait désirer récupérer\nl’ensemble de la région Ile-de-France mais nous\nallons nous contenter de l’analyse de Paris intra-muros\net des départements limitrophes.\nC’est l’un des avantage de cartiflette que de faciliter\nla récupération de fonds de carte sur un ensemble de département.\nCela évite la récupération d’un fond de carte très\nvolumineux (plus de 500Mo) pour une analyse restreinte (quelques départements).\nUn autre avantage de cartiflette est de faciliter la récupération de fonds\nde carte consolidés comme celui dont on a besoin ici : arrondissements\ndans Paris, communes ailleurs. Comme cela est expliqué dans un encadré à part,\nil s’agirait d’une opération pénible à mettre en oeuvre sans cartiflette.\nLes contours de cet espace peuvent être récupérés de la manière suivante :\n\nimport cartiflette.s3 as s3\n\nshp_communes = s3.download_vectorfile_url_all(\n    crs = 4326,\n    values = [\"75\", \"92\", \"93\", \"94\"],\n    borders=\"COMMUNE_ARRONDISSEMENT\",\n    vectorfile_format=\"topojson\",\n    filter_by=\"DEPARTEMENT\",\n    source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n    year=2022)\n\n\nshp_communes.head(3)\n\n\n\n\n\n\n\n\nid\nID\nNOM\nNOM_M\nINSEE_COM\nSTATUT\nPOPULATION\nINSEE_CAN\nINSEE_ARR\nINSEE_DEP\nINSEE_REG\nSIREN_EPCI\nsource\nINSEE_COG\ngeometry\n\n\n\n\n0\nARR_MUNI0000000009736045\nNaN\nParis 3e Arrondissement\nPARIS 3E ARRONDISSEMENT\n75056\nCapitale d'état\n34025\nNR\n1\n75\n11\n200054781\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n75103\nPOLYGON ((2.35016 48.86199, 2.35019 48.86203, ...\n\n\n1\nARR_MUNI0000000009736046\nNaN\nParis 2e Arrondissement\nPARIS 2E ARRONDISSEMENT\n75056\nCapitale d'état\n21595\nNR\n1\n75\n11\n200054781\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n75102\nPOLYGON ((2.34792 48.87069, 2.34827 48.87062, ...\n\n\n2\nARR_MUNI0000000009736545\nNaN\nParis 4e Arrondissement\nPARIS 4E ARRONDISSEMENT\n75056\nCapitale d'état\n29131\nNR\n1\n75\n11\n200054781\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n75104\nPOLYGON ((2.36849 48.85580, 2.36873 48.85482, ...\n\n\n\n\n\n\n\nOn reconnaît la structure d’un DataFrame Pandas. A cette structure s’ajoute\nune colonne geometry qui enregistre la position des limites des polygones de chaque\nobservation.\nComme vu précédemment, le système de projection est un élément important. Il permet à Python\nd’interpréter les valeurs des points (deux dimensions) en position sur\nla terre, qui n’est pas un espace plan.\n\nshp_communes.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nIci, les données sont dans le système WGS84 (code EPSG 4326).\nCe n’est pas le\nLambert-93 comme on pourrait s’y attendre, ce dernier\nétant le système légal de projection pour la France\nmétropolitaine.\nPour s’assurer qu’on a bien récupéré les contours voulus,\non peut représenter graphiquement\nles contours grâce à la méthode plot sur laquelle nous\nreviendrons :\n\nax = shp_communes.boundary.plot()\nax.set_axis_off()\n\n\n\n\n\n\n\n\n\n\n Note\nSi on ne désire pas utiliser le niveau COMMUNE_ARRONDISSEMENT,\nil est nécessaire de mettre en oeuvre une construction du fond de\ncarte en plusieurs phases.\nEn premier lieu, il est nécessaire de récupérer le niveau des communes.\n\nimport cartiflette.s3 as s3\n\nshp_communes = s3.download_vectorfile_url_all(\n    crs = 4326,\n    values = [\"75\", \"92\", \"93\", \"94\"],\n    borders=\"COMMUNE\",\n    vectorfile_format=\"topojson\",\n    filter_by=\"DEPARTEMENT\",\n    source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n    year=2022)\n\n\nshp_communes.head(4)\n\n\n\n\n\n\n\n\nid\nID\nNOM\nNOM_M\nINSEE_COM\nSTATUT\nPOPULATION\nINSEE_CAN\nINSEE_ARR\nINSEE_DEP\nINSEE_REG\nSIREN_EPCI\nsource\ngeometry\n\n\n\n\n0\nCOMMUNE_0000000009736048\nNaN\nParis\nPARIS\n75056\nCapitale d'état\n2165423\nNR\n1\n75\n11\n200054781\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.36421 48.81640, 2.36333 48.81615, ...\n\n\n0\nCOMMUNE_0000000009736037\nNaN\nLevallois-Perret\nLEVALLOIS-PERRET\n92044\nCommune simple\n66082\n16\n2\n92\n11\n200054781/200057982\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.28739 48.90364, 2.28846 48.90302, ...\n\n\n1\nCOMMUNE_0000000009736055\nNaN\nBois-Colombes\nBOIS-COLOMBES\n92009\nCommune simple\n28841\n11\n2\n92\n11\n200054781/200057990\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.26639 48.90629, 2.26645 48.90615, ...\n\n\n2\nCOMMUNE_0000000009736538\nNaN\nMalakoff\nMALAKOFF\n92046\nCommune simple\n30950\n18\n1\n92\n11\n200054781/200057966\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.27818 48.81425, 2.28066 48.81469, ...\n\n\n\n\n\n\n\n\nax = shp_communes.boundary.plot()\nax.set_axis_off()\n\n\n\n\n\n\n\n\nOn peut remarquer que la ville de Paris ne comporte pas d’arrondissements\nsur cette carte. Pour vous en convaincre, vous pouvez exécuter la\ncommande :\n\nax = shp_communes.loc[shp_communes['INSEE_DEP']==\"75\"].boundary.plot()\nax.set_axis_off()\n\n\n\n\n\n\n\n\nIl faut donc utiliser une source complémentaire.\nLe contour officiel des arrondissements est\nproduit par l’IGN séparemment des contours de communes.\nLes contours d’arrondissements sont également\ndisponibles\ngrâce à cartiflette:\n\narrondissements = s3.download_vectorfile_url_all(\n    crs = 4326,\n    values = [\"75\"],\n    borders=\"COMMUNE_ARRONDISSEMENT\",\n    vectorfile_format=\"topojson\",\n    filter_by=\"DEPARTEMENT\",\n    source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n    year=2022)\n\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE_ARRONDISSEMENT/crs=4326/DEPARTEMENT=75/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\n\n\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 28.0kiB [00:00, 167kiB/s]Downloading: : 40.1kiB [00:00, 213kiB/s]\n\n\n\nax = arrondissements.boundary.plot(alpha = 0.8, edgecolor = \"k\")\nax.set_axis_off()\n\n\n\n\n\n\n\n\nIl ne reste plus qu’à remplacer Paris par\nses arrondissements dans shp_communes.\nPour cela, on peut utiliser les méthodes\nvues dans le chapitre Pandas relatives\naux filtres et à la concaténation\nde plusieurs DataFrames:\n\nimport pandas as pd\n\nshp_communes = pd.concat(\n  [\n    shp_communes.loc[shp_communes['INSEE_DEP'] != \"75\"].to_crs(2154),\n    arrondissements.to_crs(2154)\n  ])\n\n\nax = shp_communes.boundary.plot(alpha = 0.8, edgecolor = \"k\")\nax.set_axis_off()\n\n\n\n\n\n\n\n\nCette approche fonctionne mais elle nécessite un certain nombre\nde gestes, qui sont autant de risques d’erreurs. Il est\ndonc recommandé de privilégier le niveau COMMUNE_ARRONDISSEMENT\nqui fait exactement ceci mais de manière fiable."
  },
  {
    "objectID": "content/manipulation/03_geopandas_tutorial.html#opérations-sur-les-attributs-et-les-géométries",
    "href": "content/manipulation/03_geopandas_tutorial.html#opérations-sur-les-attributs-et-les-géométries",
    "title": "Données spatiales : découverte de geopandas",
    "section": "Opérations sur les attributs et les géométries",
    "text": "Opérations sur les attributs et les géométries\n\nImport des données vélib\nSouvent, le découpage communal ne sert qu’en fond de cartes, pour donner des\nrepères. En complément de celui-ci, on peut désirer exploiter\nun autre jeu de données. On va partir des données de localisation des\nstations velib,\ndisponibles sur le site d’open data de la ville de Paris et\nrequêtables directement par l’url\nhttps://opendata.paris.fr/explore/dataset/velib-emplacement-des-stations/download/?format=geojson&timezone=Europe/Berlin&lang=fr\n\nvelib_data = 'https://opendata.paris.fr/explore/dataset/velib-emplacement-des-stations/download/?format=geojson&timezone=Europe/Berlin&lang=fr'\nstations = gpd.read_file(velib_data)\nstations.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nLes données sont dans le système de projection WGS84 qui est celui du\nsystème GPS. Celui-ci s’intègre bien avec les fonds de carte\nOpenStreetMap ou Google Maps. En toute rigueur, si on\ndésire effectuer certains calculs géométriques (mesurer des surfaces…), il est\nnécessaire de re-projeter les données dans un système qui préserve la géométrie\n(c’est le cas du Lambert 93).\nPour avoir une intuition de la localisation des stations, et notamment de la\ndensité hétérogène de celles-ci,\non peut afficher les données sur la carte des communes\nde la petite couronne. Il s’agit donc d’enrichir la carte\nprécédente d’une couche supplémentaire, à savoir la localisation\ndes stations. Au passage, on va utiliser un fond de carte\nplus esthétique:\n\nfig,ax = plt.subplots(figsize=(10, 10))\nstations.sample(200).to_crs(3857).plot(ax = ax, color = 'red', alpha = 0.4, zorder=2)\nshp_communes.to_crs(3857).plot(ax = ax, zorder=1, edgecolor = \"black\", facecolor=\"none\",\n                                                           color = None)\nctx.add_basemap(ax, source = ctx.providers.OpenStreetMap.Mapnik)\nax.set_axis_off()\n\n\n\n\n\n\n\n\nDécouvrez ci-dessous par étape les différentes lignes de commandes permettant d’afficher cette carte complète,\nétape par étape :\n1️⃣ Afficher le nuage de points de 200 stations vélibs prises au hasard\n\nfig, ax = plt.subplots(figsize=(10, 10))\nstations.sample(200).to_crs(3857).plot(ax = ax, color = 'red', alpha = 0.4, zorder=2)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\n2️⃣ Ajouter à cette couche, en-dessous, les contours des communes\n\nfig, ax = plt.subplots(figsize=(10, 10))\nstations.sample(200).to_crs(3857).plot(ax = ax, color = 'red', alpha = 0.4, zorder=2)\nshp_communes.to_crs(3857).plot(ax = ax, zorder=1, edgecolor = \"black\", facecolor=\"none\",\n                                                           color = None)\n\n\n\n\n\n\n\n\n\n\n3️⃣ Ajouter un fond de carte de type open street map grâce au package\ncontextily\n\nfig,ax = plt.subplots(figsize=(10, 10))\nstations.sample(200).to_crs(3857).plot(ax = ax, color = 'red', alpha = 0.4, zorder=2)\nshp_communes.to_crs(3857).plot(ax = ax, zorder=1, edgecolor = \"black\", facecolor=\"none\",\n                                                           color = None)\nctx.add_basemap(ax, source = ctx.providers.OpenStreetMap.Mapnik)\n\n\n\n\n\n\n\n\n\n\n4️⃣\nIl ne reste plus qu’à retirer l’axe des coordonnées, qui n’est pas très\nesthétique.\n\nfig,ax = plt.subplots(figsize=(10, 10))\nstations.sample(200).to_crs(3857).plot(ax = ax, color = 'red', alpha = 0.4, zorder=2)\nshp_communes.to_crs(3857).plot(ax = ax, zorder=1, edgecolor = \"black\", facecolor=\"none\",\n                                                           color = None)\nctx.add_basemap(ax, source = ctx.providers.OpenStreetMap.Mapnik)\nax.set_axis_off()\nax\n\n\n\n\n\n\n\n\n\n\nIn fine, on obtient la carte désirée.\n\n\nOpérations sur les attributs\nToutes les opérations possibles sur un objet Pandas le sont également\nsur un objet GeoPandas. Pour manipuler les données, et non la géométrie,\non parlera d’opérations sur les attributs.\nPar exemple, si on désire\nconnaître quelques statistiques sur la taille des stations, l’approche\nest identique à si on avait un objet Pandas classique :\n\nstations.describe()\n\n\n\n\n\n\n\n\ncapacity\n\n\n\n\ncount\n1470.000000\n\n\nmean\n31.171429\n\n\nstd\n12.784457\n\n\nmin\n0.000000\n\n\n25%\n23.000000\n\n\n50%\n29.000000\n\n\n75%\n37.000000\n\n\nmax\n200.000000\n\n\n\n\n\n\n\nPour classer les départements de la petite couronne, du plus grand au plus petit,\nprocédons en deux étapes:\n\nRécupérons le contour des communes\ngrâce à cartiflette.\nNotons qu’on pourrait récupérer directement les contours départementaux mais\npour l’exercice, nous allons le créer nous-mêmes comme agrégation\ndes contours communaux\n(voir plus bas ainsi que ce notebook Observable pour la méthode plus\nlégère qui utilise pleinement les fonctionnalités de cartiflette).\nCalculons la surface totale de ce territoire (méthode area sur un objet GeoPandas.GeoDataFrame ramenée en km², attention néamoins au système de projection comme cela est expliqué plus bas)\n\n\nshp_communes['surface'] = shp_communes.area.div(10**6)\n\nLes plus grands départements s’obtiennent par une agrégation des\nsurfaces communales :\n\nshp_communes.groupby('INSEE_DEP').sum(numeric_only = True).sort_values('surface', ascending = False)\n\n\n\n\n\n\n\n\nID\nPOPULATION\nsurface\n\n\nINSEE_DEP\n\n\n\n\n\n\n\n94\n0.0\n1407124\n244.811517\n\n\n93\n0.0\n1644903\n236.868125\n\n\n92\n0.0\n1624357\n175.570765\n\n\n75\n0.0\n2165423\n105.431335\n\n\n\n\n\n\n\nSi on veut directement les plus\ngrandes communes de la petite couronne parisienne :\n\nshp_communes.sort_values('surface', ascending = False).head(10)\n\n\n\n\n\n\n\n\nid\nID\nNOM\nNOM_M\nINSEE_COM\nSTATUT\nPOPULATION\nINSEE_CAN\nINSEE_ARR\nINSEE_DEP\nINSEE_REG\nSIREN_EPCI\nsource\ngeometry\nINSEE_COG\nsurface\n\n\n\n\n23\nCOMMUNE_0000000009735015\nNaN\nTremblay-en-France\nTREMBLAY-EN-FRANCE\n93073\nCommune simple\n36461\n20\n2\n93\n11\n200054781/200058097\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((663432.643 6875170.586, 663414.781 6...\nNaN\n22.680693\n\n\n12\nARR_MUNI0000000009736553\nNaN\nParis 16e Arrondissement\nPARIS 16E ARRONDISSEMENT\n75056\nCapitale d'état\n165523\nNR\n1\n75\n11\n200054781\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((647190.220 6864524.360, 647201.518 6...\n75116\n16.409713\n\n\n19\nARR_MUNI0000000009736532\nNaN\nParis 12e Arrondissement\nPARIS 12E ARRONDISSEMENT\n75056\nCapitale d'état\n139297\nNR\n1\n75\n11\n200054781\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((655221.113 6858576.460, 655149.729 6...\n75112\n16.380842\n\n\n7\nCOMMUNE_0000000009735500\nNaN\nAulnay-sous-Bois\nAULNAY-SOUS-BOIS\n93005\nCommune simple\n86969\n02\n2\n93\n11\n200054781/200058097\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((660415.819 6872923.194, 660423.603 6...\nNaN\n16.167599\n\n\n31\nCOMMUNE_0000000009736056\nNaN\nRueil-Malmaison\nRUEIL-MALMAISON\n92063\nCommune simple\n78317\n22\n2\n92\n11\n200054781/200057982\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((639099.023 6866521.194, 639135.281 6...\nNaN\n14.540955\n\n\n37\nCOMMUNE_0000000009736517\nNaN\nNoisy-le-Grand\nNOISY-LE-GRAND\n93051\nCommune simple\n67871\n14\n2\n93\n11\n200054781/200058790\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((664453.846 6861358.828, 664461.038 6...\nNaN\n13.138755\n\n\n0\nCOMMUNE_0000000009735515\nNaN\nSaint-Denis\nSAINT-DENIS\n93066\nSous-préfecture\n112852\n99\n3\n93\n11\n200054781/200057867\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((652098.246 6872022.511, 652102.652 6...\nNaN\n12.372213\n\n\n4\nCOMMUNE_0000000009736052\nNaN\nNanterre\nNANTERRE\n92050\nPréfecture\n96277\n99\n2\n92\n11\n200054781/200057982\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((643490.675 6867612.283, 643581.446 6...\nNaN\n12.231274\n\n\n37\nCOMMUNE_0000000009737009\nNaN\nVitry-sur-Seine\nVITRY-SUR-SEINE\n94081\nCommune simple\n95510\n99\n3\n94\n11\n200054781/200058014\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((653536.980 6852608.424, 653536.652 6...\nNaN\n11.665507\n\n\n20\nCOMMUNE_0000000009735517\nNaN\nGennevilliers\nGENNEVILLIERS\n92036\nCommune simple\n48530\n14\n2\n92\n11\n200054781/200057990\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((649583.723 6872218.087, 649556.138 6...\nNaN\n11.632021\n\n\n\n\n\n\n\nLors des étapes d’agrégation, groupby ne conserve pas les géométries. Autrement\ndit, si on effectue, par exemple, une somme en fonction d’une variable de groupe avec\nle combo groupby(...).sum(...) , on perd\nla dimension géographique.\nIl est néanmoins possible d’aggréger à la fois les géométries et les\nattribus avec la méthode dissolve:\n\nfig,ax = plt.subplots(figsize=(10, 10))\nshp_communes.dissolve(by='INSEE_DEP', aggfunc='sum').plot(ax = ax, column = \"surface\")\nax.set_axis_off()\nax\n\n/opt/mamba/lib/python3.9/site-packages/geopandas/plotting.py:730: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nPour produire l’équivalent de cette carte à un niveau France entière, il est néanmoins plus simple de directement\nrécupérer les fonds officiels des départements plutôt que d’agréger les\ncontours des communes:\n\ndep = s3.download_vectorfile_url_all(\n    values = \"metropole\",\n    crs = 4326,\n    borders = \"DEPARTEMENT\",\n    vectorfile_format=\"topojson\",\n    filter_by=\"FRANCE_ENTIERE\",\n    source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n    year=2022)\n\ndep[\"area\"] = dep.to_crs(2154).area\n\nAvant de calculer les surfaces des départements, pour éviter les déformations liées au\nsystème Mercator, nous faisons une reprojection des données à la volée. Plus de détails\npar la suite.\n\ndep.sort_values('area', ascending = False).head(3)\n\n\n\n\n\n\n\n\nid\nID\nNOM_M\nNOM\nINSEE_DEP\nINSEE_REG\nsource\nterritoire\ngeometry\narea\n\n\n\n\n33\nDEPARTEM_FXX_00000000034\nNaN\nGIRONDE\nGironde\n33\n75\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nmetropole\nMULTIPOLYGON (((-1.15275 45.60453, -1.15084 45...\n1.036783e+10\n\n\n40\nDEPARTEM_FXX_00000000041\nNaN\nLANDES\nLandes\n40\n75\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nmetropole\nPOLYGON ((-1.25361 44.46752, -1.25317 44.46762...\n9.354177e+09\n\n\n24\nDEPARTEM_FXX_00000000025\nNaN\nDORDOGNE\nDordogne\n24\n75\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nmetropole\nPOLYGON ((-0.04044 45.10261, -0.04073 45.10309...\n9.210826e+09\n\n\n\n\n\n\n\n\nax = dep.plot(column = \"area\")\nax.set_axis_off()\n\n/opt/mamba/lib/python3.9/site-packages/geopandas/plotting.py:730: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n\n\n\n\n\n\n\n\n\nOpérations sur les géométries\nOutre la représentation graphique simplifiée,\nsur laquelle nous reviendrons ultérieurement, l’intérêt principal d’utiliser\nGeoPandas est l’existence de méthodes efficaces pour\nmanipuler la dimension spatiale. Un certain nombre proviennent du\npackage\nShapely.\n\n\n Warning\nLes données sont en système de coordonnées WGS 84 ou pseudo-Mercator (epsg: 4326) et ne sont pas projetées.\nC’est un format approprié lorsqu’il s’agit d’utiliser un fonds\nde carte OpenStreetMap, Stamen, Google Maps, etc.\nMais ce n’est pas un\nformat sur lequel on désire faire des calculs car les distances sont faussées sans utiliser de projection. D’ailleurs, geopandas refusera certaines opérations\nsur des données dont le crs est 4326. On reprojette ainsi les données\ndans la projection officielle pour la métropole, le Lambert 93\n(epsg: 2154).\n\n\nComme indiqué ci-dessus, nous reprojetons les données\ndans le système Lambert 93 qui ne fausse pas les\ncalculs de distance et d’aires.\n\ncommunes = shp_communes.to_crs(2154)\nstations = stations.to_crs(2154)\n\nPar exemple, on peut recalculer la taille d’une commune ou d’arrondissement\navec la méthode area (et diviser par \\(10^6\\) pour avoir des \\(km^2\\) au lieu\ndes \\(m^2\\)):\n\ncommunes['superficie'] = communes.area.div(10**6)\ncommunes.head(3)\n\n\n\n\n\n\n\n\nid\nID\nNOM\nNOM_M\nINSEE_COM\nSTATUT\nPOPULATION\nINSEE_CAN\nINSEE_ARR\nINSEE_DEP\nINSEE_REG\nSIREN_EPCI\nsource\ngeometry\nINSEE_COG\nsurface\nsuperficie\n\n\n\n\n0\nCOMMUNE_0000000009736037\nNaN\nLevallois-Perret\nLEVALLOIS-PERRET\n92044\nCommune simple\n66082\n16\n2\n92\n11\n200054781/200057982\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((647761.341 6867306.988, 647839.223 6...\nNaN\n2.417491\n2.417491\n\n\n1\nCOMMUNE_0000000009736055\nNaN\nBois-Colombes\nBOIS-COLOMBES\n92009\nCommune simple\n28841\n11\n2\n92\n11\n200054781/200057990\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((646224.655 6867615.810, 646228.990 6...\nNaN\n1.926619\n1.926619\n\n\n2\nCOMMUNE_0000000009736538\nNaN\nMalakoff\nMALAKOFF\n92046\nCommune simple\n30950\n18\n1\n92\n11\n200054781/200057966\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((646995.323 6857373.499, 647177.485 6...\nNaN\n2.070953\n2.070953\n\n\n\n\n\n\n\nUne méthode qu’on utilise régulièrement est centroid qui, comme son nom l’indique,\nrecherche le centroïde de chaque polygone et transforme ainsi des données\nsurfaciques en données ponctuelles. Par exemple, pour\nreprésenter approximativement les centres des villages de la\nHaute-Garonne (31), après avoir téléchargé le fonds de carte adapté,\nfera\n\ncommunes_31 = s3.download_vectorfile_url_all(\n      crs = 4326,\n      values = \"31\",\n      borders=\"COMMUNE\",\n      vectorfile_format=\"topojson\",\n      filter_by=\"DEPARTEMENT\",\n      source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n      year=2022)\n\n# on reprojete en 3857 pour le fond de carte\ncommunes_31 = communes_31.to_crs(3857)\n\n# on calcule le centroide\ndep_31 = communes_31.copy()\ncommunes_31['geometry'] = communes_31['geometry'].centroid\n\nax = communes_31.plot(figsize = (10,10), color = 'red', alpha = 0.4, zorder=2)\ndep_31.to_crs(3857).plot(ax = ax, zorder=1, edgecolor = \"black\", facecolor=\"none\",\n                                                           color = None)\n#ctx.add_basemap(ax, source = ctx.providers.Stamen.Toner)\nax.set_axis_off()\nax\n\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=31/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\n\n\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 12.0kiB [00:00, 119kiB/s]Downloading: : 44.0kiB [00:00, 234kiB/s]Downloading: : 96.0kiB [00:00, 359kiB/s]Downloading: : 200kiB [00:00, 620kiB/s] Downloading: : 404kiB [00:00, 1.12MiB/s]Downloading: : 816kiB [00:00, 2.11MiB/s]Downloading: : 1.59MiB [00:00, 4.05MiB/s]Downloading: : 1.60MiB [00:00, 2.29MiB/s]\n\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "content/manipulation/03_geopandas_tutorial.html#gérer-le-système-de-projection",
    "href": "content/manipulation/03_geopandas_tutorial.html#gérer-le-système-de-projection",
    "title": "Données spatiales : découverte de geopandas",
    "section": "Gérer le système de projection",
    "text": "Gérer le système de projection\nPrécédemment, nous avons appliqué une méthode to_crs pour reprojeter\nles données dans un système de projection différent de celui du fichier\nd’origine :\n\ncommunes = communes.to_crs(2154)\nstations = stations.to_crs(2154)\n\nConcernant la gestion des projections avec GeoPandas,\nla documentation officielle est très bien\nfaite. Elle fournit notamment l’avertissement suivant qu’il est\nbon d’avoir en tête :\n\nBe aware that most of the time you don’t have to set a projection. Data loaded from a reputable source (using the geopandas.read_file() command) should always include projection information. You can see an objects current CRS through the GeoSeries.crs attribute.\nFrom time to time, however, you may get data that does not include a projection. In this situation, you have to set the CRS so geopandas knows how to interpret the coordinates.\n\n\n\n\nImage empruntée à XKCD https://xkcd.com/2256/ qu’on peut également trouver sur https://blog.chrislansdown.com/2020/01/17/a-great-map-projection-joke/\n\n\nPour déterminer le système de projection d’une base de données, on peut vérifier l’attribut crs :\n\ncommunes.crs\n\n&lt;Projected CRS: EPSG:2154&gt;\nName: RGF93 v1 / Lambert-93\nAxis Info [cartesian]:\n- X[east]: Easting (metre)\n- Y[north]: Northing (metre)\nArea of Use:\n- name: France - onshore and offshore, mainland and Corsica (France métropolitaine including Corsica).\n- bounds: (-9.86, 41.15, 10.38, 51.56)\nCoordinate Operation:\n- name: Lambert-93\n- method: Lambert Conic Conformal (2SP)\nDatum: Reseau Geodesique Francais 1993 v1\n- Ellipsoid: GRS 1980\n- Prime Meridian: Greenwich\n\n\nLes deux principales méthodes pour définir le système de projection utilisé sont :\n\ndf.set_crs : cette commande sert à préciser quel est le système de projection utilisé, c’est-à-dire comment les coordonnées (x,y) sont reliées à la surface terrestre. Cette commande ne doit pas être utilisée pour transformer le système de coordonnées, seulement pour le définir.\ndf.to_crs : cette commande sert à projeter les points d’une géométrie dans une autre, c’est-à-dire à recalculer les coordonnées selon un autre système de projection.\n\nDans le cas particulier de production de carte avec un fond OpenStreetMaps ou une carte dynamique leaflet, il est nécessaire de dé-projeter les données (par exemple à partir du Lambert-93) pour atterrir dans le système non-projeté WGS 84 (code EPSG 4326). Ce site dédié aux projections géographiques peut être utile pour retrouver le système de projection d’un fichier où il n’est pas indiqué.\nLa définition du système de projection se fait de la manière suivante (:warning: avant de le faire, se souvenir de l’avertissement !) :\ncommunes = communes.set_crs(2154)\nAlors que la reprojection (projection Albers : 5070) s’obtient de la manière suivante :\n\nshp_region = s3.download_vectorfile_url_all(\n    values = \"metropole\",\n    crs = 4326,\n    borders = \"REGION\",\n    vectorfile_format=\"topojson\",\n    filter_by=\"FRANCE_ENTIERE\",\n    source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n    year=2022)\n    \nfig,ax = plt.subplots(figsize=(10, 10))\nshp_region.to_crs(5070).plot(ax = ax)\nax\n\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=REGION/crs=4326/FRANCE_ENTIERE=metropole/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\n\n\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 29.0kiB [00:00, 178kiB/s]Downloading: : 93.0kiB [00:00, 368kiB/s]Downloading: : 289kiB [00:00, 886kiB/s] Downloading: : 629kiB [00:00, 1.59MiB/s]Downloading: : 893kiB [00:00, 1.60MiB/s]Downloading: : 2.65MiB [00:00, 6.09MiB/s]Downloading: : 3.61MiB [00:00, 7.15MiB/s]Downloading: : 4.55MiB [00:01, 7.91MiB/s]Downloading: : 4.76MiB [00:01, 4.87MiB/s]\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nOn le voit, cela modifie totalement la représentation de l’objet dans l’espace.\nClairement, cette projection n’est pas adaptée aux longitudes et latitudes françaises.\nC’est normal, il s’agit d’une projection adaptée au continent\nnord-américain (et encore, pas dans son ensemble !).\n\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n\nfig,ax = plt.subplots(figsize=(10, 10))\nworld[world.continent == \"North America\"].to_crs(5070).plot(alpha = 0.2, edgecolor = \"k\", ax = ax)\nax\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "content/manipulation/03_geopandas_tutorial.html#joindre-des-données",
    "href": "content/manipulation/03_geopandas_tutorial.html#joindre-des-données",
    "title": "Données spatiales : découverte de geopandas",
    "section": "Joindre des données",
    "text": "Joindre des données\n\nJoindre des données sur des attributs\nCe type de jointure se fait entre un objet géographique et un\ndeuxième objet, géographique ou non. A l’exception de la question\ndes géométries, il n’y a pas de différence par rapport à Pandas.\nLa seule différence avec Pandas est dans la dimension géographique.\nSi on désire conserver la dimension géographique, il faut faire\nattention à faire :\ngeopandas_object.merge(pandas_object)\nSi on utilise deux objets géographiques mais ne désire conserver qu’une seule\ndimension géographique1, on fera\ngeopandas_object1.merge(geopandas_object2)\nSeule la géométrie de l’objet de gauche\nsera conservée, même si on fait un right join.\n\n\nProlongation possible : joindre des données sur dimension géographique\nLe chapitre suivant permettra de mettre en oeuvre des\njointures géographiques.\n\n\n Hint\nLes jointures spatiales peuvent être très gourmandes en ressources (car il peut être nécessaire de croiser toutes les géométries de x avec toutes les géométries de y). Voici deux conseils qui peuvent vous aider :\n\nIl est préférable de tester les jointures géographiques sur un petit échantillon de données, pour estimer le temps et les ressources nécessaires à la réalisation de la jointure.\nIl est parfois possible d’écrire une fonction qui réduit la taille du problème. Exemple: vous voulez déterminer dans quelle commune se situe un logement dont vous connaissez les coordonnées et le département; vous pouvez écrire une fonction qui réalise pour chaque département une jointure spatiale entre les logements situés dans ce département et les communes de ce département, puis empiler les 101 tables de sorties."
  },
  {
    "objectID": "content/manipulation/03_geopandas_tutorial.html#footnotes",
    "href": "content/manipulation/03_geopandas_tutorial.html#footnotes",
    "title": "Données spatiales : découverte de geopandas",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIl est techniquement possible d’avoir un DataFrame comportant plusieurs\ngéographies. Par exemple une géométrie polygone et une géométrie point\n(le centroid). C’est néanmoins souvent compliqué à gérer et donc peu\nrecommandable.↩︎"
  },
  {
    "objectID": "content/manipulation/04a_webscraping_TP.html",
    "href": "content/manipulation/04a_webscraping_TP.html",
    "title": "Web scraping avec Python",
    "section": "",
    "text": "Le web scraping désigne les techniques d’extraction du contenu des sites internet.\nC’est une pratique très utile pour toute personne souhaitant travailler sur des informations disponibles en ligne, mais n’existant pas forcément sous la forme d’un tableau Excel.\nCe TP vous présente comment créer et exécuter des robots afin de recupérer rapidement des informations utiles à vos projets actuels ou futurs.\nIl part de quelques cas d’usages concret.\nCe chapitre est très fortement inspiré et réadapté à partir de celui de Xavier Dupré, l’ancien professeur de la matière."
  },
  {
    "objectID": "content/manipulation/04a_webscraping_TP.html#enjeux",
    "href": "content/manipulation/04a_webscraping_TP.html#enjeux",
    "title": "Web scraping avec Python",
    "section": "Enjeux",
    "text": "Enjeux\nUn certain nombre d’enjeux du web scraping ne seront évoqués\nque superficiellement dans le cadre de ce chapitre.\n\nLa zone grise de la légalité du web scraping\nEn premier lieu, en ce qui concerne la question de la légalité\nde la récupération d’information par scraping, il existe\nune zone grise. Ce n’est pas parce qu’une information est\ndisponible sur internet, directement ou avec un peu de recherche,\nqu’elle peut être récupérée et réutilisée.\nL’excellent cours d’Antoine Palazzolo évoque un certain nombre de cas\nmédiatiques et judiciaires sur cette question.\nDans le champ français, la CNIL a publié en 2020\nde nouvelles directives sur le web scraping reprécisant\nque toute donnée ne peut être réutilisée à l’insu de la personne\nà laquelle ces données appartiennent. Autrement dit, en principe,\nles données collectées par web scraping sont soumises au\nRGPD, c’est-à-dire nécessitent le consentement des personnes\nà partir desquelles la réutilisation des données est faite.\nIl est donc recommandé d’être vigilant avec les données récupérées\npar web scraping pour ne pas se mettre en faute légalement.\n\n\nStabilité et fiabilité des informations reçues\nLa récupération de données par web scraping\nest certes pratique mais elle ne correspond pas nécessairement\nà un usage pensé, ou désiré, par un fournisseur de données.\nLes données étant coûteuses à collecter et à mettre à disposition,\ncertains sites ne désirent pas nécessairement que celles-ci soient\nextraites gratuitement et facilement. A fortiori lorsque la donnée\npeut permettre à un concurrent de disposer d’une information\nutile d’un point de vue commercial (prix d’un produit concurrent, etc.).\nLes acteurs mettent donc souvent en oeuvre des stratégies pour bloquer ou\nlimiter la quantité de données scrapées. La méthode la plus\nclassique est la détection et le blocage\ndes requêtes faites par des robots plutôt que par des humains.\nPour des acteurs spécialisés, cette détection est très facile car\nde nombreuses preuves permettent d’identifier si une visite du site web\nprovient d’un utilisateur\nhumain derrière un navigateur ou d’un robot. Pour ne citer que quelques indices :\nvitesse de la navigation entre pages, rapidité à extraire la donnée,\nempreinte digitale du navigateur utilisé, capacité à répondre à des\nquestions aléatoires (captcha)…\nLes bonnes pratiques, évoquées par la suite, ont pour objectif de faire\nen sorte qu’un robot se comporte de manière civile en adoptant un comportement\nproche de celui de l’humain mais sans contrefaire le fait qu’il ne s’agit\npas d’un humain.\nIl convient d’ailleurs\nd’être prudent quant aux informations reçues par web scraping.\nLa donnée étant au coeur du modèle économique de certains acteurs, certains\nn’hésitent pas à renvoyer des données fausses aux robots\nplutôt que les bloquer. C’est de bonne guerre !\nUne autre technique piège s’appelle le honey pot. Il s’agit de pages qu’un humain\nn’irait jamais visiter - par exemple parce qu’elles n’apparaissent pas dans\nl’interface graphique - mais sur lesquelles un robot, en recherche automatique\nde contenu, va rester bloquer.\nSans aller jusqu’à la stratégie de blocage du web scraping, d’autres raisons\npeuvent expliquer qu’une récupération de données ait fonctionné par\nle passé mais ne fonctionne plus. La plus fréquente est un changement dans la structure\nd’un site web. Le web scraping présente en effet l’inconvénient d’aller chercher\nde l’information dans une structure très hiérarchisée. Un changement dans cette structure\npeut suffire à rendre un robot incapable de récupérer du contenu. Or, pour rester\nattractifs, les sites web changent fréquemment ce qui peut facilement\nrendre inopérant un robot.\nDe manière générale, l’un des principaux messages de ce\nchapitre, à retenir, est que le\nweb scraping est une solution de dernier ressort, pour des récupérations ponctuelles de données sans garantie de fonctionnement ultérieur. Il est préférable de privilégier les API lorsque celles-ci sont disponibles.\nCes dernières ressemblent à un contrat (formel ou non) entre un fournisseur de données\net un utilisateur où sont définis des besoins (les données) mais aussi des\nconditions d’accès (nombre de requêtes, volumétrie, authentification…) là\noù le web scraping est plus proche du comportement dans le Far West.\n\n\nLes bonnes pratiques\nLa possibilité de récupérer des données par l’intermédiaire\nd’un robot ne signifie pas qu’on peut se permettre de ne pas être\ncivilisé. En effet, lorsqu’il est non-maîtrisé, le\nweb scraping peut ressembler à une attaque informatique\nclassique pour faire sauter un site web : le déni de service.\nLe cours d’Antoine Palazzolo revient\nsur certaines bonnes pratiques qui ont émergé dans la communauté\ndes scrapeurs. Il est recommandé de lire cette ressource\npour en apprendre plus sur ce sujet. Y sont évoquées\nplusieurs conventions, parmi lesquelles :\n\nSe rendre, depuis la racine du site,\nsur le fichier robots.txt pour vérifier les consignes\nproposées par les développeurs du site web pour\ncadrer le comportement des robots ;\nEspacer chaque requêtes de plusieurs secondes, comme le ferait\nun humain, afin d’éviter de surcharger le site web et de le\nfaire sauter par déni de service ;\nFaire les requêtes dans les heures creuses de fréquentation du\nsite web s’il ne s’agit pas d’un site consulté internationalement.\nPar exemple, pour un site en français, lancer le robot\npendant la nuit en France métropolitaine, est une bonne pratique.\nPour lancer un robot depuis Python à une heure programmée\nà l’avance, il existe les cronjobs."
  },
  {
    "objectID": "content/manipulation/04a_webscraping_TP.html#un-détour-par-le-web-comment-fonctionne-un-site",
    "href": "content/manipulation/04a_webscraping_TP.html#un-détour-par-le-web-comment-fonctionne-un-site",
    "title": "Web scraping avec Python",
    "section": "Un détour par le Web : comment fonctionne un site ?",
    "text": "Un détour par le Web : comment fonctionne un site ?\nMême si ce TP ne vise pas à faire un cours de web, il vous faut néanmoins certaines bases sur la manière dont un site internet fonctionne afin de comprendre comment sont structurées les informations sur une page.\nUn site Web est un ensemble de pages codées en HTML qui permet de décrire à la fois le contenu et la forme d’une page Web.\nPour voir cela, ouvrez n’importe quelle page web et faites un clic-droit dessus.\n- Sous Chrome  : Cliquez ensuite sur “Affichez le code source de la page” (CTRL+U) ;\n- Sous Firefox  : “Code source de la page” (CTRL+MAJ+K) ;\n- Sous Edge  : “Affichez la page source” (CTRL+U) ;\n- Sous Safari  : voir comment faire ici\nSi vous savez quel élément vous intéresse, vous pouvez également ouvrir l’inspecteur du navigateur (clic droit sur l’élément + “Inspecter”),\npour afficher les balises encadrant votre élément de façon plus ergonomique, un peu comme un zoom.\n\nLes balises\nSur une page web, vous trouverez toujours à coup sûr des éléments comme &lt;head&gt;, &lt;title&gt;, etc. Il s’agit des codes qui vous permettent de structurer le contenu d’une page HTML et qui s’appellent des balises.\nCitons, par exemple, les balises &lt;p&gt;, &lt;h1&gt;, &lt;h2&gt;, &lt;h3&gt;, &lt;strong&gt; ou &lt;em&gt;.\nLe symbole &lt; &gt; est une balise : il sert à indiquer le début d’une partie. Le symbole &lt;/ &gt; indique la fin de cette partie. La plupart des balises vont par paires, avec une balise ouvrante et une balise fermante (par exemple &lt;p&gt; et &lt;/p&gt;).\nPar exemple, les principales balises\ndéfinissant la structure d’un tableau sont les suivantes:\n\n\n\nBalise\nDescription\n\n\n\n\n&lt;table&gt;\nTableau\n\n\n&lt;caption&gt;\nTitre du tableau\n\n\n&lt;tr&gt;\nLigne de tableau\n\n\n&lt;th&gt;\nCellule d’en-tête\n\n\n&lt;td&gt;\nCellule\n\n\n&lt;thead&gt;\nSection de l’en-tête du tableau\n\n\n&lt;tbody&gt;\nSection du corps du tableau\n\n\n&lt;tfoot&gt;\nSection du pied du tableau\n\n\n\nApplication : un tableau en HTML\nLe code HTML du tableau suivant:\n&lt;table&gt;\n&lt;caption&gt; Le Titre de mon tableau &lt;/caption&gt;\n\n   &lt;tr&gt;\n      &lt;th&gt;Prénom&lt;/th&gt;\n      &lt;th&gt;Nom&lt;/th&gt;\n      &lt;th&gt;Profession&lt;/th&gt;\n   &lt;/tr&gt;\n   &lt;tr&gt;\n      &lt;td&gt;Mike &lt;/td&gt;\n      &lt;td&gt;Stuntman&lt;/td&gt;\n      &lt;td&gt;Cascadeur&lt;/td&gt;\n   &lt;/tr&gt;\n   &lt;tr&gt;\n      &lt;td&gt;Mister&lt;/td&gt;\n      &lt;td&gt;Pink&lt;/td&gt;\n      &lt;td&gt;Gangster&lt;/td&gt;\n   &lt;/tr&gt;\n&lt;/table&gt;\nDonnera dans le navigateur :\n\n\nLe Titre de mon tableau\n\n\nPrénom\nNom\nProfession\n\n\nMike\nStuntman\nCascadeur\n\n\nMister\nPink\nGangster\n\n\n\n\n\nParent et enfant\nDans le cadre du langage HTML, les termes de parent (parent) et enfant (child) servent à désigner des élements emboîtés les uns dans les autres. Dans la construction suivante, par exemple :\n&lt;div&gt; \n    &lt;p&gt;\n       bla,bla\n    &lt;/p&gt;\n&lt;/div&gt;\nSur la page web, cela apparaitra de la manière suivante :\n\n \n    \n       bla,bla\n    \n\n\nOn dira que l’élément &lt;div&gt; est le parent de l’élément &lt;p&gt; tandis que l’élément &lt;p&gt; est l’enfant de l’élément &lt;div&gt;.\n\nMais pourquoi apprendre ça pour “scraper” ?\n\nParce que, pour bien récupérer les informations d’un site internet, il faut pouvoir comprendre sa structure et donc son code HTML. Les fonctions Python qui servent au scraping sont principalement construites pour vous permettre de naviguer entre les balises.\nAvec Python, vous allez en fait reproduire votre comportement manuel de recherche de manière\nà l’automatiser."
  },
  {
    "objectID": "content/manipulation/04a_webscraping_TP.html#scraper-avec-python-le-package-beautifulsoup",
    "href": "content/manipulation/04a_webscraping_TP.html#scraper-avec-python-le-package-beautifulsoup",
    "title": "Web scraping avec Python",
    "section": "Scraper avec Python: le package BeautifulSoup",
    "text": "Scraper avec Python: le package BeautifulSoup\n\nLes packages disponibles\nDans la première partie de ce chapitre,\nnous allons essentiellement utiliser le package BeautifulSoup4,\nen conjonction avec urllib\nou requests. Ces deux derniers packages permettent de récupérer le texte\nbrut d’une page qui sera ensuite\ninspecté via BeautifulSoup4.\nBeautifulSoup sera suffisant quand vous voudrez travailler sur des pages HTML statiques. Dès que les informations que vous recherchez sont générées via l’exécution de scripts Javascript, il vous faudra passer par des outils comme Selenium.\nDe même, si vous ne connaissez pas l’URL, il faudra passer par un framework comme Scrapy, qui passe facilement d’une page à une autre. On appelle\ncette technique le “web crawling”. Scrapy est plus complexe à manipuler que BeautifulSoup : si vous voulez plus de détails, rendez-vous sur la page du tutoriel Scrapy.\nLe web scraping est un domaine où la reproductibilité est compliquée à mettre en oeuvre.\nUne page web évolue\npotentiellement régulièrement et d’une page web à l’autre, la structure peut\nêtre très différente ce qui rend certains codes difficilement exportables.\nPar conséquent, la meilleure manière d’avoir un programme fonctionnel est\nde comprendre la structure d’une page web et dissocier les éléments exportables\nà d’autres cas d’usages des requêtes ad hoc.\n\n!pip install -q lxml\n\nimport bs4\nimport lxml\nimport pandas\nimport urllib\n\nfrom urllib import request\n\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\n\n Note\nPour être en mesure d’utiliser Selenium, il est nécessaire\nde faire communiquer Python avec un navigateur web (Firefox ou Chromium).\nLe package webdriver-manager permet de faire savoir à Python où\nse trouve ce navigateur s’il est déjà installé dans un chemin standard.\nPour l’installer, le code de la cellule ci-dessous peut être utilisé.\n\n\nPour faire fonctionner Selenium, il faut utiliser un package\nnommé webdriver-manager:\n\n!pip install webdriver-manager\n\nCollecting webdriver-manager\n  Obtaining dependency information for webdriver-manager from https://files.pythonhosted.org/packages/b1/51/b5c11cf739ac4eecde611794a0ec9df420d0239d51e73bc19eb44f02b48b/webdriver_manager-4.0.1-py2.py3-none-any.whl.metadata\n  Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: requests in /opt/mamba/lib/python3.9/site-packages (from webdriver-manager) (2.31.0)\nCollecting python-dotenv (from webdriver-manager)\n  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\nRequirement already satisfied: packaging in /opt/mamba/lib/python3.9/site-packages (from webdriver-manager) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /opt/mamba/lib/python3.9/site-packages (from packaging-&gt;webdriver-manager) (3.0.9)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/mamba/lib/python3.9/site-packages (from requests-&gt;webdriver-manager) (2.1.1)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/mamba/lib/python3.9/site-packages (from requests-&gt;webdriver-manager) (3.3)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /opt/mamba/lib/python3.9/site-packages (from requests-&gt;webdriver-manager) (1.26.11)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /opt/mamba/lib/python3.9/site-packages (from requests-&gt;webdriver-manager) (2022.9.24)\nDownloading webdriver_manager-4.0.1-py2.py3-none-any.whl (27 kB)\nInstalling collected packages: python-dotenv, webdriver-manager\nSuccessfully installed python-dotenv-1.0.0 webdriver-manager-4.0.1\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\n\nRécupérer le contenu d’une page HTML\nOn va commencer doucement. Prenons une page wikipedia,\npar exemple celle de la Ligue 1 de football, millésime 2019-2020 : Championnat de France de football 2019-2020. On va souhaiter récupérer la liste des équipes, ainsi que les url des pages Wikipedia de ces équipes.\nEtape 1️⃣ : se connecter à la page wikipedia et obtenir le code source.\nPour cela, le plus simple est d’utiliser le package urllib ou, mieux, requests.\nNous allons ici utiliser la fonction request du package urllib:\n\nurl_ligue_1 = \"https://fr.wikipedia.org/wiki/Championnat_de_France_de_football_2019-2020\"\n    \nrequest_text = request.urlopen(url_ligue_1).read()\n# print(request_text[:1000])    \n\n\ntype(request_text)\n\nbytes\n\n\nEtape 2️⃣ : utiliser le package BeautifulSoup\nqui permet de rechercher efficacement\nles balises contenues dans la chaine de caractères\nrenvoyée par la fonction request:\n\npage = bs4.BeautifulSoup(request_text, \"lxml\")\n\nSi on print l’objet page créée avec BeautifulSoup,\non voit que ce n’est plus une chaine de caractères mais bien une page HTML avec des balises.\nOn peut à présent chercher des élements à l’intérieur de ces balises.\n\n\nLa méthode find\nPar exemple, si on veut connaître le titre de la page, on utilise la méthode .find et on lui demande “title”\n\nprint(page.find(\"title\"))\n\n&lt;title&gt;Championnat de France de football 2019-2020 — Wikipédia&lt;/title&gt;\n\n\nLa methode .find ne renvoie que la première occurence de l’élément.\nPour vous en assurer vous pouvez :\n\ncopier le bout de code source obtenu lorsque vous chercher une table,\nle coller dans une cellule de votre notebook\net passer la cellule en “Markdown”\n\nLa cellule avec le copier-coller du code source donne :\n\nprint(page.find(\"table\"))\n\n&lt;table&gt;&lt;caption style=\"background-color:#99cc99;color:#000000;\"&gt;Généralités&lt;/caption&gt;&lt;tbody&gt;&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Sport&lt;/th&gt;\n&lt;td&gt;\n&lt;a href=\"/wiki/Football\" title=\"Football\"&gt;Football&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Organisateur(s)&lt;/th&gt;\n&lt;td&gt;\n&lt;a href=\"/wiki/Ligue_de_football_professionnel_(France)\" title=\"Ligue de football professionnel (France)\"&gt;LFP&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Édition&lt;/th&gt;\n&lt;td&gt;\n&lt;abbr class=\"abbr\" title=\"Quatre-vingt-deuxième (huitante-deuxième / octante-deuxième)\"&gt;82&lt;sup&gt;e&lt;/sup&gt;&lt;/abbr&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Lieu(x)&lt;/th&gt;\n&lt;td&gt;\n&lt;span class=\"datasortkey\" data-sort-value=\"France\"&gt;&lt;span class=\"flagicon\"&gt;&lt;span class=\"mw-image-border noviewer\" typeof=\"mw:File\"&gt;&lt;a class=\"mw-file-description\" href=\"/wiki/Fichier:Flag_of_France.svg\" title=\"Drapeau de la France\"&gt;&lt;img alt=\"Drapeau de la France\" class=\"mw-file-element\" data-file-height=\"600\" data-file-width=\"900\" decoding=\"async\" height=\"13\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Flag_of_France.svg/20px-Flag_of_France.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Flag_of_France.svg/30px-Flag_of_France.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Flag_of_France.svg/40px-Flag_of_France.svg.png 2x\" width=\"20\"/&gt;&lt;/a&gt;&lt;/span&gt; &lt;/span&gt;&lt;a href=\"/wiki/France\" title=\"France\"&gt;France&lt;/a&gt;&lt;/span&gt; et &lt;br/&gt;&lt;span class=\"datasortkey\" data-sort-value=\"Monaco\"&gt;&lt;span class=\"flagicon\"&gt;&lt;span class=\"mw-image-border noviewer\" typeof=\"mw:File\"&gt;&lt;a class=\"mw-file-description\" href=\"/wiki/Fichier:Flag_of_Monaco.svg\" title=\"Drapeau de Monaco\"&gt;&lt;img alt=\"Drapeau de Monaco\" class=\"mw-file-element\" data-file-height=\"800\" data-file-width=\"1000\" decoding=\"async\" height=\"16\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Flag_of_Monaco.svg/20px-Flag_of_Monaco.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Flag_of_Monaco.svg/30px-Flag_of_Monaco.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Flag_of_Monaco.svg/40px-Flag_of_Monaco.svg.png 2x\" width=\"20\"/&gt;&lt;/a&gt;&lt;/span&gt; &lt;/span&gt;&lt;a href=\"/wiki/Monaco\" title=\"Monaco\"&gt;Monaco&lt;/a&gt;&lt;/span&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Date&lt;/th&gt;\n&lt;td&gt;\nDu &lt;time class=\"nowrap date-lien\" data-sort-value=\"2019-08-09\" datetime=\"2019-08-09\"&gt;&lt;a href=\"/wiki/9_ao%C3%BBt_en_sport\" title=\"9 août en sport\"&gt;9&lt;/a&gt; &lt;a class=\"mw-redirect\" href=\"/wiki/Ao%C3%BBt_2019_en_sport\" title=\"Août 2019 en sport\"&gt;août&lt;/a&gt; &lt;a href=\"/wiki/2019_en_football\" title=\"2019 en football\"&gt;2019&lt;/a&gt;&lt;/time&gt;&lt;br/&gt;au &lt;time class=\"nowrap date-lien\" data-sort-value=\"2020-03-08\" datetime=\"2020-03-08\"&gt;&lt;a href=\"/wiki/8_mars_en_sport\" title=\"8 mars en sport\"&gt;8 mars&lt;/a&gt; &lt;a href=\"/wiki/2020_en_football\" title=\"2020 en football\"&gt;2020&lt;/a&gt;&lt;/time&gt; &lt;small&gt;(arrêt définitif)&lt;/small&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Participants&lt;/th&gt;\n&lt;td&gt;\n20 équipes&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Matchs joués&lt;/th&gt;\n&lt;td&gt;\n279 (sur 380 prévus)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Site web officiel&lt;/th&gt;\n&lt;td&gt;\n&lt;a class=\"external text\" href=\"https://www.ligue1.fr/\" rel=\"nofollow\"&gt;Site officiel&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;\n\n\nce qui est le texte source permettant de générer le tableau suivant :\n\n\n\n\nGénéralités\n\n\n\n\nSport\n\n\nFootball\n\n\n\n\nOrganisateur(s)\n\n\nLFP\n\n\n\n\nÉdition\n\n\n82e\n\n\n\n\nLieu(x)\n\n\n France et  Monaco\n\n\n\n\nDate\n\n\nDu 9 août 2019au 8 mars 2020 (arrêt définitif)\n\n\n\n\nParticipants\n\n\n20 équipes\n\n\n\n\nMatchs joués\n\n\n279 (sur 380 prévus)\n\n\n\n\nSite web officiel\n\n\nSite officiel\n\n\n\n\n\n\n\n\nLa méthode findAll\nPour trouver toutes les occurences, on utilise .findAll().\n\nprint(\"Il y a\", len(page.findAll(\"table\")), \"éléments dans la page qui sont des &lt;table&gt;\")\n\nIl y a 34 éléments dans la page qui sont des &lt;table&gt;"
  },
  {
    "objectID": "content/manipulation/04a_webscraping_TP.html#exercice-guidé-obtenir-la-liste-des-équipes-de-ligue-1",
    "href": "content/manipulation/04a_webscraping_TP.html#exercice-guidé-obtenir-la-liste-des-équipes-de-ligue-1",
    "title": "Web scraping avec Python",
    "section": "Exercice guidé : obtenir la liste des équipes de Ligue 1",
    "text": "Exercice guidé : obtenir la liste des équipes de Ligue 1\nDans le premier paragraphe de la page “Participants”,\non a le tableau avec les résultats de l’année.\n\n\n Exercice 1 : Récupérer les participants de la Ligue 1\nPour cela, nous allons procéder en 6 étapes:\n\nTrouver le tableau\nRécupérer chaque ligne du table\nNettoyer les sorties en ne gardant que le texte sur une ligne\nGénéraliser sur toutes les lignes\nRécupérer les entêtes du tableau\nFinalisation du tableau\n\n\n\n1️⃣ Trouver le tableau\n\n# on identifie le tableau en question : c'est le premier qui a cette classe \"wikitable sortable\"\ntableau_participants = page.find('table', {'class' : 'wikitable sortable'})\n\n\nprint(tableau_participants)\n\n\n\n\n\nClub\n\n\nDernièremontée\n\n\nBudget[3]en M€\n\n\nClassement2018-2019\n\n\nEntraîneur\n\n\nDepuis\n\n\nStade\n\n\nCapacitéen L1[4]\n\n\nNombrede saisonsen L1\n\n\n\n\nParis Saint-Germain\n\n\n1974\n\n\n637\n\n\n1er\n\n\n Thomas Tuchel\n\n\n2018\n\n\nParc des Princes\n\n\n47 929\n\n\n46\n\n\n\n\nLOSC Lille\n\n\n2000\n\n\n120\n\n\n2e\n\n\n Christophe Galtier\n\n\n2017\n\n\nStade Pierre-Mauroy\n\n\n49 712\n\n\n59\n\n\n\n\nOlympique lyonnais\n\n\n1989\n\n\n310\n\n\n3e\n\n\n Rudi Garcia\n\n\n2019\n\n\nGroupama Stadium\n\n\n57 206\n\n\n60\n\n\n\n\nAS Saint-Étienne\n\n\n2004\n\n\n100\n\n\n4e\n\n\n Claude Puel\n\n\n2019\n\n\nStade Geoffroy-Guichard\n\n\n41 965\n\n\n66\n\n\n\n\nOlympique de Marseille\n\n\n1996\n\n\n110\n\n\n5e\n\n\n André Villas-Boas\n\n\n2019\n\n\nOrange Vélodrome\n\n\n66 226\n\n\n69\n\n\n\n\nMontpellier HSC\n\n\n2009\n\n\n40\n\n\n6e\n\n\n Michel Der Zakarian\n\n\n2017\n\n\nStade de la Mosson\n\n\n22 000\n\n\n27\n\n\n\n\nOGC Nice\n\n\n2002\n\n\n50\n\n\n7e\n\n\n Patrick Vieira\n\n\n2018\n\n\nAllianz Riviera\n\n\n35 596\n\n\n60\n\n\n\n\nStade de Reims\n\n\n2018\n\n\n45\n\n\n8e\n\n\n David Guion\n\n\n2017\n\n\nStade Auguste-Delaune\n\n\n20 546\n\n\n35\n\n\n\n\nNîmes Olympique\n\n\n2018\n\n\n27\n\n\n9e\n\n\n Bernard Blaquart\n\n\n2015\n\n\nStade des Costières\n\n\n15 788\n\n\n35\n\n\n\n\nStade rennais FC\n\n\n1994\n\n\n65\n\n\n10e\n\n\n Julien Stéphan\n\n\n2018\n\n\nRoazhon Park\n\n\n29 194\n\n\n62\n\n\n\n\nRC Strasbourg Alsace\n\n\n2017\n\n\n43\n\n\n11e\n\n\n Thierry Laurey\n\n\n2016\n\n\nStade de la Meinau\n\n\n26 109\n\n\n58\n\n\n\n\nFC Nantes\n\n\n2013\n\n\n70\n\n\n12e\n\n\n Christian Gourcuff\n\n\n2019\n\n\nStade de la Beaujoire - Louis Fonteneau\n\n\n35 322\n\n\n51\n\n\n\n\nSCO d’Angers\n\n\n2015\n\n\n32\n\n\n13e\n\n\n Stéphane Moulin\n\n\n2011\n\n\nStade Raymond-Kopa\n\n\n14 582\n\n\n27\n\n\n\n\nGirondins de Bordeaux\n\n\n1992\n\n\n70\n\n\n14e\n\n\n Paulo Sousa\n\n\n2019\n\n\nMatmut Atlantique\n\n\n42 115\n\n\n66\n\n\n\n\nAmiens SC\n\n\n2017\n\n\n30\n\n\n15e\n\n\n Luka Elsner\n\n\n2019\n\n\nStade Crédit Agricole la Licorne\n\n\n12 999\n\n\n2\n\n\n\n\nToulouse FC\n\n\n2003\n\n\n35\n\n\n16e\n\n\n Denis Zanko\n\n\n2020\n\n\nStadium de Toulouse\n\n\n33 033\n\n\n32\n\n\n\n\nAS Monaco\n\n\n2013\n\n\n220\n\n\n17e\n\n\n Robert Moreno\n\n\n2019\n\n\nStade Louis-II\n\n\n16 500\n\n\n60\n\n\n\n\nDijon FCO\n\n\n2016\n\n\n38\n\n\n18e\n\n\n Stéphane Jobard\n\n\n2019\n\n\nParc des Sports Gaston-Gérard\n\n\n15 459\n\n\n4\n\n\n\n\nFC Metz\n\n\n2019\n\n\n40\n\n\n1er (Ligue 2)\n\n\n Vincent Hognon\n\n\n2019\n\n\nStade Saint-Symphorien\n\n\n25 865\n\n\n61\n\n\n\n\nStade brestois 29\n\n\n2019\n\n\n30\n\n\n2e (Ligue 2)\n\n\n Olivier Dall’Oglio\n\n\n2019\n\n\nStade Francis-Le Blé\n\n\n14 920\n\n\n13\n\n\n\n\n\n\n2️⃣ Récupérer chaque ligne du tableau\nOn recherche d’abord toutes les lignes du tableau avec la balise tr\n\ntable_body = tableau_participants.find('tbody')\nrows = table_body.find_all('tr')\n\nOn obtient une liste où chaque élément est une des lignes du tableau\nPour illustrer cela, on va d’abord afficher la première ligne.\nCelle-ci correspont aux entêtes de colonne:\n\nprint(rows[0])\n\n&lt;tr&gt;\n&lt;th scope=\"col\"&gt;Club\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Dernière&lt;br/&gt;montée\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Budget&lt;sup class=\"reference\" id=\"cite_ref-3\"&gt;&lt;a href=\"#cite_note-3\"&gt;&lt;span class=\"cite_crochet\"&gt;[&lt;/span&gt;3&lt;span class=\"cite_crochet\"&gt;]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;&lt;br/&gt;en M&lt;a href=\"/wiki/Euro\" title=\"Euro\"&gt;€&lt;/a&gt;\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Classement&lt;br/&gt;&lt;a href=\"/wiki/Championnat_de_France_de_football_2018-2019\" title=\"Championnat de France de football 2018-2019\"&gt;2018-2019&lt;/a&gt;\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Entraîneur\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Depuis\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Stade\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Capacité&lt;br/&gt;en L1&lt;sup class=\"reference\" id=\"cite_ref-4\"&gt;&lt;a href=\"#cite_note-4\"&gt;&lt;span class=\"cite_crochet\"&gt;[&lt;/span&gt;4&lt;span class=\"cite_crochet\"&gt;]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Nombre&lt;br/&gt;de saisons&lt;br/&gt;en L1\n&lt;/th&gt;&lt;/tr&gt;\n\n\nLa seconde ligne va correspondre à la ligne du premier club présent dans le tableau:\n\nprint(rows[1])\n\n&lt;tr bgcolor=\"#97DEFF\"&gt;\n&lt;td&gt;&lt;a href=\"/wiki/Paris_Saint-Germain_Football_Club\" title=\"Paris Saint-Germain Football Club\"&gt;Paris Saint-Germain&lt;/a&gt;\n&lt;/td&gt;\n&lt;td&gt;1974\n&lt;/td&gt;\n&lt;td&gt;637\n&lt;/td&gt;\n&lt;td&gt;&lt;span data-sort-value=\"101 !\"&gt;&lt;/span&gt;&lt;abbr class=\"abbr\" title=\"Premier\"&gt;1&lt;sup&gt;er&lt;/sup&gt;&lt;/abbr&gt;\n&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;span class=\"flagicon\"&gt;&lt;span class=\"mw-image-border noviewer\" typeof=\"mw:File\"&gt;&lt;a class=\"mw-file-description\" href=\"/wiki/Fichier:Flag_of_Germany.svg\" title=\"Drapeau : Allemagne\"&gt;&lt;img alt=\"\" class=\"mw-file-element\" data-file-height=\"600\" data-file-width=\"1000\" decoding=\"async\" height=\"12\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Flag_of_Germany.svg/20px-Flag_of_Germany.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Flag_of_Germany.svg/30px-Flag_of_Germany.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Flag_of_Germany.svg/40px-Flag_of_Germany.svg.png 2x\" width=\"20\"/&gt;&lt;/a&gt;&lt;/span&gt;&lt;/span&gt; &lt;a href=\"/wiki/Thomas_Tuchel\" title=\"Thomas Tuchel\"&gt;Thomas Tuchel&lt;/a&gt;\n&lt;/td&gt;\n&lt;td&gt;2018\n&lt;/td&gt;\n&lt;td&gt;&lt;a href=\"/wiki/Parc_des_Princes\" title=\"Parc des Princes\"&gt;Parc des Princes&lt;/a&gt;\n&lt;/td&gt;\n&lt;td&gt;47 929\n&lt;/td&gt;\n&lt;td&gt;46\n&lt;/td&gt;&lt;/tr&gt;\n\n\n3️⃣ Nettoyer les sorties en ne gardant que le texte sur une ligne\nOn va utiliser l’attribut text afin de se débarasser de toute la couche de HTML qu’on obtient à l’étape 2.\nUn exemple sur la ligne du premier club :\n- on commence par prendre toutes les cellules de cette ligne, avec la balise td.\n- on fait ensuite une boucle sur chacune des cellules et on ne garde que le texte de la cellule avec l’attribut text.\n- enfin, on applique la méthode strip() pour que le texte soit bien mis en forme (sans espace inutile etc).\n\ncols = rows[1].find_all('td')\nprint(cols[0])\nprint(cols[0].text.strip())\n\n&lt;td&gt;&lt;a href=\"/wiki/Paris_Saint-Germain_Football_Club\" title=\"Paris Saint-Germain Football Club\"&gt;Paris Saint-Germain&lt;/a&gt;\n&lt;/td&gt;\nParis Saint-Germain\n\n\n\nfor ele in cols : \n    print(ele.text.strip())\n\nParis Saint-Germain\n1974\n637\n1er\nThomas Tuchel\n2018\nParc des Princes\n47 929\n46\n\n\n4️⃣ Généraliser sur toutes les lignes :\n\nfor row in rows:\n    cols = row.find_all('td')\n    cols = [ele.text.strip() for ele in cols]\n    print(cols)\n\n[]\n['Paris Saint-Germain', '1974', '637', '1er', 'Thomas Tuchel', '2018', 'Parc des Princes', '47\\xa0929', '46']\n['LOSC Lille', '2000', '120', '2e', 'Christophe Galtier', '2017', 'Stade Pierre-Mauroy', '49\\xa0712', '59']\n['Olympique lyonnais', '1989', '310', '3e', 'Rudi Garcia', '2019', 'Groupama Stadium', '57\\xa0206', '60']\n['AS Saint-Étienne', '2004', '100', '4e', 'Claude Puel', '2019', 'Stade Geoffroy-Guichard', '41\\xa0965', '66']\n['Olympique de Marseille', '1996', '110', '5e', 'André Villas-Boas', '2019', 'Orange Vélodrome', '66\\xa0226', '69']\n['Montpellier HSC', '2009', '40', '6e', 'Michel Der Zakarian', '2017', 'Stade de la Mosson', '22\\xa0000', '27']\n['OGC Nice', '2002', '50', '7e', 'Patrick Vieira', '2018', 'Allianz Riviera', '35\\xa0596', '60']\n['Stade de Reims', '2018', '45', '8e', 'David Guion', '2017', 'Stade Auguste-Delaune', '20\\xa0546', '35']\n['Nîmes Olympique', '2018', '27', '9e', 'Bernard Blaquart', '2015', 'Stade des Costières', '15\\xa0788', '35']\n['Stade rennais FC', '1994', '65', '10e', 'Julien Stéphan', '2018', 'Roazhon Park', '29\\xa0194', '62']\n['RC Strasbourg Alsace', '2017', '43', '11e', 'Thierry Laurey', '2016', 'Stade de la Meinau', '26\\xa0109', '58']\n['FC Nantes', '2013', '70', '12e', 'Christian Gourcuff', '2019', 'Stade de la Beaujoire - Louis Fonteneau', '35\\xa0322', '51']\n['SCO d’Angers', '2015', '32', '13e', 'Stéphane Moulin', '2011', 'Stade Raymond-Kopa', '14\\xa0582', '27']\n['Girondins de Bordeaux', '1992', '70', '14e', 'Paulo Sousa', '2019', 'Matmut Atlantique', '42\\xa0115', '66']\n['Amiens SC', '2017', '30', '15e', 'Luka Elsner', '2019', 'Stade Crédit Agricole la Licorne', '12\\xa0999', '2']\n['Toulouse FC', '2003', '35', '16e', 'Denis Zanko', '2020', 'Stadium de Toulouse', '33\\xa0033', '32']\n['AS Monaco', '2013', '220', '17e', 'Robert Moreno', '2019', 'Stade Louis-II', '16\\xa0500', '60']\n['Dijon FCO', '2016', '38', '18e', 'Stéphane Jobard', '2019', 'Parc des Sports Gaston-Gérard', '15\\xa0459', '4']\n['FC Metz', '2019', '40', '1er (Ligue 2)', 'Vincent Hognon', '2019', 'Stade Saint-Symphorien', '25\\xa0865', '61']\n['Stade brestois 29', '2019', '30', '2e (Ligue 2)', \"Olivier Dall'Oglio\", '2019', 'Stade Francis-Le Blé', '14\\xa0920', '13']\n\n\nOn a bien réussi à avoir les informations contenues dans le tableau des participants du championnat.\nMais la première ligne est étrange : c’est une liste vide …\nIl s’agit des en-têtes : elles sont reconnues par la balise th et non td.\nOn va mettre tout le contenu dans un dictionnaire, pour le transformer ensuite en DataFrame pandas :\n\ndico_participants = dict()\nfor row in rows:\n    cols = row.find_all('td')\n    cols = [ele.text.strip() for ele in cols]\n    if len(cols) &gt; 0 : \n        dico_participants[cols[0]] = cols[1:]\ndico_participants\n\n{'Paris Saint-Germain': ['1974',\n  '637',\n  '1er',\n  'Thomas Tuchel',\n  '2018',\n  'Parc des Princes',\n  '47\\xa0929',\n  '46'],\n 'LOSC Lille': ['2000',\n  '120',\n  '2e',\n  'Christophe Galtier',\n  '2017',\n  'Stade Pierre-Mauroy',\n  '49\\xa0712',\n  '59'],\n 'Olympique lyonnais': ['1989',\n  '310',\n  '3e',\n  'Rudi Garcia',\n  '2019',\n  'Groupama Stadium',\n  '57\\xa0206',\n  '60'],\n 'AS Saint-Étienne': ['2004',\n  '100',\n  '4e',\n  'Claude Puel',\n  '2019',\n  'Stade Geoffroy-Guichard',\n  '41\\xa0965',\n  '66'],\n 'Olympique de Marseille': ['1996',\n  '110',\n  '5e',\n  'André Villas-Boas',\n  '2019',\n  'Orange Vélodrome',\n  '66\\xa0226',\n  '69'],\n 'Montpellier HSC': ['2009',\n  '40',\n  '6e',\n  'Michel Der Zakarian',\n  '2017',\n  'Stade de la Mosson',\n  '22\\xa0000',\n  '27'],\n 'OGC Nice': ['2002',\n  '50',\n  '7e',\n  'Patrick Vieira',\n  '2018',\n  'Allianz Riviera',\n  '35\\xa0596',\n  '60'],\n 'Stade de Reims': ['2018',\n  '45',\n  '8e',\n  'David Guion',\n  '2017',\n  'Stade Auguste-Delaune',\n  '20\\xa0546',\n  '35'],\n 'Nîmes Olympique': ['2018',\n  '27',\n  '9e',\n  'Bernard Blaquart',\n  '2015',\n  'Stade des Costières',\n  '15\\xa0788',\n  '35'],\n 'Stade rennais FC': ['1994',\n  '65',\n  '10e',\n  'Julien Stéphan',\n  '2018',\n  'Roazhon Park',\n  '29\\xa0194',\n  '62'],\n 'RC Strasbourg Alsace': ['2017',\n  '43',\n  '11e',\n  'Thierry Laurey',\n  '2016',\n  'Stade de la Meinau',\n  '26\\xa0109',\n  '58'],\n 'FC Nantes': ['2013',\n  '70',\n  '12e',\n  'Christian Gourcuff',\n  '2019',\n  'Stade de la Beaujoire - Louis Fonteneau',\n  '35\\xa0322',\n  '51'],\n 'SCO d’Angers': ['2015',\n  '32',\n  '13e',\n  'Stéphane Moulin',\n  '2011',\n  'Stade Raymond-Kopa',\n  '14\\xa0582',\n  '27'],\n 'Girondins de Bordeaux': ['1992',\n  '70',\n  '14e',\n  'Paulo Sousa',\n  '2019',\n  'Matmut Atlantique',\n  '42\\xa0115',\n  '66'],\n 'Amiens SC': ['2017',\n  '30',\n  '15e',\n  'Luka Elsner',\n  '2019',\n  'Stade Crédit Agricole la Licorne',\n  '12\\xa0999',\n  '2'],\n 'Toulouse FC': ['2003',\n  '35',\n  '16e',\n  'Denis Zanko',\n  '2020',\n  'Stadium de Toulouse',\n  '33\\xa0033',\n  '32'],\n 'AS Monaco': ['2013',\n  '220',\n  '17e',\n  'Robert Moreno',\n  '2019',\n  'Stade Louis-II',\n  '16\\xa0500',\n  '60'],\n 'Dijon FCO': ['2016',\n  '38',\n  '18e',\n  'Stéphane Jobard',\n  '2019',\n  'Parc des Sports Gaston-Gérard',\n  '15\\xa0459',\n  '4'],\n 'FC Metz': ['2019',\n  '40',\n  '1er (Ligue 2)',\n  'Vincent Hognon',\n  '2019',\n  'Stade Saint-Symphorien',\n  '25\\xa0865',\n  '61'],\n 'Stade brestois 29': ['2019',\n  '30',\n  '2e (Ligue 2)',\n  \"Olivier Dall'Oglio\",\n  '2019',\n  'Stade Francis-Le Blé',\n  '14\\xa0920',\n  '13']}\n\n\n\ndata_participants = pandas.DataFrame.from_dict(dico_participants,orient='index')\ndata_participants.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\nParis Saint-Germain\n1974\n637\n1er\nThomas Tuchel\n2018\nParc des Princes\n47 929\n46\n\n\nLOSC Lille\n2000\n120\n2e\nChristophe Galtier\n2017\nStade Pierre-Mauroy\n49 712\n59\n\n\nOlympique lyonnais\n1989\n310\n3e\nRudi Garcia\n2019\nGroupama Stadium\n57 206\n60\n\n\nAS Saint-Étienne\n2004\n100\n4e\nClaude Puel\n2019\nStade Geoffroy-Guichard\n41 965\n66\n\n\nOlympique de Marseille\n1996\n110\n5e\nAndré Villas-Boas\n2019\nOrange Vélodrome\n66 226\n69\n\n\n\n\n\n\n\n5️⃣ Récupérer les en-têtes du tableau:\n\nfor row in rows:\n    cols = row.find_all('th')\n    print(cols)\n    if len(cols) &gt; 0 : \n        cols = [ele.get_text(separator=' ').strip().title() for ele in cols]\n        columns_participants = cols\n\n[&lt;th scope=\"col\"&gt;Club\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Dernière&lt;br/&gt;montée\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Budget&lt;sup class=\"reference\" id=\"cite_ref-3\"&gt;&lt;a href=\"#cite_note-3\"&gt;&lt;span class=\"cite_crochet\"&gt;[&lt;/span&gt;3&lt;span class=\"cite_crochet\"&gt;]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;&lt;br/&gt;en M&lt;a href=\"/wiki/Euro\" title=\"Euro\"&gt;€&lt;/a&gt;\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Classement&lt;br/&gt;&lt;a href=\"/wiki/Championnat_de_France_de_football_2018-2019\" title=\"Championnat de France de football 2018-2019\"&gt;2018-2019&lt;/a&gt;\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Entraîneur\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Depuis\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Stade\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Capacité&lt;br/&gt;en L1&lt;sup class=\"reference\" id=\"cite_ref-4\"&gt;&lt;a href=\"#cite_note-4\"&gt;&lt;span class=\"cite_crochet\"&gt;[&lt;/span&gt;4&lt;span class=\"cite_crochet\"&gt;]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Nombre&lt;br/&gt;de saisons&lt;br/&gt;en L1\n&lt;/th&gt;]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n\n\n\ncolumns_participants\n\n['Club',\n 'Dernière Montée',\n 'Budget [ 3 ] En M €',\n 'Classement 2018-2019',\n 'Entraîneur',\n 'Depuis',\n 'Stade',\n 'Capacité En L1 [ 4 ]',\n 'Nombre De Saisons En L1']\n\n\n6️⃣ Finalisation du tableau\n\ndata_participants.columns = columns_participants[1:]\n\n\ndata_participants.head()\n\n\n\n\n\n\n\n\nDernière Montée\nBudget [ 3 ] En M €\nClassement 2018-2019\nEntraîneur\nDepuis\nStade\nCapacité En L1 [ 4 ]\nNombre De Saisons En L1\n\n\n\n\nParis Saint-Germain\n1974\n637\n1er\nThomas Tuchel\n2018\nParc des Princes\n47 929\n46\n\n\nLOSC Lille\n2000\n120\n2e\nChristophe Galtier\n2017\nStade Pierre-Mauroy\n49 712\n59\n\n\nOlympique lyonnais\n1989\n310\n3e\nRudi Garcia\n2019\nGroupama Stadium\n57 206\n60\n\n\nAS Saint-Étienne\n2004\n100\n4e\nClaude Puel\n2019\nStade Geoffroy-Guichard\n41 965\n66\n\n\nOlympique de Marseille\n1996\n110\n5e\nAndré Villas-Boas\n2019\nOrange Vélodrome\n66 226\n69"
  },
  {
    "objectID": "content/manipulation/04a_webscraping_TP.html#pour-aller-plus-loin",
    "href": "content/manipulation/04a_webscraping_TP.html#pour-aller-plus-loin",
    "title": "Web scraping avec Python",
    "section": "Pour aller plus loin",
    "text": "Pour aller plus loin\n\nRécupération des localisations des stades\nEssayez de comprendre pas à pas ce qui est fait dans les étapes qui suivent (la récupération d’informations supplémentaires en naviguant dans les pages des différents clubs).\n\nimport urllib\nimport pandas as pd\nimport bs4 \n\ndivision=[]\nequipe=[]\nstade=[]\nlatitude_stade=[]        \nlongitude_stade=[]     \n\nurl_list=[\"http://fr.wikipedia.org/wiki/Championnat_de_France_de_football_2019-2020\", \"http://fr.wikipedia.org/wiki/Championnat_de_France_de_football_de_Ligue_2_2019-2020\"]\n\nfor url_ligue in url_list :\n       \n    print(url_ligue)\n    sock = urllib.request.urlopen(url_ligue).read() \n    page=bs4.BeautifulSoup(sock)\n\n# Rechercher les liens des équipes dans la liste disponible sur wikipedia \n\n    for team in page.findAll('span' , {'class' : 'toponyme'}) :  \n        \n        # Indiquer si c'est de la ligue 1 ou de la ligue 2\n        \n        if url_ligue==url_list[0] :\n            division.append(\"L1\")\n        else :\n            division.append(\"L2\")\n\n       # Trouver le nom et le lien de l'équipe\n            \n        if team.find('a')!=None :\n            team_url=team.find('a').get('href')\n            name_team=team.find('a').get('title')\n            equipe.append(name_team)\n            url_get_info = \"http://fr.wikipedia.org\"+team_url\n            print(url_get_info)\n \n       # aller sur la page de l'équipe\n           \n            search = urllib.request.urlopen(url_get_info).read()\n            search_team=bs4.BeautifulSoup(search)\n\n       # trouver le stade             \n            compteur = 0\n            for stadium in search_team.findAll('tr'):\n                for x in stadium.findAll('th' , {'scope' : 'row'} ) :\n                    if x.contents[0].string==\"Stade\" and compteur == 0:\n                        compteur = 1\n                        # trouver le lien du stade et son nom\n                        url_stade=stadium.findAll('a')[1].get('href')\n                        name_stadium=stadium.findAll('a')[1].get('title')\n                        stade.append(name_stadium)\n                        url_get_stade = \"http://fr.wikipedia.org\"+url_stade\n                        print(url_get_stade)\n                        \n                        # Aller sur la page du stade et trouver ses coodronnées géographiques\n                        \n                        search_stade = urllib.request.urlopen(url_get_stade).read()\n                        soup_stade=bs4.BeautifulSoup(search_stade) \n                        kartographer = soup_stade.find('a',{'class': \"mw-kartographer-maplink\"})\n                        if kartographer == None :\n                          latitude_stade.append(None)\n                          longitude_stade.append(None) \n                        else :\n                            for coordinates in kartographer :\n                                print(coordinates)\n                                liste =   coordinates.split(\",\")          \n                                latitude_stade.append(str(liste[0]).replace(\" \", \"\") + \"'\")\n                                longitude_stade.append(str(liste[1]).replace(\" \", \"\") + \"'\")\n                            \n\ndict = {'division' : division , 'equipe': equipe, 'stade': stade, 'latitude': latitude_stade, 'longitude' : longitude_stade}\ndata = pd.DataFrame(dict)\ndata = data.dropna()\n\n\ndata.head(5)\n\n\n\n\n\n\n\n\ndivision\nequipe\nstade\nlatitude\nlongitude\n\n\n\n\n0\nL1\nParis Saint-Germain Football Club\nParc des Princes\n48° 50′ 29″ N'\n2° 15′ 11″ E'\n\n\n1\nL1\nLOSC Lille\nStade Pierre-Mauroy\n50° 36′ 43″ N'\n3° 07′ 50″ E'\n\n\n2\nL1\nOlympique lyonnais\nParc Olympique lyonnais\n45° 45′ 55″ N'\n4° 58′ 55″ E'\n\n\n3\nL1\nAssociation sportive de Saint-Étienne\nStade Geoffroy-Guichard\n45° 27′ 39″ N'\n4° 23′ 25″ E'\n\n\n4\nL1\nOlympique de Marseille\nStade Vélodrome\n43° 16′ 11″ N'\n5° 23′ 45″ E'\n\n\n\n\n\n\n\nOn va transformer les coordonnées en degrés en coordonnées numériques\nafin d’être en mesure de faire une carte.\n\nimport re\n\ndef dms2dd(degrees, minutes, seconds, direction):\n    dd = float(degrees) + float(minutes)/60 + float(seconds)/(60*60);\n    if direction in ('S', 'O'):\n        dd *= -1\n    return dd\n\ndef parse_dms(dms):\n    parts = re.split('[^\\d\\w]+', dms)\n    lat = dms2dd(parts[0], parts[1], parts[2], parts[3])\n    #lng = dms2dd(parts[4], parts[5], parts[6], parts[7])\n    return lat\n\n\ndata['latitude'] = data['latitude'].apply(parse_dms)\ndata['longitude'] = data['longitude'].apply(parse_dms)\n\nTous les éléments sont en place pour faire une belle carte à ce stade. On\nva utiliser folium pour celle-ci, qui est présenté dans la partie\nvisualisation.\n\n\nCarte des stades avec folium\n\nimport geopandas as gpd\nfrom pathlib import Path\nimport folium\n\ngdf = gpd.GeoDataFrame(\n    data, geometry=gpd.points_from_xy(data.longitude, data.latitude))\n\nPath(\"leaflet\").mkdir(parents=True, exist_ok=True)\n\ncenter = gdf[['latitude', 'longitude']].mean().values.tolist()\nsw = gdf[['latitude', 'longitude']].min().values.tolist()\nne = gdf[['latitude', 'longitude']].max().values.tolist()\n\nm = folium.Map(location = center, tiles='openstreetmap')\n\n# I can add marker one by one on the map\nfor i in range(0,len(gdf)):\n    folium.Marker([gdf.iloc[i]['latitude'], gdf.iloc[i]['longitude']], popup=gdf.iloc[i]['stade']).add_to(m) \n\nm.fit_bounds([sw, ne])\n\nLa carte obtenue doit ressembler à la suivante :\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "content/manipulation/04a_webscraping_TP.html#récupérer-des-informations-sur-les-pokemons",
    "href": "content/manipulation/04a_webscraping_TP.html#récupérer-des-informations-sur-les-pokemons",
    "title": "Web scraping avec Python",
    "section": "Récupérer des informations sur les pokemons",
    "text": "Récupérer des informations sur les pokemons\nLe prochain exercice pour mettre en pratique le web scraping\nconsiste à récupérer des informations sur les\npokemons à partir du\nsite internet pokemondb.net.\n\nVersion non guidée\n\n\n Exercice 2 : Les pokemon (version non guidée)\nPour cet exercice, nous vous demandons d’obtenir différentes informations sur les pokémons :\n\nles informations personnelles des 893 pokemons sur le site internet pokemondb.net.\nLes informations que nous aimerions obtenir au final dans un DataFrame sont celles contenues dans 4 tableaux :\n\n\nPokédex data\nTraining\nBreeding\nBase stats\n\n\nNous aimerions que vous récupériez également les images de chacun des pokémons et que vous les enregistriez dans un dossier\n\n\nPetit indice : utilisez les modules request et shutil\nPour cette question, il faut que vous cherchiez de vous même certains éléments, tout n’est pas présent dans le TD.\n\n\n\nPour la question 1, l’objectif est d’obtenir le code source d’un tableau comme\ncelui qui suit\n(Pokemon Nincada).\n\n\n\nPokédex data\n\n\n\n\n\n\nNational №\n\n\n290\n\n\n\n\nType\n\n\nBug Ground\n\n\n\n\nSpecies\n\n\nTrainee Pokémon\n\n\n\n\nHeight\n\n\n0.5 m (1′08″)\n\n\n\n\nWeight\n\n\n5.5 kg (12.1 lbs)\n\n\n\n\nAbilities\n\n\n1. Compound EyesRun Away (hidden ability)\n\n\n\n\nLocal №\n\n\n042 (Ruby/Sapphire/Emerald)111 (X/Y — Central Kalos)043 (Omega Ruby/Alpha Sapphire)104 (Sword/Shield)\n\n\n\n\n\n\n\n\n\n\nTraining\n\n\n\n\n\n\nEV yield\n\n\n1 Defense\n\n\n\n\nCatch rate\n\n\n255 (33.3% with PokéBall, full HP)\n\n\n\n\nBase Friendship\n\n\n70 (normal)\n\n\n\n\nBase Exp.\n\n\n53\n\n\n\n\nGrowth Rate\n\n\nErratic\n\n\n\n\n\n\n\n\nBreeding\n\n\n\n\n\n\nEgg Groups\n\n\nBug\n\n\n\n\nGender\n\n\n50% male, 50% female\n\n\n\n\nEgg cycles\n\n\n15 (3,599–3,855 steps)\n\n\n\n\n\n\n\n\n\n\n\n\n\nBase stats\n\n\n\n\n\n\n\nHP\n\n\n31\n\n\n\n\n\n\n\n172\n\n\n266\n\n\n\n\nAttack\n\n\n45\n\n\n\n\n\n\n\n85\n\n\n207\n\n\n\n\nDefense\n\n\n90\n\n\n\n\n\n\n\n166\n\n\n306\n\n\n\n\nSp. Atk\n\n\n30\n\n\n\n\n\n\n\n58\n\n\n174\n\n\n\n\nSp. Def\n\n\n30\n\n\n\n\n\n\n\n58\n\n\n174\n\n\n\n\nSpeed\n\n\n40\n\n\n\n\n\n\n\n76\n\n\n196\n\n\n\n\n\n\nTotal\n\n\n266\n\n\n\n\nMin\n\n\nMax\n\n\n\n\n\n\n\n\nPour la question 2, l’objectif est d’obtenir\nles images des pokemon.\n\n\nVersion guidée\nLes prochaines parties permettront de faire l’exercice ci-dessus\nétape par étape,\nde manière guidée.\nNous souhaitons tout d’abord obtenir les\ninformations personnelles de tous\nles pokemons sur pokemondb.net.\nLes informations que nous aimerions obtenir au final pour les pokemons sont celles contenues dans 4 tableaux :\n\nPokédex data\nTraining\nBreeding\nBase stats\n\nNous proposons ensuite de récupérer et afficher les images.\n\nEtape 1: constituer un DataFrame de caractéristiques\n\n\n Exercice 2b : Les pokémons (version guidée)\nPour récupérer les informations, le code devra être divisé en plusieurs étapes :\n\nTrouvez la page principale du site et la transformer en un objet intelligible pour votre code.\nLes fonctions suivantes vous seront utiles :\n\n\nurllib.request.Request\nurllib.request.urlopen\nbs4.BeautifulSoup\n\n\nCréez une fonction qui permet de récupérer la page d’un pokémon à partir de son nom.\nA partir de la page de bulbasaur, obtenez les 4 tableaux qui nous intéressent :\n\n\non va chercher l’élément suivant : ('table', { 'class' : \"vitals-table\"})\npuis stocker ses éléments dans un dictionnaire\n\n\nRécupérez par ailleurs la liste de noms des pokémons qui nous permettra de faire une boucle par la suite. Combien trouvez-vous de pokémons ?\nEcrire une fonction qui récupère l’ensemble des informations sur les dix premiers pokémons de la liste et les intègre dans un DataFrame\n\n\n\nA l’issue de la question 3,\nvous devriez obtenir une liste de caractéristiques proche de celle-ci:\n\n\n{'National №': '0001',\n 'name': 'bulbasaur',\n 'Type': ' Grass Poison ',\n 'Species': 'Seed Pokémon',\n 'Height': '0.7\\xa0m (2′04″)',\n 'Weight': '6.9\\xa0kg (15.2\\xa0lbs)',\n 'Abilities': '1. OvergrowChlorophyll (hidden ability)',\n 'Local №': \"0001 (Red/Blue/Yellow)0226 (Gold/Silver/Crystal)0001 (FireRed/LeafGreen)0231 (HeartGold/SoulSilver)0080 (X/Y — Central Kalos)0001 (Let's Go Pikachu/Let's Go Eevee)0068 (The Isle of Armor)\",\n 'EV yield': ' 1 Sp. Atk ',\n 'Catch rate': ' 45 (5.9% with PokéBall, full HP) ',\n 'Base Friendship': ' 50 (normal) ',\n 'Base Exp.': '64',\n 'Growth Rate': 'Medium Slow',\n 'Egg Groups': 'Grass, Monster',\n 'Gender': '87.5% male, 12.5% female',\n 'Egg cycles': '20 (4,884–5,140 steps) ',\n 'HP': '45',\n 'Attack': '49',\n 'Defense': '49',\n 'Sp. Atk': '65',\n 'Sp. Def': '65',\n 'Speed': '45'}\n\n\nLa structure est ici en dictionnaire, ce qui est pratique.\nEnfin, vous pouvez intégrer les informations\ndes dix premiers pokémons à un\nDataFrame, qui aura l’aspect suivant :\n\n\n\n\n\n\n\n\n\nNational №\nname\nType\nSpecies\nHeight\nWeight\nAbilities\nLocal №\nEV yield\nCatch rate\n...\nGrowth Rate\nEgg Groups\nGender\nEgg cycles\nHP\nAttack\nDefense\nSp. Atk\nSp. Def\nSpeed\n\n\n\n\n0\n0001\nbulbasaur\nGrass Poison\nSeed Pokémon\n0.7 m (2′04″)\n6.9 kg (15.2 lbs)\n1. OvergrowChlorophyll (hidden ability)\n0001 (Red/Blue/Yellow)0226 (Gold/Silver/Crysta...\n1 Sp. Atk\n45 (5.9% with PokéBall, full HP)\n...\nMedium Slow\nGrass, Monster\n87.5% male, 12.5% female\n20 (4,884–5,140 steps)\n45\n49\n49\n65\n65\n45\n\n\n1\n0002\nivysaur\nGrass Poison\nSeed Pokémon\n1.0 m (3′03″)\n13.0 kg (28.7 lbs)\n1. OvergrowChlorophyll (hidden ability)\n0002 (Red/Blue/Yellow)0227 (Gold/Silver/Crysta...\n1 Sp. Atk, 1 Sp. Def\n45 (5.9% with PokéBall, full HP)\n...\nMedium Slow\nGrass, Monster\n87.5% male, 12.5% female\n20 (4,884–5,140 steps)\n60\n62\n63\n80\n80\n60\n\n\n2\n0003\nvenusaur\nGrass Poison\nSeed Pokémon\n2.0 m (6′07″)\n100.0 kg (220.5 lbs)\n1. OvergrowChlorophyll (hidden ability)\n0003 (Red/Blue/Yellow)0228 (Gold/Silver/Crysta...\n2 Sp. Atk, 1 Sp. Def\n45 (5.9% with PokéBall, full HP)\n...\nMedium Slow\nGrass, Monster\n87.5% male, 12.5% female\n20 (4,884–5,140 steps)\n80\n82\n83\n100\n100\n80\n\n\n3\n0004\ncharmander\nFire\nLizard Pokémon\n0.6 m (2′00″)\n8.5 kg (18.7 lbs)\n1. BlazeSolar Power (hidden ability)\n0004 (Red/Blue/Yellow)0229 (Gold/Silver/Crysta...\n1 Speed\n45 (5.9% with PokéBall, full HP)\n...\nMedium Slow\nDragon, Monster\n87.5% male, 12.5% female\n20 (4,884–5,140 steps)\n39\n52\n43\n60\n50\n65\n\n\n4\n0005\ncharmeleon\nFire\nFlame Pokémon\n1.1 m (3′07″)\n19.0 kg (41.9 lbs)\n1. BlazeSolar Power (hidden ability)\n0005 (Red/Blue/Yellow)0230 (Gold/Silver/Crysta...\n1 Sp. Atk, 1 Speed\n45 (5.9% with PokéBall, full HP)\n...\nMedium Slow\nDragon, Monster\n87.5% male, 12.5% female\n20 (4,884–5,140 steps)\n58\n64\n58\n80\n65\n80\n\n\n\n\n5 rows × 22 columns\n\n\n\n\n\nEtape 2: récupérer et afficher des photos de Pokemon\nNous aimerions que vous récupériez également les images des 5 premiers pokémons\net que vous les enregistriez dans un dossier.\n\n\n Exercice 2b : Les pokémons (version guidée)\n\nLes URL des images des pokemon prennent la forme “https://img.pokemondb.net/artwork/{pokemon}.jpg”.\nUtiliser les modules requests et shutil pour télécharger\net enregistrer en local les images.\nImporter ces images stockées au format JPEG dans Python grâce à la fonction imread du package skimage.io\n\n\n\n\n!pip install scikit-image\n\nRequirement already satisfied: scikit-image in /opt/mamba/lib/python3.9/site-packages (0.21.0)\nRequirement already satisfied: numpy&gt;=1.21.1 in /opt/mamba/lib/python3.9/site-packages (from scikit-image) (1.26.0)\nRequirement already satisfied: scipy&gt;=1.8 in /opt/mamba/lib/python3.9/site-packages (from scikit-image) (1.11.2)\nRequirement already satisfied: networkx&gt;=2.8 in /opt/mamba/lib/python3.9/site-packages (from scikit-image) (3.1)\nRequirement already satisfied: pillow&gt;=9.0.1 in /opt/mamba/lib/python3.9/site-packages (from scikit-image) (10.0.0)\nRequirement already satisfied: imageio&gt;=2.27 in /opt/mamba/lib/python3.9/site-packages (from scikit-image) (2.31.4)\nRequirement already satisfied: tifffile&gt;=2022.8.12 in /opt/mamba/lib/python3.9/site-packages (from scikit-image) (2023.9.18)\nRequirement already satisfied: PyWavelets&gt;=1.1.1 in /opt/mamba/lib/python3.9/site-packages (from scikit-image) (1.4.1)\nRequirement already satisfied: packaging&gt;=21 in /opt/mamba/lib/python3.9/site-packages (from scikit-image) (21.3)\nRequirement already satisfied: lazy_loader&gt;=0.2 in /opt/mamba/lib/python3.9/site-packages (from scikit-image) (0.3)\nRequirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /opt/mamba/lib/python3.9/site-packages (from packaging&gt;=21-&gt;scikit-image) (3.0.9)\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv"
  },
  {
    "objectID": "content/manipulation/04a_webscraping_TP.html#selenium-mimer-le-comportement-dun-utilisateur-internet",
    "href": "content/manipulation/04a_webscraping_TP.html#selenium-mimer-le-comportement-dun-utilisateur-internet",
    "title": "Web scraping avec Python",
    "section": "Selenium : mimer le comportement d’un utilisateur internet",
    "text": "Selenium : mimer le comportement d’un utilisateur internet\nJusqu’à présent,\nnous avons raisonné comme si nous connaissions toujours l’url qui nous intéresse.\nDe plus, les pages que nous visitons sont “statiques”,\nelles ne dépendent pas d’une action ou d’une recherche de l’internaute.\nNous allons voir à présent comment nous en sortir pour remplir\ndes champs sur un site web et récupérer ce qui nous intéresse.\nLa réaction d’un site web à l’action d’un utilisateur passe régulièrement par\nl’usage de JavaScript dans le monde du développement web.\nLe package Selenium permet\nde reproduire, depuis un code automatisé, le comportement\nmanuel d’un utilisateur. Il permet ainsi\nd’obtenir des informations du site qui ne sont pas dans le\ncode HTML mais qui apparaissent uniquement à la suite de\nl’exécution de script JavaScript en arrière plan.\nSelenium se comporte comme un utilisateur lambda sur internet :\nil clique sur des liens, il remplit des formulaires, etc.\n\nPremier exemple en scrapant un moteur de recherche\nDans cet exemple, nous allons essayer d’aller sur le\nsite de Bing Actualités\net entrer dans la barre de recherche un sujet donné.\nPour tester, nous allons faire une recherche avec le mot-clé “Trump”.\nL’installation de Selenium nécessite d’avoir Chromium qui est un\nnavigateur Google Chrome minimaliste.\nLa version de chromedriver\ndoit être &gt;= 2.36 et dépend de la version de Chrome que vous avez sur votre environnement\nde travail. Pour installer cette version minimaliste de Chrome sur un environnement\nLinux, vous pouvez vous référer à l’encadré dédié.\n\n\n Installation de Selenium\nD’abord, il convient d’installer les dépendances.\nSur Colab, vous pouvez utiliser les commandes suivantes :\n\n!sudo apt-get update\n!sudo apt install -y unzip xvfb libxi6 libgconf-2-4 -y\n!sudo apt install chromium-chromedriver -y\n!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n\n\nSi vous êtes sur le SSP Cloud, vous pouvez\nexécuter les commandes suivantes :\n\n!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb -O /tmp/chrome.deb\n!sudo apt-get update\n!sudo -E apt-get install -y /tmp/chrome.deb\n!pip install chromedriver-autoinstaller selenium\n\nimport chromedriver_autoinstaller\nchromedriver_autoinstaller.install()\n\n\nVous pouvez ensuite installer Selenium.\nPar exemple, depuis une\ncellule de Notebook :\n\n\nAprès avoir installé Chromium,\nil est nécessaire d’indiquer à Python où\nle trouver. Si vous êtes sur Linux et que vous\navez suivi les consignes précédentes, vous pouvez faire :\n\nimport selenium\nfrom webdriver_manager.chrome import ChromeDriverManager\n\npath_to_web_driver = ChromeDriverManager().install()\n\nEn premier lieu, il convient d’initialiser le comportement\nde Selenium en répliquant les paramètres\ndu navigateur. Pour cela, on va d’abord initialiser\nnotre navigateur avec quelques options :\n\nimport time\n\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\n\nchrome_options = webdriver.ChromeOptions()\nchrome_options.add_argument('--headless')\nchrome_options.add_argument('--no-sandbox')\nchrome_options.add_argument('--disable-dev-shm-usage')\n#chrome_options.add_argument('--verbose') \n\nPuis on lance le navigateur :\n\nfrom selenium.webdriver.chrome.service import Service\nservice = Service(executable_path=path_to_web_driver)\n\nbrowser = webdriver.Chrome(service=service,\n                           options=chrome_options)\n\nOn va sur le site de Bing Actualités,\net on lui indique le mot clé que nous souhaitons chercher.\nEn l’occurrence, on s’intéresse aux actualités de Donald Trump.\nAprès avoir inspecté la page depuis les outils de développement du navigateur,\non voit que la barre de recherche est un élement du code appelé q (comme query).\nOn va ainsi demander à selenium de chercher cet élément:\n\nbrowser.get('https://www.bing.com/news')\n\nsearch = browser.find_element(\"name\", \"q\")\nprint(search)\nprint([search.text, search.tag_name, search.id])\n\n# on envoie à cet endroit le mot qu'on aurait tapé dans la barre de recherche\nsearch.send_keys(\"Trump\")\n\nsearch_button = browser.find_element(\"xpath\", \"//input[@id='sb_form_go']\") \nsearch_button.click()\n\nSelenium permet de capturer l’image qu’on verrait dans le navigateur\navec get_screenshot_as_png. Cela peut être utile pour vérifier qu’on\na fait la bonne action:\n\n\n\n\n\n\n\n\n\nEnfin, on peut extraire les résultats. Plusieurs\nméthodes sont disponibles. La méthode la plus\npratique, lorsqu’elle est disponible,\nest d’utiliser le XPath qui est un chemin\nnon ambigu pour accéder à un élement. En effet,\nplusieurs éléments peuvent partager la même classe ou\nle même attribut ce qui peut faire qu’une recherche\nde ce type peut renvoyer plusieurs échos.\nPour déterminer le XPath d’un objet, les outils\nde développeurs de votre site web sont pratiques.\nPar exemple, sous Firefox, une fois que vous\navez trouvé un élément dans l’inspecteur, vous\npouvez faire click droit &gt; Copier &gt; XPath.\nEnfin, pour mettre fin à notre session, on demande\nà Python de quitter le navigateur:\n\nbrowser.quit()\n\nOn a obtenu les résultats suivants :\n\n\n['https://www.msn.com/en-us/money/companies/republican-megadonor-mercer-family-weighs-backing-trump-as-they-maintain-massive-war-chest/ar-AA1kijjY', 'https://news.yahoo.com/not-law-legal-experts-torch-140917639.html', 'https://www.msn.com/en-us/news/other/trump-critics-appeal-colorado-ruling-that-said-insurrectionist-ban-doesn-t-apply-to-presidents/ar-AA1kit2w', 'https://www.msn.com/en-us/news/politics/louisiana-fraud-case-could-silence-donald-trump/ar-AA1khSUH', 'https://www.msn.com/en-us/news/politics/trump-fraud-trial-trump-continues-to-assail-judge-clerk/ar-AA1hyBsB', 'https://www.nytimes.com/2023/11/21/us/politics/trump-democrats-biden.html', 'https://www.msn.com/en-us/news/other/ex-trump-aide-says-democracy-won-t-survive-if-he-wins-2024-election/ar-AA1ki3uq', 'https://www.cbsnews.com/video/appeals-court-considers-reinstating-trumps-gag-order-in-2020-election-case/']\n\n\nLes autres méthodes utiles de Selenium:\n\n\n\n\n\n\n\nMéthode\nRésultat\n\n\n\n\nfind_element(****).click()\nUne fois qu’on a trouvé un élément réactif, notamment un bouton, on peut cliquer dessus pour activer une nouvelle page\n\n\nfind_element(****).send_keys(\"toto\")\nUne fois qu’on a trouvé un élément, notamment un champ où s’authentifier, on peut envoyer une valeur, ici “toto”.\n\n\n\n\n\nUtiliser Selenium pour jouer à 2048\nDans cet exemple, on utilise le module pour que Python\nappuie lui même sur les touches du clavier afin de jouer à 2048.\nNote : ce bout de code ne donne pas une solution à 2048,\nil permet juste de voir ce qu’on peut faire avec Selenium.\n\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.common.keys import Keys\n\n# on ouvre la page internet du jeu 2048\nservice = Service(executable_path=path_to_web_driver)\n\nbrowser = webdriver.Chrome(service=service,\n                           options=chrome_options)\nbrowser.get('https://play2048.co//')\n\n# Ce qu'on va faire : une boucle qui répète inlassablement la même chose : haut / droite / bas / gauche\n\n# on commence par cliquer sur la page pour que les touches sachent \nbutton = browser.find_element(\"class name\", 'grid-container')\nbrowser.execute_script(\"arguments[0].click();\", button)\ntime.sleep(0.5)\n\ngrid = browser.find_element(\"tag name\", 'body')\n\n# pour savoir quels coups faire à quel moment, on crée un dictionnaire\ndirection = {0: Keys.UP, 1: Keys.RIGHT, 2: Keys.DOWN, 3: Keys.LEFT}\ncount = 0\n\nwhile True:\n    try: # on vérifie que le bouton \"Try again\" n'est pas là - sinon ça veut dire que le jeu est fini\n        retryButton = browser.find_element(\"link text\",'Try again')\n        scoreElem = browser.find_element(\"class name\", 'score-container')\n        break\n    except:\n        #Do nothing.  Game is not over yet\n        pass\n    # on continue le jeu - on appuie sur la touche suivante pour le coup d'après\n    count += 1\n    grid.send_keys(direction[count % 4]) \n    time.sleep(0.1)\n\nprint('Score final : {} en {} coups'.format(scoreElem.text, count))    \nbrowser.quit()"
  },
  {
    "objectID": "content/manipulation/04a_webscraping_TP.html#exercice-supplémentaire",
    "href": "content/manipulation/04a_webscraping_TP.html#exercice-supplémentaire",
    "title": "Web scraping avec Python",
    "section": "Exercice supplémentaire",
    "text": "Exercice supplémentaire\nPour découvrir une autre application possible du web scraping, vous pouvez également vous lancer dans le sujet 5 de l’édition 2023 d’un hackathon non compétitif organisé par l’Insee :\n\nSur Github\nSur le SSPCloud\n\nLe contenu de la section NLP du cours pourra vous être utile pour la seconde partie du sujet !"
  },
  {
    "objectID": "content/manipulation/04b_regex_TP.html#introduction",
    "href": "content/manipulation/04b_regex_TP.html#introduction",
    "title": "Maîtriser les expressions régulières",
    "section": "Introduction",
    "text": "Introduction\nPython offre énormément de fonctionalités très pratiques pour la manipulation de données\ntextuelles. C’est l’une des raisons de son\nsuccès dans la communauté du traitement automatisé du langage (NLP, voir partie dédiée).\nDans les chapitres précédents, nous avons parfois été amenés à chercher des éléments textuels basiques. Cela était possible avec la méthode str.find du package Pandas qui constitue une version vectorisée de la méthode find\nde base. Nous avons d’ailleurs\npu utiliser cette dernière directement, notamment lorsqu’on a fait du web scraping.\nCependant, cette fonction de recherche\ntrouve rapidement ses limites.\nPar exemple, si on désire trouver à la fois les occurrences d’un terme au singulier\net au pluriel, il sera nécessaire d’utiliser\nau moins deux fois la méthode find.\nPour des verbes conjugués, cela devient encore plus complexe, en particulier si ceux-ci changent de forme selon le sujet.\nPour des expressions compliquées, il est conseillé d’utiliser les expressions régulières,\nou “regex”. C’est une fonctionnalité qu’on retrouve dans beaucoup de langages. C’est une forme de grammaire qui permet de rechercher des expressions.\nUne partie du contenu de cette partie\nest une adaptation de la\ndocumentation collaborative sur R nommée utilitR à laquelle j’ai participé. Ce chapitre reprend aussi du contenu du\nlivre R for Data Science qui présente un chapitre\ntrès pédagogique sur les regex.\nNous allons utiliser le package re pour illustrer nos exemples d’expressions\nrégulières. Il s’agit du package de référence, qui est utilisé, en arrière-plan,\npar Pandas pour vectoriser les recherches textuelles.\n\nimport re\nimport pandas as pd\n\n\n\n Hint\nLes expressions régulières (regex) sont notoirement difficiles à maîtriser. Il existe des outils qui facilitent le travail avec les expressions régulières.\n\nL’outil de référence pour ceci est [https://regex101.com/] qui permet de tester des regex en Python\ntout en ayant une explication qui accompagne ce test\nDe même pour ce site qui comporte une cheat sheet en bas de la page.\nLes jeux de Regex Crossword permettent d’apprendre les expressions régulières en s’amusant\n\nIl peut être pratique de demander à des IA assistantes, comme Github Copilot ou ChatGPT, une\npremière version d’une regex en expliquant le contenu qu’on veut extraire.\nCela peut faire économiser pas mal de temps, sauf quand l’IA fait preuve d’une confiance excessive\net vous propose avec aplomb une regex totalement fausse…"
  },
  {
    "objectID": "content/manipulation/04b_regex_TP.html#principe",
    "href": "content/manipulation/04b_regex_TP.html#principe",
    "title": "Maîtriser les expressions régulières",
    "section": "Principe",
    "text": "Principe\nLes expressions régulières sont un outil permettant de décrire un ensemble de chaînes de caractères possibles selon une syntaxe précise, et donc de définir un motif (ou pattern). Les expressions régulières servent par exemple lorsqu’on veut extraire une partie d’une chaîne de caractères, ou remplacer une partie d’une chaîne de caractères. Une expression régulière prend la forme d’une chaîne de caractères, qui peut contenir à la fois des éléments littéraux et des caractères spéciaux qui ont un sens logique.\nPar exemple, \"ch.+n\" est une expression régulière qui décrit le motif suivant : la chaîne littérale ch, suivi de n’importe quelle chaîne d’au moins un caractère (.+), suivie de la lettre n. Dans la chaîne \"J'ai un chien.\", la sous-chaîne \"chien\" correspond à ce motif. De même pour \"chapeau ron\" dans \"J'ai un chapeau rond\". En revanche, dans la chaîne \"La soupe est chaude.\", aucune sous-chaîne ne correpsond à ce motif (car aucun n n’apparaît après le ch).\nPour s’en convaincre, nous pouvons déjà regarder\nles deux premiers cas:\n\npattern = \"ch.+n\"\nprint(re.search(pattern, \"J'ai un chien.\"))\nprint(re.search(pattern, \"J'ai un chapeau rond.\"))\n\n&lt;re.Match object; span=(8, 13), match='chien'&gt;\n&lt;re.Match object; span=(8, 19), match='chapeau ron'&gt;\n\n\nCependant, dans le dernier cas, nous ne trouvons pas\nle pattern recherché:\n\nprint(re.search(pattern, \"La soupe est chaude.\"))\n\nNone\n\n\nLa regex précédente comportait deux types de caractères:\n\nles caractères littéraux : lettres et nombres qui sont reconnus de manière littérale\nles méta-caractères : symboles qui ont un sens particulier dans les regex.\n\nLes principaux méta-caractères sont ., +, *, [, ], ^ et $ mais il\nen existe beaucoup d’autres.\nParmi cet ensemble, on utilise principalement les quantifieurs (., +, *…),\nles classes de caractères (ensemble qui sont délimités par [ et ])\nou les ancres (^, $…)\nDans l’exemple précédent,\nnous retrouvions deux quantifieurs accolés .+. Le premier (.) signifie n’importe quel caractère1. Le deuxième (+) signifie “répète le pattern précédent”.\nDans notre cas, la combinaison .+ permet ainsi de répéter n’importe quel caractère avant de trouver un n.\nLe nombre de fois est indeterminé: cela peut ne pas être pas nécessaire d’intercaler des caractères avant le n\nou cela peut être nécessaire d’en intercepter plusieurs :\n\nprint(re.search(pattern, \"J'ai un chino\"))\nprint(re.search(pattern, \"J'ai un chiot très mignon.\"))\n\n&lt;re.Match object; span=(8, 12), match='chin'&gt;\n&lt;re.Match object; span=(8, 25), match='chiot très mignon'&gt;\n\n\n\nClasses de caractères\nLors d’une recherche, on s’intéresse aux caractères et souvent aux classes de caractères : on cherche un chiffre, une lettre, un caractère dans un ensemble précis ou un caractère qui n’appartient pas à un ensemble précis. Certains ensembles sont prédéfinis, d’autres doivent être définis à l’aide de crochets.\nPour définir un ensemble de caractères, il faut écrire cet ensemble entre crochets. Par exemple, [0123456789] désigne un chiffre. Comme c’est une séquence de caractères consécutifs, on peut résumer cette écriture en [0-9].\nPar\nexemple, si on désire trouver tous les pattern qui commencent par un c suivi\nd’un h puis d’une voyelle (a, e, i, o, u), on peut essayer\ncette expression régulière.\n\nre.findall(\"[c][h][aeiou]\", \"chat, chien, veau, vache, chèvre\")\n\n['cha', 'chi', 'che']\n\n\nIl serait plus pratique d’utiliser Pandas dans ce cas pour isoler les\nlignes qui répondent à la condition logique (en ajoutant les accents\nqui ne sont pas compris sinon):\n\nimport pandas as pd\ntxt = pd.Series(\"chat, chien, veau, vache, chèvre\".split(\", \"))\ntxt.str.match(\"ch[aeéèiou]\")\n\n0     True\n1     True\n2    False\n3    False\n4     True\ndtype: bool\n\n\nCependant, l’usage ci-dessus des classes de caractères\nn’est pas le plus fréquent.\nOn privilégie celles-ci pour identifier des\npattern complexe plutôt qu’une suite de caractères littéraux.\nLes tableaux d’aide mémoire illustrent une partie des\nclasses de caractères les plus fréquentes\n([:digit:] ou \\d…)\n\n\nQuantifieurs\nNous avons rencontré les quantifieurs avec notre première expression\nrégulière. Ceux-ci contrôlent le nombre de fois\nqu’un pattern est rencontré.\nLes plus fréquents sont:\n\n? : 0 ou 1 match ;\n+ : 1 ou plus de matches ;\n* : 0 or more matches.\n\nPar exemple, colou?r permettra de matcher à la fois l’écriture américaine et anglaise\n\nre.findall(\"colou?r\", \"Did you write color or colour?\")\n\n['color', 'colour']\n\n\nCes quantifiers peuvent bien sûr être associés à\nd’autres types de caractères, notamment les classes de caractères.\nCela peut être extrèmement pratique.\nPar exemple, \\d+ permettra de capturer un ou plusieurs chiffres, \\s?\npermettra d’ajouter en option un espace,\n[\\w]{6,8} un mot entre six et huit lettres qu’on écrira…\nIl est aussi possible de définir le nombre de répétitions\navec {}:\n\n{n} matche exactement n fois ;\n{n,} matche au moins n fois ;\n{n,m} matche entre n et m fois.\n\nCependant, la répétition des termes\nne s’applique par défaut qu’au dernier\ncaractère précédent le quantifier.\nOn peut s’en convaincre avec l’exemple ci-dessus:\n\nprint(re.match(\"toc{4}\",\"toctoctoctoc\"))\n\nNone\n\n\nPour pallier ce problème, il existe les parenthèses.\nLe principe est le même qu’avec les règles numériques:\nles parenthèses permettent d’introduire une hiérarchie.\nPour reprendre l’exemple précédent, on obtient\nbien le résultat attendu grâce aux parenthèses:\n\nprint(re.match(\"(toc){4}\",\"toctoctoctoc\"))\nprint(re.match(\"(toc){5}\",\"toctoctoctoc\"))\nprint(re.match(\"(toc){2,4}\",\"toctoctoctoc\"))\n\n&lt;re.Match object; span=(0, 12), match='toctoctoctoc'&gt;\nNone\n&lt;re.Match object; span=(0, 12), match='toctoctoctoc'&gt;\n\n\n\n\n Note\nL’algorithme des expressions régulières essaye toujours de faire correspondre le plus grand morceau à l’expression régulière.\nPar exemple, soit une chaine de caractère HTML:\n\ns = \"&lt;h1&gt;Super titre HTML&lt;/h1&gt;\"\n\nL’expression régulière re.findall(\"&lt;.*&gt;\", s) correspond, potentiellement,\nà trois morceaux :\n\n&lt;h1&gt;\n&lt;/h1&gt;\n&lt;h1&gt;Super titre HTML&lt;/h1&gt;\n\nC’est ce dernier qui sera choisi, car le plus grand. Pour\nsélectionner le plus petit,\nil faudra écrire les multiplicateurs comme ceci : *?, +?.\nEn voici quelques exemples:\n\ns = \"&lt;h1&gt;Super titre HTML&lt;/h1&gt;\\n&lt;p&gt;&lt;code&gt;Python&lt;/code&gt; est un langage très flexible&lt;/p&gt;\"\nprint(re.findall(\"&lt;.*&gt;\", s))\nprint(re.findall(\"&lt;p&gt;.*&lt;/p&gt;\", s))\nprint(re.findall(\"&lt;p&gt;.*?&lt;/p&gt;\", s))\nprint(re.compile(\"&lt;.*?&gt;\").findall(s))\n\n['&lt;h1&gt;Super titre HTML&lt;/h1&gt;', '&lt;p&gt;&lt;code&gt;Python&lt;/code&gt; est un langage très flexible&lt;/p&gt;']\n['&lt;p&gt;&lt;code&gt;Python&lt;/code&gt; est un langage très flexible&lt;/p&gt;']\n['&lt;p&gt;&lt;code&gt;Python&lt;/code&gt; est un langage très flexible&lt;/p&gt;']\n['&lt;h1&gt;', '&lt;/h1&gt;', '&lt;p&gt;', '&lt;code&gt;', '&lt;/code&gt;', '&lt;/p&gt;']\n\n\n\n\n\n\nAide-mémoire\nLe tableau ci-dessous peut servir d’aide-mémoire\nsur les regex:\n\n\n\n\n\n\n\nExpression régulière\nSignification\n\n\n\n\n\"^\"\nDébut de la chaîne de caractères\n\n\n\"$\"\nFin de la chaîne de caractères\n\n\n\"\\\\.\"\nUn point\n\n\n\".\"\nN’importe quel caractère\n\n\n\".+\"\nN’importe quelle suite de caractères non vide\n\n\n\".*\"\nN’importe quelle suite de caractères, éventuellement vi\n\n\n\"[:alnum:]\"\nUn caractère alphanumérique\n\n\n\"[:alpha:]\"\nUne lettre\n\n\n\"[:digit:]\"\nUn chiffre\n\n\n\"[:lower:]\"\nUne lettre minuscule\n\n\n\"[:punct:]\"\nUn signe de ponctuation\n\n\n\"[:space:]\"\nun espace\n\n\n\"[:upper:]\"\nUne lettre majuscule\n\n\n\"[[:alnum:]]+\"\nUne suite d’au moins un caractère alphanumérique\n\n\n\"[[:alpha:]]+\"\nUne suite d’au moins une lettre\n\n\n\"[[:digit:]]+\"\nUne suite d’au moins un chiffre\n\n\n\"[[:lower:]]+\"\nUne suite d’au moins une lettre minuscule\n\n\n\"[[:punct:]]+\"\nUne suite d’au moins un signe de ponctuation\n\n\n\"[[:space:]]+\"\nUne suite d’au moins un espace\n\n\n\"[[:upper:]]+\"\nUne suite d’au moins une lettre majuscule\n\n\n\"[[:alnum:]]*\"\nUne suite de caractères alphanumériques, éventuellement vide\n\n\n\"[[:alpha:]]*\"\nUne suite de lettres, éventuellement vide\n\n\n\"[[:digit:]]*\"\nUne suite de chiffres, éventuellement vide\n\n\n\"[[:lower:]]*\"\nUne suite de lettres minuscules, éventuellement vide\n\n\n\"[[:upper:]]*\"\nUne suite de lettres majuscules, éventuellement vide\n\n\n\"[[:punct:]]*\"\nUne suite de signes de ponctuation, éventuellement vide\n\n\n\"[^[:alpha:]]+\"\nUne suite d’au moins un caractère autre qu’une lettre\n\n\n\"[^[:digit:]]+\"\nUne suite d’au moins un caractère autre qu’un chiffre\n\n\n\"\\|\"\nL’une des expressions x ou y est présente\n\n\n[abyz]\nUn seul des caractères spécifiés\n\n\n[abyz]+\nUn ou plusieurs des caractères spécifiés (éventuellement répétés)\n\n\n[^abyz]\nAucun des caractères spécifiés n’est présent\n\n\n\nCertaines classes de caractères bénéficient d’une syntaxe plus légère car\nelles sont très fréquentes. Parmi-celles:\n\n\n\n\n\n\n\nExpression régulière\nSignification\n\n\n\n\n\\d\nN’importe quel chiffre\n\n\n\\D\nN’importe quel caractère qui n’est pas un caractère\n\n\n\\s\nN’importe quel espace (espace, tabulation, retour à la ligne)\n\n\n\\S\nN’importe quel caractère qui n’est pas un espace\n\n\n\\w\nN’importe quel type de mot (lettres et nombres)\n\n\n\\W\nN’importe quel ensemble qui n’est pas un mot (lettres et nombres)\n\n\n\nDans l’exercice suivant, vous allez pouvoir mettre en pratique\nles exemples précédents sur une regex un peu plus complète.\nCet exercice ne nécessite pas la connaissance des subtilités\ndu package re, vous n’aurez besoin que de re.findall.\nCet exercice utilisera la chaine de caractère suivante :\n\ns = \"\"\"date 0 : 14/9/2000\ndate 1 : 20/04/1971     date 2 : 14/09/1913     date 3 : 2/3/1978\ndate 4 : 1/7/1986     date 5 : 7/3/47     date 6 : 15/10/1914\ndate 7 : 08/03/1941     date 8 : 8/1/1980     date 9 : 30/6/1976\"\"\"\ns\n\n'date 0 : 14/9/2000\\ndate 1 : 20/04/1971     date 2 : 14/09/1913     date 3 : 2/3/1978\\ndate 4 : 1/7/1986     date 5 : 7/3/47     date 6 : 15/10/1914\\ndate 7 : 08/03/1941     date 8 : 8/1/1980     date 9 : 30/6/1976'\n\n\n\n\n Exercice 1\n\nOn va d’abord s’occuper d’extraire le jour de naissance.\n\nLe premier chiffre du jour est 0, 1, 2 ou 3. Traduire cela sous la forme d’une séquence [X-X]\nLe deuxième chiffre du jour est lui entre 0 et 9. Traduire cela sous la séquence adéquate\nRemarquez que le premier jour est facultatif. Intercaler entre les deux classes de caractère adéquate\nle quantifieur qui convient\nAjouter le slash à la suite du motif\nTester avec re.findall. Vous devriez obtenir beaucoup plus d’échos que nécessaire.\nC’est normal, à ce stade la\nregex n’est pas encore finalisée\n\nSuivre la même logique pour les mois en notant que les mois du calendrier grégorien ne dépassent\njamais la première dizaine. Tester avec re.findall\nDe même pour les années de naissance en notant que jusqu’à preuve du contraire, pour des personnes vivantes\naujourd’hui, les millénaires concernés sont restreints. Tester avec re.findall\nCette regex n’est pas naturelle, on pourrait très bien se satisfaire de classes de\ncaractères génériques \\d même si elles pourraient, en pratique, nous sélectionner des\ndates de naissance non possibles (43/78/4528 par exemple). Cela permettrait\nd’alléger la regex afin de la rendre plus intelligible. Ne pas oublier l’utilité des quantifieurs.\nComment adapter la regex pour qu’elle soit toujours valide pour nos cas mais permette aussi de\ncapturer les dates de type YYYY/MM/DD ? Tester sur 1998/07/12\n\n\n\nA l’issue de la question 1, vous devriez avoir ce résultat :\n\n\n['14/',\n '9/',\n '20/',\n '04/',\n '14/',\n '09/',\n '2/',\n '3/',\n '1/',\n '7/',\n '7/',\n '3/',\n '15/',\n '10/',\n '08/',\n '03/',\n '8/',\n '1/',\n '30/',\n '6/']\n\n\nA l’issue de la question 2, vous devriez avoir ce résultat, qui\ncommence à prendre forme:\n\n\n['14/9',\n '20/04',\n '14/09',\n '2/3',\n '1/7',\n '7/3',\n '15/10',\n '08/03',\n '8/1',\n '30/6']\n\n\nA l’issue de la question 3, on parvient bien\nà extraire les dates :\n\n\n['14/9/2000',\n '20/04/1971',\n '14/09/1913',\n '2/3/1978',\n '1/7/1986',\n '7/3/47',\n '15/10/1914',\n '08/03/1941',\n '8/1/1980',\n '30/6/1976']\n\n\nSi tout va bien, à la question 5, votre regex devrait\nfonctionner:\n\n\n['14/9/2000',\n '20/04/1971',\n '14/09/1913',\n '2/3/1978',\n '1/7/1986',\n '7/3/47',\n '15/10/1914',\n '08/03/1941',\n '8/1/1980',\n '30/6/1976',\n '1998/07/12']"
  },
  {
    "objectID": "content/manipulation/04b_regex_TP.html#principales-fonctions-de-re",
    "href": "content/manipulation/04b_regex_TP.html#principales-fonctions-de-re",
    "title": "Maîtriser les expressions régulières",
    "section": "Principales fonctions de re",
    "text": "Principales fonctions de re\nVoici un tableau récapitulatif des principales\nfonctions du package re suivi d’exemples.\nNous avons principalement\nutilisé jusqu’à présent re.findall qui est\nl’une des fonctions les plus pratiques du package.\nre.sub et re.search sont également bien pratiques.\nLes autres sont moins vitales mais peuvent dans des\ncas précis être utiles.\n\n\n\n\n\n\n\nFonction\nObjectif\n\n\n\n\nre.match(&lt;regex&gt;, s)\nTrouver et renvoyer le premier match de l’expression régulière &lt;regex&gt; à partir du début du string s\n\n\nre.search(&lt;regex&gt;, s)\nTrouver et renvoyer le premier match de l’expression régulière &lt;regex&gt; quelle que soit sa position dans le string s\n\n\nre.finditer(&lt;regex&gt;, s)\nTrouver et renvoyer un itérateur stockant tous les matches de l’expression régulière &lt;regex&gt; quelle que soit leur(s) position(s) dans le string s. En général, on effectue ensuite une boucle sur cet itérateur\n\n\nre.findall(&lt;regex&gt;, s)\nTrouver et renvoyer tous les matches de l’expression régulière &lt;regex&gt; quelle que soit leur(s) position(s) dans le string s sous forme de liste\n\n\nre.sub(&lt;regex&gt;, new_text, s)\nTrouver et remplacer tous les matches de l’expression régulière &lt;regex&gt; quelle que soit leur(s) position(s) dans le string s\n\n\n\nPour illustrer ces fonctions, voici quelques exemples:\n\nExemple de re.match 👇\nre.match ne peut servir qu’à capturer un pattern en début\nde string. Son utilité est donc limitée.\nCapturons néanmoins toto :\n\nre.match(\"(to){2}\", \"toto à la plage\")\n\n&lt;re.Match object; span=(0, 4), match='toto'&gt;\n\n\n\n\n\nExemple de re.search 👇\nre.search est plus puissant que re.match, on peut\ncapturer des termes quelle que soit leur position\ndans un string. Par exemple, pour capturer age :\n\nre.search(\"age\", \"toto a l'age d'aller à la plage\")\n\n&lt;re.Match object; span=(9, 12), match='age'&gt;\n\n\nEt pour capturer exclusivement “age” en fin\nde string :\n\nre.search(\"age$\", \"toto a l'age d'aller à la plage\")\n\n&lt;re.Match object; span=(28, 31), match='age'&gt;\n\n\n\n\n\nExemple de re.finditer 👇\nre.finditer est, à mon avis,\nmoins pratique que re.findall. Son utilité\nprincipale par rapport à re.findall\nest de capturer la position dans un champ textuel:\n\ns = \"toto a l'age d'aller à la plage\"\nfor match in re.finditer(\"age\", s):\n    start = match.start()\n    end = match.end()\n    print(f'String match \"{s[start:end]}\" at {start}:{end}')\n\nString match \"age\" at 9:12\nString match \"age\" at 28:31\n\n\n\n\n\nExemple de re.sub 👇\nre.sub permet de capturer et remplacer des expressions.\nPar exemple, remplaçons “age” par “âge”. Mais attention,\nil ne faut pas le faire lorsque le motif est présent dans “plage”.\nOn va donc mettre une condition négative: capturer “age” seulement\ns’il n’est pas en fin de string (ce qui se traduit en regex par ?!$)\n\nre.sub(\"age(?!$)\", \"âge\", \"toto a l'age d'aller à la plage\")\n\n\"toto a l'âge d'aller à la plage\"\n\n\n\n\n\n\n Quand utiliser re.compile et les raw strings ?\nre.compile peut être intéressant lorsque\nvous utilisez une expression régulière plusieurs fois dans votre code.\nCela permet de compiler l’expression régulière en un objet reconnu par re,\nce qui peut être plus efficace en termes de performance lorsque l’expression régulière\nest utilisée à plusieurs reprises ou sur des données volumineuses.\nLes chaînes brutes (raw string) sont des chaînes de caractères spéciales en Python,\nqui commencent par r. Par exemple r\"toto à la plage\".\nElles peuvent être intéressantes\npour éviter que les caractères d’échappement ne soient interprétés par Python\nPar exemple, si vous voulez chercher une chaîne qui contient une barre oblique inverse \\ dans une chaîne, vous devez utiliser une chaîne brute pour éviter que la barre oblique inverse ne soit interprétée comme un caractère d’échappement (\\t, \\n, etc.).\nLe testeur https://regex101.com/ suppose d’ailleurs que\nvous utilisez des raw string, cela peut donc être utile de s’habituer à les utiliser."
  },
  {
    "objectID": "content/manipulation/04b_regex_TP.html#généralisation-avec-pandas",
    "href": "content/manipulation/04b_regex_TP.html#généralisation-avec-pandas",
    "title": "Maîtriser les expressions régulières",
    "section": "Généralisation avec Pandas",
    "text": "Généralisation avec Pandas\nLes méthodes de Pandas sont des extensions de celles de re\nqui évitent de faire une boucle pour regarder,\nligne à ligne, une regex. En pratique, lorsqu’on traite des\nDataFrames, on utilise plutôt l’API Pandas que re. Les\ncodes de la forme df.apply(lambda x: re.&lt;fonction&gt;(&lt;regex&gt;,x), axis = 1)\nsont à bannir car très peu efficaces.\nLes noms changent parfois légèrement par rapport à leur\néquivalent re.\n\n\n\n\n\n\n\nMéthode\nDescription\n\n\n\n\nstr.count()\nCompter le nombre d’occurrences du pattern dans chaque ligne\n\n\nstr.replace()\nRemplacer le pattern par une autre valeur. Version vectorisée de re.sub()\n\n\nstr.contains()\nTester si le pattern apparaît, ligne à ligne. Version vectorisée de re.search()\n\n\nstr.extract()\nExtraire les groupes qui répondent à un pattern et les renvoyer dans une colonne\n\n\nstr.findall()\nTrouver et renvoyer toutes les occurrences d’un pattern. Si une ligne comporte plusieurs échos, une liste est renvoyée. Version vectorisée de re.findall()\n\n\n\nA ces fonctions, s’ajoutent les méthodes str.split() et str.rsplit() qui sont bien pratiques.\n\nExemple de str.count 👇\nOn peut compter le nombre de fois qu’un pattern apparaît avec\nstr.count\n\ndf = pd.DataFrame({\"a\": [\"toto\", \"titi\"]})\ndf['a'].str.count(\"to\")\n\n0    2\n1    0\nName: a, dtype: int64\n\n\n\n\n\nExemple de str.replace 👇\nRemplaçons le motif “ti” en fin de phrase\n\ndf = pd.DataFrame({\"a\": [\"toto\", \"titi\"]})\ndf['a'].str.replace(\"ti$\", \" punch\")\n\n0    toto\n1    titi\nName: a, dtype: object\n\n\n\n\n\nExemple de str.contains 👇\nVérifions les cas où notre ligne termine par “ti” :\n\ndf = pd.DataFrame({\"a\": [\"toto\", \"titi\"]})\ndf['a'].str.contains(\"ti$\")\n\n0    False\n1     True\nName: a, dtype: bool\n\n\n\n\n\nExemple de str.findall 👇\n\ndf = pd.DataFrame({\"a\": [\"toto\", \"titi\"]})\ndf['a'].str.findall(\"to\")\n\n0    [to, to]\n1          []\nName: a, dtype: object\n\n\n\n\n\n\n Warning\nA l’heure actuelle, il n’est pas nécessaire d’ajouter l’argument regex = True mais cela\ndevrait être le cas dans une future version de Pandas.\nCela peut valoir le coup de s’habituer à l’ajouter."
  },
  {
    "objectID": "content/manipulation/04b_regex_TP.html#pour-en-savoir-plus",
    "href": "content/manipulation/04b_regex_TP.html#pour-en-savoir-plus",
    "title": "Maîtriser les expressions régulières",
    "section": "Pour en savoir plus",
    "text": "Pour en savoir plus\n\ndocumentation collaborative sur R nommée utilitR\nR for Data Science\nRegular Expression HOWTO dans la documentation officielle de Python\nL’outil de référence [https://regex101.com/] pour tester des expressions régulières\nCe site qui comporte une cheat sheet en bas de la page.\nLes jeux de Regex Crossword permettent d’apprendre les expressions régulières en s’amusant"
  },
  {
    "objectID": "content/manipulation/04b_regex_TP.html#exercices-supplémentaires",
    "href": "content/manipulation/04b_regex_TP.html#exercices-supplémentaires",
    "title": "Maîtriser les expressions régulières",
    "section": "Exercices supplémentaires",
    "text": "Exercices supplémentaires\n\nExtraction d’adresses email\nIl s’agit d’un usage classique des regex\n\ntext_emails = 'Hello from toto@gmail.com to titi.grominet@yahoo.com about the meeting @2PM'\n\n\n\n Exercice : extraction d'adresses email\nUtiliser la structure d’une adresse mail [XXXX]@[XXXX] pour récupérer\nce contenu\n\n\n\n\n['toto@gmail.com', 'titi.grominet@yahoo.com']\n\n\n\n\nExtraire des années depuis un DataFrame Pandas\nL’objectif général de l’exercice est de nettoyer des colonnes d’un DataFrame en utilisant des expressions régulières.\n\n\n Exercice\nLa base en question contient des livres de la British Library et quelques informations les concernant. Le jeu de données est disponible ici : https://raw.githubusercontent.com/realpython/python-data-cleaning/master/Datasets/BL-Flickr-Images-Book.csv\nLa colonne “Date de Publication” n’est pas toujours une année, il y a parfois d’autres informations. Le but de l’exercice est d’avoir une date de publication du livre propre et de regarder la distribution des années de publications.\nPour ce faire, vous pouvez :\n\nSoit choisir de réaliser l’exercice sans aide. Votre lecture de l’énoncé s’arrête donc ici. Vous devez alors faire attention à bien regarder vous-même la base de données et la transformer avec attention.\nSoit suivre les différentes étapes qui suivent pas à pas.\n\nVersion guidée 👇\n\nLire les données depuis l’url https://raw.githubusercontent.com/realpython/python-data-cleaning/master/Datasets/BL-Flickr-Images-Book.csv. Attention au séparateur\nNe garder que les colonnes ['Identifier', 'Place of Publication', 'Date of Publication', 'Publisher', 'Title', 'Author']\nObserver la colonne ‘Date of Publication’ et remarquer le problème sur certaines lignes (par exemple la ligne 13)\nCommencez par regarder le nombre d’informations manquantes. On ne pourra pas avoir mieux après la regex, et normalement on ne devrait pas avoir moins…\nDéterminer la forme de la regex pour une date de publication. A priori, il y a 4 chiffres qui forment une année.\nUtiliser la méthode str.extract() avec l’argument expand = False (pour ne conserver que la première date concordant avec notre pattern)?\nOn a 2 NaN qui n’étaient pas présents au début de l’exercice. Quels sont-ils et pourquoi ?\nQuelle est la répartition des dates de publications dans le jeu de données ? Vous pouvez par exemple afficher un histogramme grâce à la méthode plot avec l’argument kind =\"hist\".\n\n\n\n\nVoici par exemple le problème qu’on demande de détecter à la question 3 :\n\n\n\n\n\n\n\n\n\nDate of Publication\nTitle\n\n\n\n\n13\n1839, 38-54\nDe Aardbol. Magazijn van hedendaagsche land- e...\n\n\n14\n1897\nCronache Savonesi dal 1500 al 1570 ... Accresc...\n\n\n15\n1865\nSee-Saw; a novel ... Edited [or rather, writte...\n\n\n16\n1860-63\nGéodésie d'une partie de la Haute Éthiopie,...\n\n\n17\n1873\n[With eleven maps.]\n\n\n18\n1866\n[Historia geográfica, civil y politica de la ...\n\n\n19\n1899\nThe Crisis of the Revolution, being the story ...\n\n\n\n\n\n\n\n\n\n181\n\n\nGrâce à notre regex (question 5), on obtient ainsi un DataFrame plus conforme à nos attentes\n\n\n\n\n\n\n\n\n\nDate of Publication\nyear\n\n\n\n\n0\n1879 [1878]\n1879\n\n\n7\nNaN\nNaN\n\n\n13\n1839, 38-54\n1839\n\n\n16\n1860-63\n1860\n\n\n23\n1847, 48 [1846-48]\n1847\n\n\n...\n...\n...\n\n\n8278\n1883, [1884]\n1883\n\n\n8279\n1898-1912\n1898\n\n\n8283\n1831, 32\n1831\n\n\n8284\n[1806]-22\n1806\n\n\n8286\n1834-43\n1834\n\n\n\n\n1759 rows × 2 columns\n\n\n\nQuant aux nouveaux NaN,\nil s’agit de lignes qui ne contenaient pas de chaînes de caractères qui ressemblaient à des années:\n\n\n\n\n\n\n\n\n\nDate of Publication\nyear\n\n\n\n\n1081\n112. G. & W. B. Whittaker\nNaN\n\n\n7391\n17 vols. University Press\nNaN\n\n\n\n\n\n\n\nEnfin, on obtient l’histogramme suivant des dates de publications:\n\n\n&lt;Axes: ylabel='Frequency'&gt;"
  },
  {
    "objectID": "content/manipulation/04b_regex_TP.html#footnotes",
    "href": "content/manipulation/04b_regex_TP.html#footnotes",
    "title": "Maîtriser les expressions régulières",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nN’importe quel caractère à part le retour à la ligne (\\n). Ceci est à garder en tête, j’ai déjà perdu des heures à chercher pourquoi mon . ne capturait pas ce que je voulais qui s’étalait sur plusieurs lignes…↩︎"
  },
  {
    "objectID": "content/visualisation/index.html",
    "href": "content/visualisation/index.html",
    "title": "Partie 2: visualiser les données",
    "section": "",
    "text": "La visualisation de données est l’art et la science de représenter visuellement des informations complexes et abstraites à l’aide d’éléments visuels.\nSon objectif principal est de synthétiser l’information présente dans un ensemble de données afin de faciliter\nla compréhension des enjeux de celle-ci pour une analyse ultérieure.\nLa visualisation de données permet, entre autres, de mettre en évidence des tendances, des corrélations ou\ndes anomalies qui pourraient être difficiles voire impossibles à saisir simplement en examinant des données brutes, ces dernières nécessitant\nune certaine mise en contexte pour porter du sens.\nLa visualisation de données joue un rôle crucial dans le\nprocessus d’analyse de données en fournissant des moyens visuels pour explorer, interpréter et communiquer des informations.\nElle facilite la communication entre experts de la données, décideurs et grand public,\nen permettant de raconter des histoires basées sur les données de manière plus convaincante et engageante.\nLa visualisation de données a une place à part dans\nl’ensemble des techniques de la data science.\nElle intervient à tous les stades du processus de\nproduction de la donnée, de\nl’amont (analyse exploratoire) à\nl’aval (restitution à des publics multiples) et\npeut, si elle est bien construite, permettre de\nsaisir de manière intuitive la structure des données\nou les enjeux de son analyse.\nArt de la synthèse, la visualisation de données\nest également l’art de raconter une histoire et\npeut même, lorsqu’elle est bien construite, prétendre\nau rang de production artistique.\nLa dataviz est un métier en soi dont on trouve de\nplus en plus de praticiens dans les titres de presse\nou dans des entreprises\nspécialisées (Datawrapper par exemple).\nSans prétendre construire\ndes visualisations aussi riches que celles des spécialistes,\ntout data scientist se doit d’être en mesure de pouvoir\nproduire rapidement quelques visualisations permettant\nde synthétiser les jeux de données à sa disposition.\nUne visualisation claire et lisible tout en restant simple\npeut être meilleure qu’un discours pour faire passer un message.\nJe recommande notamment\nce post de blog\nd’Eric Mauvière qui revient sur deux graphiques dans une publication\nrécente\ndu Service statistique du Ministère de la Santé (DREES)\net montre la manière dont on peut améliorer le message transmis\npar des figures :\nDe même qu’un discours, une visualisation est une communication\npour laquelle un locuteur - la personne construisant la visualisation -\ncherche à transmettre une information à un récepteur - éventuellement\nla même personne que le locuteur puisqu’une visualisation peut\nêtre construite pour soi-même dans une analyse exploratoire. Il n’est\ndonc pas surprenant qu’à l’époque où la sémiologie occupait une\npart importante dans les débats intellectuels, notamment autour\nde la figure de Roland Barthes, le concept de sémiologie\ngraphique ait émergé\nautour de la personne de Jacques Bertin (Bertin 1967; Palsky 2017).\nCette approche permet de réfléchir sur la pertinence des\ntechniques mises en oeuvre pour transmettre un message\ngraphique et de nombreuses visualisations, si elles\nsuivaient quelques-unes de ces règles, pourraient\nêtre améliorées à peu de frais.\nL’Insee a publié, il y a quelques années, un guide de\nsémiologie graphique très utile qu’il\nest intéressant de consulter de temps en temps (Insee 2018).\nPour revenir à notre cours,\nnous présenterons dans cette partie quelques librairies\net visualisations basiques en Python permettant de\npartir sur de bonnes bases. Les ressources pour\napprofondir et progresser dans l’art de la visualisation\nne manquent pas, comme cet ouvrage (Wilke 2019)."
  },
  {
    "objectID": "content/visualisation/index.html#lécosystème-python",
    "href": "content/visualisation/index.html#lécosystème-python",
    "title": "Partie 2: visualiser les données",
    "section": "L’écosystème Python",
    "text": "L’écosystème Python\nL’écosystème Python pour la valorisation de données est très riche et\ntrès éclaté.\nIl est\npossible de consacrer des livres entiers à celui-ci (Dale 2022).\nPython propose\nde nombreuses librairies pour produire de manière rapide et relativement\nsimple des visualisations de données1.\nLes librairies graphiques se distinguent principalement en deux familles:\n\nLes librairies de représentations figées. Celles-ci ont plutôt vocation à être intégrées\ndans des publications figées type PDF ou documents texte. Nous présenterons\nprincipalement Matplotlib et Seaborn mais il en existe d’autres,\ncomme Plotnine.\nLes librairies de représentations dynamiques. Celles-ci sont adaptées à des représentations\nweb et offrent la possibilité aux lecteurs d’agir sur la représentation graphique affichée.\nLes librairies qui proposent ces fonctionnalités reposent généralement sur JavaScript, l’écosystème\ndu développement web, pour lequel elles offrent un point d’entrée via Python.\nNous évoquerons principalement Plotly et Folium dans cette famille mais il existe de nombreux\nautres frameworks dans ce domaine2.\n\nDans le domaine de la visualisation, ce cours adopte le parti pris\nd’explorer quelques\nlibrairies centrales à partir d’un nombre restreint d’exemples en\nrépliquant des graphiques qu’on peut trouver sur le site d’open data de la\nmairie de Paris.\nLa meilleure école pour la visualisation est la pratique sur des jeux de données.\n\nLes applications de visualisation\nCette partie du cours se focalise sur des représentations synthétiques simples.\nElle n’évoque pas (encore ?) la construction d’applications de visualisation\nde données où un ensemble de graphiques se mettent à jour de manière synchrone\nen fonction d’actions d’utilisateurs.\nCeci dépasse en effet le cadre d’un cours d’introduction car cela implique\nde maîtriser des concepts plus complexes comme l’interaction entre une page\nweb et un serveur (local). Néanmoins, j’ai déjà construit\navec Romain Avouac\nun tutoriel 101 très détaillé sur Streamlit\n(permettant de créer une application type Yuka)\npour une formation à l’Insee."
  },
  {
    "objectID": "content/visualisation/index.html#résumé-de-cette-partie",
    "href": "content/visualisation/index.html#résumé-de-cette-partie",
    "title": "Partie 2: visualiser les données",
    "section": "Résumé de cette partie",
    "text": "Résumé de cette partie\nCette partie est divisée en deux et chaque chapitre est lui-même\ndual, selon qu’on s’intéresse aux représentations figées\nou dynamiques :\n\nDans un premier temps, nous évoquerons des\nreprésentations graphiques standards (histogrammes, diagrammes\nen barre…) pour synthétiser certaines informations quantitatives ;\n\nLes représentations fixes reposeront sur Pandas, Matplotlib et Seaborn\nLes graphiques réactifs s’appuieront sur Plotly\n\nDans un deuxième temps, nous présenterons les représentations\ncartographiques:\n\nLes cartes fixes avec Geopandas ou Geoplot\nLes cartes réactives avec Folium (adaptation Python de la librairie Leaflet.js)"
  },
  {
    "objectID": "content/visualisation/index.html#références-utiles",
    "href": "content/visualisation/index.html#références-utiles",
    "title": "Partie 2: visualiser les données",
    "section": "Références utiles",
    "text": "Références utiles\nLa visualisation de données est un art qui s’apprend, au début, principalement\npar la pratique. Néanmoins, il n’est pas évident de produire\ndes visualisations lisibles et ergonomiques\net il est utile de s’inspirer d’exemples de\nspécialistes (les grands titres de presse disposent d’excellentes visualisations).\nVoici quelques ressources utiles sur ces sujets :\n\nDatawrapper propose un excellent blog sur les\nbonnes pratiques de visualisation, notamment\navec les articles de Lisa Charlotte Muth. Je recommande notamment cet article sur\nles couleurs ou\ncelui-ci sur les textes ;\nLe blog d’Eric Mauvière ;\n“La Sémiologie graphique de Jacques Bertin a cinquante ans” ;\nLes visualisations trending sur Observable ;\nLe New York Times (les rois de la dataviz) revient tous les ans sur les meilleures visualisations\nde l’année dans la veine du data scrollytelling. Voir par exemple la rétrospective de l’année 2022.\n\nEt quelques références supplémentaires, citées dans cette introduction :\n\n\nBertin, Jacques. 1967. Sémiologie Graphique. Paris: Mouton/Gauthier-Villars.\n\n\nDale, Kyran. 2022. Data Visualization with Python and JavaScript. \" O’Reilly Media, Inc.\".\n\n\nInsee. 2018. “Guide de Sémiologie Cartographique.”\n\n\nPalsky, Gilles. 2017. “La sémiologie Graphique de Jacques Bertin a Cinquante Ans.” Visions Carto (En Ligne).\n\n\nWilke, Claus O. 2019. Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures. O’Reilly Media."
  },
  {
    "objectID": "content/visualisation/index.html#footnotes",
    "href": "content/visualisation/index.html#footnotes",
    "title": "Partie 2: visualiser les données",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPour être honnête, Python est sur ce point un peu moins agréable\nque R qui bénéficie de\nl’incontournable librairie ggplot2.\nN’étant pas\nconstruite sur la grammaire des graphiques,\nla principe librairie de graphiques en Python qu’est Matplotlib est plus fastidieuse\nà utiliser que ggplot2.\nseaborn, que nous présenterons,\nfacilite un peu le travail de représentation graphique mais, là encore, il est difficile de faire\nplus malléable et universel que ggplot2.\nLa librairie plotnine vise à proposer une implémentation similaire\nà ggplot pour les utilisateurs de Python. Son développement est à suivre.↩︎\nA cet égard, je recommande vivement de suivre l’actualité de la dataviz\nsur la plateforme Observable qui tend à\nrapprocher les communautés des spécialistes de la dataviz et des analystes\nde données. La librairie Plot pourrait devenir\nun nouveau standard dans les prochaines années, sorte d’intermédiaire\nentre ggplot et d3.↩︎"
  },
  {
    "objectID": "content/visualisation/maps.html",
    "href": "content/visualisation/maps.html",
    "title": "De belles cartes avec python : mise en pratique",
    "section": "",
    "text": "La pratique de la cartographie se fera, dans ce cours, en répliquant des cartes qu’on peut trouver sur\nla page de l’open-data de la ville de Paris\nici.\nNote\nProduire de belles cartes demande du temps mais aussi du bon sens. En fonction de la structure des données, certaines représentations sont à éviter voire à exclure. L’excellent guide disponible ici propose quelques règles et évoque les erreurs à éviter lorsqu’on désire effectuer des\nreprésentations spatiales.\nCelui-ci reprend un guide de sémiologie cartographique\nproduit par l’Insee qui propose de nombreux conseils pratiques pour produire des représentations\ncartographiques sensées.\nCe TP vise à initier :\nLes données utilisées sont :\nAttention\nCertaines librairies géographiques dépendent de rtree qui est parfois difficile à installer car\nce package dépend de librairies compilées qui sont compliquées à installer sur Windows.\nPour installer rtree sur Windows, le mieux est d’utiliser Anaconda.\nAvant de pouvoir commencer, il est nécessaire d’installer quelques\npackages au préalable:\n# Sur colab\n!pip install pandas fiona shapely pyproj rtree # à faire obligatoirement en premier pour utiliser rtree ou pygeos pour les jointures spatiales\n!pip install contextily\n!pip install geopandas\n!pip install geoplot\nDans la première partie, nous allons utiliser les packages suivants :\nimport pandas as pd\nimport geopandas as gpd\nimport contextily as ctx\nimport geoplot\nimport matplotlib.pyplot as plt\nimport folium\n\n/opt/mamba/lib/python3.9/site-packages/geopandas/_compat.py:123: UserWarning:\n\nThe Shapely GEOS version (3.12.0-CAPI-1.18.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n\n/tmp/ipykernel_3997/3556787358.py:2: UserWarning:\n\nShapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n\nimport os\nos.environ['USE_PYGEOS'] = '0'\nimport geopandas\n\nIn a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html)."
  },
  {
    "objectID": "content/visualisation/maps.html#première-carte-avec-lapi-matplotlib-de-geopandas",
    "href": "content/visualisation/maps.html#première-carte-avec-lapi-matplotlib-de-geopandas",
    "title": "De belles cartes avec python : mise en pratique",
    "section": "Première carte avec l’API matplotlib de geopandas",
    "text": "Première carte avec l’API matplotlib de geopandas\n\n\n Exercice 1: Importer les données\nImporter les données de compteurs de vélos en deux temps.\n\nD’abord, les comptages peuvent être trouvés à l’adresse https://minio.lab.sspcloud.fr/projet-formation/diffusion/python-datascientist/bike.csv. ⚠️ Il s’agit de données\ncompressées au format gzip, il faut donc utiliser l’option compression. Nommer cet objet comptages.\nImporter les données de localisation des compteurs à partir de l’url https://parisdata.opendatasoft.com/api/explore/v2.1/catalog/datasets/comptage-velo-compteurs/exports/geojson?lang=fr&timezone=Europe%2FBerlin. Nommer cet objet compteurs.\nFaire attention à deux valeurs aberrantes. Utiliser\nla fonctionalité str.contains pour exclure les\nobservations contenant “Bike IN” ou “Bike OUT”\ndans la variable\nnom_compteur\nOn va également utiliser les données d’arrondissements de la ville de Paris. Importer ces données depuis https://opendata.paris.fr/explore/dataset/arrondissements/download/?format=geojson&timezone=Europe/Berlin&lang=fr. Nommer cet objet arrondissements.\nUtiliser la méthode plot pour représenter les localisations des compteurs dans l’espace. C’est, on peut l’avouer, peu informatif sans apport extérieur. Il va donc falloir travailler un peu l’esthétique\n\n\n\n\ncompteurs = compteurs.loc[~compteurs[\"nom_compteur\"].str.contains(r\"(Bike IN|Bike OUT)\")]\n\n/tmp/ipykernel_3997/3602001210.py:1: UserWarning:\n\nThis pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n\n\n\n\n\n Warning\nOn serait tenté de faire un merge de la base compteurs et comptages.\nEn l’occurrence, il s’agirait d’un produit cartésien puisqu’il s’agit de faire exploser la base spatiale.\nAvec des données spatiales, c’est souvent une très mauvaise idée. Cela duplique les points, créant des difficultés à représenter les données mais aussi ralentit les calculs.\nSauf à utiliser la méthode dissolve (qui va agréger k fois la même géométrie…), les géométries sont perdues lorsqu’on effectue des groupby.\n\n\nMaintenant, tout est prêt pour une première carte. matplotlib fonctionne selon\nle principe des couches. On va de la couche la plus lointaine à celle le plus\nen surface. L’exception est lorsqu’on ajoute un fond de carte contextily via\nctx.add_basemap: on met cet appel en dernier.\n\n\n Exercice 2: Première carte\nReprésenter une carte des compteurs avec le fonds de carte des arrondissements\n\nFaire attention à avoir des arrondissements dont l’intérieur est transparent (argument à utiliser: facecolor).\nFaire des bordures d’arrondissements noires et affichez les compteurs en rouge.\nPour obtenir un graphique plus grand, vous pouvez utiliser l’argument figsize = (10,10).\nPour les localisations, les points doivent être rouges en étant plus transparent au centre (argument à utiliser: alpha)\n\n\n\nVous devriez obtenir cette carte:\n\n\n\n\n\n\n\n\n\n\n\n Exercice 3 : Ajouter un fonds de carte avec contextily\nRepartir de la carte précédente.\n\nUtiliser ctx.add_basemap pour ajouter un fonds de carte. Pour ne pas afficher les axes, vous pouvez utiliser ax.set_axis_off().\n\n:warning: Par défaut, contextily désire un système de projection (crs) qui est le Web Mercator (epsg: 3857). Il faut changer la valeur de l’argument crs.\n:warning: Avec les versions anciennes des packages, il faut utiliser .to_string sur un objet CRS pour qu’il soit reconnu par contextily. Sur des versions récentes, la valeur numérique du code EPSG est suffisante.\n\nTrouver un fonds de carte plus esthétique, qui permette de visualiser les grands axes, parmi ceux possibles. Pour tester l’esthétique, vous pouvez utiliser cet url. La documentation de référence sur les tuiles disponibles est ici\n\n\n\n\nax.get_figure()\n\n\n\n\n\n\n\n\n\nax.get_figure()\n\n\n\n\n\n\n\n\n\nax.get_figure().savefig(\"featured_maps.png\")\n\nLe principe de la heatmap est de construire, à partir d’un nuage de point bidimensionnel, une distribution 2D lissée. La méthode repose sur les estimateurs à noyaux qui sont des méthodes de lissage local.\n\n\n Exercice 3 : Ajouter un fonds de carte avec contextily\nPour le moment, la fonction geoplot.kdeplot n’incorpore pas toutes les fonctionalités de seaborn.kdeplot. Pour être en mesure de construire une heatmap avec des données pondérées (cf. cette issue dans le dépôt seaborn), il y a une astuce. Il faut simuler k points de valeur 1 autour de la localisation observée. La fonction ci-dessous, qui m’a été bien utile, est pratique\n\nimport numpy as np\ndef expand_points(shapefile,\n                  index_var = \"grid_id\",\n                  weight_var = 'prop',\n                  radius_sd = 100,\n                  crs = 2154):\n    \"\"\"\n    Multiply number of points to be able to have a weighted heatmap\n    :param shapefile: Shapefile to consider\n    :param index_var: Variable name to set index\n    :param weight_var: Variable that should be used\n    :param radius_sd: Standard deviation for the radius of the jitter\n    :param crs: Projection system that should be used. Recommended option\n      is Lambert 93 because points will be jitterized using meters\n    :return:\n      A geopandas point object with as many points by index as weight\n    \"\"\"\n\n    shpcopy = shapefile\n    shpcopy = shpcopy.set_index(index_var)\n    shpcopy['npoints'] = np.ceil(shpcopy[weight_var])\n    shpcopy['geometry'] = shpcopy['geometry'].centroid\n    shpcopy['x'] = shpcopy.geometry.x\n    shpcopy['y'] = shpcopy.geometry.y\n    shpcopy = shpcopy.to_crs(crs)\n    shpcopy = shpcopy.loc[np.repeat(shpcopy.index.values, shpcopy.npoints)]\n    shpcopy['x'] = shpcopy['x'] + np.random.normal(0, radius_sd, shpcopy.shape[0])\n    shpcopy['y'] = shpcopy['y'] + np.random.normal(0, radius_sd, shpcopy.shape[0])\n\n    gdf = gpd.GeoDataFrame(\n        shpcopy,\n        geometry = gpd.points_from_xy(shpcopy.x, shpcopy.y),\n        crs = crs)\n\n    return gdf\n\n\n\n\n\n Exercice 4 : Data cleaning avant de pouvoir faire une heatmap\n\nCalculer le trafic moyen, pour chaque station, entre 7 heures et 10 heures (bornes incluses) et nommer cet objet df1. Faire la même chose, en nommant df2, pour le trafic entre 17 et 20 heures (bornes incluses)\nNous allons désormais préparer les données de manière à faire une heatmap. Après avoir compris ce que permet de faire la fonction expand_points ci-dessus, créer une fonction explode_data qui suive les étapes suivantes.\n\n\nConvertir un DataFrame dans le système de projection Lambert 93 (epsg: 2154)\nAppliquer expand_points aux noms de variable adéquats. Vous pouvez fixer la valeur de radius_sd à 100.\nReconvertir l’output au format WGS84 (epsg: 4326)\n\n\nAppliquer cette fonction à df1 et df2\n\n\n\n\n\n Exercice 5 : Heatmap, enfin !\nReprésenter, pour ces deux moments de la journée, la heatmap du trafic de vélo avec geoplot.kdeplot. Pour cela :\n\nAppliquer la fonction geoplot.kdeplot avec comme consignes :\n\nd’utiliser les arguments shade=True et shade_lowest=True pour colorer l’intérieur des courbes de niveaux obtenues ;\nd’utiliser une palette de couleur rouge avec une transparence modérée (alpha = 0.6)\nd’utiliser l’argument clip pour ne pas déborder hors de Paris (en cas de doute, se référer à l’aide de geoplot.kdeplot)\nL’argument bw (pour bandwidth) détermine le plus ou moins fort lissage spatial. Vous pouvez partir d’un bandwidth égal à 0.01 et le faire varier pour voir l’effet sur le résultat\n\nNe pas oublier d’ajouter les arrondissements. Avec geoplot, il faut utiliser geoplot.polyplot.\n\n\n\n\nax.get_figure()"
  },
  {
    "objectID": "content/visualisation/maps.html#des-cartes-réactives-grâce-à-folium",
    "href": "content/visualisation/maps.html#des-cartes-réactives-grâce-à-folium",
    "title": "De belles cartes avec python : mise en pratique",
    "section": "Des cartes réactives grâce à folium",
    "text": "Des cartes réactives grâce à folium\nDe plus en plus de données de visualisation reposent sur la cartographie réactive. Que ce soit dans l’exploration des données ou dans la représentation finale de résultats, la cartographie réactive est très appréciable.\nfolium offre une interface très flexible et très facile à prendre à main. Les cartes sont construites grâce à la librairie JavaScript Leaflet.js mais, sauf si on désire aller loin dans la customisation du résultat, il n’est pas nécessaire d’avoir des notions dans le domaine.\nUn objet folium se construit par couche. La première est l’initialisation de la carte. Les couches suivantes sont les éléments à mettre en valeur. L’initialisation de la carte nécessite la définition d’un point central (paramètre location) et d’un zoom de départ (zoom_start). Plutôt que de fournir manuellement le point central et le zoom on peut :\n\nDéterminer le point central en construisant des colonnes longitudes et latitudes et en prenant la moyenne de celles-ci ;\nUtiliser la méthode fit_bounds qui cale la carte sur les coins sud-ouest et nord-est. En supposant que la carte s’appelle m, on fera m.fit_bounds([sw, ne])\n\nLe bout de code suivant permet de calculer le centre de la carte\n\ncompteurs['lon'] = compteurs.geometry.x\ncompteurs['lat'] = compteurs.geometry.y\ncenter = compteurs[['lat', 'lon']].mean().values.tolist()\nprint(center)\n\n[48.8546374609375, 2.349252640625]\n\n\nAlors que le code suivant permet de calculer les coins:\n\nsw = compteurs[['lat', 'lon']].min().values.tolist()\nne = compteurs[['lat', 'lon']].max().values.tolist()\nprint(sw, ne)\n\n[48.81964, 2.26526] [48.898946, 2.41143]\n\n\n\n\n Hint\nSi un fond gris s’affiche, c’est qu’il y a un problème de localisation ou d’accès à internet. Pour le premier cas, cela provient généralement d’un problème de projection ou d’une inversion des longitudes et latitudes.\nLes longitudes représentent les x (axe ouest-est) et les latitudes y (axe sud-nord). De manière contrintuitive, folium attend qu’on lui fournisse les données sous la forme [latitude, longitude] donc [y,x]\n\n\n\n\n Exercice 6 : Visualiser la localisation des stations\n\nCalculer le centre centerde la carte des données compteurs. Il s’obtient en agrègeant l’ensemble des géométries, calculant le centroid et récupèrant la valeur sous forme de liste. Avec une logique similaire, calculez les bornes du sud-ouest sw et du nord-est ne de la carte.\nReprésenter la localisation des stations en utilisant un zoom optimal.\n\n\n\n\n# Afficher la carte\nm\n\n\n\n Exercice 7: Représenter les stations\nFaire la même carte, avec des ronds proportionnels au nombre de comptages :\n\nPour le rayon de chaque cercle, vous pouvez appliquer la règle 500*x/max(x) (règle au doigt mouillé)\nVous pouvez réduire la taille des bordures de cercle avec l’option weight = 1 et fixer la couleur avec color = 'grey'\n(Optionnel) Colorer en rouge les 10 plus grosses stations. L’opacité étant, par défaut, un peu faible, le paramètre fill_opacity = 0.4 améliore le rendu.\n(Optionnel) Afficher, en supplément du nom du compteur lorsqu’on clique, la valeur du comptage en revenant à la ligne\n\n\n\nLa carte obtenue doit ressembler à la suivante :\n\n# Afficher la carte\nm"
  },
  {
    "objectID": "content/visualisation/maps.html#exercices-supplémentaires",
    "href": "content/visualisation/maps.html#exercices-supplémentaires",
    "title": "De belles cartes avec python : mise en pratique",
    "section": "Exercices supplémentaires",
    "text": "Exercices supplémentaires\n\nDensité de population dans la petite couronne parisienne\nPour cet exercice, le package cartiflette\nva être pratique pour récupérer un fonds de carte mélangeant arrondissements\nparisiens et communes dans les autres villes.\nNous allons privilégier une carte à ronds proportionnels (bubble map)\naux cartes chorolèpthes qui trompent\nl’oeil. Les instructions d’installation du package topojson\nsont disponibles dans la partie manipulation\n\n\n Exercice: bubble map de densité des populations\n\nRécupérer le fond de carte des départements 75, 92, 93 et 94\navec cartiflette. Pour cela, utiliser download_vectorfile_url_all\ndepuis cartiflette.s3 en fixant l’option level à COMMUNE_ARRONDISSEMENT.\nNommer cet objet df.\nAfin que les calculs ultérieurs de surface ne soient pas faussés,\nassurez-vous que les données sont en Lambert 93 en reprojetant\nnos contours (code EPSG: 2154).\nCréer un objet departements avec dissolve pour également disposer\nd’un fond de carte des départements\nCréer une variable surface et utilisant la méthode area. L’unité\ndoit être le km², il faut donc diviser par \\(10^6\\)\nCréer une variable densite\nUtiliser pd.cut avec les seuils 5000, 15000 et 30000 personnes\npar km². Vous pouvez utiliser l’option label pour dénommer les tranches\nCréer un GeoDataFrame de points en utilisant la méthode centroid. Celui-ci\nnous servira à localiser le centre de nos ronds.\nReprésenter la densité communale sous forme de carte avec ronds proportionnels.\nVous pouvez utiliser la variable créée à la question 5 pour les couleurs.\n\n\n\nLa carte obtenue devrait ressembler à celle-ci:\n\n\n/opt/mamba/lib/python3.9/site-packages/geopandas/plotting.py:730: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\nText(0.3, 0.15, 'Source: IGN - AdminExpress')"
  },
  {
    "objectID": "content/visualisation/maps.html#références-supplémentaires",
    "href": "content/visualisation/maps.html#références-supplémentaires",
    "title": "De belles cartes avec python : mise en pratique",
    "section": "Références supplémentaires",
    "text": "Références supplémentaires\n\nGeocomputation with Python"
  },
  {
    "objectID": "content/modelisation/0_preprocessing.html",
    "href": "content/modelisation/0_preprocessing.html",
    "title": "Préparation des données pour construire un modèle",
    "section": "",
    "text": "Ce chapitre utilise le jeu de données présenté dans l’introduction\nde cette partie :\nles données de vote aux élections présidentielles américaines de 2020 au niveau des comtés\ncroisées à des variables socio-démographiques.\nLe code de consitution de la base de données\nest disponible sur Github.\nL’exercice 1 permet, à ceux qui le désirent, d’essayer de le reconstituer pas à pas.\nLe guide utilisateur de Scikit est une référence précieuse,\nà consulter régulièrement. La partie sur le preprocessing est\ndisponible ici.\nL’objectif de ce chapitre est de présenter quelques éléments de\npréparation des données. Il s’agit d’une étape fondamentale, à ne\npas négliger. Les modèles reposent sur certaines hypothèses, généralement\nrelatives à la distribution théorique des variables qui y sont intégrées.\nIl est nécessaire de faire correspondre la distribution empirique\nà ces hypothèses ce qui implique un travail de restructuration des données.\nCelui-ci permettra d’avoir des résultats de modélisation plus pertinents.\nNous verrons dans le chapitre sur les pipelines comment industrialiser\nces étapes de preprocessing afin de se simplifier la vie pour appliquer\nun modèle sur un jeu de données différent de celui sur lequel il a été estimé.\nScikit-Learn \nscikit-learn est aujourd’hui la librairie de référence dans l’écosystème du\nMachine Learning. Il s’agit d’une librairie qui, malgré les très nombreuses\nméthodes implémentées, présente l’avantage d’être un point d’entrée unifié.\nCet aspect unifié est l’une des raisons du succès précoce de celle-ci. R n’a\nbénéficié que plus récemment d’une librairie unifiée,\nà savoir tidymodels.\nUne autre raison du succès de scikit est son approche opérationnelle: la mise\nen production de modèles développés via les pipelines scikit est peu coûteuse.\nUn chapitre spécial de ce cours est dédié aux pipelines.\nAvec Romain Avouac, nous proposons un cours plus avancé\nen dernière année d’ENSAE où nous présentons certains enjeux relatifs\nà la mise en production de modèles développés avec scikit.\nLe coeur de l’équipe de développement de scikit-learn est situé\nà l’Inria 🇫🇷.\nPour découvrir la richesse de l’écosystème scikit, il\nest recommandé de suivre le\nMOOC scikit,\ndéveloppé dans le cadre de l’initiative Inria Academy.\nLes packages suivants sont nécessaires pour importer et visualiser\nles données d’élection:\n!pip install --upgrade xlrd #colab bug verson xlrd\n!pip install geopandas\nDans ce chapitre, nous allons nous focaliser sur la préparation\ndes données à faire en amont du travail de modélisation.\nCette étape est indispensable pour s’assurer de la cohérence\nentre les données et les hypothèses de modélisation mais aussi\npour produire des analyses valides scientifiquement.\nLa démarche générale que nous adopterons dans ce chapitre, et\nqui sera ensuite raffinée dans les prochains chapitres,\nest la suivante:\nC’est l’approche classique du machine learning. On découpe\nl’ensemble des données disponibles en deux parties, échantillons\nd’apprentissage et de validation. Le premier sert à entraîner\nun modèle et la qualité des prédictions de celui-ci est\névaluée sur le deuxième pour limiter\nle biais de surapprentissage. Le chapitre suivant approfondira\ncette question de l’évaluation des modèles. A ce stade de notre\nprogression, on se concentrera dans ce chapitre\nsur la question des données. La librairie Scikit est non seulement\nparticulièrement\npratique parce qu’elle propose énormément d’algorithmes de machine learning\nmais aussi parce qu’elle facilite la préparation des données en amont,\nce qui est l’objet de ce chapitre.\nNéanmoins, avant de se concentrer sur la préparation des données, nous\nallons passer un peu de temps à explorer la structure des données\nà partir de laquelle nous désirons construire une modélisation. Ceci\nest indispensable afin de comprendre la nature de celles-ci et choisir\nune modélisation adéquate."
  },
  {
    "objectID": "content/modelisation/0_preprocessing.html#construction-de-la-base-de-données",
    "href": "content/modelisation/0_preprocessing.html#construction-de-la-base-de-données",
    "title": "Préparation des données pour construire un modèle",
    "section": "Construction de la base de données",
    "text": "Construction de la base de données\nLes sources de données étant diverses, le code qui construit la base finale est directement fourni.\nLe travail de construction d’une base unique\nest un peu fastidieux mais il s’agit d’un bon exercice, que vous pouvez tenter,\npour réviser Pandas :\n\n\n Exercice 1 : Importer les données des élections US\nCet exercice est OPTIONNEL\n\nTélécharger et importer le shapefile depuis ce lien\nExclure les Etats suivants : “02”, “69”, “66”, “78”, “60”, “72”, “15”\nImporter les résultats des élections depuis ce lien\nImporter les bases disponibles sur le site de l’USDA en faisant attention à renommer les variables de code FIPS de manière identique\ndans les 4 bases\nMerger ces 4 bases dans une base unique de caractéristiques socioéconomiques\nMerger aux données électorales à partir du code FIPS\nMerger au shapefile à partir du code FIPS. Faire attention aux 0 à gauche dans certains codes. Il est\nrecommandé d’utiliser la méthode str.lstrip pour les retirer\nImporter les données des élections 2000 à 2016 à partir du MIT Election Lab?\nLes données peuvent être directement requêtées depuis l’url\nhttps://dataverse.harvard.edu/api/access/datafile/3641280?gbrecs=false\nCréer une variable share comptabilisant la part des votes pour chaque candidat.\nNe garder que les colonnes \"year\", \"FIPS\", \"party\", \"candidatevotes\", \"share\"\nFaire une conversion long to wide avec la méthode pivot_table pour garder une ligne\npar comté x année avec en colonnes les résultats de chaque candidat dans cet état.\nMerger à partir du code FIPS au reste de la base.\n\n\n\nSi vous ne faites pas l’exercice 1, pensez à charger les données en executant la fonction get_data.py :\n\nimport requests\n\nurl = 'https://raw.githubusercontent.com/linogaliana/python-datascientist/master/content/modelisation/get_data.py'\nr = requests.get(url, allow_redirects=True)\nopen('getdata.py', 'wb').write(r.content)\n\nimport getdata\nvotes = getdata.create_votes_dataframes()\n\nCe code introduit une base nommée votes dans l’environnement. Il s’agit d’une\nbase rassemblant les différentes sources. Elle a l’aspect\nsuivant:\n\nvotes.head(3)\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nCOUNTYNS\nAFFGEOID\nGEOID\nNAME\nLSAD\nALAND\nAWATER\ngeometry\n...\nshare_2008_democrat\nshare_2008_other\nshare_2008_republican\nshare_2012_democrat\nshare_2012_other\nshare_2012_republican\nshare_2016_democrat\nshare_2016_other\nshare_2016_republican\nwinner\n\n\n\n\n0\n29\n227\n00758566\n0500000US29227\n29227\nWorth\n06\n690564983\n493903\nPOLYGON ((-94.63203 40.57176, -94.53388 40.570...\n...\n0.363714\n0.034072\n0.602215\n0.325382\n0.041031\n0.633588\n0.186424\n0.041109\n0.772467\nrepublican\n\n\n1\n31\n061\n00835852\n0500000US31061\n31061\nFranklin\n06\n1491355860\n487899\nPOLYGON ((-99.17940 40.35068, -98.72683 40.350...\n...\n0.284794\n0.019974\n0.695232\n0.250000\n0.026042\n0.723958\n0.149432\n0.045427\n0.805140\nrepublican\n\n\n2\n36\n013\n00974105\n0500000US36013\n36013\nChautauqua\n06\n2746047476\n1139407865\nPOLYGON ((-79.76195 42.26986, -79.62748 42.324...\n...\n0.495627\n0.018104\n0.486269\n0.425017\n0.115852\n0.459131\n0.352012\n0.065439\n0.582550\nrepublican\n\n\n\n\n3 rows × 383 columns\n\n\n\nLa carte choroplèthe suivante permet de visualiser rapidement les résultats\n(l’Alaska et Hawaï ont été exclus).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# republican : red, democrat : blue\ncolor_dict = {'republican': '#FF0000', 'democrats': '#0000FF'}\n\nfig, ax = plt.subplots(figsize = (12,12))\ngrouped = votes.groupby('winner')\nfor key, group in grouped:\n    group.plot(ax=ax, column='winner', label=key, color=color_dict[key])\nplt.axis('off')\n\n(-127.6146362, -64.0610978, 23.253819649999997, 50.628669349999996)\n\n\n\n\n\n\n\n\n\nLes cartes choroplèthes peuvent donner une impression fallacieuse\nce qui explique que\nce type de carte a servi\nde justification pour contester les résultats du vote.\nEn effet, un biais\nconnu des représentations choroplèthes est qu’elles donnent une importance\nvisuelle excessive aux grands espaces. Or, ceux-ci sont souvent des espaces\npeu denses et influencent donc moins la variable d’intérêt (en l’occurrence\nle taux de vote en faveur des républicains/démocrates). Une représentation à\nprivilégier pour ce type de phénomènes est les\nronds proportionnels (voir Insee (2018), “Le piège territorial en cartographie”).\nLe GIF “Land does not vote, people do”\nqui avait eu un certain succès en 2020 propose un autre mode de visualisation.\nLa carte originale a été construite avec JavaScript. Cependant,\non dispose avec Python de plusieurs outils\npour répliquer, à faible coût, cette carte\ngrâce à\nl’une des surcouches à JavaScript vues dans la partie visualisation.\nEn l’occurrence, on peut utiliser Plotly pour tenir compte de la population\net faire une carte en ronds proportionnels.\nLe code suivant permet de construire une carte adaptée:\n\nimport plotly\nimport plotly.graph_objects as go\nimport pandas as pd\nimport geopandas as gpd\n\n\ncentroids = votes.copy()\ncentroids.geometry = centroids.centroid\ncentroids['size'] = centroids['CENSUS_2010_POP'] / 10000  # to get reasonable plotable number\n\ncolor_dict = {\"republican\": '#FF0000', 'democrats': '#0000FF'}\ncentroids[\"winner\"] =  np.where(centroids['votes_gop'] &gt; centroids['votes_dem'], 'republican', 'democrats') \n\n\ncentroids['lon'] = centroids['geometry'].x\ncentroids['lat'] = centroids['geometry'].y\ncentroids = pd.DataFrame(centroids[[\"county_name\",'lon','lat','winner', 'CENSUS_2010_POP',\"state_name\"]])\ngroups = centroids.groupby('winner')\n\ndf = centroids.copy()\n\ndf['color'] = df['winner'].replace(color_dict)\ndf['size'] = df['CENSUS_2010_POP']/6000\ndf['text'] = df['CENSUS_2010_POP'].astype(int).apply(lambda x: '&lt;br&gt;Population: {:,} people'.format(x))\ndf['hover'] = df['county_name'].astype(str) +  df['state_name'].apply(lambda x: ' ({}) '.format(x)) + df['text']\n\nfig_plotly = go.Figure(\n  data=go.Scattergeo(\n  locationmode = 'USA-states',\n  lon=df[\"lon\"], lat=df[\"lat\"],\n  text = df[\"hover\"],\n  mode = 'markers',\n  marker_color = df[\"color\"],\n  marker_size = df['size'],\n  hoverinfo=\"text\"\n  )\n)\n\nfig_plotly.update_traces(\n  marker = {'opacity': 0.5, 'line_color': 'rgb(40,40,40)', 'line_width': 0.5, 'sizemode': 'area'}\n)\n\nfig_plotly.update_layout(\n  title_text = \"Reproduction of the \\\"Acres don't vote, people do\\\" map &lt;br&gt;(Click legend to toggle traces)\",\n  showlegend = True,\n  geo = {\"scope\": 'usa', \"landcolor\": 'rgb(217, 217, 217)'}\n)\n\n\nfig_plotly.show()\n\n\n                                                \n\n\nLes cercles proportionnels permettent ainsi à l’oeil de se concentrer sur les\nzones les plus denses et non sur les grands espaces. Cette fois, on voit bien\nque le vote démocrate est majoritaire, ce que cachait l’aplat de couleur."
  },
  {
    "objectID": "content/modelisation/0_preprocessing.html#explorer-la-structure-des-données",
    "href": "content/modelisation/0_preprocessing.html#explorer-la-structure-des-données",
    "title": "Préparation des données pour construire un modèle",
    "section": "Explorer la structure des données",
    "text": "Explorer la structure des données\nLa première étape nécessaire à suivre avant de se lancer dans la modélisation\nest de déterminer les variables à inclure dans le modèle.\nLes fonctionnalités de Pandas sont, à ce niveau, suffisantes pour explorer des structures simples.\nNéanmoins, lorsqu’on est face à un jeu de données présentant de\nnombreuses variables explicatives (features en machine learning, covariates en économétrie),\nil est souvent judicieux d’avoir une première étape de sélection de variables,\nce que nous verrons par la suite dans la partie dédiée.\nAvant d’être en mesure de sélectionner le meilleur ensemble de variables explicatives,\nnous allons en prendre un nombre restreint et arbitraire.\nLa première tâche est de représenter les relations entre les données,\nnotamment la relation des variables explicatives\nà la variable dépendante (le score du parti républicain)\nainsi que les relations entre les variables explicatives.\n\n\n Exercice 2 : Regarder les corrélations entre les variables\n\nCréer un DataFrame df2 plus petit avec les variables winner, votes_gop, Unemployment_rate_2019,\nMedian_Household_Income_2019,\nPercent of adults with less than a high school diploma, 2015-19,\nPercent of adults with a bachelor's degree or higher, 2015-19\nReprésenter grâce à un graphique la matrice de corrélation. Vous pouvez utiliser le package seaborn et sa fonction seaborn.\nReprésenter une matrice de nuages de points des variables de la base df2 avec pd.plotting.scatter_matrix\n(optionnel) Refaire ces figures avec Plotly qui offre également la possibilité de faire une matrice de corrélation.\n\n\n\nLa matrice construite avec seaborn (question 2) aura l’aspect suivant :\n\n\n&lt;Axes: &gt;\n\n\nAlors que celle construite directement avec corr de Pandas\nressemblera plutôt à ce tableau :\n\n\n\n\n\n\n\n \nvotes_gop\nUnemployment_rate_2019\nMedian_Household_Income_2019\nPercent of adults with less than a high school diploma, 2015-19\nPercent of adults with a bachelor's degree or higher, 2015-19\n\n\n\n\nvotes_gop\n1.00\n-0.08\n0.35\n-0.11\n0.37\n\n\nUnemployment_rate_2019\n-0.08\n1.00\n-0.43\n0.36\n-0.36\n\n\nMedian_Household_Income_2019\n0.35\n-0.43\n1.00\n-0.51\n0.71\n\n\nPercent of adults with less than a high school diploma, 2015-19\n-0.11\n0.36\n-0.51\n1.00\n-0.59\n\n\nPercent of adults with a bachelor's degree or higher, 2015-19\n0.37\n-0.36\n0.71\n-0.59\n1.00\n\n\n\n\n\nLe nuage de point obtenu à l’issue de la question 3 ressemblera à :\n\n\n\n\n\n\n\n\n\n\n\narray([[&lt;Axes: xlabel='votes_gop', ylabel='votes_gop'&gt;,\n        &lt;Axes: xlabel='Unemployment_rate_2019', ylabel='votes_gop'&gt;,\n        &lt;Axes: xlabel='Median_Household_Income_2019', ylabel='votes_gop'&gt;,\n        &lt;Axes: xlabel='Percent of adults with less than a high school diploma, 2015-19', ylabel='votes_gop'&gt;,\n        &lt;Axes: xlabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\", ylabel='votes_gop'&gt;],\n       [&lt;Axes: xlabel='votes_gop', ylabel='Unemployment_rate_2019'&gt;,\n        &lt;Axes: xlabel='Unemployment_rate_2019', ylabel='Unemployment_rate_2019'&gt;,\n        &lt;Axes: xlabel='Median_Household_Income_2019', ylabel='Unemployment_rate_2019'&gt;,\n        &lt;Axes: xlabel='Percent of adults with less than a high school diploma, 2015-19', ylabel='Unemployment_rate_2019'&gt;,\n        &lt;Axes: xlabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\", ylabel='Unemployment_rate_2019'&gt;],\n       [&lt;Axes: xlabel='votes_gop', ylabel='Median_Household_Income_2019'&gt;,\n        &lt;Axes: xlabel='Unemployment_rate_2019', ylabel='Median_Household_Income_2019'&gt;,\n        &lt;Axes: xlabel='Median_Household_Income_2019', ylabel='Median_Household_Income_2019'&gt;,\n        &lt;Axes: xlabel='Percent of adults with less than a high school diploma, 2015-19', ylabel='Median_Household_Income_2019'&gt;,\n        &lt;Axes: xlabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\", ylabel='Median_Household_Income_2019'&gt;],\n       [&lt;Axes: xlabel='votes_gop', ylabel='Percent of adults with less than a high school diploma, 2015-19'&gt;,\n        &lt;Axes: xlabel='Unemployment_rate_2019', ylabel='Percent of adults with less than a high school diploma, 2015-19'&gt;,\n        &lt;Axes: xlabel='Median_Household_Income_2019', ylabel='Percent of adults with less than a high school diploma, 2015-19'&gt;,\n        &lt;Axes: xlabel='Percent of adults with less than a high school diploma, 2015-19', ylabel='Percent of adults with less than a high school diploma, 2015-19'&gt;,\n        &lt;Axes: xlabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\", ylabel='Percent of adults with less than a high school diploma, 2015-19'&gt;],\n       [&lt;Axes: xlabel='votes_gop', ylabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\"&gt;,\n        &lt;Axes: xlabel='Unemployment_rate_2019', ylabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\"&gt;,\n        &lt;Axes: xlabel='Median_Household_Income_2019', ylabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\"&gt;,\n        &lt;Axes: xlabel='Percent of adults with less than a high school diploma, 2015-19', ylabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\"&gt;,\n        &lt;Axes: xlabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\", ylabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\"&gt;]],\n      dtype=object)\n\n\nLe résultat de la question 4 devrait, quant à lui,\nressembler au graphique suivant :"
  },
  {
    "objectID": "content/modelisation/0_preprocessing.html#transformer-les-données",
    "href": "content/modelisation/0_preprocessing.html#transformer-les-données",
    "title": "Préparation des données pour construire un modèle",
    "section": "Transformer les données",
    "text": "Transformer les données\nLes différences d’échelle ou de distribution entre les variables peuvent\ndiverger des hypothèses sous-jacentes dans les modèles.\nPar exemple, dans le cadre\nde la régression linéaire, les variables catégorielles ne sont pas traitées à la même\nenseigne que les variables ayant valeur dans \\(\\mathbb{R}\\). Une variable\ndiscrète (prenant un nombre fini de valeurs) devra être transformées en suite de\nvariables 0/1 par rapport à une modalité de référence pour être en adéquation\navec les hypothèses de la régression linéaire.\nOn appelle ce type de transformation\none-hot encoding, sur lequel nous reviendrons. Il s’agit d’une transformation,\nparmi d’autres, disponibles dans scikit pour mettre en adéquation un jeu de\ndonnées et des hypothèses mathématiques.\nL’ensemble de ces tâches s’appelle le preprocessing. L’un des intérêts\nd’utiliser Scikit est qu’on peut considérer qu’une tâche de preprocessing\nest, en fait, une tâche d’apprentissage. En effet, le preprocessing\nconsiste à apprendre des paramètres d’une structure\nde données (par exemple estimer moyennes et variances pour les retrancher à chaque\nobservation) et on peut très bien appliquer ces paramètres\nà des observations qui n’ont pas servi à construire\nceux-ci. Ainsi, en gardant en tête l’approche générale avec Scikit\n\n\n\n\n\nnous allons voir deux processus très classiques de preprocessing :\n\nLa standardisation transforme des données pour que la distribution empirique suive une loi \\(\\mathcal{N}(0,1)\\).\nLa normalisation transforme les données de manière à obtenir une norme (\\(\\mathcal{l}_1\\) ou \\(\\mathcal{l}_2\\)) unitaire. Autrement dit, avec la norme adéquate, la somme des éléments est égale à 1.\n\n\n\n Warning\nPour un statisticien,\nle terme normalization dans le vocable Scikit peut avoir un sens contre-intuitif.\nOn s’attendrait à ce que la normalisation consiste à transformer une variable de manière à ce que \\(X \\sim \\mathcal{N}(0,1)\\).\nC’est, en fait, la standardisation en Scikit qui fait cela.\n\n\n\nStandardisation\nLa standardisation consiste à transformer des données pour que la distribution empirique suive une loi \\(\\mathcal{N}(0,1)\\). Pour être performants, la plupart des modèles de machine learning nécessitent souvent d’avoir des données dans cette distribution.\n\n\n Exercice 3: Standardisation\n\nStandardiser la variable Median_Household_Income_2019 (ne pas écraser les valeurs !) et regarder l’histogramme avant/après normalisation.\n\nNote : On obtient bien une distribution centrée à zéro et on pourrait vérifier que la variance empirique soit bien égale à 1. On pourrait aussi vérifier que ceci est vrai également quand on transforme plusieurs colonnes à la fois.\n\nCréer scaler, un Transformer que vous construisez sur les 1000 premières lignes de votre DataFrame df2 à l’exception de la variable à expliquer winner. Vérifier la moyenne et l’écart-type de chaque colonne sur ces mêmes observations.\n\nNote : Les paramètres qui seront utilisés pour une standardisation ultérieure sont stockés dans les attributs .mean_ et .scale_\nOn peut voir ces attributs comme des paramètres entraînés sur un certain jeu de\ndonnées et qu’on peut réutiliser sur un autre, à condition que les\ndimensions coïncident.\n\nAppliquer scaler sur les autres lignes du DataFrame et comparer les distributions obtenues de la variable Median_Household_Income_2019.\n\nNote : Une fois appliqués à un autre DataFrame, on peut remarquer que la distribution n’est pas exactement centrée-réduite dans le DataFrame sur lequel les paramètres n’ont pas été estimés. C’est normal, l’échantillon initial n’était pas aléatoire, les moyennes et variances de cet échantillon n’ont pas de raison de coïncider avec les moments de l’échantillon complet.\n\n\n\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n\n\n&lt;Axes: xlabel='y_standard', ylabel='Count'&gt;\n\n\n\n\n\n\n\n\n\n\n\nMoyenne de chaque variable sur 1000 premières observations avant :  [ 1.73616500e+04  3.84530000e+00  5.51891150e+04  1.29669150e+01\n  2.15813433e+01 -2.73468885e-02]\nEcart-type de chaque variable sur 1000 premières observations avant :  [3.28113703e+04 1.28903822e+00 1.33256197e+04 6.45536365e+00\n 9.41139584e+00 9.23129044e-01]\nMoyenne de chaque variable sur 1000 premières observations après :  [-3.37507799e-17  2.66453526e-17  1.58095759e-16  1.42108547e-17\n  1.24344979e-17 -1.59872116e-17]\nEcart-type de chaque variable sur 1000 premières observations après :  [1. 1. 1. 1. 1. 1.]\n\n\n\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n\n\n&lt;Axes: ylabel='Count'&gt;\n\n\n\n\n\n\n\n\n\n\n\nNormalisation\nLa normalisation est l’action de transformer les données de manière\nà obtenir une norme (\\(\\mathcal{l}_1\\) ou \\(\\mathcal{l}_2\\)) unitaire.\nAutrement dit, avec la norme adéquate, la somme des éléments est égale à 1.\nPar défaut, la norme est dans \\(\\mathcal{l}_2\\).\nCette transformation est particulièrement utilisée en classification de texte ou pour effectuer du clustering.\n\n\n Exercice 4 : Normalisation\n\nNormaliser la variable Median_Household_Income_2019 (ne pas écraser les valeurs !) et regarder l’histogramme avant/après normalisation.\nVérifier que la norme \\(\\mathcal{l}_2\\) est bien égale à 1.\n\n\n\n\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n\n\n&lt;Axes: ylabel='Count'&gt;\n\n\n\n\n\n\n\n\n\n\n\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n\n\n\n\nEncodage des valeurs catégorielles\nLes données catégorielles doivent être recodées\nsous forme de valeurs numériques pour être intégrés aux modèles de machine learning.\nCela peut être fait de plusieurs manières :\n\nLabelEncoder: transforme un vecteur [\"a\",\"b\",\"c\"] en vecteur numérique [0,1,2].\nCette approche a l’inconvénient d’introduire un ordre dans les modalités, ce qui n’est pas toujours souhaitable\nOrdinalEncoder: une version généralisée du LabelEncoder qui a vocation à s’appliquer sur des matrices (\\(X\\)),\nalors que LabelEncoder s’applique plutôt à un vecteur (\\(y\\))\npandas.get_dummies effectue une opération de dummy expansion.\nUn vecteur de taille n avec K catégories sera transformé en matrice de taille \\(n \\times K\\)\npour lequel chaque colonne sera une variable dummy pour la modalité k.\nIl y a ici \\(K\\) modalités et il y a donc multicolinéarité.\nAvec une régression linéaire avec constante,\nil convient de retirer une modalité avant l’estimation.\nOneHotEncoder est une version généralisée (et optimisée) de la dummy expansion.\nIl a plutôt vocation à s’appliquer sur les features (\\(X\\)) du modèle\n\n\n\n Exercice 5 : Encoder des variables catégorielles\n\nCréer df qui conserve uniquement les variables state_name et county_name dans votes.\nAppliquer à state_name un LabelEncoder\nNote : Le résultat du label encoding est relativement intuitif, notamment quand on le met en relation avec le vecteur initial.\nRegarder la dummy expansion de state_name\nAppliquer un OrdinalEncoder à df[['state_name', 'county_name']]\nNote : Le résultat du ordinal encoding est cohérent avec celui du label encoding\nAppliquer un OneHotEncoder à df[['state_name', 'county_name']]\n\nNote : scikit optimise l’objet nécessaire pour stocker le résultat d’un modèle de transformation. Par exemple, le résultat de l’encoding One Hot est un objet très volumineux. Dans ce cas, scikit utilise une matrice Sparse.\n\n\n\n\narray([[23, 'Missouri'],\n       [25, 'Nebraska'],\n       [30, 'New York'],\n       ...,\n       [41, 'Texas'],\n       [41, 'Texas'],\n       [41, 'Texas']], dtype=object)\n\n\n\n\n\n\n\n\n\n\n\nAlabama\nArizona\nArkansas\nCalifornia\nColorado\nConnecticut\nDelaware\nDistrict of Columbia\nFlorida\nGeorgia\n...\nSouth Dakota\nTennessee\nTexas\nUtah\nVermont\nVirginia\nWashington\nWest Virginia\nWisconsin\nWyoming\n\n\n\n\n0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3102\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3103\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3104\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3105\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3106\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n3107 rows × 49 columns\n\n\n\n\n\narray([23., 25., 30., ..., 41., 41., 41.])\n\n\n\n\n&lt;3107x1891 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 6214 stored elements in Compressed Sparse Row format&gt;"
  },
  {
    "objectID": "content/modelisation/0_preprocessing.html#références",
    "href": "content/modelisation/0_preprocessing.html#références",
    "title": "Préparation des données pour construire un modèle",
    "section": "Références",
    "text": "Références\n\n\nInsee. 2018. “Guide de Sémiologie Cartographique.”"
  },
  {
    "objectID": "content/modelisation/2_SVM.html",
    "href": "content/modelisation/2_SVM.html",
    "title": "Classification: premier modèle avec les SVM",
    "section": "",
    "text": "Pour pouvoir importer les données,\nnous allons importer les données suivantes:\nNous allons partir du même jeu de données que précédemment,\nc’est-à-dire les résultats des élections US 2020 présentés dans l’introduction\nde cette partie: les données de vote aux élections présidentielles américaines\ncroisées à des variables sociodémographiques.\nLe code\nest disponible sur Github.\nimport requests\n\nurl = 'https://raw.githubusercontent.com/linogaliana/python-datascientist/master/content/modelisation/get_data.py'\nr = requests.get(url, allow_redirects=True)\nopen('getdata.py', 'wb').write(r.content)\n\nimport getdata\nvotes = getdata.create_votes_dataframes()\nPour ce TP, nous aurons besoin des packages suivants :"
  },
  {
    "objectID": "content/modelisation/2_SVM.html#la-méthode-des-svm-support-vector-machines",
    "href": "content/modelisation/2_SVM.html#la-méthode-des-svm-support-vector-machines",
    "title": "Classification: premier modèle avec les SVM",
    "section": "La méthode des SVM (Support Vector Machines)",
    "text": "La méthode des SVM (Support Vector Machines)\nL’une des méthodes de machine learning les\nplus utilisées en classification sont les SVM (Support Vector Machines).\nIl s’agit de trouver, dans un système de projection adéquat (noyau ou kernel),\nles paramètres de l’hyperplan (en fait d’un hyperplan à marges maximales)\nséparant les classes de données :\n\n\n\n\n\n\n\n Formalisation mathématique\nLes SVM sont l’une des méthodes de machine learning les plus intuitives\ndu fait de l’interprétation géométrique simple de la méthode. Il s’agit\naussi d’un des algorithmes de machine learning à la formalisation\nla moins complexe pour les praticiens ayant des notions en statistique\ntraditionnelle. Cette boîte revient dessus. Néanmoins,\ncelle-ci n’est pas nécessaire à la compréhension du chapitre.\nEn machine learning, plus que les détails mathématiques, l’important\nest d’avoir des intuitions.\nL’objectif des SVM est, rappelons-le, de trouver un hyperplan qui permette\nde séparer les différentes classes au mieux. Par exemple, dans un espace\nà deux dimensions, il s’agit de trouver une droite avec des marges\nqui permette de séparer au mieux l’espace en partie avec\ndes labels homogènes.\nOn peut, sans perdre de généralité,\nsupposer que le problème consiste à supposer l’existence d’une loi de probabilité \\(\\mathbb{P}(x,y)\\) (\\(\\mathbb{P} \\to \\{-1,1\\}\\)) qui est inconnue. Le problème de discrimination\nvise à construire un estimateur de la fonction de décision idéale qui minimise la probabilité d’erreur, autrement dit\n\\[\n\\theta = \\arg\\min_\\Theta \\mathbb{P}(h_\\theta(X) \\neq y |x)\n\\]\nLes SVM les plus simples sont les SVM linéaires. Dans ce cas, on suppose qu’il existe un séparateur linéaire qui permet d’associer chaque classe à son signe:\n\\[\nh_\\theta(x) = \\text{signe}(f_\\theta(x)) ; \\text{ avec } f_\\theta(x) = \\theta^T x + b\n\\]\navec \\(\\theta \\in \\mathbb{R}^p\\) et \\(w \\in \\mathbb{R}\\).\n\n\n\n\n\nLorsque des observations sont linéairement séparables,\nil existe une infinité de frontières de décision linéaire séparant les deux classes. Le “meilleur” choix est de prendre la marge maximale permettant de séparer les données. La distance entre les deux marges est \\(\\frac{2}{||\\theta||}\\). Donc maximiser cette distance entre deux hyperplans revient à minimiser \\(||\\theta||^2\\) sous la contrainte \\(y_i(\\theta^Tx_i + b) \\geq 1\\).\nDans le cas non linéairement séparable, la hinge loss \\(\\max\\big(0,y_i(\\theta^Tx_i + b)\\big)\\) permet de linéariser la fonction de perte:\n\n\n\n\n\nce qui donne le programme d’optimisation suivant :\n\\[\n\\frac{1}{n} \\sum_{i=1}^n \\max\\big(0,y_i(\\theta^Tx_i + b)\\big) + \\lambda ||\\theta||^2\n\\]\nLa généralisation au cas non linéaire implique d’introduire des noyaux transformant l’espace de coordonnées des observations."
  },
  {
    "objectID": "content/modelisation/2_SVM.html#application",
    "href": "content/modelisation/2_SVM.html#application",
    "title": "Classification: premier modèle avec les SVM",
    "section": "Application",
    "text": "Application\nPour appliquer un modèle de classification, il nous faut\ntrouver une variable dichotomique. Le choix naturel est\nde prendre la variable dichotomique qu’est la victoire ou\ndéfaite d’un des partis.\nMême si les Républicains ont perdu en 2020, ils l’ont emporté\ndans plus de comtés (moins peuplés). Nous allons considérer\nque la victoire des Républicains est notre label 1 et la défaite 0.\n\n\n Exercice 1 : Premier algorithme de classification\n\nCréer une variable dummy appelée y dont la valeur vaut 1 quand les républicains l’emportent.\nEn utilisant la fonction prête à l’emploi nommée train_test_split de la librairie sklearn.model_selection,\ncréer des échantillons de test (20 % des observations) et d’estimation (80 %) avec comme features: 'Unemployment_rate_2019', 'Median_Household_Income_2019', 'Percent of adults with less than a high school diploma, 2015-19', \"Percent of adults with a bachelor's degree or higher, 2015-19\" et comme label la variable y.\n\nNote: Il se peut que vous ayez le warning suivant:\n\nA column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel()\n\nNote : Pour éviter ce warning à chaque fois que vous estimez votre modèle, vous pouvez utiliser DataFrame[['y']].values.ravel() plutôt que DataFrame[['y']] lorsque vous constituez vos échantillons.\n\nEntraîner un classifieur SVM avec comme paramètre de régularisation C = 1. Regarder les mesures de performance suivante : accuracy, f1, recall et precision.\nVérifier la matrice de confusion : vous devriez voir que malgré des scores en apparence pas si mauvais, il y a un problème notable.\nRefaire les questions précédentes avec des variables normalisées. Le résultat est-il différent ?\nChanger de variables x. Utiliser uniquement le résultat passé du vote démocrate et le revenu (votes_gop et Median_Household_Income_2019). Regarder les résultats, notamment la matrice de confusion.\n[OPTIONNEL] Faire une 5-fold validation croisée pour déterminer le paramètre C idéal.\n\n\n\nA l’issue de la question 3,\nle classifieur avec C = 1\ndevrait avoir les performances suivantes :\n\n\n\n\nScore\n\n\n\n\nAccuracy\n0.882637\n\n\nRecall\n0.897297\n\n\nPrecision\n0.968872\n\n\nF1\n0.931712\n\n\n\nLa matrice de confusion associée\nprend cette forme:\n\n\n\n\n\n\n\n\n\n\nA l’issue de la question 6,\nle nouveau classifieur avec devrait avoir les performances suivantes :\n\n\n\n\nScore\n\n\n\n\nAccuracy\n0.882637\n\n\nRecall\n0.897297\n\n\nPrecision\n0.968872\n\n\nF1\n0.931712\n\n\n\nEt la matrice de confusion associée:"
  },
  {
    "objectID": "content/modelisation/4_featureselection.html",
    "href": "content/modelisation/4_featureselection.html",
    "title": "Sélection de variables : une introduction",
    "section": "",
    "text": "Pour illustrer le travail de données nécessaire pour faire de la sélection de variables,\nnous allons partir du même jeu de données que précédemment,\nc’est-à-dire les résultats des élections US 2020 présentés dans l’introduction\nde cette partie: les données de vote aux élections présidentielles américaines\ncroisées à des variables sociodémographiques.\nLe code\nest disponible sur Github.\n#!pip install --upgrade xlrd #colab bug verson xlrd\n#!pip install geopandas\n\nimport requests\n\nurl = 'https://raw.githubusercontent.com/linogaliana/python-datascientist/master/content/modelisation/get_data.py'\nr = requests.get(url, allow_redirects=True)\nopen('getdata.py', 'wb').write(r.content)\n\nimport getdata\nvotes = getdata.create_votes_dataframes()\nJusqu’à présent, nous avons supposé que les variables utiles à la prévision du\nvote Républicain étaient connues du modélisateur. Nous n’avons ainsi exploité qu’une partie\nlimitée des variables disponibles dans nos données. Néanmoins, outre le fléau\ncomputationnel que représenterait la construction d’un modèle avec un grand\nnombre de variables, le choix d’un nombre restreint de variables\n(modèle parcimonieux) limite le risque de sur-apprentissage.\nComment, dès-lors, choisir le bon nombre de variables et la meilleure\ncombinaison de ces variables ? Il existe de multiples méthodes, parmi lesquelles :"
  },
  {
    "objectID": "content/modelisation/4_featureselection.html#principe-du-lasso",
    "href": "content/modelisation/4_featureselection.html#principe-du-lasso",
    "title": "Sélection de variables : une introduction",
    "section": "Principe du LASSO",
    "text": "Principe du LASSO\n\nPrincipe général\nLa classe des modèles de feature selection est ainsi très vaste et regroupe\nun ensemble très diverse de modèles. Nous allons nous focaliser sur le LASSO\n(Least Absolute Shrinkage and Selection Operator)\nqui est une extension de la régression linéaire qui vise à sélectionner des\nmodèles sparses. Ce type de modèle est central dans le champ du\nCompressed sensing (où on emploie plutôt le terme\nde L1-regularization que de LASSO). Le LASSO est un cas particulier des\nrégressions elastic-net dont un autre cas fameux est la régression ridge.\nContrairement à la régression linéaire classique, elles fonctionnent également\ndans un cadre où \\(p&gt;N\\), c’est à dire où le nombre de régresseurs est très grand puisque supérieur\nau nombre d’observations.\n\n\nPénalisation\nEn adoptant le principe d’une fonction objectif pénalisée,\nle LASSO permet de fixer un certain nombre de coefficients à 0.\nLes variables dont la norme est non nulle passent ainsi le test de sélection.\n\n\n Hint\nLe LASSO est un programme d’optimisation sous contrainte. On cherche à trouver l’estimateur \\(\\beta\\) qui minimise l’erreur quadratique (régression linéaire) sous une contrainte additionnelle régularisant les paramètres:\n\\[\n\\min_{\\beta} \\frac{1}{2}\\mathbb{E}\\bigg( \\big( X\\beta - y  \\big)^2 \\bigg) \\\\\n\\text{s.t. } \\sum_{j=1}^p |\\beta_j| \\leq t\n\\]\nCe programme se reformule grâce au Lagrangien est permet ainsi d’obtenir un programme de minimisation plus maniable :\n\\[\n\\beta^{\\text{LASSO}} = \\arg \\min_{\\beta} \\frac{1}{2}\\mathbb{E}\\bigg( \\big( X\\beta - y  \\big)^2 \\bigg) + \\alpha \\sum_{j=1}^p |\\beta_j| = \\arg \\min_{\\beta} ||y-X\\beta||_{2}^{2} + \\alpha ||\\beta||_1\n\\]\noù \\(\\lambda\\) est une réécriture de la régularisation précédente.\n\n\n\n\nPremière régression LASSO\nAvant de se lancer dans les exercices, on va éliminer quelques colonnes redondantes,\ncelles qui concernent les votes des partis concurrents (forcément très\ncorrélés au vote Républicain…) :\n\ndf2 = votes.loc[:,~votes.columns.str.endswith(\n  ('_democrat','_green','_other', 'per_point_diff', 'per_dem')\n  )]\n\nNous allons utiliser par la suite les fonctions ou\npackages suivants :\n\n# packages utiles\nimport numpy as np\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Lasso\nimport sklearn.metrics\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import lasso_path\nimport seaborn as sns\n\n\n\n Exercice 1 : Premier LASSO\nOn cherche toujours à prédire la variable per_gop.\n\nPréparez les variables à utiliser.\n\n\nNe garder que les colonnes numériques (idéalement on transformerait\nles variables non numériques en numériques)\nRemplacer les valeurs infinies par des NaN et les valeurs manquantes par 0.\nStandardiser les features (c’est-à-dire les variables autres que la variable per_gop) avec StandardScaler\n\n\nOn cherche toujours à prédire la variable per_gop. Créez un échantillon d’entraînement et un échantillon test.\nEstimer un modèle LASSO pénalisé avec \\(alpha = 0.1\\). Afficher les valeurs des coefficients. Quelles variables ont une valeur non nulle ?\nMontrer que les variables sélectionnées sont parfois très corrélées.\nComparer la performance de ce modèle parcimonieux avec celle d’un modèle avec plus de variables\nUtiliser la fonction lasso_path pour évaluer le nombre de paramètres sélectionnés par LASSO lorsque \\(\\alpha\\)\nvarie (parcourir \\(\\alpha \\in [0.001,0.01,0.02,0.025,0.05,0.1,0.25,0.5,0.8,1.0]\\) ).\n\n\n\nA l’issue de la question 3,\nles variables sélectionnées sont :\n\n\n['ALAND',\n 'FIPS_y',\n 'INTERNATIONAL_MIG_2017',\n 'DOMESTIC_MIG_2014',\n 'DOMESTIC_MIG_2017',\n 'RESIDUAL_2010',\n 'RESIDUAL_2019',\n 'R_death_2012',\n 'R_death_2019',\n 'R_NATURAL_INC_2019',\n 'R_INTERNATIONAL_MIG_2011',\n 'R_DOMESTIC_MIG_2012',\n \"Percent of adults with a bachelor's degree or higher, 1990\",\n 'Percent of adults with a high school diploma only, 2000',\n \"Percent of adults with a bachelor's degree or higher, 2000\",\n \"Percent of adults with a bachelor's degree or higher, 2015-19\",\n 'Rural_urban_continuum_code_2013',\n 'Metro_2013',\n 'Unemployment_rate_2002',\n 'Unemployment_rate_2003',\n 'Unemployment_rate_2012',\n 'Rural-urban_Continuum_Code_2003',\n 'Rural-urban_Continuum_Code_2013',\n 'CI90LB517P_2019',\n 'candidatevotes_2016_republican',\n 'share_2012_republican',\n 'share_2016_republican']\n\n\nCertaines variables font sens, comme les variables d’éducation par exemple. Notamment, un des meilleurs prédicteurs pour le score des Républicains en 2020 est… le score des Républicains (et mécaniquement des démocrates) en 2016.\nPar ailleurs, on sélectionne des variables redondantes. Une phase plus approfondie de nettoyage des données serait en réalité nécessaire.\n\n\n\n\n\n\n\n\n\nOn voit que plus \\(\\alpha\\) est élevé, moins le modèle sélectionne de variables."
  },
  {
    "objectID": "content/modelisation/4_featureselection.html#validation-croisée-pour-sélectionner-le-modèle",
    "href": "content/modelisation/4_featureselection.html#validation-croisée-pour-sélectionner-le-modèle",
    "title": "Sélection de variables : une introduction",
    "section": "Validation croisée pour sélectionner le modèle",
    "text": "Validation croisée pour sélectionner le modèle\nQuel \\(\\alpha\\) faut-il privilégier ? Pour cela,\nil convient d’effectuer une validation croisée afin de choisir le modèle pour\nlequel les variables qui passent la phase de sélection permettent de mieux\nprédire le résultat Républicain :\n\nfrom sklearn.linear_model import LassoCV\n\ndf3 = df2.select_dtypes(include=np.number)\ndf3.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf3 = df3.fillna(0)\nscaler = StandardScaler()\nyindex = df3.columns.get_loc(\"per_gop\")\ndf3_scale = scaler.fit(df3).transform(df3)\n# X_train, X_test , y_train, y_test = train_test_split(np.delete(data, yindex, axis = 1),data[:,yindex], test_size=0.2, random_state=0)\n\nlcv = LassoCV(alphas=my_alphas, fit_intercept=False,random_state=0,cv=5).fit(np.delete(df3_scale, yindex, axis = 1), df3_scale[:,yindex])\n\n\nprint(\"alpha optimal :\", lcv.alpha_)\n\nalpha optimal : 0.001\n\n\n\nlasso2 = Lasso(fit_intercept=True, alpha = lcv.alpha_).fit(X_train,y_train)\nfeatures_selec2 = df2.select_dtypes(include=np.number).drop(\"per_gop\", axis = 1).columns[np.abs(lasso2.coef_)&gt;0].tolist()\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning:\n\nObjective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.486e+03, tolerance: 6.352e+01\n\n\n\nLes variables sélectionnées sont :\n\nprint(features_selec2)\n\n['ALAND', 'AWATER', 'votes_gop', 'diff', 'Rural-urban_Continuum Code_2003', 'Rural-urban_Continuum Code_2013', 'Urban_Influence_Code_2013', 'Economic_typology_2015', 'CENSUS_2010_POP', 'N_POP_CHG_2013', 'N_POP_CHG_2016', 'N_POP_CHG_2017', 'N_POP_CHG_2018', 'N_POP_CHG_2019', 'Births_2011', 'Births_2015', 'Deaths_2015', 'Deaths_2017', 'Deaths_2018', 'NATURAL_INC_2012', 'NATURAL_INC_2013', 'NATURAL_INC_2014', 'NATURAL_INC_2016', 'NATURAL_INC_2018', 'INTERNATIONAL_MIG_2010', 'INTERNATIONAL_MIG_2011', 'INTERNATIONAL_MIG_2012', 'INTERNATIONAL_MIG_2013', 'INTERNATIONAL_MIG_2014', 'INTERNATIONAL_MIG_2015', 'INTERNATIONAL_MIG_2016', 'INTERNATIONAL_MIG_2017', 'INTERNATIONAL_MIG_2018', 'INTERNATIONAL_MIG_2019', 'DOMESTIC_MIG_2010', 'DOMESTIC_MIG_2012', 'DOMESTIC_MIG_2013', 'DOMESTIC_MIG_2015', 'DOMESTIC_MIG_2016', 'DOMESTIC_MIG_2018', 'NET_MIG_2011', 'NET_MIG_2014', 'NET_MIG_2018', 'NET_MIG_2019', 'RESIDUAL_2010', 'RESIDUAL_2011', 'RESIDUAL_2012', 'RESIDUAL_2013', 'RESIDUAL_2014', 'RESIDUAL_2015', 'RESIDUAL_2016', 'RESIDUAL_2017', 'RESIDUAL_2018', 'RESIDUAL_2019', 'GQ_ESTIMATES_BASE_2010', 'GQ_ESTIMATES_2013', 'GQ_ESTIMATES_2015', 'GQ_ESTIMATES_2017', 'R_birth_2011', 'R_birth_2013', 'R_birth_2014', 'R_birth_2016', 'R_birth_2017', 'R_birth_2019', 'R_death_2011', 'R_death_2012', 'R_death_2013', 'R_death_2014', 'R_death_2015', 'R_death_2016', 'R_death_2017', 'R_death_2018', 'R_death_2019', 'R_NATURAL_INC_2012', 'R_NATURAL_INC_2015', 'R_NATURAL_INC_2017', 'R_NATURAL_INC_2018', 'R_NATURAL_INC_2019', 'R_INTERNATIONAL_MIG_2011', 'R_INTERNATIONAL_MIG_2012', 'R_INTERNATIONAL_MIG_2013', 'R_INTERNATIONAL_MIG_2014', 'R_INTERNATIONAL_MIG_2015', 'R_INTERNATIONAL_MIG_2016', 'R_INTERNATIONAL_MIG_2017', 'R_INTERNATIONAL_MIG_2018', 'R_INTERNATIONAL_MIG_2019', 'R_DOMESTIC_MIG_2011', 'R_DOMESTIC_MIG_2012', 'R_DOMESTIC_MIG_2013', 'R_DOMESTIC_MIG_2015', 'R_DOMESTIC_MIG_2016', 'R_DOMESTIC_MIG_2018', 'R_NET_MIG_2014', 'R_NET_MIG_2017', 'R_NET_MIG_2019', '2003 Rural-urban Continuum Code', 'Less than a high school diploma, 1970', 'High school diploma only, 1970', 'Some college (1-3 years), 1970', 'Four years of college or higher, 1970', 'Percent of adults with less than a high school diploma, 1970', 'Percent of adults with a high school diploma only, 1970', 'Percent of adults completing some college (1-3 years), 1970', 'Percent of adults completing four years of college or higher, 1970', 'Less than a high school diploma, 1980', 'High school diploma only, 1980', 'Some college (1-3 years), 1980', 'Four years of college or higher, 1980', 'Percent of adults with less than a high school diploma, 1980', 'Percent of adults with a high school diploma only, 1980', 'Percent of adults completing some college (1-3 years), 1980', 'Percent of adults completing four years of college or higher, 1980', 'Less than a high school diploma, 1990', 'Percent of adults with less than a high school diploma, 1990', 'Percent of adults with a high school diploma only, 1990', 'Less than a high school diploma, 2000', 'High school diploma only, 2000', \"Some college or associate's degree, 2000\", \"Bachelor's degree or higher, 2000\", 'Percent of adults with less than a high school diploma, 2000', 'Percent of adults with a high school diploma only, 2000', \"Percent of adults completing some college or associate's degree, 2000\", \"Percent of adults with a bachelor's degree or higher, 2000\", 'Less than a high school diploma, 2015-19', 'High school diploma only, 2015-19', \"Some college or associate's degree, 2015-19\", \"Bachelor's degree or higher, 2015-19\", 'Percent of adults with less than a high school diploma, 2015-19', 'Percent of adults with a high school diploma only, 2015-19', \"Percent of adults completing some college or associate's degree, 2015-19\", \"Percent of adults with a bachelor's degree or higher, 2015-19\", 'Metro_2013', 'Unemployed_2000', 'Unemployment_rate_2000', 'Unemployment_rate_2001', 'Unemployed_2002', 'Unemployment_rate_2002', 'Unemployed_2003', 'Unemployment_rate_2003', 'Civilian_labor_force_2004', 'Employed_2004', 'Unemployment_rate_2004', 'Civilian_labor_force_2005', 'Unemployed_2005', 'Unemployment_rate_2005', 'Civilian_labor_force_2006', 'Unemployed_2006', 'Unemployment_rate_2006', 'Unemployed_2007', 'Unemployment_rate_2007', 'Unemployed_2008', 'Unemployment_rate_2008', 'Employed_2009', 'Unemployment_rate_2009', 'Employed_2010', 'Unemployment_rate_2010', 'Civilian_labor_force_2011', 'Employed_2011', 'Unemployed_2011', 'Civilian_labor_force_2012', 'Employed_2012', 'Unemployed_2012', 'Unemployment_rate_2012', 'Unemployed_2013', 'Unemployment_rate_2013', 'Unemployed_2014', 'Unemployment_rate_2014', 'Civilian_labor_force_2015', 'Employed_2015', 'Unemployment_rate_2015', 'Unemployed_2016', 'Unemployment_rate_2016', 'Unemployed_2017', 'Unemployment_rate_2017', 'Unemployed_2018', 'Unemployment_rate_2018', 'Unemployment_rate_2019', 'Med_HH_Income_Percent_of_State_Total_2019', 'Rural-urban_Continuum_Code_2003', 'Urban_Influence_Code_2003', 'Rural-urban_Continuum_Code_2013', 'POVALL_2019', 'CI90LBALL_2019', 'CI90UBALL_2019', 'CI90LBALLP_2019', 'CI90UBALLP_2019', 'POV017_2019', 'CI90LB017_2019', 'CI90UB017_2019', 'CI90LB017P_2019', 'CI90LB517_2019', 'CI90UB517_2019', 'PCTPOV517_2019', 'CI90LB517P_2019', 'CI90LBINC_2019', 'CI90UBINC_2019', 'candidatevotes_2000_republican', 'candidatevotes_2004_republican', 'candidatevotes_2008_republican', 'candidatevotes_2012_republican', 'candidatevotes_2016_republican', 'share_2000_republican', 'share_2008_republican', 'share_2012_republican', 'share_2016_republican']\n\n\n\ndf2.select_dtypes(include=np.number).drop(\"per_gop\", axis = 1).columns[np.abs(lasso2.coef_)&gt;0]\nnlasso = sum(np.abs(lasso2.coef_)&gt;0)\n\nCela correspond à un modèle avec 206 variables sélectionnées.\n\n\n Hint\nDans le cas où le modèle paraîtrait trop peu parcimonieux, il faudrait revoir la phase de définition des variables pertinentes pour comprendre si des échelles différentes de certaines variables ne seraient pas plus appropriées (par exemple du log)."
  },
  {
    "objectID": "content/modelisation/6_pipeline.html#pourquoi-utiliser-les-pipelines",
    "href": "content/modelisation/6_pipeline.html#pourquoi-utiliser-les-pipelines",
    "title": "Premier pas vers l’industrialisation avec les pipelines scikit",
    "section": "Pourquoi utiliser les pipelines ?",
    "text": "Pourquoi utiliser les pipelines ?\nLes chapitres précédents ont permis de montrer des bouts de code\népars pour entraîner des modèles ou faire du preprocessing.\nCette démarche est intéressante pour tâtonner mais risque d’être coûteuse\nultérieurement s’il est nécessaire d’ajouter une étape de preprocessing\nou de changer d’algorithme.\nHeureusement, scikit propose un excellent outil pour proposer un cadre\ngénéral pour créer une chaîne de production machine learning. Il\ns’agit des\npipelines.\nIls présentent de nombreux intérêts, parmi lesquels:\n\nIls sont très pratiques et lisibles. On rentre des données en entrée, on n’appelle qu’une seule fois les méthodes fit et predict ce qui permet de s’assurer une gestion cohérente des transformations de variables, par exemple après l’appel d’un StandardScaler\nLa modularité rend aisée la mise à jour d’un pipeline et renforce la capacité à le réutiliser\nIls permettent de facilement chercher les hyperparamètres d’un modèle. Sans pipeline, écrire un code qui fait du tuning d’hyperparamètres peut être pénible. Avec les pipelines, c’est une ligne de code.\nLa sécurité d’être certain que les étapes de preprocessing sont bien appliquées aux jeux de données désirés avant l’estimation.\n\n\n\n Hint\nUn des intérêts des pipelines scikit est qu’ils fonctionnent aussi avec\ndes méthodes qui ne sont pas issues de scikit.\nIl est possible d’introduire un modèle de réseau de neurones Keras dans\nun pipeline scikit.\nPour introduire un modèle économétrique statsmodels\nc’est un peu plus coûteux mais nous allons proposer des exemples\nqui peuvent servir de modèle et qui montrent que c’est faisable\nsans trop de difficulté."
  },
  {
    "objectID": "content/modelisation/6_pipeline.html#comment-créer-un-pipeline",
    "href": "content/modelisation/6_pipeline.html#comment-créer-un-pipeline",
    "title": "Premier pas vers l’industrialisation avec les pipelines scikit",
    "section": "Comment créer un pipeline",
    "text": "Comment créer un pipeline\nUn pipeline est un enchaînement d’opérations qu’on code en enchainant\ndes pairs (clé, valeur):\n\nla clé est le nom du pipeline, cela peut être utile lorsqu’on va\nreprésenter le pipeline sous forme de diagramme acyclique (visualisation DAG)\nou qu’on veut afficher des informations sur une étape\nla valeur représente la transformation à mettre en oeuvre dans le pipeline\n(c’est-à-dire, à l’exception de la dernière étape,\nmettre en oeuvre une méthode transform et éventuellement une\ntransformation inverse).\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\n\nestimators = [('reduce_dim', PCA()), ('clf', SVC())]\npipe = Pipeline(estimators)\n\n\n\n Hint\nIl est pratique de visualiser un pipeline sous forme de DAG.\nPour cela, dans un notebook, on utilise la configuration\nsuivante:\n#| eval: false\nfrom sklearn import set_config\nset_config(display='diagram') \n\n\npipe\nAu sein d’une étape de pipeline, les paramètres d’un estimateur\nsont accessibles avec la notation &lt;estimator&gt;__&lt;parameter&gt;.\nCela permet de fixer des valeurs pour les arguments des fonctions scikit\nqui sont appelées au sein d’un pipeline.\nC’est cela qui rendra l’approche des pipelines particulièrement utile\npour la grid search:\n\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {\"reduce_dim__n_components\":[2, 5, 10], \"clf__C\":[0.1, 10, 100]}\ngrid_search = GridSearchCV(pipe, param_grid=param_grid)\n\n\nDonnées utilisées\nNous allons utiliser les données de transactions immobilières DVF pour chercher\nla meilleure manière de prédire, sachant les caractéristiques d’un bien, son\nprix.\nCes données peuvent être importées directement depuis data.gouv:\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nmutations = pd.read_csv('https://www.data.gouv.fr/fr/datasets/r/3004168d-bec4-44d9-a781-ef16f41856a2', sep = \"|\", decimal=\",\")\n\nOn propose d’enrichir la base de quelques variables qui pourraient servir\nultérieurement:\n\nmutations['Date mutation'] = pd.to_datetime(mutations['Date mutation'], format = \"%d/%m/%Y\")\nmutations['year'] = mutations['Date mutation'].dt.year\nmutations['month'] = mutations['Date mutation'].dt.month\nmutations['dep'] = mutations['Code postal'].astype(str).str[:2]\nmutations['lprix'] = np.log(mutations[\"Valeur fonciere\"])\n\nSi vous travaillez avec les données de 2020, n’oubliez pas\nd’intégrer l’effet\nconfinement strict dans vos modèles. Pour cela, vous pouvez créer une variable\nindicatrice entre les dates en question:\n\nmutations['confinement'] = mutations['Date mutation'].between(pd.to_datetime(\"2020-03-17\"), pd.to_datetime(\"2020-05-03\")).astype(int)\n\nLes données DVF proposent une observation par transaction. Ces transactions\npeuvent concerner plusieurs lots.\nPour simplifier,\non va créer une variable de surface qui agrège les différentes informations\nde surface disponibles dans le jeu de données. En effet, les variables\nen question sont très corrélées les unes entre elles :\n\ng.figure.get_figure()\n\nLes agréger revient à supposer que le modèle de fixation des prix est le même\nentre chaque lot. C’est une hypothèse simplificatrice qu’une personne plus\nexperte du marché immobilier, ou qu’une approche propre de sélection\nde variable pourrait amener à nier\n\nmutations['surface'] = mutations.loc[:, mutations.columns[mutations.columns.str.startswith('Surface Carrez')].tolist()].sum(axis = 1)"
  },
  {
    "objectID": "content/modelisation/6_pipeline.html#un-premier-pipeline-random-forest-sur-des-variables-standardisées",
    "href": "content/modelisation/6_pipeline.html#un-premier-pipeline-random-forest-sur-des-variables-standardisées",
    "title": "Premier pas vers l’industrialisation avec les pipelines scikit",
    "section": "Un premier pipeline: random forest sur des variables standardisées",
    "text": "Un premier pipeline: random forest sur des variables standardisées\nNotre premier pipeline va nous permettre d’intégrer ensemble:\n\nUne étape de preprocessing avec la standardisation de variables\nUne étape d’estimation du prix en utilisant un modèle de random forest\n\nPour le moment, on va prendre comme acquis un certain nombre de variables\nexplicatives (les features) et les hyperparamètres du modèle\n\nDéfinition des ensembles train/test\nNous allons donc nous restreindre à un sous-ensemble de colonnes dans un\npremier temps :\n\nxvars = ['dep', 'Nombre de lots', 'Code type local', 'surface', 'Nombre pieces principales']\nxvars2 = pd.Series(xvars).str.replace(\" \",\"_\").tolist()\n\nmutations2 = mutations.loc[:, xvars + [\"Valeur fonciere\"]]\n\nNous allons également ne conserver que les transactions inférieures à 5 millions\nd’euros (on anticipe que celles ayant un montant supérieur sont des transactions\nexceptionnelles dont le mécanisme de fixation du prix diffère)\n\nmutations2  = mutations2.dropna()\nmutations2 = mutations2.loc[mutations2['Valeur fonciere'] &lt; 5e6] #keep only values below 10 millions\n\nmutations2.columns = mutations2.columns.str.replace(\" \", \"_\")\nnumeric_features = mutations2.columns[~mutations2.columns.isin(['dep','Code_type_local', 'confinement'])].tolist()\ncategorical_features = ['dep','Code_type_local']\n\nAu passage, nous avons abandonné la variable de code postal pour privilégier\nla commune afin de réduire la dimension de notre jeu de données. Si on voulait\nvraiment avoir un bon modèle, il faudrait faire autrement car le code postal\nest probablement un très bon prédicteur du prix d’un bien, une fois que\nles caractéristiques du bien sont contrôlées.\nNous allons stratifier notre échantillonage de train/test par département\nafin de tenir compte, de manière minimale, de la géographie.\nPour accélérer les calculs pour ce tutoriel, nous n’allons considérer que\n20% des transactions observées sur chaque département.\n\nfrom sklearn.model_selection import train_test_split\n\nmutations2 = mutations2.groupby('dep').sample(frac = 0.2, random_state = 123)\n\nX_train, X_test, y_train, y_test = train_test_split(mutations2[xvars2], mutations2[[\"Valeur_fonciere\"]].values.ravel(), test_size = 0.2, random_state = 0, stratify=mutations2[['dep']])\n\n\n\nDéfinition du premier pipeline\nNous allons donc partir d’un random forest avec des valeurs d’hyperparamètres\ndonnées.\nLes random forest sont une méthode d’aggrégation1 d’arbres de décision.\nOn calcule \\(K\\) arbres de décision et en tire, par une méthode d’agrégation,\nune règle de décision moyenne qu’on va appliquer pour tirer une\nprédiction de nos données.\n\n\n\n\n\nC’est un article de Léo Breiman (2001)2, statisticien à Berkeley, qui\nest à l’origine du succès des random forests. L’un des intérêts\ndes random forest est qu’il existe des méthodes pour déterminer\nl’importance relative de chaque variable dans la prédiction.\nPour commencer, nous allons fixer la taille des arbres de décision avec\nl’hyperparamètre max_depth = 2.\n\nfrom sklearn.ensemble import RandomForestRegressor\nregr = RandomForestRegressor(max_depth=2, random_state=123)\n\nNotre pipeline va intégrer les étapes suivantes :\n\nPreprocessing :\n\nLes variables numériques vont être standardisées avec un StandardScaler.\nPour cela, nous allons utiliser la liste numeric_features définie précédemment.\nLes variables catégorielles vont être explosées avec un one hot encoding\n(méthode OneHotEncoder de scikit)\nPour cela, nous allons utiliser la liste categorical_features\n\nRandom forest : nous allons appliquer l’estimateur regr défini plus haut\n\nJ’ajoute en commentaire un exemple de comment s’introduirait une imputation\nde valeurs manquantes. La version 1.0 de scikit facilite l’intégration\nd’étapes complexes dans les pipelines3. Si vous utilisez une\nversion antérieure à la 1.0 de scikit, vous pouvez vous rendre dans\nla section Annexe pour avoir des exemples de définition alternative\n(attention cependant, vous ne pourrez récupérer le nom des features\ntransformées comme ici, ce qui peut pénaliser l’analyse d’importance\nde variables)\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import make_column_transformer\n\nnumeric_pipeline = make_pipeline(\n  #SimpleImputer(),\n  StandardScaler()\n)\ntransformer = make_column_transformer(\n    (numeric_pipeline, numeric_features[:-1]),\n    (OneHotEncoder(sparse = False, handle_unknown = \"ignore\"), categorical_features))\npipe = Pipeline(steps=[('preprocessor', transformer),\n                      ('randomforest', regr)])\n\nNous avons construit ce pipeline sous forme de couches successives. La couche\nrandomforest prendra automatiquement le résultat de la couche preprocessor\nen input. La couche features permet d’introduire de manière relativement\nsimple (quand on a les bonnes méthodes) la complexité du preprocessing\nsur données réelles dont les types divergent.\nOn peut visualiser le graphe et ainsi se représenter la manière dont\nce pipeline opère:\npipe\nMaintenant, il ne reste plus qu’à estimer notre modèle sur l’ensemble\nd’entraînement. C’est très simple avec un pipeline : il suffit d’utiliser\nde mettre à jour le pipeline avec la méthode fit\nOn peut utiliser le nom du pipeline en conjonction de certaines méthodes\npour appliquer cette étape sur un jeu de données pour visualiser\nl’effet de la transformation.\nPar exemple, pour visualiser le jeu de données transformé avant l’étape\nd’estimation, on peut\nfaire\n\npipe[:-1].fit_transform(X_train)\n\nDe même, si on veut récupérer le nom des features en sortie du preprocessing,\non utilisera la méthode .get_feature_names_out qui est bien pratique\n(c’est cette méthode qui est plus complexe à appeler dans les versions scikit\nancienne qui nous a fait privilégier le pipeline ci-dessous)\n\nfeatures_names=pipe['preprocessor'].get_feature_names_out()\nfeatures_names\n\n\n\nVariable importance\nOn ne va représenter, parmi notre ensemble important de colonnes, que celles\nqui ont une importance non nulle. Grâce à notre vecteur features_names,\non va pouvoir facilement afficher le nom des colonnes en question (et donc\ncomprendre les features déterminantes)\nL’importance va être définie à partir\nde la mesure d’impureté4\nOn voit donc que deux variables déterminantes sont des effets fixes\ngéographiques (qui servent à ajuster de la différence de prix entre\nParis et les Hauts de Seine et le reste de la France), une autre variable\nest un effet fixe type de bien. Les deux variables qui pourraient introduire\nde la variabilité, à savoir la surface et, dans une moindre mesure, le\nnombre de lots, ont une importance moindre.\n\n\n Note\nIdéalement, on utiliserait yellowbrick pour représenter l’importance des variables\nMais en l’état actuel du pipeline on a beaucoup de variables dont le poids\nest nul qui viennent polluer la visualisation. Vous pouvez\nconsulter la\ndocumentation de yellowbrick sur ce sujet\n\n\n\n\nPrédiction\nL’analyse de l’importance de variables permet de mieux comprendre\nle fonctionnement interne des random forests.\nOn obtient un modèle dont les performances sont les suivantes :\n\nfrom sklearn.metrics import mean_squared_error\n\n\ncompar = pd.DataFrame([y_test, pipe.predict(X_test)]).T\ncompar.columns = ['obs','pred']\ncompar['diff'] = compar.obs - compar.pred\n\nprint(\"Le RMSE sur le jeu de test est {:,}\".format(\n   int(np.sqrt(mean_squared_error(y_test, pipe.predict(X_test))))\n))\n\nLe RMSE n’est pas très bon. Pour comprendre pourquoi, représentons\nnotre nuage de point des valeurs observées et prédites:\nC’est très décevant. La prédiction a trop peu de variabilité pour capturer\nla variance des prix observée. Cela vient du fait que les variables\nayant de l’importance dans la prédiction sont principalement des effets fixes,\nqui ne permettent donc qu’une variabilité limitée.\n\ng.figure.get_figure()"
  },
  {
    "objectID": "content/modelisation/6_pipeline.html#recherche-des-hyperparamètres-optimaux-avec-une-validation-croisée",
    "href": "content/modelisation/6_pipeline.html#recherche-des-hyperparamètres-optimaux-avec-une-validation-croisée",
    "title": "Premier pas vers l’industrialisation avec les pipelines scikit",
    "section": "Recherche des hyperparamètres optimaux avec une validation croisée",
    "text": "Recherche des hyperparamètres optimaux avec une validation croisée\nOn détecte que le premier modèle n’est pas très bon et ne nous aidera\npas vraiment à évaluer de manière fiable la maison de nos rêves.\nOn va essayer de voir si notre modèle ne serait pas meilleur avec des\nhyperparamètres plus adaptés. Après tout, nous avons choisi par défaut\nla profondeur de l’arbre mais c’était un choix au doigt mouillé.\nQuels sont les hyperparamètres qu’on peut essayer d’optimiser ?\n\npipe['randomforest'].get_params()\n\nUn détour par la documentation\nnous aide à comprendre ceux sur lesquels on va jouer. Par exemple, il serait\nabsurde de jouer sur le paramètre random_state qui est la racine du générateur\npseudo-aléatoire.\nComme l’objectif est de se concentrer sur la démarche plus qu’essayer de\ntrouver un bon modèle,\nnous allons également réduire la taille des données pour accélérer\nles calculs\n\nmutations2 = mutations2.groupby('dep').sample(frac = 0.5, random_state = 123)\n\nX_train, X_test, y_train, y_test = train_test_split(mutations2[xvars2], mutations2[[\"Valeur_fonciere\"]].values.ravel(), test_size = 0.2, random_state = 0, stratify=mutations2[['dep']])\n\nX = pd.concat((X_train, X_test), axis=0)\nY = np.concatenate([y_train,y_test])\n\nNous allons nous contenter de jouer sur les paramètres:\n\nn_estimators: Le nombre d’arbres de décision que notre forêt contient\nmax_depth: La profondeur de chaque arbre\n\nIl existe plusieurs manières de faire de la validation croisée. Nous allons ici\nutiliser la grid search qui consiste à estimer et tester le modèle sur chaque\ncombinaison d’une grille de paramètres et sélectionner le couple de valeurs\ndes hyperparamètres amenant à la meilleure prédiction. Par défaut, scikit\neffectue une 5-fold cross validation. Nous n’allons pas changer\nce comportement.\nComme expliqué précédemment, les paramètres s’appelent sous la forme\n&lt;step&gt;__&lt;parameter_name&gt;\nLa validation croisée pouvant être très consommatrice de temps, nous\nn’allons l’effectuer que sur un nombre réduit de valeurs de notre grille.\nIl est possible de passer la liste des valeurs à passer au crible sous\nforme de liste (comme pour l’argument max_depth ci-dessous) ou\nsous forme d’array (comme pour l’argument n_estimators) ce qui est\nsouvent pratique pour générer un criblage d’un intervalle avec np.linspace.\nOn peut récupérer les paramètres optimaux avec la méthode best_params_:\n\ngrid_search.best_params_\n\nOn pourra aussi ré-utiliser le modèle optimal de la manière suivante :\ngrid_search.best_estimator_\nToutes les performances sur les ensembles d’échantillons et de test sur la grille\nd’hyperparamètres sont disponibles dans l’attribut:\n\nperf_random_forest = pd.DataFrame(grid_search.cv_results_)\n\nRegardons les résultats moyens pour chaque valeur des hyperparamètres:\nGlobalement, à profondeur d’arbre donnée, le nombre d’arbres change\nmarginalement la performance (cela détériore\nla performance quand la profondeur est de 4, cela améliore quand\non fixe la profondeur de 2).\nEn revanche, changer la profondeur de l’arbre améliore la\nperformance de manière plus marquée.\nMaintenant, il nous reste à re-entraîner le modèle avec ces nouveaux\nparamètres sur l’ensemble du jeu de train et l’évaluer sur l’ensemble\ndu jeu de test:\nOn obtient le RMSE suivant :\n\nprint(\"Le RMSE sur le jeu de test est {:,}\".format(\n   int(np.sqrt(mean_squared_error(y_test, pipe_optimal.predict(X_test))))\n))\n\nEt si on regarde la qualité en prédiction:\n\ng.figure.get_figure()\n\nOn obtient plus de variance dans la prédiction, c’est déjà un peu mieux.\nCependant, cela reste décevant pour plusieurs raisons:\n\nnous n’avons pas fait d’étape de sélection de variable\nnous n’avons pas chercher à déterminer si la variable à prédire la plus\npertinente était le prix ou une transformation de celle-ci\n(par exemple le prix au \\(m^2\\))\n\n\nfeatures_names=pipe_optimal['preprocessor'].get_feature_names_out()\nimportances = pipe_optimal['randomforest'].feature_importances_\nstd = np.std([tree.feature_importances_ for tree in pipe_optimal['randomforest'].estimators_], axis=0)\n\nforest_importances = pd.Series(importances[importances&gt;0], index=features_names[importances&gt;0])\n\nfig, ax = plt.subplots()\nforest_importances.plot.bar(yerr=std[importances&gt;0], ax=ax)\nax.set_title(\"Feature importances using MDI\")\nax.set_ylabel(\"Mean decrease in impurity\")\nfig.tight_layout()\n\n\nRemarque sur la performance\nLes estimations sont, par défaut, menées de manière séquentielle (l’une après\nl’autre). Nous sommes cependant face à un problème\nembarassingly parallel.\nPour gagner en performance, il est recommandé d’utiliser l’argument\nn_jobs=-1.\n\n\nRemarque sur l’échantillonnage\nEn l’état actuel de l’échantillonnage entre train et test au sein de la\ngrid search,\non est face à un problème de data leaking car l’échantillon\nn’est pas balancé entre nos classes (les départements).\nCertaines classes se\nretrouvent hors de l’échantillon d’estimation mais dans l’échantillon de prédiction.\nAutrement dit, notre pipeline de preprocessing se retrouve à devoir\nnettoyer des valeurs qu’il ne connaît pas.\nNous avons choisi une option, dans notre pipeline pour se faciliter la vie\nà ce propos. Nous ne rencontrons pas d’erreur car nous avons utilisé l’option\nhandle_unknown = \"ignore\" plutôt que\nhandle_unknown = \"error\" (défaut) dans le one hot encoding.\nCette option est dangereuse et n’est pas recommandée pour un vrai pipeline.\nDe manière générale, il vaut mieux adopter une approche de\nprogrammation défensive en n’hésitant pas à renvoyer une erreur si la\nstructure du DataFrame de prédiction diffère vraiment de celle du DataFrame\nd’entraînement.\nPour éviter cette erreur, il serait mieux de définir explicitement le schéma de\nvalidation croisée à mettre en oeuvre.\nPrécédemment, nous avions utilisé un échantillonnage stratifié.\nCela pourrait être fait ici avec\nla méthode StratifiedGroupKFold (plus d’éléments à venir)\nfrom sklearn.model_selection import StratifiedGroupKFold\ncv = StratifiedGroupKFold(n_splits=5)\n#grid_search.fit(pd.concat((X_train, X_test), axis=0), np.concatenate([y_train,y_test]), cv = cv, groups = pd.concat((X_train, X_test), axis=0)['dep'])"
  },
  {
    "objectID": "content/modelisation/6_pipeline.html#eléments-supplémentaires-à-venir",
    "href": "content/modelisation/6_pipeline.html#eléments-supplémentaires-à-venir",
    "title": "Premier pas vers l’industrialisation avec les pipelines scikit",
    "section": "Eléments supplémentaires à venir",
    "text": "Eléments supplémentaires à venir\nCe chapitre est amené à être enrichi des éléments suivants\n(cf. #207)\n\nComparaison performance entre modèles grâce aux pipelines\nIntégration d’une étape de sélection de variable dans un pipeline\nstatsmodels dans un pipeline\nKeras dans un pipeline"
  },
  {
    "objectID": "content/modelisation/6_pipeline.html#annexes-pipelines-alternatifs",
    "href": "content/modelisation/6_pipeline.html#annexes-pipelines-alternatifs",
    "title": "Premier pas vers l’industrialisation avec les pipelines scikit",
    "section": "Annexes : pipelines alternatifs",
    "text": "Annexes : pipelines alternatifs\n\nPréalable : quelques méthodes pour gagner en flexibilité dans le preprocessing\nNotre DataFrame comporte des types hétérogènes de variables:\n\nDes variables numériques dont les variances sont très hétérogènes\nDes variables textuelles qui mériteraient un recodage sous forme numérique\nDes variables discrètes dont les modalités devraient être éclatées (one hot encoding)\n\nPour gagner en flexibilité, nous allons proposer certaines méthodes qui permettent\nd’appliquer les étapes de preprocessing adéquates à un sous-ensemble de\nvariables5. Ces méthodes ne sont plus nécessaires dans les versions\nrécentes de scikit.\nPour cela, il convient d’adopter l’approche de la programmation orientée objet.\nOn va créer des classes avec des méthodes transform et fit_transform\nqui pourront ainsi être intégrées directement dans les pipelines, comme s’il\ns’agissait de méthodes issues de scikit.\nLa première généralise LabelEncoder à un sous-ensemble de colonnes:\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nclass MultiColumnLabelEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)\n\nLa seconde généralise cette fois le one hot encoding à un sous ensemble de\nfonctions\n\nclass MultiColumnOneHotEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                output[col] = OneHotEncoder(sparse=False).fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = OneHotEncoder(sparse=False).fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)\n\nLes méthodes suivantes vont nous permettre de passer en arguments les noms\nde colonnes pour intégrer la récupération des bonnes colonnes de nos\ndataframes dans le pipeline:\n\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n\nclass Columns(BaseEstimator, TransformerMixin):\n    def __init__(self, names=None):\n        self.names = names\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X):\n        return X[self.names]\n\nclass Normalize(BaseEstimator, TransformerMixin):\n    def __init__(self, func=None, func_param={}):\n        self.func = func\n        self.func_param = func_param\n\n    def transform(self, X):\n        if self.func != None:\n            return self.func(X, **self.func_param)\n        else:\n            return X\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\nEnfin, on va créer une méthode intermédiaire sous forme de hack\n(elle prend une matrice en entrée et renvoie la même matrice)\npour\npouvoir facilement récupérer notre matrice de feature afin de vérifier\nses caractéristiques (notamment le nombre de colonnes disponibles):\n\nclass Collect(BaseEstimator, TransformerMixin):\n\n    def transform(self, X):\n        #print(X.shape)\n        #self.shape = shape\n        # what other output you want\n        return X\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n\nfrom sklearn.pipeline import make_pipeline, FeatureUnion\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\n\npipe2 = Pipeline([\n    (\"features\", FeatureUnion([\n        ('numeric', make_pipeline(Columns(names=numeric_features[:-1]),StandardScaler())),\n        ('categorical', make_pipeline(Columns(names=categorical_features),OneHotEncoder(sparse=False)))\n    ])),\n    ('identity', Collect()),\n    ('randomforest', regr)\n])\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('numeric', StandardScaler(), numeric_features[:-1]),\n        ('categorical', OneHotEncoder(sparse=False, handle_unknown = \"ignore\"), categorical_features)])\n\npipe3 = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('randomforest', regr)])"
  },
  {
    "objectID": "content/modelisation/6_pipeline.html#références",
    "href": "content/modelisation/6_pipeline.html#références",
    "title": "Premier pas vers l’industrialisation avec les pipelines scikit",
    "section": "Références",
    "text": "Références\n\nBreiman L (2001). “Random Forests”. Machine Learning. 45 (1): 5–32."
  },
  {
    "objectID": "content/modelisation/6_pipeline.html#footnotes",
    "href": "content/modelisation/6_pipeline.html#footnotes",
    "title": "Premier pas vers l’industrialisation avec les pipelines scikit",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEn machine learning on retrouve un principe inspiré du\nbootstrap\nqui permet d’agréger un ensemble d’estimateurs en un estimateur “moyennisé”.\nIl s’agit du bagging.\nEn économétrie, le bootstrap consiste à ré-estimer sur K sous-échantillons\naléatoires des données un estimateur afin d’en tirer, par exemple, un intervalle\nde confiance empirique à 95%. Le principe du bagging est le même. On ré-estime\nK fois notre estimateur (par exemple un arbre de décision) et propose une\nrègle d’agrégation pour en tirer une règle moyennisée et donc une prédiction.↩︎\nExtrait de ce blog:\nGini importance (or mean decrease impurity), which is computed from the Random Forest structure. Let’s look how the Random Forest is constructed. It is a set of Decision Trees. Each Decision Tree is a set of internal nodes and leaves. In the internal node, the selected feature is used to make decision how to divide the data set into two separate sets with similars responses within. The features for internal nodes are selected with some criterion, which for classification tasks can be gini impurity or infomation gain, and for regression is variance reduction. We can measure how each feature decrease the impurity of the split (the feature with highest decrease is selected for internal node). For each feature we can collect how on average it decreases the impurity. The average over all trees in the forest is the measure of the feature importance.↩︎\nVoir ce thread stackoverflow↩︎\nExtrait de ce blog:\nGini importance (or mean decrease impurity), which is computed from the Random Forest structure. Let’s look how the Random Forest is constructed. It is a set of Decision Trees. Each Decision Tree is a set of internal nodes and leaves. In the internal node, the selected feature is used to make decision how to divide the data set into two separate sets with similars responses within. The features for internal nodes are selected with some criterion, which for classification tasks can be gini impurity or infomation gain, and for regression is variance reduction. We can measure how each feature decrease the impurity of the split (the feature with highest decrease is selected for internal node). For each feature we can collect how on average it decreases the impurity. The average over all trees in the forest is the measure of the feature importance.↩︎\nUn certain nombre des éléments suivants ont été glannés, par ci par là,\ndepuis stackoverflow.↩︎"
  },
  {
    "objectID": "content/NLP/01_intro.html",
    "href": "content/NLP/01_intro.html",
    "title": "Quelques éléments pour comprendre les enjeux du NLP",
    "section": "",
    "text": "Le NLP est un domaine immense de recherche. Cette page est une introduction\nfort incomplète à la question. Il s’agit de montrer la logique, quelques exemples\navec Python \net s’amuser avec comme base d’exemple un livre formidable :books: :\nLe Comte de Monte Cristo.\nDans le cadre de l’introduction au NLP que vous pouvez retrouver dans\nles différents chapitres, nous évoquons principalement les champs suivants du NLP:\nCela laisse de côté des champs très actifs de recherche\ndu NLP, notamment l’analyse de sentiment ou les modèles de\nlangage (modèles GPT par exemple). Les outils découverts\ndans cette partie du cours permettront, si vous le désirez,\nde bénéficier d’une base solide pour approfondir tel ou tel\nsujet."
  },
  {
    "objectID": "content/NLP/01_intro.html#base-dexemple",
    "href": "content/NLP/01_intro.html#base-dexemple",
    "title": "Quelques éléments pour comprendre les enjeux du NLP",
    "section": "Base d’exemple",
    "text": "Base d’exemple\nLa base d’exemple est le Comte de Monte Cristo d’Alexandre Dumas.\nIl est disponible\ngratuitement sur le site\nProject Gutemberg comme des milliers\nd’autres livres du domaine public. La manière la plus simple de le récupérer\nest de télécharger avec le package request le fichier texte et le retravailler\nlégèrement pour ne conserver que le corpus du livre :\n\nfrom urllib import request\n\nurl = \"https://www.gutenberg.org/files/17989/17989-0.txt\"\nresponse = request.urlopen(url)\nraw = response.read().decode('utf8')\n\ndumas = raw.split(\"*** START OF THE PROJECT GUTENBERG EBOOK LE COMTE DE MONTE-CRISTO, TOME I ***\")[1].split(\"*** END OF THE PROJECT GUTENBERG EBOOK LE COMTE DE MONTE-CRISTO, TOME I ***\")[0]\n\nimport re\n\ndef clean_text(text):\n    text = text.lower() # mettre les mots en minuscule\n    text = \" \".join(text.split())\n    return text\n\ndumas = clean_text(dumas)\n\ndumas[10000:10500]\n\n\" mes yeux. --vous avez donc vu l'empereur aussi? --il est entré chez le maréchal pendant que j'y étais. --et vous lui avez parlé? --c'est-à-dire que c'est lui qui m'a parlé, monsieur, dit dantès en souriant. --et que vous a-t-il dit? --il m'a fait des questions sur le bâtiment, sur l'époque de son départ pour marseille, sur la route qu'il avait suivie et sur la cargaison qu'il portait. je crois que s'il eût été vide, et que j'en eusse été le maître, son intention eût été de l'acheter; mais je lu\""
  },
  {
    "objectID": "content/NLP/01_intro.html#la-particularité-des-données-textuelles",
    "href": "content/NLP/01_intro.html#la-particularité-des-données-textuelles",
    "title": "Quelques éléments pour comprendre les enjeux du NLP",
    "section": "La particularité des données textuelles",
    "text": "La particularité des données textuelles\n\nObjectif\nLe natural language processing (NLP) ou\ntraitement automatisé de la langue (TAL) en Français,\nvise à extraire de l’information de textes à partir d’une analyse statistique du contenu.\nCette définition permet d’inclure de nombreux champs d’applications au sein\ndu NLP (traduction, analyse de sentiment, recommandation, surveillance, etc. ) ainsi que de méthodes.\nCette approche implique de transformer un texte, qui est une information compréhensible par un humain, en un nombre, information appropriée pour un ordinateur et une approche statistique ou algorithmique.\nTransformer une information textuelle en valeurs numériques propres à une analyse statistique n’est pas une tâche évidente. Les données textuelles sont non structurées puisque l’information cherchée, qui est propre à chaque analyse, est perdue au milieu d’une grande masse d’informations qui doit, de plus, être interprétée dans un certain contexte (un même mot ou une phrase n’ayant pas la même signification selon le contexte).\nSi cette tâche n’était pas assez difficile comme ça, on peut ajouter d’autres difficultés propres à l’analyse textuelle car ces données sont :\n\nbruitées : ortographe, fautes de frappe…\nchangeantes : la langue évolue avec de nouveaux mots, sens…\ncomplexes : structures variables, accords…\nambigues : synonymie, polysémie, sens caché…\npropres à chaque langue : il n’existe pas de règle de passage unique entre deux langues\ngrande dimension : des combinaisons infinies de séquences de mots\n\n\n\nMéthode\nL’unité textuelle peut être le mot ou encore une séquence de n\nmots (un n-gramme) ou encore une chaîne de caractères (e.g. la\nponctuation peut être signifiante). On parle de token. L’analyse textuelle vise à transformer le texte en données\nnumériques manipulables.\nOn peut ensuite utiliser diverses techniques (clustering,\nclassification supervisée) suivant l’objectif poursuivi pour exploiter\nl’information transformée. Mais les étapes de nettoyage de texte sont indispensables car sinon un algorithme sera incapable de détecter une information pertinente dans l’infini des possibles."
  },
  {
    "objectID": "content/NLP/01_intro.html#nettoyer-un-texte",
    "href": "content/NLP/01_intro.html#nettoyer-un-texte",
    "title": "Quelques éléments pour comprendre les enjeux du NLP",
    "section": "Nettoyer un texte",
    "text": "Nettoyer un texte\nLes wordclouds sont des représentations graphiques assez pratiques pour visualiser\nles mots les plus fréquents. Elles sont très simples à implémenter en Python\navec le module wordcloud qui permet même d’ajuster la forme du nuage à\nune image :\n\n\nimport wordcloud\nimport numpy as np\nimport io\nimport requests\nimport PIL\nimport matplotlib.pyplot as plt\n\nimg = \"https://raw.githubusercontent.com/linogaliana/python-datascientist/master/content/NLP/book.png\"\nbook_mask = np.array(PIL.Image.open(io.BytesIO(requests.get(img).content)))\n\nfig = plt.figure()\n\ndef make_wordcloud(corpus):\n    wc = wordcloud.WordCloud(background_color=\"white\", max_words=2000, mask=book_mask, contour_width=3, contour_color='steelblue')\n    wc.generate(corpus)\n    return wc\n\nplt.imshow(make_wordcloud(dumas), interpolation='bilinear')\nplt.axis(\"off\")\n#plt.show()\n#plt.savefig('word.png', bbox_inches='tight')\n\n\n(-0.5, 1429.5, 783.5, -0.5)\n(a) Nuage de mot produit à partir du Comte de Monte Cristo\n\n\n\n\n\n\n(b)\n\n\n\nFigure 1: ?(caption)\n\n\nCela montre clairement qu’il est nécessaire de nettoyer notre texte. Le nom\ndu personnage principal, Dantès, est ainsi masqué par un certain nombre\nd’articles ou mots de liaison qui perturbent l’analyse. Ces mots sont des\nstop-words. La librairie NLTK (Natural Language ToolKit), librairie\nde référence dans le domaine du NLP, permet de facilement retirer ces\nstopwords (cela pourrait également être fait avec\nla librairie plus récente, spaCy). Avant cela, il est nécessaire\nde transformer notre texte en le découpant par unités fondamentales (les tokens).\nLes exemples suivants, extraits de Galiana and Castillo (2022), montrent l’intérêt du\nnettoyage de textes lorsqu’on désire comparer des corpus\nentre eux. En l’occurrence, il s’agit de comparer un corpus de\nnoms de produits dans des collectes automatisées de produits\nde supermarché (scanner-data) avec des noms de produits\ndans les données de l’OpenFoodFacts, une base de données\ncontributive. Sans nettoyage, le bruit l’emporte sur le signal\net il est impossible de déceler des similarités entre les jeux\nde données. Le nettoyage permet d’harmoniser\nun peu ces jeux de données pour avoir une chance d’être en\nmesure de les comparer.\n\n\n\n\n\nOpenFoodFacts avant nettoyage\n\n\n\n\n\nScanner-data avant nettoyage\n\n\n\n\n\n\n\n\n\nOpenFoodFacts après nettoyage\n\n\n\n\n\nScanner-data après nettoyage\n\n\n\n\n\nTokenisation\n\n\n Hint\nLors de la première utilisation de NLTK, il est nécessaire de télécharger\nquelques éléments nécessaires à la tokenisation, notamment la ponctuation.\nPour cela, il est recommandé d’utiliser la commande suivante :\nimport nltk\nnltk.download('punkt')\n\n\n\nimport nltk\nnltk.download('punkt')\n\n[nltk_data] Downloading package punkt to /github/home/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n\n\nTrue\n\n\nLa tokenisation consiste à découper un texte en morceaux. Ces morceaux\npourraient être des phrases, des chapitres, des n-grammes ou des mots. C’est\ncette dernière option que l’on va choisir, plus simple pour retirer les\nstopwords :\n\nimport nltk\n\nwords = nltk.word_tokenize(dumas, language='french')\nwords[1030:1050]\n\n['que',\n 'voulez-vous',\n ',',\n 'monsieur',\n 'edmond',\n ',',\n 'reprit',\n \"l'armateur\",\n 'qui',\n 'paraissait',\n 'se',\n 'consoler',\n 'de',\n 'plus',\n 'en',\n 'plus',\n ',',\n 'nous',\n 'sommes',\n 'tous']\n\n\nOn remarque que les mots avec apostrophes sont liés en un seul, ce qui est\npeut-être faux sur le plan de la grammaire mais peu avoir un sens pour une\nanalyse statistique. Il reste des signes de ponctuation qu’on peut éliminer\navec la méthode isalpha:\n\nwords = [word for word in words if word.isalpha()]\nwords[1030:1050]\n\n['assez',\n 'sombre',\n 'obséquieux',\n 'envers',\n 'ses',\n 'supérieurs',\n 'insolent',\n 'envers',\n 'ses',\n 'subordonnés',\n 'aussi',\n 'outre',\n 'son',\n 'titre',\n 'comptable',\n 'qui',\n 'est',\n 'toujours',\n 'un',\n 'motif']\n\n\nComme indiqué ci-dessus, pour télécharger\nle corpus de ponctuation, il est\nnécessaire d’exécuter la ligne de\ncommande suivante :\n\n\nRetirer les stop-words\nLe jeu de données est maintenant propre. On peut désormais retirer les\nmots qui n’apportent pas de sens et servent seulement à faire le\nlien entre deux prépositions. On appelle ces mots des\nstop words dans le domaine du NLP.\n\n\n Hint\nLors de la première utilisation de NLTK, il est nécessaire de télécharger\nles stopwords.\nimport nltk\nnltk.download('stopwords')\n\n\nComme indiqué ci-dessus, pour télécharger\nle corpus de stopwords1, il est\nnécessaire d’exécuter la ligne de\ncommande suivante :\n\nimport nltk\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to /github/home/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n\n\nTrue\n\n\n\nfrom nltk.corpus import stopwords\nprint(stopwords.words(\"french\"))\n\nstop_words = set(stopwords.words('french'))\n\n\nwords = [w for w in words if not w in stop_words]\nprint(words[1030:1050])\n\n['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n['celui', 'dantès', 'a', 'déposé', 'passant', 'comment', 'paquet', 'déposer', 'danglars', 'rougit', 'passais', 'devant', 'porte', 'capitaine', 'entrouverte', 'vu', 'remettre', 'paquet', 'cette', 'lettre']\n\n\nCes retraitements commencent à porter leurs fruits puisque des mots ayant plus\nde sens commencent à se dégager, notamment les noms des personnages\n(Fernand, Mercédès, Villefort, etc.)\n\nwc = make_wordcloud(' '.join(words))\n\nfig = plt.figure()\n\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\n\n(-0.5, 1429.5, 783.5, -0.5)\n\n\n\n\n\n\n\n\n\n\n\nStemming\nPour réduire la complexité d’un texte, on peut tirer partie de\n“classes d’équivalence” : on peut\nconsidérer que différentes formes d’un même mot (pluriel,\nsingulier, conjugaison) sont équivalentes et les remplacer par une\nmême forme dite canonique. Il existe deux approches dans le domaine :\n\nla lemmatisation qui requiert la connaissance des statuts\ngrammaticaux (exemple : chevaux devient cheval)\nla racinisation (stemming) plus fruste mais plus rapide, notamment\nen présence de fautes d’orthographes. Dans ce cas, chevaux peut devenir chev\nmais être ainsi confondu avec chevet ou cheveux\n\nLa racinisation est plus simple à mettre en oeuvre car elle peut s’appuyer sur\ndes règles simples pour extraire la racine d’un mot.\nPour réduire un mot dans sa forme “racine”, c’est-à-dire en s’abstrayant des\nconjugaisons ou variations comme les pluriels, on applique une méthode de\nstemming. Le but du stemming est de regrouper de\nnombreuses variantes d’un mot comme un seul et même mot.\nPar exemple, une fois que l’on applique un stemming, “chats” et “chat”\ndeviennent un même mot.\nCette approche a l’avantage de réduire la taille du vocabulaire à maîtriser\npour l’ordinateur et le modélisateur. Il existe plusieurs algorithmes de\nstemming, notamment le Porter Stemming Algorithm ou le\nSnowball Stemming Algorithm. Nous pouvons utiliser ce dernier en Français :\n\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer(language='french')\n\nstemmed = [stemmer.stem(word) for word in words]\nprint(stemmed[1030:1050])\n\n['celui', 'dantes', 'a', 'dépos', 'pass', 'comment', 'paquet', 'dépos', 'danglar', 'roug', 'pass', 'dev', 'port', 'capitain', 'entrouvert', 'vu', 'remettr', 'paquet', 'cet', 'lettr']\n\n\nA ce niveau, les mots commencent à être moins intelligibles par un humain.\nLa machine prendra le relais, on lui a préparé le travail\n\n\n Note\nIl existe aussi le stemmer suivant :\nfrom nltk.stem.snowball import FrenchStemmer\nstemmer = FrenchStemmer()"
  },
  {
    "objectID": "content/NLP/01_intro.html#reconnaissance-des-entités-nommées",
    "href": "content/NLP/01_intro.html#reconnaissance-des-entités-nommées",
    "title": "Quelques éléments pour comprendre les enjeux du NLP",
    "section": "Reconnaissance des entités nommées",
    "text": "Reconnaissance des entités nommées\nCette étape n’est pas une étape de préparation mais illustre la capacité\ndes librairies Python a extraire du sens d’un texte. La librairie\nspaCy permet de faire de la reconnaissance d’entités nommées, ce qui peut\nêtre pratique pour extraire rapidement certains personnages de notre oeuvre.\n\n\nLa librairie spaCy\nNTLK est la librairie historique d’analyse textuelle en Python. Elle existe\ndepuis les années 1990. L’utilisation industrielle du NLP dans le monde\nde la data science est néanmoins plus récente et doit beaucoup à la collecte\naccrue de données non structurées par les réseaux sociaux. Cela a amené à\nun renouvelement du champ du NLP, tant dans le monde de la recherche que dans\nsa mise en application dans l’industrie de la donnée.\nLe package spaCy est l’un des packages qui a permis\ncette industrialisation des méthodes de NLP. Conçu autour du concept\nde pipelines de données, il est beaucoup plus pratique à mettre en oeuvre\npour une chaîne de traitement de données textuelles mettant en oeuvre\nplusieurs étapes de transformation des données.\n\n\n#!pip install deplacy\n#!python -m spacy download fr_core_news_sm\nimport spacy\n\nnlp=spacy.load(\"fr_core_news_sm\")\ndoc = nlp(dumas)\nimport spacy\nfrom spacy import displacy\ndisplacy.render(doc, style=\"ent\", jupyter=True)"
  },
  {
    "objectID": "content/NLP/01_intro.html#représentation-dun-texte-sous-forme-vectorielle",
    "href": "content/NLP/01_intro.html#représentation-dun-texte-sous-forme-vectorielle",
    "title": "Quelques éléments pour comprendre les enjeux du NLP",
    "section": "Représentation d’un texte sous forme vectorielle",
    "text": "Représentation d’un texte sous forme vectorielle\nUne fois nettoyé, le texte est plus propice à une représentation vectorielle.\nEn fait, implicitement, on a depuis le début adopté une démarche bag of words.\nIl s’agit d’une représentation, sans souci de contexte (ordre des mots, contexte d’utilisation),\noù chaque token représente un élément dans un vocabulaire de taille \\(|V|\\).\nOn peut ainsi avoir une représentation matricielle les occurrences de\nchaque token dans plusieurs documents (par exemple plusieurs livres,\nchapitres, etc.) pour, par exemple, en déduire une forme de similarité.\nAfin de réduire la dimension de la matrice bag of words,\non peut s’appuyer sur des pondérations.\nOn élimine ainsi certains mots très fréquents ou au contraire très rares.\nLa pondération la plus simple est basée sur la fréquence des mots dans le document.\nC’est l’objet de la métrique tf-idf (term frequency - inverse document frequency)\nabordée dans un prochain chapitre."
  },
  {
    "objectID": "content/NLP/01_intro.html#footnotes",
    "href": "content/NLP/01_intro.html#footnotes",
    "title": "Quelques éléments pour comprendre les enjeux du NLP",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLe corpus de stop-words de NLTK\nest relativement limité. Il est recommandé\nde privilégier celui de spaCy, plus\ncomplet, pour éliminer plus de mots\nvalises.↩︎"
  },
  {
    "objectID": "content/NLP/03_lda.html",
    "href": "content/NLP/03_lda.html",
    "title": "Latent Dirichlet Allocation (LDA)",
    "section": "",
    "text": "Cette page approfondit les exercices présentés dans la\nsection précédente.\nOn va ainsi continuer notre exploration de la littérature anglophones :\nLes données sont disponibles dans la base de\ndonnées spooky.csv et peuvent être\nimportées par Python en utilisant directement l’url\nhttps://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv.\nLe but va être dans un premier temps de regarder dans le détail les termes les plus fréquents utilisés par les auteurs, et les représenter graphiquement.\nCe notebook est librement inspiré de :\nLa LDA est une technique d’estimation bayésienne.\nLe cours d’Alberto Brietti\nsur le sujet constitue une très bonne ressource pour comprendre\nles fondements de cette technique."
  },
  {
    "objectID": "content/NLP/03_lda.html#librairies-nécessaires",
    "href": "content/NLP/03_lda.html#librairies-nécessaires",
    "title": "Latent Dirichlet Allocation (LDA)",
    "section": "Librairies nécessaires",
    "text": "Librairies nécessaires\nCette page évoquera les principales librairies pour faire du NLP, notamment :\n\nWordCloud\nNLTK\nSpaCy\nKeras\nTensorFlow\n\n\n\n Hint\nComme dans la partie précédente, il faut télécharger quelques\néléments pour que NTLK puisse fonctionner correctement. Pour cela, faire:\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('genesis')\nnltk.download('omw-1.4')\n\n\nLa liste des modules à importer est assez longue, la voici :\n\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('genesis')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n#from IPython.display import display\nimport base64\nimport string\nimport re\nimport nltk\n\nfrom collections import Counter\nfrom time import time\n# from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords\nfrom sklearn.metrics import log_loss\nimport matplotlib.pyplot as plt\n\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\n\n[nltk_data] Downloading package stopwords to /github/home/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /github/home/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package genesis to /github/home/nltk_data...\n[nltk_data]   Package genesis is already up-to-date!\n[nltk_data] Downloading package wordnet to /github/home/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /github/home/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!"
  },
  {
    "objectID": "content/NLP/03_lda.html#données-utilisées",
    "href": "content/NLP/03_lda.html#données-utilisées",
    "title": "Latent Dirichlet Allocation (LDA)",
    "section": "Données utilisées",
    "text": "Données utilisées\nSi vous avez déjà lu la section précédente et importé les données, vous\npouvez passer à la section suivante\nLe code suivant permet d’importer le jeu de données spooky:\n\nimport pandas as pd\n\nurl='https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv'\nimport pandas as pd\ntrain = pd.read_csv(url,\n                    encoding='latin-1')\ntrain.columns = train.columns.str.capitalize()\n                    \ntrain['ID'] = train['Id'].str.replace(\"id\",\"\")\ntrain = train.set_index('Id')\n\nLe jeu de données met ainsi en regard un auteur avec une phrase qu’il a écrite:\n\ntrain.head()\n\n\n\n\n\n\n\n\nText\nAuthor\nID\n\n\nId\n\n\n\n\n\n\n\nid26305\nThis process, however, afforded me no means of...\nEAP\n26305\n\n\nid17569\nIt never once occurred to me that the fumbling...\nHPL\n17569\n\n\nid11008\nIn his left hand was a gold snuff box, from wh...\nEAP\n11008\n\n\nid27763\nHow lovely is spring As we looked from Windsor...\nMWS\n27763\n\n\nid12958\nFinding nothing else, not even gold, the Super...\nHPL\n12958\n\n\n\n\n\n\n\nLes étapes de preprocessing sont expliquées dans le chapitre précédent. On applique les étapes suivantes :\n\nTokeniser\nRetirer la ponctuation et les stopwords\nLemmatiser le texte\n\n\nlemma = WordNetLemmatizer()\n\ntrain_clean = (train\n    .groupby([\"ID\",\"Author\"])\n    .apply(lambda s: nltk.word_tokenize(' '.join(s['Text'])))\n    .apply(lambda words: [word for word in words if word.isalpha()])\n)\n\nfrom nltk.corpus import stopwords  \nstop_words = set(stopwords.words('english'))\n\ntrain_clean = (train_clean\n    .apply(lambda words: [lemma.lemmatize(w) for w in words if not w in stop_words])\n    .reset_index(name='tokenized')\n)\n\ntrain_clean.head(2)\n\n\n\n\n\n\n\n\nID\nAuthor\ntokenized\n\n\n\n\n0\n00001\nMWS\n[Idris, well, content, resolve, mine]\n\n\n1\n00002\nHPL\n[I, faint, even, fainter, hateful, modernity, ..."
  },
  {
    "objectID": "content/NLP/03_lda.html#principe-de-la-lda-latent-dirichlet-allocation",
    "href": "content/NLP/03_lda.html#principe-de-la-lda-latent-dirichlet-allocation",
    "title": "Latent Dirichlet Allocation (LDA)",
    "section": "Principe de la LDA (Latent Dirichlet Allocation)",
    "text": "Principe de la LDA (Latent Dirichlet Allocation)\nLe modèle Latent Dirichlet Allocation (LDA) est un modèle probabiliste génératif qui permet\nde décrire des collections de documents de texte ou d’autres types de données discrètes. LDA fait\npartie d’une catégorie de modèles appelés “topic models”, qui cherchent à découvrir des structures\nthématiques cachées dans des vastes archives de documents.\nCeci permet d’obtenir des méthodes\nefficaces pour le traitement et l’organisation des documents de ces archives : organisation automatique\ndes documents par sujet, recherche, compréhension et analyse du texte, ou même résumer des\ntextes.\nAujourd’hui, ce genre de méthodes s’utilisent fréquemment dans le web, par exemple pour\nanalyser des ensemble d’articles d’actualité, les regrouper par sujet, faire de la recommandation\nd’articles, etc.\nLa LDA est une méthode qui considère les corpus comme des mélanges de sujets et\nde mots. Chaque document peut être représenté comme le résultat d’un mélange :\n\nde sujets\net, au sein de ces sujets, d’un choix de mots.\n\nL’estimation des\nparamètres de la LDA passe par l’estimation des distributions des variables\nlatentes à partir des données observées (posterior inference).\nMathématiquement, on peut se représenter la LDA comme une\ntechnique de maximisation de log vraisemblance avec un algorithme EM (expectation maximisation)\ndans un modèle de mélange.\nLa matrice termes-documents qui sert de point de départ est la suivante :\n\n\n\n\nword_1\nword_2\nword_3\n…\nword_J\n\n\n\n\ndoc_1\n3\n0\n1\n…\n0\n\n\n…\n…\n…\n…\n…\n…\n\n\ndoc_N\n1\n0\n0\n…\n5\n\n\n\nOn dit que cette matrice est sparse (creuse en Français) car elle contient principalement des 0. En effet, un document n’utilise qu’une partie mineure du vocabulaire complet.\nLa LDA consiste à transformer cette matrice sparsedocument-terme en deux matrices de moindre dimension:\n\nUne matrice document-sujet\nUne matrice sujet-mots\n\nEn notant \\(K_i\\) le sujet \\(i\\). On obtient donc\n\nUne matrice document-sujet ayant la structure suivante :\n\n\n\n\n\nK_1\nK_2\nK_3\n…\nK_M\n\n\n\n\ndoc_1\n1\n0\n1\n…\n0\n\n\n…\n…\n…\n…\n…\n…\n\n\ndoc_N\n1\n1\n1\n…\n0\n\n\n\n\nUne matrice sujets-mots ayant la structure suivante :\n\n\n\n\n\nword_1\nword_2\nword_3\n…\nword_J\n\n\n\n\nK_1\n1\n0\n0\n…\n0\n\n\n…\n…\n…\n…\n…\n…\n\n\nK_M\n1\n1\n1\n…\n0\n\n\n\nCes deux matrices ont l’interprétation suivante :\n\nLa première nous renseigne sur la présence d’un sujet dans un document\nLa seconde nous renseigne sur la présence d’un mot dans un sujet\n\nEn fait, le principe de la LDA est de construire ces deux matrices à partir des fréquences d’apparition des mots dans le texte.\nOn va se concentrer sur Edgar Allan Poe.\n\ncorpus = train_clean[train_clean[\"Author\"] == \"EAP\"]"
  },
  {
    "objectID": "content/NLP/03_lda.html#entraîner-une-lda",
    "href": "content/NLP/03_lda.html#entraîner-une-lda",
    "title": "Latent Dirichlet Allocation (LDA)",
    "section": "Entraîner une LDA",
    "text": "Entraîner une LDA\nIl existe plusieurs manières d’entraîner une LDA.\nNous allons utiliser Scikit ici avec la méthode LatentDirichletAllocation.\nComme expliqué dans la partie modélisation :\n\nOn initialise le modèle ;\nOn le met à jour avec la méthode fit.\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\n# Initialise the count vectorizer with the English stop words\ncount_vectorizer = CountVectorizer(stop_words='english')# Fit and transform the processed titles\ncount_data = count_vectorizer.fit_transform(corpus['tokenized'].apply(lambda s: ' '.join(s)))\n\n# Tweak the two parameters below\nnumber_topics = 5\nnumber_words = 10# Create and fit the LDA model\nlda = LatentDirichletAllocation(n_components=11, max_iter=5,\n                                learning_method = 'online',\n                                learning_offset = 50.,\n                                random_state = 0,\n                                n_jobs = 1)\nlda.fit(count_data)"
  },
  {
    "objectID": "content/NLP/03_lda.html#visualiser-les-résultats",
    "href": "content/NLP/03_lda.html#visualiser-les-résultats",
    "title": "Latent Dirichlet Allocation (LDA)",
    "section": "Visualiser les résultats",
    "text": "Visualiser les résultats\nOn peut déjà commencer par utiliser une fonction pour afficher les\nrésultats :\n\n# Helper function\ndef print_topics(model, count_vectorizer, n_top_words):\n    words = count_vectorizer.get_feature_names_out()\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"\\nTopic #%d:\" % topic_idx)\n        print(\" \".join([words[i]\n                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n\nprint_topics(lda, count_vectorizer, number_words)\n\n\nTopic #0:\narm looking thousand respect hour table woman rest ah seen\n\nTopic #1:\nsaid dupin ha end write smith chair phenomenon quite john\n\nTopic #2:\ntime thing say body matter course day place object immediately\n\nTopic #3:\nmere memory felt sat movement case sole green principle bone\n\nTopic #4:\ndoor room open small friend lady replied night window hand\n\nTopic #5:\nword man day idea good point house shall mind say\n\nTopic #6:\neye figure form left sea hour ordinary life deep world\n\nTopic #7:\nfoot great little earth let le year nature come nearly\n\nTopic #8:\nhand strange head color hair spoken read ear ghastly neck\n\nTopic #9:\ncame looked shadow low dream like death light spirit tree\n\nTopic #10:\neye know heart saw character far tell oh voice wall\n\n\nLa représentation sous forme de liste de mots n’est pas la plus pratique…\nOn peut essayer de se représenter un wordcloud de chaque sujet pour mieux voir si cette piste est pertinente :\n\ntf_feature_names = count_vectorizer.get_feature_names_out()\n\ndef wordcloud_lda(lda, tf_feature_names):\n\n  fig, axs = plt.subplots(len(lda.components_) // 3 + 1, 3)\n  \n  for i in range(len(lda.components_)):\n      corpus_lda = lda.components_[i]\n      first_topic_words = [tf_feature_names[l] for l in corpus_lda.argsort()[:-50-1:-1]]\n      k = i // 3\n      j = (i - k*3)\n      wordcloud = WordCloud(stopwords=stop_words, background_color=\"black\",width = 2500, height = 1800)\n      wordcloud = wordcloud.generate(\" \".join(first_topic_words))\n      axs[k][j].set_title(\"Wordcloud pour le \\nsujet {}\".format(i))\n      axs[k][j].axis('off')\n      axs[k][j].imshow(wordcloud)\n  \n  r = len(lda.components_) % 3\n  [fig.delaxes(axs[len(lda.components_) // 3,k-1]) for k in range(r+1, 3+1) if r != 0]\n\nwc = wordcloud_lda(lda, tf_feature_names)\nwc\n\n\n\n\n\n\n\n\n\nwc\n\nLe module pyLDAvis offre quelques visualisations bien pratiques lorsqu’on\ndésire représenter de manière synthétique les résultats d’une LDA et observer la distribution sujet x mots.\n\n\n Hint\nDans un notebook faire :\nimport pyLDAvis.sklearn\n\npyLDAvis.enable_notebook()\nPour les utilisateurs de Windows, il est nécessaire d’ajouter l’argument\nn_jobs = 1. Sinon, Python tente d’entraîner le modèle avec de la\nparallélisation. Le problème est que les processus sont des FORKs, ce que\nWindows ne supporte pas. Sur un système Unix (Linux, Mac OS), on peut se passer de cet\nargument.\n\n\n\n#!pip install pyLDAvis #à faire en haut du notebook sur colab\nimport pyLDAvis\nimport pyLDAvis.sklearn\n\n# pyLDAvis.enable_notebook()\nvis_data = pyLDAvis.sklearn.prepare(lda, count_data, count_vectorizer, n_jobs = 1)\npyLDAvis.display(vis_data)\n\n\nChaque bulle représente un sujet. Plus la bulle est grande, plus il y a de documents qui traitent de ce sujet.\n\nPlus les barres sont loin les unes des autres, plus elles sont différentes. Un bon modèle aura donc tendance à avoir de grandes bulles qui ne se recoupent pas. Ce n’est pas vraiment le cas ici…\n\nLes barres bleues représentent la fréquence de chaque mot dans le corpus.\nLes barres rouges représentent une estimation du nombre de termes générés dans un sujet précis. La barre rouge la plus longue correspond au mot le plus utilisé dans ce sujet."
  },
  {
    "objectID": "content/NLP/03_lda.html#références",
    "href": "content/NLP/03_lda.html#références",
    "title": "Latent Dirichlet Allocation (LDA)",
    "section": "Références",
    "text": "Références\n\nLe poly d’Alberto Brietti"
  },
  {
    "objectID": "content/NLP/05_exo_supp.html",
    "href": "content/NLP/05_exo_supp.html",
    "title": "Exercices supplémentaires",
    "section": "",
    "text": "Cette page approfondit certains aspects présentés dans les autres tutoriels. Il s’agit d’une suite d’exercice, avec corrections, pour présenter d’autres aspects du NLP ou pratiquer sur des données différentes\n\nExploration des libellés de l’openfood database\n{{% box status=“exercise” title=“Exercise: les noms de produits dans l’openfood database” icon=“fas fa-pencil-alt” %}}\nL’objectif de cet exercice est d’analyser les termes les plus fréquents\ndans les noms de produits de l’openfood database. Au passage, cela permet de réviser les étapes de preprocessing (LIEN XXXXX) et d’explorer les enjeux de reconnaissance d’entités nommées.\n{{% /box %}}\nDans cet exercice:\n\ntokenisation (nltk)\nretrait des stop words (nltk)\nnuage de mots (wordcloud)\nreconnaissance du langage (fasttext)\nreconnaissance d’entités nommées (spacy)\n\nle tout sur l’OpenFood Database, une base de données alimentaire qui est enrichie de manière collaborative.\n{{% box status=“hint” title=“Hint” icon=“fa fa-lightbulb” %}}\nPour pouvoir utiliser les modèles pré-entraînés de spaCy, il faut les télécharger. La méthode préconisée est d’utiliser, depuis un terminal, la commande suivante\npython -m spacy download fr_core_news_sm\nDans un notebook jupyter, il se peut qu’il soit nécessaire de relancer le kernel.\nSi l’accès à la ligne de commande n’est pas possible, ou si la commande échoue, il est possible de télécharger le modèle pré-entraîné directement depuis une session Python\nimport spacy\nspacy.cli.download('fr_core_news_sm')\n{{% /box %}}\n\nImporter le modèle de reconnaissance de langage qui sera utilisé par la suite\nainsi que le corpus Français utilisé par spacy\n\n\nimport tempfile\nimport os\nimport spacy\n\ntemp_dir = tempfile.NamedTemporaryFile()\ntemp_dir = temp_dir.name\n\nos.system(\"wget -O {} https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\".format( \"%s.model.bin\" % temp_dir))\nspacy.cli.download('fr_core_news_sm')\n\n\nImporter les données de l’openfood database à partir du code suivant\n\n\nimport pandas as pd\nimport urllib.request\n\n\nurllib.request.urlretrieve('https://static.openfoodfacts.org/data/en.openfoodfacts.org.products.csv', \"%s.openfood.csv\" % temp_dir)\ndf_openfood = pd.read_csv(\"%s.openfood.csv\" % temp_dir, delimiter=\"\\t\",\n                          usecols=['product_name'], encoding = 'utf-8', dtype = \"str\")\n\nCes données devraient avoir l’aspect suivant :\n\ndf_openfood.iloc[:2, :5]\n\n\nCréer une fonction de nettoyage des noms de produits effectuant les\nétapes suivantes :\n\n\ntokeniser le texte en question\nretirer la ponctuation et les stopwords\n\nAppliquer cette fonction à l’ensemble des noms de produits (variable\nproduct_name)\n\nEffectuer un nuage de mot sur les libellés avant et après nettoyage\npour comprendre la structure du corpus en question.\nLe résultat devrait avoir l’apparence suivante\n\n\nimport wordcloud as wc\nimport matplotlib.pyplot as plt\n\n\ndef graph_wordcloud(data, by = None, valueby = None, yvar = \"Text\"):\n    if (by is not None) & (valueby is not None):        \n        txt = data[data[by]==valueby][yvar].astype(str)\n    else:\n        txt = data[yvar].astype(str)\n    all_text = ' '.join([text for text in txt])\n    wordcloud = wc.WordCloud(width=800, height=500,\n                          random_state=21,\n                      max_words=2000).generate(all_text)\n    return wordcloud\n\ndef graph_wordcloud_by(data, by, yvar = \"Text\"):\n    n_topics = data[by].unique().tolist()\n    width=20\n    height=80\n    rows = len(n_topics)//2\n    cols = 2\n    fig=plt.figure(figsize=(width, height))\n    axes = []\n    for i in range(cols*rows):\n        b = graph_wordcloud(data, by = by, valueby = n_topics[i], yvar = yvar)\n        axes.append( fig.add_subplot(rows, cols, i+1) )\n        axes[-1].set_title(\"{}\".format(n_topics[i]))  \n        plt.imshow(b)\n        plt.axis('off')\n        plt.savefig('{}.png'.format(yvar), bbox_inches='tight')\n\n\ndef wordcount_words(data, yvar, by = None):\n    plt.figure( figsize=(15,15) )\n    if by is None:\n        wordcloud = graph_wordcloud(data, yvar = yvar, by = by)\n        plt.imshow(wordcloud)\n        plt.axis('off')\n        plt.savefig('{}.png'.format(yvar), bbox_inches='tight')\n    else:\n        graph_wordcloud_by(data, by = by, yvar = yvar)\n\nwordcount_words(df_openfood, yvar = \"product_name\")\nwordcount_words(df_openfood, \"tokenized\")\n\n\nUtiliser la librairie Fasttext pour extraire les noms de produits\nfrançais\n\n\nAppliquer le modèle téléchargé précédemment pour déterminer le langage\nNe récupérer que les libellés français\n\n\nimport fasttext\n\nPRETRAINED_MODEL_PATH = \"%s.model.bin\" % temp_dir\nmodel = fasttext.load_model(PRETRAINED_MODEL_PATH)\nnewcols = ['language','score_language']\ndf_openfood[newcols] = pd.DataFrame(df_openfood['product_name'].astype(str).apply(lambda s: list(model.predict(s))).apply(lambda l: [l[0][0],l[1][0]]).tolist(), columns = newcols)\ndf_openfood['language'] = df_openfood['language'].str.replace(\"__label__\",\"\")\ndf_openfood_french = df_openfood[df_openfood['language'] == \"fr\"]\ndf_openfood_french.head(2)\n\n\nVisualiser avec spacy.displacy le résultat d’une reconnaissance\nd’entités nommées sur 50 données aléatoires. Cela vous semble-t-il satisfaisant ?\n\n\nimport spacy\nimport fr_core_news_sm\n\nnlp = fr_core_news_sm.load()\n\nexample = \" \\n \".join(df_openfood_french['product_name'].astype(\"str\").sample(50))\n\nfrom spacy import displacy\nhtml = displacy.render(nlp(example), style='ent', page=True)\n\n\nprint(html)\n\n\nRécupérer dans un vecteur les entités nommées reconnues par spaCy.\nRegarder les entités reconnues dans les 20 premiers libellés de produits\n\n\nx = []\nfor doc in nlp.pipe(df_openfood_french.head(20)['product_name'].astype(\"unicode\"), disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n    # Do something with the doc here\n    x.append([(ent.text, ent.label_) for ent in doc.ents])\n    \nx\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@book{galiana2023,\n  author = {Galiana, Lino},\n  title = {Python Pour La Data Science},\n  date = {2023},\n  url = {https://pythonds.linogaliana.fr/},\n  doi = {10.5281/zenodo.8229676},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGaliana, Lino. 2023. Python Pour La Data Science. https://doi.org/10.5281/zenodo.8229676."
  },
  {
    "objectID": "content/modern-ds/continuous_integration.html",
    "href": "content/modern-ds/continuous_integration.html",
    "title": "Intégration continue avec Python",
    "section": "",
    "text": "Cette page sera actualisée prochainement, une version plus à jour et plus complète peut être trouvée sur https://ensae-reproductibilite.github.io/website/\nL’un des apports principaux des innovations\nrécentes de la data science est la\nmanière dont des projets, malgré\nleur complexité, peuvent facilement\nêtre converti en projets pérennes\nà partir\nd’un prototype bien construit.\nEn s’inspirant de l’approche devops ,\nméthode de travail qui consiste à adopter un certain\nnombre de gestes pour\nautomatiser la production de livrables ou de tests\ndès la\nconception du produit, les data scientists\nont adopté une méthode de travail très efficace\npour favoriser la réutilisation de leur travail\npar d’autres équipes que celles à l’origine de\nla conception du protype initial.\nCette approche devops a été reprise et étendue\npour donner un autre buzz-word, le MLops.\nIl s’agit d’une approche qui vise à créer\net mettre à disposition des modèles de machine\nlearning de manière fiable et automatisée\nà chaque nouvelle étape du projet, en parallèle\nde la mise à jour du code ayant produit ces\noutput.\nCes nouvelles méthodes de travail permettent\ndes gains substantiels de productivité\npour les équipes développant des modèles\net réduit fortement le coût de reprise d’un\ncode par une équipe en charge de sa\npérenisation. Ce coût est en effet le principal\nfrein à la mise en production de nouveaux\nprojets ce qui peut représenter un gâchis\nnon négligeable de temps et de ressources.\nComme nous l’expliquons avec Romain Avouac\ndans un cours de dernière année de l’ENSAE\n(https://ensae-reproductibilite.github.io/website/),\nl’adoption de certaines bonnes pratiques\nde développement de code et d’une démarche\nexploitant les dernières innovations de\nla data science peut substantiellement\naugmenter les chances d’un succès\nd’un projet. Le nouveau paradigme, qui\nconsiste à intégrer en amont du projet\ncertaines contraintes de la production\net tester continuellement la manière dont les\nlivrables évoluent, évite que la mise\nen production d’un projet, qui est coûteuse\nen temps et en ressources, n’aboutisse qu’au\nmoment où le projet est déjà caduc\n(car les données ou les besoins ont évolués…)."
  },
  {
    "objectID": "content/modern-ds/continuous_integration.html#fonctionnement-des-actions-github",
    "href": "content/modern-ds/continuous_integration.html#fonctionnement-des-actions-github",
    "title": "Intégration continue avec Python",
    "section": "Fonctionnement des actions Github",
    "text": "Fonctionnement des actions Github\nLes actions Github fonctionnent par couches successives au sein desquelles\non effectue un certain nombre d’instructions.\nLa meilleure manière d’apprendre les actions Github est, certes, de lire la\ndocumentation officielle mais surtout,\nà mon avis, de regarder quelques pipelines pour comprendre la démarche.\nL’un des intérêts des Github Actions est la possibilité d’avoir un pipeline\nproposant une intrication de langages différents pour avoir une chaine de\nproduction qui propose les outils les plus efficaces pour répondre à un\nobjectif en limitant les verrous techniques.\nPar exemple, le pipeline de ce cours, disponible\nsur Github propose une intrication des langages\nPython et R avec des technologies Anaconda (pour contrôler\nl’environnement Python comme expliqué dans les chapitres précédents)\net Javascript (pour le déploiement d’un site web avec le service tiers\nNetlify)2. Cette chaîne de production multi-langage permet que\nles mêmes fichiers sources génèrent un site web et des notebooks disponibles\nsur plusieurs environnements.\n\n\nname: Production deployment\n\non:\n  push:\n    branches:\n      - main\n      - master\n      - light\n\njobs:\n  pages:\n    name: Render-Blog\n    runs-on: ubuntu-latest\n    container: linogaliana/python-datascientist:latest\n    if: ${{ !github.event.pull_request.head.repo.fork }}\n    steps:\n      - uses: actions/checkout@v2\n        with:\n          #fetch-depth: 0\n          ref: ${{ github.event.pull_request.head.ref }}\n          repository: ${{github.event.pull_request.head.repo.full_name}}\n      - name: Configure safe.directory  # Workaround for actions/checkout#760\n        run: git config --global --add safe.directory /__w/python-datascientist/python-datascientist\n      - name: Render website\n        run: |\n          quarto render --to html\n      - name: Publish to Pages\n        if: github.ref == 'refs/heads/master'\n        run: |\n          git config --global user.email quarto-github-actions-publish@example.com\n          git config --global user.name \"Quarto GHA Workflow Runner\"\n          quarto publish gh-pages . --no-render --no-browser\n\n\n\nLes couches qui constituent les étapes du pipeline\nportent ainsi le nom de steps. Un step peut comporter un certain\nnombre d’instructions ou exécuter des instructions pré-définies.\nL’une de ces instructions prédéfinies est, par exemple,\nl’installation de Python\nou l’initialisation d’un environnement conda.\nLa documentation officielle de Github propose un\nfichier qui peut servir de modèle\npour tester un script Python voire l’uploader de manière automatique\nsur Pypi."
  },
  {
    "objectID": "content/modern-ds/continuous_integration.html#intégration-continue-avec-python-tester-un-notebook",
    "href": "content/modern-ds/continuous_integration.html#intégration-continue-avec-python-tester-un-notebook",
    "title": "Intégration continue avec Python",
    "section": "Intégration continue avec Python: tester un notebook",
    "text": "Intégration continue avec Python: tester un notebook\nCette section n’est absolument pas exhaustive. Au contraire, elle ne fournit\nqu’un exemple minimal pour expliquer la logique de l’intégration continue. Il\nne s’agit ainsi pas d’une garantie absolue de reproductibilité d’un notebook.\nGithub propose une action officielle pour utiliser Python dans un\npipeline d’intégration continue. Elle est disponible sur le\nMarketPlace Github.\nIl s’agit d’un bon point de départ, à enrichir.\nLe fichier qui contrôle les instructions exécutées dans l’environnement Actions\ndoit se trouver dans le dossier .github/workflows/\n(:warning: ne pas oublier le point au début du\nnom du dossier). Il doit être au format YAML avec une extension .yml\nou .yaml.\nIl peut avoir n’importe quel nom néanmoins il\nvaut mieux lui donner un nom signifiant,\npar exemple prod.yml pour un fichier contrôlant une chaîne de production.\n\nLister les dépendances\nAvant d’écrire les instructions à exécuter par Github, il faut définir un\nenvironnement d’exécution car Github ne connaît pas la configuration Python\ndont vous avez besoin.\nIl convient ainsi de lister les dépendances nécessaires dans un fichier\nrequirements.txt (si on utilise un environnement virtuel)\nou un fichier environment.yml (si on préfère\nutiliser un environnement conda).\nBien que le principe sous-jacent soit légèrement différent,\nces fichiers ont la même fonction:\npermettre la création d’un environnement ex-nihilo\navec un certain nombre de dépendances pré-installées3.\nSi on fait le choix de l’option environment.yml,\nle fichier prendra ainsi la forme\nsuivante, à enrichir en fonction de la\nrichesse de l’environnement souhaité. :\nchannels:\n  - conda-forge\n\ndependencies:\n  - python&gt;=3.10\n  - jupyter\n  - jupytext\n  - matplotlib\n  - nbconvert\n  - numpy\n  - pandas\n  - scipy\n  - seaborn\nLe même fichier sous le format requirements.txt aura\nla forme suivante :\njupyter\njupytext\nmatplotlib\nnbconvert\nnumpy\npandas\nscipy\nseaborn\nSous leur apparente équivalence, au-delà de\nla question du formatage, ces fichiers ont\ndeux différences principales :\n\nla version minimale de Python est définie dans\nle fichier environment.yml alors qu’elle ne l’est\npas dans un fichier requirements.txt. C’est\nparce que le second installe les dépendances dans\nun environnement déjà existant par ailleurs alors\nque le premier peut servir à créer l’environnement\navec une certaine configuration de Python ;\nle mode d’installation des packages n’est pas le\nmême. Avec un environment.yml on installera des\npackages via conda alors qu’avec un requirements.txt\non privilégiera plutôt pip4.\n\nDans le cas de l’environnement conda,\nle choix du channel conda-forge vise à contrôler le dépôt utilisé par\nAnaconda.\n\n\n Hint\nLa conda forge est un dépôt de package alternatif\nau canal par défaut d’Anaconda qui est maintenu par\nl’équipe de développeurs officiels d’Anaconda.\nComme cette dernière cherche en priorité à\nassurer la stabilité de l’écosystème Anaconda,\nles versions de package évoluent moins vite\nque le rythme voulu par les développeurs de\npackages. Pour cette raison, un dépôt\nalternatif, où les montées de version sont\nplus simples parce qu’elles dépendent des\ndéveloppeurs de chaque package, a émergé.\nIl s’agit de la conda forge. Lorsqu’on\ndésire utiliser des fonctionalités récentes\nde l’écosystème de la data science,\nil est conseillé de l’utiliser.\n\n\nNe pas oublier de mettre ce fichier sous contrôle de version et de l’envoyer\nsur le dépôt par un push.\n\n\nCréer un environnement reproductible dans Github Actions\nDeux approches sont possibles à ce niveau, selon le degré\nde reproductibilité désiré5:\n\nCréer l’environnement via une action existante. L’action\nconda-incubator/setup-miniconda@v2\nest un bon point de départ.\nCréer l’environnement dans une image Docker.\n\nLa deuxième solution permet de contrôler de manière\nbeaucoup plus fine l’environnement dans lequel\nPython s’éxécutera ainsi que la manière dont\nl’environnement sera créé6. Néanmoins, elle nécessite\ndes connaissances plus poussées dans la principe\nde la conteneurisation qui peuvent être coûteuses\nà acquérir. Selon l’ambition du projet, notamment\nles réutilisation qu’il désire,\nun data scientist pourra privilégier\ntelle ou telle option. Les deux solutions sont présentées\ndans l’exemple fil-rouge du cours que nous\ndonnons avec Romain Avouac\n(https://ensae-reproductibilite.github.io/website/application/).\n\n\nTester un notebook myfile.ipynb\nDans cette partie, on va supposer que le notebook à tester s’appelle myfile.ipynb\net se trouve à la racine du dépôt. Les\ndépendances pour l’exécuter sont\nlistées dans un fichier requirements.txt.\nLe modèle suivant, expliqué en dessous, fournit un modèle de recette pour\ntester un notebook. Supposons que ce fichier soit présent\ndans un chemin .github/workflows/test-notebook.yml\n\n\nEnvironnement virtuel\n\nname: Test notebook execution using Github Actions\n\non: [push]\n\njobs:\n  build-linux:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up Python 3.10\n      uses: actions/setup-python@v3\n      with:\n        python-version: '3.10'\n      - shell: bash\n      run: |\n        python --version\n    - name: Install dependencies\n      run:\n        pip install -r requirements.txt\n        pip install jupyter nbconvert\n    - name: Test jupyter from command line\n      run:\n        jupyter nbconvert --execute --to notebook --inplace myfile.ipynb\n    - uses: actions/upload-artifact@v3\n      with:\n        name: Notebook\n        path: myfile.ipynb\n        retention-days: 5\n\n\n\nEnvironnement conda\n\nname: Test notebook execution using Github Actions\n\non: [push]\n\njobs:\n  build-linux:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up Python 3.10\n      uses: actions/setup-python@v3\n      with:\n        python-version: '3.10'\n    - name: Add conda to system path\n      run: |\n        # $CONDA is an environment variable pointing to the root of the miniconda directory\n        echo $CONDA/bin &gt;&gt; $GITHUB_PATH\n    - name: Install dependencies\n      run: |\n        conda env update --file environment.yml --name base\n        conda install jupyter nbconvert\n    - name: Test jupyter from command line\n      run:\n        jupyter nbconvert --execute --to notebook --inplace myfile.ipynb\n    - uses: actions/upload-artifact@v3\n      with:\n        name: Notebook\n        path: myfile.ipynb\n        retention-days: 5\n\nDans les deux cas, la démarche est la même:\n\non récupère les fichiers présents dans le dépôt\n(action checkout) ;\non installe Python ;\non installe les dépendances pour exécuter le code.\nDans l’approche conda, il est également nécessaire\nde faire quelques configurations supplémentaires (notamment\najouter conda aux logiciels reconnus par la ligne\nde commande) ;\non teste le notebook en ligne de commande et remplace\ncelui existant, sur la machine temporaire, par la version\nproduite sur cet environnement neutre.\non rend possible le téléchargement du\nnotebook produit automatiquement pendant 5 jours7. Ceci\nrepose sur les artefacts qui sont un élément récupéré\ndes machines temporaires qui n’existent plus dès que le\ncode a fini d’être exécuté.\n\nCes actions sont exécutées à chaque interaction avec\nle dépôt distant (push), quelle que soit la\nbranche. A partir de ce modèle, il est possible de\nraffiner pour, par exemple, automatiquement\nfaire un commit du notebook validé et le pusher\nvia le robot Github8"
  },
  {
    "objectID": "content/modern-ds/continuous_integration.html#les-services-de-mise-à-disposition-de-github-et-gitlab",
    "href": "content/modern-ds/continuous_integration.html#les-services-de-mise-à-disposition-de-github-et-gitlab",
    "title": "Intégration continue avec Python",
    "section": "Les services de mise à disposition de Github et Gitlab",
    "text": "Les services de mise à disposition de Github et Gitlab\nGithub et Gitlab, les deux plateformes de partage\nde code, proposent non seulement des services\ngratuits d’intégration continue mais aussi des services\nde mise à disposition de sites web pleinement intégrés\naux services de stockage de code.\nCes services, Gitlab Pages et Github Pages, auxquels\non peut associer le service externe Netlify qui répond\nau même principe9 permettent, à chaque modification\ndu code source d’un projet, de reconstruire le site web (le livrable)\nqui peut être directement produit à partir de certains fichiers\n(des slides revealJS par exemple) ou qui\nsert d’output à l’intégration continue après compilation\nde fichiers plus complexes (des fichiers quarto par exemple).\nChaque dépôt sur Github ou Gitlab peut ainsi être associé\nà un URL de déploiement disponible sur internet. A chaque\ncommit sur le dépôt, le site web qui sert de livrable\nest ainsi mis à jour. La version déployée à partir de la\nbranche principale peut ainsi être considérée\ncomme la version de production alors que les branches\nsecondaires peuvent servir d’espace bac à sable pour\nvérifier que des changements dans le code source\nne mettent pas en péril le livrable. Cette méthode,\nqui sécurise la production d’un livrable sous forme\nde site web, est ainsi particulièrement appréciable."
  },
  {
    "objectID": "content/modern-ds/continuous_integration.html#les-services-externes-disponibles-sans-infrastructure-spéciale",
    "href": "content/modern-ds/continuous_integration.html#les-services-externes-disponibles-sans-infrastructure-spéciale",
    "title": "Intégration continue avec Python",
    "section": "Les services externes disponibles sans infrastructure spéciale",
    "text": "Les services externes disponibles sans infrastructure spéciale\nPour fonctionner, l’intégration continue\nnécessite de mettre en oeuvre des environnements normalisés.\nComme évoqué précédemment,\nla technologie sous-jacente est celle de la conteneurisation.\nLes images qui servent de point de départ au lancement\nd’un conteneur sont elles-mêmes mises à disposition\ndans des espaces communautaires (des registres d’images).\nIl en existe plusieurs, les plus connus étant\nle dockerhub ou le registry de Gitlab.\nCes registres servent d’espaces de stockage pour des images,\nqui sont des objets volumineux (potentiellement plusieurs\nGigas) mais aussi d’espace de mutualisation en permettant\nà d’autres de réutiliser une image prête à l’emploi ou,\nau contraire, à partir de\nlaquelle on peut ajouter un certain nombre de couches\npour obtenir l’environnement minimal\nde reproductibilité.\nIl est possible d’utiliser certaines actions Github\nprête à l’emploi pour constuire une image Docker\nà partir d’un fichier Dockerfile. Après avoir\ncrée une connexion entre un compte sur la\nplateforme Github et l’autre sur DockerHub,\nune mise à disposition automatisée d’un livrable\nsous forme d’image Docker est ainsi possible.\nUne image Docker peut offrir une grande variété\nd’output. Elle peut servir uniquement à\nmettre à disposition un environnement de\nreproductibilité mais elle peut servir à mettre\nà disposition, pour les personnes maîtrisant\nDocker, des output plus raffinés. Par exemple,\ndans le cours que nous donnons à l’ENSAE, nous\nmontrons comment docker peut servir à\nmettre à disposition à un utilisateur tiers\nune application minimaliste (construite avec flask)\nqu’il fera tourner\nsur son ordinateur.\nSi une image Docker peut être très utile pour la mise\nà disposition, elle nécessite pour sa réutilisation\nun niveau avancé d’expertise en programmation.\nCela ne conviendra pas à tous les publics. Certains\nne désireront que bénéficier d’une application interactive\noù ils pourrons visualiser certains résultats en fonction\nd’actions comme des filtres sur des sous-champs ou le choix\nde certaines plages de données. D’autres publics seront\nplutôt intéressé par la réutilisation d’un programme\nou des résultats d’un modèle sous forme d’API mais n’auront\npas l’infrastructure interne pour faire tourner le code\nd’origine ou une image Docker. C’est pour répondre à ces\nlimites qu’il peut devenir intéressant, pour une équipe\nde data science de développer une architecture\nkubernetes interne, si l’organisation en a les moyens, ou\nde payer un fournisseur de service, comme AWS, qui permet\ncela."
  },
  {
    "objectID": "content/modern-ds/continuous_integration.html#kubernetes-le-sommet-de-la-pente-du-déploiement",
    "href": "content/modern-ds/continuous_integration.html#kubernetes-le-sommet-de-la-pente-du-déploiement",
    "title": "Intégration continue avec Python",
    "section": "Kubernetes: le sommet de la pente du déploiement",
    "text": "Kubernetes: le sommet de la pente du déploiement\nKubernetes est une technologie qui pousse la logique\nde la conteneurisation à son paroxysme.\nIl s’agit d’un système open-source, développé\npar Google, permettant\nd’automatiser le déploiement, la mise à l’échelle\net la gestion d’applications conteneurisées.\nGrâce à Kubernetes, une application, par exemple\nun site web proposant de la réactivité,\npeut être mise à disposition et reporter les calculs,\nlorsqu’ils sont nécessaires, sur\nun serveur. L’utilisation de Kubernetes dans\nun projet de data science permet ainsi\nd’anticiper à la fois l’interface d’une application\nvalorisant un projet mais aussi le fonctionnement\ndu back-office, par exemple en testant la capacité\nde charge de cette application. Une introduction\nà Kubernetes orienté donnée peut être trouvée dans\nle cours dédié à la mise en production\nque nous donnons avec Romain Avouac et dans ce\npost de blog très bien fait.\nDans les grandes organisations, où les rôles sont\nplus spécialisés que dans les petites structures,\nce ne sont pas nécessairement les data scientists\nqui devront maîtriser Kubernetes mais plutôt\nles data-architect ou les data-engineer. Néanmoins,\nles data scientists devront être capable de\ndialoguer avec eux et mettre en oeuvre une méthode\nde travail adaptée (celle-ci reposera en principe sur\nl’approche CI/CD). Dans les petites structures, les\ndata scientist peuvent être en mesure\nde mettre en oeuvre le déploiement en continu. En\nrevanche, il est plus rare, dans ces structures,\noù les moyens humains de maintenance sont limités,\nque les serveurs sur lesquels fonctionnent Kubernetes\nsoient détenus en propres. En général, ils sont loués\ndans des services de paiement à la demande de type\nAWS."
  },
  {
    "objectID": "content/modern-ds/continuous_integration.html#footnotes",
    "href": "content/modern-ds/continuous_integration.html#footnotes",
    "title": "Intégration continue avec Python",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCes services d’intégration continue étaient utilisés lorsque Github\nne proposait pas encore de service intégré, comme le faisait Gitlab.\nIls sont de moins en moins fréquemment utilisés.↩︎\nPour réduire le temps nécessaire pour construire le site web, ce\npipeline s’appuie sur un environnement Docker construit sur un autre dépôt\ndisponible également sur Github\n.\nCelui-ci part d’une configuration système Linux et construit un environnement\nAnaconda à partir d’un fichier environment.yml qui liste toutes les dépendances\nnécessaires pour exécuter les morceaux de code du site web.\nCet environnement Anaconda est construit grâce à l’outil mamba qui permet\nd’aller beaucoup plus vite dans la constitution d’environnements que ne le\npermet conda.↩︎\nSur la différence entre les environnements virtuels\net les environnements conda, voir\ncette partie de cours\nplus avancé que nous donnons\navec Romain Avouac sur la mise en production\nde projets data science.↩︎\nIl est possible d’installer une partie des packages\navec pip en définissant un champ pip dans le\nfichier environment.yml. Néanmoins, les concepteurs\nd’Anaconda recommandent d’être prudent avec cette méthode\nqui présente certes l’avantage d’accélérer le temps de\ncréation de l’environnement mais peut créer des\ndifficultés avec des librairies nécessitant d’autres\nlangages système comme le C.↩︎\nLe point de vue que nous défendons avec\nRomain Avouac dans notre cours sur la reproductibilité\nest qu’il s’agit d’un continuum dans lequel on investit\nplus ou moins en fonction de ses contraintes, de ses\nbesoins, de ses compétences, du temps humain qu’on\npeut dédier à développer des output reproductibles\net le temps gagné en développant une telle approche.\nSelon où on se trouve sur ce cursus, en fonction\ndes solutions déjà existantes qu’on peut trouver\nsur internet, on va plus ou moins raffiner\nnotre intégration et nos déploiements\ncontinus.↩︎\nIl est recommandé de ne pas garder la période de rétention\ndes artefacts par défaut car celle-ci est assez longue (90 jours).\nLes output pouvant être assez volumineux et expirant rapidement\n(en général ce qui nous intéresse est la dernière ou l’avant\ndernière version de l’_output), pour des raisons écologiques,\nil est recommandé de fixer des périodes courtes. Cela peut être\nfait directement dans le fichier configurant l’intégration\ncontinue comme ici ou dans les paramètres par défaut\ndu dépôt pour que cette règle s’applique à toutes les\nproductions faites par intégration continue.↩︎\nIl est recommandé de ne pas garder la période de rétention\ndes artefacts par défaut car celle-ci est assez longue (90 jours).\nLes output pouvant être assez volumineux et expirant rapidement\n(en général ce qui nous intéresse est la dernière ou l’avant\ndernière version de l’_output), pour des raisons écologiques,\nil est recommandé de fixer des périodes courtes. Cela peut être\nfait directement dans le fichier configurant l’intégration\ncontinue comme ici ou dans les paramètres par défaut\ndu dépôt pour que cette règle s’applique à toutes les\nproductions faites par intégration continue.↩︎\nIl s’agit du service utilisé, par exemple,\npour ce cours. Netlify est un service de mise à disposition\nqui offre des fonctionalités plus complètes que celles\npermises par Gitlab Pages et Github Pages. Outre cet\navantage, il est plus facile à configurer que Github Pages\nqui nécessite l’usage d’une branche dédiée nommée gh-pages,\nce qui peut\nrebutant.↩︎\nIl s’agit du service utilisé, par exemple,\npour ce cours. Netlify est un service de mise à disposition\nqui offre des fonctionalités plus complètes que celles\npermises par Gitlab Pages et Github Pages. Outre cet\navantage, il est plus facile à configurer que Github Pages\nqui nécessite l’usage d’une branche dédiée nommée gh-pages,\nce qui peut\nrebutant.↩︎"
  },
  {
    "objectID": "content/modern-ds/s3.html",
    "href": "content/modern-ds/s3.html",
    "title": "Les nouveaux modes d’accès aux données : le format parquet et les données sur le cloud",
    "section": "",
    "text": "Ce chapitre est une introduction à la question\ndu stockage des données et aux innovations\nrécentes dans ce domaine. L’objectif\nest d’abord de présenter les avantages\ndu format Parquet et la manière dont\non peut utiliser les\nlibrairies pyarrow\nou duckdb pour traiter\nde manière efficace des données volumineuses\nau format Parquet. Ensuite, on présentera\nla manière dont ce format parquet s’intègre\nbien avec des systèmes de stockage cloud,\nqui tendent à devenir la norme dans le monde\nde la data science."
  },
  {
    "objectID": "content/modern-ds/s3.html#principe-du-stockage-de-la-donnée",
    "href": "content/modern-ds/s3.html#principe-du-stockage-de-la-donnée",
    "title": "Les nouveaux modes d’accès aux données : le format parquet et les données sur le cloud",
    "section": "Principe du stockage de la donnée",
    "text": "Principe du stockage de la donnée\nPour comprendre les apports du format Parquet, il est nécessaire\nde faire un détour pour comprendre la manière dont une information\nest stockée et accessible à un langage de traitement de la donnée.\nIl existe deux approches dans le monde du stockage de la donnée.\nLa première est celle de la base de données relationnelle. La seconde est le\nprincipe du fichier.\nLa différence entre les deux est dans la manière dont l’accès aux\ndonnées est organisé."
  },
  {
    "objectID": "content/modern-ds/s3.html#les-fichiers",
    "href": "content/modern-ds/s3.html#les-fichiers",
    "title": "Les nouveaux modes d’accès aux données : le format parquet et les données sur le cloud",
    "section": "Les fichiers",
    "text": "Les fichiers\nDans un fichier, les données sont organisées selon un certain format et\nle logiciel de traitement de la donnée va aller chercher et structurer\nl’information en fonction de ce format. Par exemple, dans un fichier\n.csv, les différentes informations seront stockées au même niveau\navec un caractère pour les séparer (la virgule , dans les .csv anglosaxons, le point virgule dans les .csv français, la tabulation dans les .tsv). Le fichier suivant\nnom ; profession \nAstérix ; \nObélix ; Tailleur de menhir ;\nAssurancetourix ; Barde\nsera ainsi organisé naturellement sous forme tabulée par Python\n\n\n\n\n\n\n\n\n\nnom\nprofession\n\n\n\n\n0\nAstérix\n\n\n\n1\nObélix\nTailleur de menhir\n\n\n2\nAssurancetourix\nBarde\n\n\n\n\n\n\n\nA propos des fichiers de ce type, on parle de fichiers plats car\nles enregistrements relatifs à une observation sont stockés ensemble,\nsans hiérarchie.\nCertains formats de données vont permettre d’organiser les informations\nde manière différente. Par exemple, le format JSON va\nhiérarchiser différemment la même information [^1]:\n[\n  {\n    \"nom\": \"Astérix\"\n  },\n  {\n    \"nom\": \"Obélix\",\n    \"profession\": \"Tailleur de menhir\"\n  },\n  {\n    \"nom\": \"Assurancetourix\",\n    \"profession\": \"Barde\"\n  }\n]\n\n\n Hint \nLa différence entre le CSV et le format JSON va au-delà d’un simple “formattage” des données.\nPar sa nature non tabulaire, le format JSON permet des mises à jour beaucoup plus facile de la donnée dans les entrepôts de données.\nPar exemple, un site web qui collecte de nouvelles données n’aura pas à mettre à jour l’ensemble de ses enregistrements antérieurs\npour stocker la nouvelle donnée (par exemple pour indiquer que pour tel ou tel client cette donnée n’a pas été collectée)\nmais pourra la stocker dans\nun nouvel item. Ce sera à l’outil de requête (Python ou un autre outil)\nde créer une relation entre les enregistrements stockés à des endroits\ndifférents.\nCe type d’approche flexible est l’un des fondements de l’approche NoSQL,\nsur laquelle nous allons revenir, qui a permis l’émergence de technologies au coeur de l’écosystème actuel du big-data comme Hadoop ou ElasticSearch.\n\n\nCette fois, quand on n’a pas d’information, on ne se retrouve pas avec nos deux séparateurs accolés (cf. la ligne “Astérix”) mais l’information\nn’est tout simplement pas collectée.\n\n\n Note\nIl se peut très bien que l’information sur une observation soit disséminée\ndans plusieurs fichiers dont les formats diffèrent.\nPar exemple, dans le domaine des données géographiques,\nlorsqu’une donnée est disponible sous format de fichier(s), elle peut l’être de deux manières!\n\nSoit la donnée est stockée dans un seul fichier qui mélange contours géographiques et valeurs attributaires\n(la valeur associée à cette observation géographique, par exemple le taux d’abstention). Ce principe est celui du geojson.\nSoit la donnée est stockée dans plusieurs fichiers qui sont spécialisés: un fichier va stocker les contours géographiques,\nl’autre les données attributaires et d’autres fichiers des informations annexes (comme le système de projection). Ce principe est celui du shapefile.\nC’est alors le logiciel qui requête\nles données (Python par exemple) qui saura où aller chercher l’information\ndans les différents fichiers et associer celle-ci de manière cohérente.\n\n\n\nUn concept supplémentaire dans le monde du fichier est celui du file system. Le file system est\nle système de localisation et de nommage des fichiers.\nPour simplifier, le file system est la manière dont votre ordinateur saura\nretrouver, dans son système de stockage, les bits présents dans tel ou tel fichier\nappartenant à tel ou tel dossier."
  },
  {
    "objectID": "content/modern-ds/s3.html#les-bases-de-données",
    "href": "content/modern-ds/s3.html#les-bases-de-données",
    "title": "Les nouveaux modes d’accès aux données : le format parquet et les données sur le cloud",
    "section": "Les bases de données",
    "text": "Les bases de données\nLa logique des bases de données est différente. Elle est plus systémique.\nUn système de gestion de base de données (Database Management System)\nest un logiciel qui gère à la fois le stockage d’un ensemble de données reliée,\npermet de mettre à jour celle-ci (ajout ou suppression d’informations, modification\ndes caractéristiques d’une table…)\net qui gère également\nles modalités d’accès à la donnée (type de requête, utilisateurs\nayant les droits en lecture ou en écriture…).\nLa relation entre les entités présentes dans une base de données\nprend généralement la forme d’un schéma en étoile. Une base va centraliser\nles informations disponibles qui seront ensuite détaillées dans des tables\ndédiées.\n\nSource: La documentation Databricks sur le schéma en étoile\nLe logiciel associé à la base de données fera ensuite le lien\nentre ces tables à partir de requêtes SQL. L’un des logiciels les plus efficaces dans ce domaine\nest PostgreSQL. Python est tout à fait\nutilisable pour passer une requête SQL à un gestionnaire de base de données.\nLes packages sqlalchemy et psycopg2\npeuvent servir à utiliser PostgreSQL pour requêter une\nbase de donnée ou la mettre à jour.\nLa logique de la base de données est donc très différente de celle du fichier.\nCes derniers sont beaucoup plus légers pour plusieurs raisons.\nD’abord, parce qu’ils sont moins adhérents à\nun logiciel gestionnaire. Là où le fichier ne nécessite, pour la gestion,\nqu’un file system, installé par défaut sur\ntout système d’exploitation, une base de données va nécessiter un\nlogiciel spécialisé. L’inconvénient de l’approche fichier, sous sa forme\nstandard, est qu’elle\nne permet pas une gestion fine des droits d’accès et amène généralement à une\nduplication de la donnée pour éviter que la source initiale soit\nré-écrite (involontairement ou de manière intentionnelle par un utilisateur malveillant).\nRésoudre ce problème est l’une des\ninnovations des systèmes cloud, sur lesquelles nous reviendrons en évoquant le\nsystème S3.\nUn deuxième inconvénient de l’approche base de données par\nrapport à l’approche fichier, pour un utilisateur de Python,\nest que les premiers nécessitent l’intermédiation du logiciel de gestion\nde base de données là où, dans le second cas, on va se contenter d’une\nlibrairie, donc un système beaucoup plus léger,\nqui sait comment transformer la donnée brute en DataFrame.\nPour ces raisons, entre autres, les bases de données sont donc moins à la\nmode dans l’écosystème récent de la data science que les fichiers."
  },
  {
    "objectID": "content/modern-ds/s3.html#lire-un-parquet-en-python-la-librairie-pyarrow",
    "href": "content/modern-ds/s3.html#lire-un-parquet-en-python-la-librairie-pyarrow",
    "title": "Les nouveaux modes d’accès aux données : le format parquet et les données sur le cloud",
    "section": "Lire un parquet en Python: la librairie pyarrow",
    "text": "Lire un parquet en Python: la librairie pyarrow\nLa librairie pyarrow permet la lecture et l’écriture\nde fichiers parquet avec Python1. Elle repose\nsur un type particulier de dataframe, le pyarrow.Table\nqui peut être utilisé en substitut ou en complément\ndu DataFrame\nde pandas. Il est recommandé de régulièrement\nconsulter la documentation officielle de pyarrow\nconcernant la lecture et écriture de fichiers et celle relative\naux manipulations de données.\nPour illustrer les fonctionalités de pyarrow,\nrepartons de notre CSV initial que nous allons\nenrichir d’une nouvelle variable numérique\net que nous\nallons\nconvertir en objet pyarrow avant de l’écrire au format parquet:\n\nimport pandas as pd\nfrom io import StringIO \nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\ns = \"\"\"\nnom;cheveux;profession\nAstérix;blond;\nObélix;roux;Tailleur de menhir\nAssurancetourix;blond;Barde\n\"\"\"\n\nsource = StringIO(s)\n\ndf = pd.read_csv(source, sep = \";\", index_col=False)\ndf[\"taille\"] = [155, 190, 175]\ntable = pa.Table.from_pandas(df)\n\ntable\n\npq.write_table(table, 'example.parquet')\n\n\n\n Hint \nL’utilisation des noms pa pour pyarrow et pq pour\npyarrow.parquet est une convention communautaire\nqu’il est recommandé de suivre.\n\n\nPour importer et traiter ces données, on peut conserver\nles données sous le format pyarrow.Table\nou transformer en pandas.DataFrame. La deuxième\noption est plus lente mais présente l’avantage\nde permettre ensuite d’appliquer toutes les\nmanipulations offertes par l’écosystème\npandas qui est généralement mieux connu que\ncelui d’Arrow.\nSupposons qu’on ne s’intéresse qu’à la taille et à la couleur\nde cheveux de nos gaulois.\nIl n’est pas nécessaire d’importer l’ensemble de la base, cela\nferait perdre du temps pour rien. On appelle\ncette approche le column pruning qui consiste à\nne parcourir, dans le fichier, que les colonnes qui nous\nintéressent. Du fait du stockage orienté colonne du parquet,\nil suffit de ne considérer que les blocs qui nous\nintéressent (alors qu’avec un CSV il faudrait scanner tout\nle fichier avant de pouvoir éliminer certaines colonnes).\nCe principe du column pruning se matérialise avec\nl’argument columns dans parquet.\nEnsuite, avec pyarrow, on pourra utiliser pyarrow.compute pour\neffectuer des opérations directement sur une table\nArrow :\n\nimport pyarrow.compute as pc\n\ntable = pq.read_table('example.parquet', columns=['taille', 'cheveux'])\n\ntable.group_by(\"cheveux\").aggregate([(\"taille\", \"mean\")])\n\nLa manière équivalente de procéder en passant\npar l’intermédiaire de pandas est\n\ntable = pq.read_table('example.parquet', columns=['taille', 'cheveux'])\n\ntable.to_pandas().groupby(\"cheveux\")[\"taille\"].mean()\n\ncheveux\nblond    165.0\nroux     190.0\nName: taille, dtype: float64\n\n\nIci, comme les données sont peu volumineuses, deux des\navantages du parquet par rapport\nau CSV (données moins\nvolumineuses et vitesse de l’import)\nne s’appliquent pas vraiment.\n\n\n Note\nUn autre principe d’optimisation de la performance qui est\nau coeur de la librairie Arrow est le filter pushdown\n(ou predicate pushdown).\nQuand on exécute un filtre de sélection de ligne\njuste après avoir chargé un jeu de données,\nArrow va essayer de le mettre en oeuvre lors de l’étape de lecture\net non après. Autrement dit, Arrow va modifier le plan\nd’exécution pour pousser le filtre en amont de la séquence d’exécution\nafin de ne pas essayer de lire les lignes inutiles."
  },
  {
    "objectID": "content/modern-ds/s3.html#quest-ce-que-le-système-de-stockage-s3",
    "href": "content/modern-ds/s3.html#quest-ce-que-le-système-de-stockage-s3",
    "title": "Les nouveaux modes d’accès aux données : le format parquet et les données sur le cloud",
    "section": "Qu’est-ce que le système de stockage S3 ?",
    "text": "Qu’est-ce que le système de stockage S3 ?\nDans les entreprises et administrations,\nun nombre croissant de données sont\ndisponibles depuis un système de stockage\nnommé S3.\nLe système S3 (Simple Storage System) est un système de stockage développé\npar Amazon et qui est maintenant devenu une référence pour le stockage en ligne.\nIl s’agit d’une architecture à la fois\nsécurisée (données cryptées, accès restreints) et performante.\nLe concept central du système S3 est le bucket.\nUn bucket est un espace (privé ou partagé) où on peut stocker une\narborescence de fichiers. Pour accéder aux fichiers figurant\ndans un bucket privé, il faut des jetons d’accès (l’équivalent d’un mot de passe)\nreconnus par le serveur de stockage. On peut alors lire et écrire dans le bucket.\n\n\n Note\nLes exemples suivants seront réplicables pour les utilisateurs de la plateforme\nSSP Cloud\n\nIls peuvent également l’être pour des utilisateurs ayant un\naccès à AWS, il suffit de changer l’URL du endpoint\nprésenté ci-dessous."
  },
  {
    "objectID": "content/modern-ds/s3.html#comment-faire-avec-python",
    "href": "content/modern-ds/s3.html#comment-faire-avec-python",
    "title": "Les nouveaux modes d’accès aux données : le format parquet et les données sur le cloud",
    "section": "Comment faire avec Python ?",
    "text": "Comment faire avec Python ?\n\nLes librairies principales\nL’interaction entre ce système distant de fichiers et une session locale de Python\nest possible grâce à des API. Les deux principales librairies sont les suivantes :\n\nboto3, une librairie créée et maintenue par Amazon ;\ns3fs, une librairie qui permet d’interagir avec les fichiers stockés à l’instar d’un filesystem classique.\n\nLa librairie pyarrow que nous avons déjà présentée permet également\nde traiter des données stockées sur le cloud comme si elles\nétaient sur le serveur local. C’est extrêmement pratique\net permet de fiabiliser la lecture ou l’écriture de fichiers\ndans une architecture cloud.\nUn exemple, assez court, est disponible\ndans la documentation officielle\nIl existe également d’autres librairies permettant de gérer\ndes pipelines de données (chapitre à venir) de manière\nquasi indifférente entre une architecture locale et une architecture\ncloud. Parmi celles-ci, nous présenterons quelques exemples\navec snakemake.\nEn arrière-plan, snakemake\nva utiliser boto3 pour communiquer avec le système\nde stockage.\nEnfin, selon le même principe du comme si les données\nétaient en local, il existe l’outil en ligne de commande\nmc (Minio Client) qui permet de gérer par des lignes\nde commande Linux les dépôts distants comme s’ils étaient\nlocaux.\nToutes ces librairies offrent la possibilité de se connecter depuis Python,\nà un dépôt de fichiers distant, de lister les fichiers disponibles dans un\nbucket, d’en télécharger un ou plusieurs ou de faire de l’upload\nNous allons présenter quelques-unes des opérations les plus fréquentes,\nen mode cheatsheet."
  },
  {
    "objectID": "content/modern-ds/s3.html#connexion-à-un-bucket",
    "href": "content/modern-ds/s3.html#connexion-à-un-bucket",
    "title": "Les nouveaux modes d’accès aux données : le format parquet et les données sur le cloud",
    "section": "Connexion à un bucket",
    "text": "Connexion à un bucket\nPar la suite, on va utiliser des alias pour les trois valeurs suivantes, qui servent\nà s’authentifier.\nkey_id = 'MY_KEY_ID'\naccess_key = 'MY_ACCESS_KEY'\ntoken = \"MY_TOKEN\"\nCes valeurs peuvent être également disponibles dans\nles variables d’environnement de Python. Comme il s’agit d’une information\nd’authentification personnelle, il ne faut pas stocker les vraies valeurs de ces\nvariables dans un projet, sous peine de partager des traits d’identité sans le\nvouloir lors d’un partage de code.\n\nboto3 👇\nAvec boto3, on créé d’abord un client puis on exécute des requêtes dessus.\nPour initialiser un client, il suffit, en supposant que l’url du dépôt S3 est\n\"https://minio.lab.sspcloud.fr\", de faire:\nimport boto3\ns3 = boto3.client(\"s3\",endpoint_url = \"https://minio.lab.sspcloud.fr\")\n\n\n\nS3FS 👇\nLa logique est identique avec s3fs.\nSi on a des jetons d’accès à jour et dans les variables d’environnement\nadéquates:\nimport s3fs\nfs = s3fs.S3FileSystem(\n  client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})\n\n\n\nArrow 👇\nLa logique d’Arrow est proche de celle de s3fs. Seuls les noms\nd’arguments changent\nSi on a des jetons d’accès à jour et dans les variables d’environnement\nadéquates:\nfrom pyarrow import fs\ns3 = fs.S3FileSystem(endpoint_override=\"http://\"+\"minio.lab.sspcloud.fr\")\n\n\n\nSnakemake 👇\nLa logique de Snakemake est, quant à elle,\nplus proche de celle de boto3. Seuls les noms\nd’arguments changent\nSi on a des jetons d’accès à jour et dans les variables d’environnement\nadéquates:\nfrom snakemake.remote.S3 import RemoteProvider as S3RemoteProvider\nS3 = S3RemoteProvider(host = \"https://\" + os.getenv('AWS_S3_ENDPOINT'))\n\n\nIl se peut que la connexion à ce stade soit refusée (HTTP error 403).\nCela peut provenir\nd’une erreur dans l’URL utilisé. Cependant, cela reflète plus généralement\ndes paramètres d’authentification erronés.\n\nboto3 👇\nLes paramètres d’authentification sont des arguments supplémentaires:\nimport boto3\ns3 = boto3.client(\"s3\",endpoint_url = \"https://minio.lab.sspcloud.fr\",\n                  aws_access_key_id=key_id, \n                  aws_secret_access_key=access_key, \n                  aws_session_token = token)\n\n\n\nS3FS 👇\nLa logique est la même, seuls les noms d’arguments diffèrent\nimport s3fs\nfs = s3fs.S3FileSystem(\n  client_kwargs={'endpoint_url': 'https://'+'minio.lab.sspcloud.fr'},\n  key = key_id, secret = access_key,\n  token = token)\n\n\n\nArrow 👇\nTout est en argument cette fois:\nfrom pyarrow import fs\n\ns3 = fs.S3FileSystem(\n    access_key = key_id,\n    secret_key = access_key,\n    session_token = token,\n    endpoint_override = 'https://'+'minio.lab.sspcloud.fr',\n    scheme = \"https\"\n    )\n\n\n\nSnakemake 👇\nLa logique est la même, seuls les noms d’arguments diffèrent\nfrom snakemake.remote.S3 import RemoteProvider as S3RemoteProvider\nS3 = S3RemoteProvider(host = \"https://\" + os.getenv('AWS_S3_ENDPOINT'), access_key_id=key_id, secret_access_key=access_key)\n\n\n\n\n Note\nDans le SSP Cloud,\nlorsque l’initialisation du service Jupyter du SSP Cloud est récente\n(moins de 12 heures), il est possible d’utiliser\nautomatiquement les jetons stockés automatiquement à la création du dépôt.\nSi on désire accéder aux données du SSP Cloud depuis une session Python du\ndatalab (service VSCode, Jupyter…),\nil faut remplacer l’url par http://minio.lab.sspcloud.fr"
  },
  {
    "objectID": "content/modern-ds/s3.html#lister-les-fichiers",
    "href": "content/modern-ds/s3.html#lister-les-fichiers",
    "title": "Les nouveaux modes d’accès aux données : le format parquet et les données sur le cloud",
    "section": "Lister les fichiers",
    "text": "Lister les fichiers\nS’il n’y a pas d’erreur à ce stade, c’est que la connexion est bien effective.\nPour le vérifier, on peut essayer de faire la liste des fichiers disponibles\ndans un bucket auquel on désire accéder.\nPar exemple, on peut vouloir\ntester l’accès aux bases FILOSOFI (données de revenu localisées disponibles\nsur https://www.insee.fr) au sein du bucket donnees-insee.\n\nboto3 👇\nPour cela,\nla méthode list_objects offre toutes les options nécessaires:\nimport boto3\ns3 = boto3.client(\"s3\",endpoint_url = \"https://minio.lab.sspcloud.fr\")\nfor key in s3.list_objects(Bucket='donnees-insee', Prefix='diffusion/FILOSOFI')['Contents']:\n    print(key['Key'])\n\n\n\nS3FS 👇\nPour lister les fichiers, c’est la méthode ls (celle-ci ne liste pas par\ndéfaut les fichiers de manière récursive comme boto3):\nimport s3fs\nfs = s3fs.S3FileSystem(\n  client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})\nfs.ls(\"donnees-insee/diffusion\")\n\n\n\nArrow 👇\nfrom pyarrow import fs\ns3 = fs.S3FileSystem(endpoint_override='https://'+'minio.lab.sspcloud.fr')\ns3.get_file_info(fs.FileSelector('donnees-insee/diffusion', recursive=True))\n\n\n\nmc 👇\nmc ls -r"
  },
  {
    "objectID": "content/modern-ds/s3.html#télécharger-un-fichier-depuis-s3-pour-lenregistrer-en-local",
    "href": "content/modern-ds/s3.html#télécharger-un-fichier-depuis-s3-pour-lenregistrer-en-local",
    "title": "Les nouveaux modes d’accès aux données : le format parquet et les données sur le cloud",
    "section": "Télécharger un fichier depuis S3 pour l’enregistrer en local",
    "text": "Télécharger un fichier depuis S3 pour l’enregistrer en local\nCette méthode n’est en général pas recommandée car, comme on va le voir\npar la suite, il est possible de lire à la volée des fichiers. Cependant,\ntélécharger un fichier depuis le cloud pour l’écrire sur le disque\nlocal peut parfois être utile (par exemple, lorsqu’il est nécessaire\nde dézipper un fichier).\n\nboto3 👇\nOn utilise cette fois la méthode download_file\nimport boto3\ns3 = boto3.client(\"s3\",endpoint_url = \"https://minio.lab.sspcloud.fr\")\ns3.download_file('donnees-insee', \"diffusion/FILOSOFI/2014/FILOSOFI_COM.csv\", 'data.csv')\n\n\n\nS3FS 👇\nimport s3fs\nfs = s3fs.S3FileSystem(\n  client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})\nfs.download('donnees-insee/diffusion/FILOSOFI/2014/FILOSOFI_COM.csv','test.csv')\n\n\n\nSnakemake 👇\nfrom snakemake.remote.S3 import RemoteProvider as S3RemoteProvider\nS3 = S3RemoteProvider(host = \"https://\" + os.getenv('AWS_S3_ENDPOINT'))\nbucket = \"mon-bucket\"\n\nrule ma_super_regle_s3:\n    input:\n        fichier = S3.remote(f'{bucket}/moninput.csv')\n    output:\n        fichier='mon_dossier_local/monoutput.csv'\n    run:\n        shell(\"cp {input[0]} {output[0]}\")\n\n\n\nmc 👇\nmc cp \"donnees-insee/FILOSOFI/2014/FILOSOFI_COM.csv\" 'data.csv'"
  },
  {
    "objectID": "content/modern-ds/s3.html#lire-un-fichier-directement",
    "href": "content/modern-ds/s3.html#lire-un-fichier-directement",
    "title": "Les nouveaux modes d’accès aux données : le format parquet et les données sur le cloud",
    "section": "Lire un fichier directement",
    "text": "Lire un fichier directement\nLa méthode précédente n’est pas optimale. En effet, l’un des intérêts des API\nest qu’on peut traiter un fichier sur S3 comme s’il s’agissait d’un fichier\nsur son PC. Cela est d’ailleurs une manière plus sécurisée de procéder puisqu’on\nlit les données à la volée, sans les écrire dans un filesystem local.\n\nboto3 👇\nimport boto3\ns3 = boto3.client(\"s3\",endpoint_url = \"https://minio.lab.sspcloud.fr\")\nobj = s3.get_object(Bucket='donnees-insee', Key=\"diffusion/FILOSOFI/2014/FILOSOFI_COM.csv\")\ndf = pd.read_csv(obj['Body'], sep = \";\")\ndf.head(2)\n\n\n\nS3FS 👇\nLe code suivant devrait permettre d’effectuer la même opération avec s3fs\nimport pandas as pd\nimport s3fs\nfs = s3fs.S3FileSystem(\n  client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})\ndf = pd.read_csv(fs.open('{}/{}'.format('donnees-insee', \"diffusion/FILOSOFI/2014/FILOSOFI_COM.csv\"),\n                         mode='rb'), sep = \";\"\n                 )\n\ndf.head(2)\n\n\n\nSnakemake 👇\nfrom snakemake.remote.S3 import RemoteProvider as S3RemoteProvider\nS3 = S3RemoteProvider(host = \"https://\" + os.getenv('AWS_S3_ENDPOINT'))\nbucket = \"mon-bucket\"\n\nrule ma_super_regle_s3:\n    input:\n        fichier = S3.remote(f'{bucket}/moninput.csv')\n    run:\n        import pandas as pd\n        df = pd.read_csv(input.fichier)\n        # PLUS D'OPERATIONS\n\n\n\nArrow 👇\nArrow est une librairie qui permet de lire des CSV.\nIl est néanmoins\nbeaucoup plus pratique d’utiliser le format parquet avec arrow.\nDans un premier temps, on configure le filesystem avec les\nfonctionalités d’Arrow (cf. précédemment).\n\nfrom pyarrow import fs\n\ns3 = fs.S3FileSystem(endpoint_override='http://'+'minio.lab.sspcloud.fr')\n\nPour lire un csv, on fera:\nfrom pyarrow import fs\nfrom pyarrow import csv\n\ns3 = fs.S3FileSystem(endpoint_override='https://'+'minio.lab.sspcloud.fr')\n\nwith s3.open_input_file(\"donnees-insee/diffusion/FILOSOFI/2014/FILOSOFI_COM.csv\") as file:\n    df = csv.read_csv(file, parse_options=csv.ParseOptions(delimiter=\";\")).to_pandas()\nPour un fichier au format parquet, la démarche est plus simple grâce à l’argument\nfilesystem dans pyarrow.parquet.ParquetDataset :\nimport pyarrow.parquet as pq\n\n#bucket = \"\"\n#parquet_file=\"\"\ndf = pq.ParquetDataset(f'{bucket}/{parquet_file}', filesystem=s3).read_pandas().to_pandas()"
  },
  {
    "objectID": "content/modern-ds/s3.html#uploader-un-fichier",
    "href": "content/modern-ds/s3.html#uploader-un-fichier",
    "title": "Les nouveaux modes d’accès aux données : le format parquet et les données sur le cloud",
    "section": "Uploader un fichier",
    "text": "Uploader un fichier\n\nboto3 👇\ns3.upload_file(file_name, bucket, object_name)\n\n\n\nS3FS 👇\nfs.put(filepath, f\"{bucket}/{object_name}\", recursive=True)\n\n\n\nArrow 👇\nSupposons que df soit un pd.DataFrame\nDans un système local, on convertirait\nen table Arrow puis on écrirait en parquet\n(voir la documentation officielle).\nQuand on est sur un système S3, il s’agit seulement d’ajouter\nnotre connexion à S3 dans l’argument filesystem\n(voir la page sur ce sujet dans la documentation Arrow)\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\ntable = pa.Table.from_pandas(df)\npq.write_table(table, f\"{bucket}/{path}\", filesystem=s3)\n\n\n\nSnakemake 👇\nfrom snakemake.remote.S3 import RemoteProvider as S3RemoteProvider\nS3 = S3RemoteProvider(host = \"https://\" + os.getenv('AWS_S3_ENDPOINT'))\nbucket = \"mon-bucket\"\n\nrule ma_super_regle_s3:\n    input:\n        fichier='mon_dossier_local/moninput.csv'\n    output:\n        fichier=S3.remote(f'{bucket}/monoutput.csv')\n    run:\n        shell(\"cp output.fichier input.fichier\")\n\n\n\nmc 👇\nmc cp 'data.csv' \"MONBUCKET/monoutput.csv\""
  },
  {
    "objectID": "content/modern-ds/s3.html#pour-aller-plus-loin",
    "href": "content/modern-ds/s3.html#pour-aller-plus-loin",
    "title": "Les nouveaux modes d’accès aux données : le format parquet et les données sur le cloud",
    "section": "Pour aller plus loin",
    "text": "Pour aller plus loin\n\nLa documentation sur MinIO du SSPCloud"
  },
  {
    "objectID": "content/modern-ds/s3.html#footnotes",
    "href": "content/modern-ds/s3.html#footnotes",
    "title": "Les nouveaux modes d’accès aux données : le format parquet et les données sur le cloud",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nElle permet aussi la lecture et l’écriture\nde .csv.↩︎\nD’ailleurs, les générations n’ayant connu nativement\nque ce type de stockage ne sont pas familiarisées\nau concept de file system et préfèrent\npayer le temps de recherche. Voir\ncet article\nsur le sujet.↩︎\nD’ailleurs, les générations n’ayant connu nativement\nque ce type de stockage ne sont pas familiarisées\nau concept de file system et préfèrent\npayer le temps de recherche. Voir\ncet article\nsur le sujet.↩︎\nD’ailleurs, les générations n’ayant connu nativement\nque ce type de stockage ne sont pas familiarisées\nau concept de file system et préfèrent\npayer le temps de recherche. Voir\ncet article\nsur le sujet.↩︎"
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html",
    "href": "content/modern-ds/elastic_intro.html",
    "title": "Introduction à ElasticSearch pour la recherche textuelle",
    "section": "",
    "text": "Pour essayer les exemples présents dans ce tutoriel :\nCe chapitre a été écrit avec Milena Suarez-Castillo\net présente quelques éléments qui servent de base à un travail en cours\nsur les inégalités socioéconomiques dans les\nchoix de consommation alimentaire."
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#réplication-de-ce-chapitre",
    "href": "content/modern-ds/elastic_intro.html#réplication-de-ce-chapitre",
    "title": "Introduction à ElasticSearch pour la recherche textuelle",
    "section": "Réplication de ce chapitre",
    "text": "Réplication de ce chapitre\nCe chapitre est plus exigeant en termes d’infrastructures que les précédents.\nSi la première partie de ce chapitre peut être menée avec une\ninstallation standard de Python, ce n’est pas le cas de la\ndeuxième qui nécessite un serveur ElasticSearch. Les utilisateurs du\nSSP Cloud pourront répliquer les exemples de ce cours\ncar cette technologie est disponible (que ce soit pour indexer une base ou\npour requêter une base existante).\n⚠️ Ce\nchapitre nécessite une version particulière du\npackage ElasticSearch pour tenir compte de l’héritage de la version 7 du moteur Elastic.\nPour cela, faire\n\n!pip install elasticsearch==8.2.0\n!pip install unidecode\n!pip install rapidfuzz\n!pip install xlrd\n\nLa première partie de ce tutoriel ne nécessite pas d’architecture particulière et\npeut ainsi être exécutée en utilisant les packages suivants :\n\nimport time\nimport pandas as pd\n\nLe script functions.py, disponible sur Github,\nregroupe un certain nombre de fonctions utiles permettant\nd’automatiser certaines tâches de nettoyage classiques\nen NLP.\n\n\n Hint\nPlusieurs méthodes peuvent être mises en oeuvre pour récupérer\nle script d’utilitaires. Vous pouvez trouver en dessous\nde cet encadré une méthode qui va chercher la dernière\nversion sur le dépôt Github du cours\n\n\n\nimport requests\nurl = \"https://github.com/linogaliana/python-datascientist/raw/master/content/modern-ds/functions.py\"\nr = requests.get(url, allow_redirects=True)\n\nopen('functions.py', 'wb').write(r.content)\n\nAprès l’avoir récupéré (cf. encadré dédié),\nil convient d’importer les fonctions sous forme de module:\n\nimport functions as fc"
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#cas-dusage",
    "href": "content/modern-ds/elastic_intro.html#cas-dusage",
    "title": "Introduction à ElasticSearch pour la recherche textuelle",
    "section": "Cas d’usage",
    "text": "Cas d’usage\nCe notebook recense et propose d’appréhender quelques outils utilisés\npour le papier présenté aux\nJournées de Méthodologie Statistiques 2022: Galiana and Suarez-Castillo, “Fuzzy matching on big-data: an illustration with scanner data and crowd-sourced nutritional data”\n(travail en cours!)\nOn va partir du cas d’usage suivant :\n\nCombien de calories dans ma recette de cuisine de ce soir? Combien de calories dans mes courses de la semaine?\n\nL’objectif est de reconstituer, à partir de libellés de produits, les caractéristiques nutritionnelles d’une recette.\nLe problème est que les libellés des tickets de caisse ne sont pas des champs textuels très propres, ils contiennent,\npar exemple, beaucoup d’abbréviations, toutes n’étant pas évidentes.\nVoici par exemple une série de noms de produits qu’on va utiliser par la suite:\n\nticket = ['CROISSANTS X6 400G',\n          'MAQUEREAUX MOUTAR.',\n          'IGP OC SAUVIGNON B',\n          'LAIT 1/2 ECRM UHT',\n          '6 OEUFS FRAIS LOCA',\n          'ANANAS C2',\n          'L POMME FUDJI X6 CAL 75/80 1KG ENV',\n          'PLT MIEL',\n          'STELLA ARTOIS X6',\n          'COTES DU LUBERON AIGUEBRUN 75C']\n\nA ces produits, s’ajoutent les ingrédients suivants, issus de la\nrecette du velouté de potiron et carottes de Marmiton\nqui sera notre plat principal :\n\ningredients = ['500 g de carottes',\n '2 pommes de terre',\n \"1 gousse d'ail\",\n '1/2 l de lait',\n '1/2 l de bouillon de volaille',\n \"1 cuillère à soupe de huile d'olive\",\n '1 kg de potiron',\n '1 oignon',\n '10 cl de crème liquide (facultatif)']\n\nEssayer de récupérer par web scraping cette liste est un bon exercice pour réviser\nles concepts vus précédemment\nOn va donc créer une liste de course compilant\nces deux\nlistes hétérogènes de noms de produits:\n\nlibelles = ticket + ingredients\n\nOn part avec cette liste dans notre supermarché virtuel. L’objectif sera de trouver\nune méthode permettant de passer à l’échelle:\nautomatiser les traitements, effectuer des recherches efficaces, garder une certaine généralité et flexibilité.\nCe chapitre montrera par l’exemple l’intérêt d’Elastic par rapport à une solution\nqui n’utiliserait que du Python."
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#les-bases-offrant-des-informations-nutritionnelles",
    "href": "content/modern-ds/elastic_intro.html#les-bases-offrant-des-informations-nutritionnelles",
    "title": "Introduction à ElasticSearch pour la recherche textuelle",
    "section": "Les bases offrant des informations nutritionnelles",
    "text": "Les bases offrant des informations nutritionnelles\nPour un nombre restreint de produits, on pourrait bien sûr chercher à\nla main les caractéristiques des produits en utilisant les\nfonctionalités d’un moteur de recherche:\n\n\n\n\n\nCependant, cette approche serait très fastidieuse et\nnécessiterait de récuperer, à la main, chaque caractéristique\npour chaque produit. Ce n’est donc pas envisageable.\nLes données disponibles sur Google viennent de l’USDA,\nl’équivalent américain de notre Ministère de l’Agriculture.\nCependant, pour des recettes comportant des noms de produits français, ainsi que\ndes produits potentiellement transformés, ce n’est pas très pratique d’utiliser\nune base de données de produits agricoles en Français. Pour cette raison,\nnous proposons d’utiliser les deux bases suivantes,\nqui servent de base au travail de\nGaliana and Suarez Castillo (2022)\n\nL’OpenFoodFacts database qui est une base\ncollaborative française de produits alimentaires. Issue d’un projet Data4Good, il s’agit d’une\nalternative opensource et opendata à la base de données de l’application Yuka.\nLa table de composition nutritionnelle Ciqual produite par l’Anses. Celle-ci\npropose la composition nutritionnelle moyenne des aliments les plus consommés en France. Il s’agit d’une base de données\nenrichie par rapport à celle de l’USDA puisqu’elle ne se cantonne pas aux produits agricoles non transformés.\nAvec cette base, il ne s’agit pas de trouver un produit exact mais essayer de trouver un produit type proche du produit\ndont on désire connaître les caractéristiques."
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#import",
    "href": "content/modern-ds/elastic_intro.html#import",
    "title": "Introduction à ElasticSearch pour la recherche textuelle",
    "section": "Import",
    "text": "Import\nQuelques fonctions utiles sont regroupées dans le script functions.py et importées dans le notebook.\nLa base OpenFood peut être récupérée en ligne\nvia la fonction fc.import_openfood. Néanmoins, cette opération nécessitant\nun certain temps (les données brutes faisant autour de 2Go), nous proposons une méthode\npour les utilisateurs du SSP Cloud où une version est disponible sur\nl’espace de stockage.\nLa base Ciqual, qui plus légère, est récupérée elle directement en ligne\nvia la fonction fc.import_ciqual.\n\n# Pour les utilisateurs du SSP Cloud\nopenfood = fc.import_openfood_s3()\n# Pour les utilisateurs hors du SSP Cloud\n# openfood = fc.import_openfood()\nciqual = fc.import_ciqual()\n\n\nopenfood.head()\n\n\nciqual.head()"
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#quest-ce-quelastic",
    "href": "content/modern-ds/elastic_intro.html#quest-ce-quelastic",
    "title": "Introduction à ElasticSearch pour la recherche textuelle",
    "section": "Qu’est-ce qu’Elastic ?",
    "text": "Qu’est-ce qu’Elastic ?\nElasticSearch c’est un logiciel qui fournit un moteur de recherche installé sur\nun serveur (ou une machine personnelle) qu’il est possible de requêter depuis un client\n(une session Python par exemple).\nC’est un moteur de recherche\ntrès performant, puissant et flexible, extrêmement utilisé dans le domaine de la datascience\nsur données textuelles.\nUn cas d’usage est par exemple de trouver,\ndans un corpus de grande dimension\n(plusieurs sites web, livres…), un certain texte en s’autorisant des termes voisins\n(verbes conjugués, fautes de frappes…).\nUn index est une collection de documents dans lesquels on souhaite chercher, préalablement ingérés dans un moteur de recherche les documents sont les établissements.\nL’indexation consiste à pré-réaliser les traitements des termes des documents pour gagner en efficacité lors de la phase de recherche.\nL’indexation est faite une fois pour de nombreuses recherches potentielles, pour lesquelles la rapidité de réponse peut être cruciale.\nAprès avoir indexé une base, on effectuera des requêtes qui sont des recherches\nd’un document dans la base indexé (équivalent de notre web) à partir de\ntermes de recherche normalisés.\nLe principe est le même que celui d’un moteur de recherche du web comme Google.\nD’un côté, l’ensemble à parcourir est indexé pour être en\nmesure de parcourir de manière efficace l’ensemble du corpus.\nDe l’autre côté, la phase de recherche permet de retrouver l’élément du corpus le\nplus cohérent avec la requête de recherche.\nL’indexation consiste, par exemple,\nà pré-définir des traitements des termes du corpus pour gagner en efficacité\nlors de la phase de recherche. En effet, l’indexation est une opération peu fréquente\npar rapport à la recherche. Pour cette dernière, l’efficacité est cruciale (un site web\nqui prend plusieurs secondes à interpréter une requête simple ne sera pas utilisé). Mais, pour\nl’indexation, ceci est moins crucial.\nLes documents sont constitués de variables, les champs (‘fields’),\ndont le type est spécifié (“text”, “keywoard”, “geo_point”, “numeric”…) à l’indexation.\nElasticSearch propose une interface graphique nommée Kibana.\nCelle-ci est pratique\npour tester des requêtes et pour superviser le serveur Elastic. Cependant,\npour le passage à l’échelle, notamment pour mettre en lien une base indexée dans\nElastic avec une autre source de données, les API proposées par ElasticSearch\nsont beaucoup plus pratiques. Ces API permettent de connecter une session Python (idem pour R)\nà un serveur Elastic afin de communiquer avec lui\n(échanger des flux via une API REST)."
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#elasticsearch-et-python",
    "href": "content/modern-ds/elastic_intro.html#elasticsearch-et-python",
    "title": "Introduction à ElasticSearch pour la recherche textuelle",
    "section": "ElasticSearch et Python",
    "text": "ElasticSearch et Python\nEn Python, le package officiel est elasticsearch.\nCe dernier permet de configurer les paramètres pour interagir avec un serveur, indexer\nune ou plusieurs bases, envoyer de manière automatisée un ensemble de requêtes\nau serveur, récupérer les résultats directement dans une session Python…"
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#premier-essai-les-produits-ciqual-les-plus-similaires-aux-produits-de-la-recette",
    "href": "content/modern-ds/elastic_intro.html#premier-essai-les-produits-ciqual-les-plus-similaires-aux-produits-de-la-recette",
    "title": "Introduction à ElasticSearch pour la recherche textuelle",
    "section": "Premier essai: les produits Ciqual les plus similaires aux produits de la recette",
    "text": "Premier essai: les produits Ciqual les plus similaires aux produits de la recette\nOn pourrait écrire une fonction qui prend en argument\nune liste de libellés d’intérêt et une liste de candidat au match et\nrenvoie le libellé le plus proche.\nCependant, le risque est que cet algorithme soit relativement lent s’il n’est pas codé\nparfaitement.\nIl est, à mon avis, plus simple, quand\non est habitué à la logique Pandas,\nde faire un produit cartésien pour obtenir un vecteur mettant en miroir\nchaque produit de notre recette avec l’ensembles des produits Ciqual et ensuite comparer les deux vecteurs pour prendre,\npour chaque produit, le meilleur match.\nLes bases étant de taille limitée, le produit cartésien n’est pas problématique.\nAvec des bases plus conséquentes, une stratégie plus parcimonieuse en mémoire devrait être envisagée.\nPour faire cette opération, on va utiliser la fonction match_product de\nnote script d’utilitaires.\n\ndist_leven = fc.match_product(libelles, ciqual)\ndist_leven\n\nCette première étape naïve est décevante à plusieurs égards:\n\nCertes, on a des matches cohérent (par exemple “Oignon rouge, cru” et “1 oignon”)\nmais on a plus de couples incohérents ;\nLe temps de calcul peut apparaître faible mais le passage à l’échelle risque d’être compliqué ;\nLes besoins mémoires sont potentiellement importants lors de l’appel à\nrapidfuzz.process.extract ce qui peut bloquer le passage à l’échelle ;\nLa distance textuelle n’est pas nécessairement la plus pertinente.\n\nOn a, en fait, négligé une étape importante: la normalisation (ou nettoyage des textes) présentée dans la\npartie NLP, notamment:\n\nharmonisation de la casse, suppression des accents…\nsuppressions des mots outils (e.g. ici on va d’abord négliger les quantités pour trouver la nature de l’aliment, en particulier pour Ciqual)\n\n\n\n\n\n\nScanner-data avant nettoyage\n\n\n\n\n\nOpenFood data avant nettoyage\n\n\n\n\n\n\n\nScanner-data après nettoyage\n\n\n\n\n\nOpenFood data après nettoyage\n\n\n\n\nFaisons donc en apparence un retour en arrière qui sera\nnéanmoins salvateur pour améliorer\nla pertinence des liens faits entre nos\nbases de données."
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#objectif",
    "href": "content/modern-ds/elastic_intro.html#objectif",
    "title": "Introduction à ElasticSearch pour la recherche textuelle",
    "section": "Objectif",
    "text": "Objectif\nLe preprocessing correspond à l’ensemble des opérations\nayant lieu avant l’analyse à proprement parler.\nIci, ce preprocessing est intéressant à plusieurs\négards:\n\nIl réduit le bruit dans nos jeux de données (par exemple des mots de liaisons) ;\nIl permet de normaliser et harmoniser les syntaxes dans nos différentes sources.\n\nL’objectif est ainsi de réduire nos noms de produits à la substantifique moelle\npour améliorer la pertinence de la recherche.\nPour être pertinent, le preprocessing comporte généralement deux types de\ntraitements. En premier lieu, ceux qui sont généraux et applicables\nà tous types de corpus textuels: retrait des stopwords, de la ponctuation, etc.\nles méthodes disponibles dans la partie NLP.\nEnsuite, il est nécessaire de mettre en oeuvre des nettoyages plus spécifiques à chaque corpus.\nPar exemple dans la source Ciqual,\nla cuisson est souvent renseignée et bruite les appariemments."
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#démarche",
    "href": "content/modern-ds/elastic_intro.html#démarche",
    "title": "Introduction à ElasticSearch pour la recherche textuelle",
    "section": "Démarche",
    "text": "Démarche\n\n\n Exercice 1: preprocessing\n\nPour transformer les lettres avec accents en leur équivalent\nsans accent, la fonction unidecode\n(du package du même nom) est pratique.\nLa tester sur le jeu de données ciqual en créant une nouvelle\ncolonne nommée libel_clean\nLa casse différente selon les jeux de données peut être pénalisante\npour trouver des produits similaires. Pour éviter ces problèmes,\nmettre tout en majuscule.\nLes informations sur les quantités ou le packaging peuvent apporter\ndu bruit dans notre comparaison. Nous allons retirer ces mots,\nà travers la liste ['KG','CL','G','L','CRUE?S?', 'PREEMBALLEE?S?'],\nqu’on peut considérer comme un dictionnaire de stop-words métier.\nPour cela, il convient d’utiliser une expression régulière dans la méthode\nstr.replace de Pandas.\nAvec ceux-ci, on va utiliser la liste des stop-words de\nla librairie nltk pour retirer les stop-words classiques (_“le”,“la”, etc.).\nLa librairie SpaCy, plus riche, pourrait être utilisée ; nous laissons\ncela sous la forme d’exercice supplémentaire.\nOn a encore des signes de ponctuation ou des chiffres qui peuvent\npoluer la comparaison. Les retirer grâce à la méthode replace et\nune regex [^a-zA-Z]\nEnfin, par sécurité, on peut supprimer les espaces multiples.\nUtiliser la regex '([ ]{2,})' pour cela. Observer le résultat\nfinal.\n(Optionnel). Comme exercice supplémentaire, faire la même chose avec les\npipelines SpaCy.\n\n\n\nA l’issue de la question 1, le jeu de données ciqual devrait\nressembler à celui-ci:\nAprès avoir mis en majuscule, on se retrouve avec le jeu de données\nsuivant:\nAprès retrait des stop-words, nos libellés prennent\nla forme suivante :\nLa regex pour éliminer les caractères de ponctuation permet ainsi d’obtenir:\nEnfin, à l’issue de la question 5, le DataFrame obtenu est le suivant :\nCes étapes de nettoyage ont ainsi permis de concentrer l’information\ndans les noms de produits sur ce qui l’identifie vraiment."
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#approche-systématique",
    "href": "content/modern-ds/elastic_intro.html#approche-systématique",
    "title": "Introduction à ElasticSearch pour la recherche textuelle",
    "section": "Approche systématique",
    "text": "Approche systématique\nPour systématiser cette approche à nos différents DataFrame, rien de mieux\nqu’une fonction. Celle-ci est présente dans le module functions\nsous le nom clean_libelle.\n\nfrom functions import clean_libelle\n\nPour résumer l’exercice précédent, cette fonction va :\n\nHarmoniser la casse et retirer les accents (voir functions.py) ;\nRetirer tout les caractères qui ne sont pas des lettres (chiffres, ponctuations) ;\nRetirer les caractères isolés.\n\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\n\nstop_words = ['KG','CL','G','L','CRUE?S?', 'PREEMBALLEE?S?']\nstop_words += [l.upper() for l in stopwords.words('french')]\n\nreplace_regex = {r'[^A-Z]': ' ', r'\\b[A-Z0-9]{1,2}?\\b':' '} # \n\nCela permet d’obtenir les bases nettoyées suivantes :\n\nciqual = clean_libelle(ciqual, yvar = 'alim_nom_fr', replace_regex = replace_regex, stopWords = stop_words)\nciqual.sample(10)\n\n\nopenfood = clean_libelle(openfood, yvar = 'product_name', replace_regex = replace_regex, stopWords = stop_words)\nopenfood.sample(10)\n\n\ncourses = pd.DataFrame(libelles, columns = ['libel'])\ncourses = clean_libelle(courses, yvar = 'libel', replace_regex = replace_regex, stopWords = stop_words)\ncourses.sample(10)\n\nLes noms de produits sont déjà plus harmonisés.\nVoyons voir si cela permet de trouver un\nmatch dans l’Openfood database:\n\ndist_leven_openfood = fc.match_product(courses[\"libel_clean\"], openfood, \"libel_clean\")\ndist_leven_openfood.sample(10)\n\nPas encore parfait, mais on progresse sur les produits appariés!\nConcernant le temps de calcul, les quelques secondes nécessaires à\nce calcul peuvent apparaître un faible prix à payer. Cependant,\nil convient de rappeler que le nombre de produits dans l’ensemble\nde recherche est faible. Cette solution n’est donc pas généralisable."
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#réduire-les-temps-de-recherche",
    "href": "content/modern-ds/elastic_intro.html#réduire-les-temps-de-recherche",
    "title": "Introduction à ElasticSearch pour la recherche textuelle",
    "section": "Réduire les temps de recherche",
    "text": "Réduire les temps de recherche\nFinalement, l’idéal serait de disposer d’un moteur de recherche adapté à notre besoin,\ncontenant les produits candidats, que l’on pourrait interroger, rapide en lecture, capable de classer les échos renvoyés par pertinence, que l’on pourrait requêter de manière flexible.\nPar exemple, on pourrait vouloir signaler qu’un\nécho nous intéresse seulement si la donnée calorique n’est pas manquante.\nOn pourrait même vouloir qu’il effectue pour nous des prétraitements sur les données.\nCela paraît beaucoup demander. Mais c’est exactement ce que fait ElasticSearch."
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#créer-un-cluster-elastic-sur-le-datalab",
    "href": "content/modern-ds/elastic_intro.html#créer-un-cluster-elastic-sur-le-datalab",
    "title": "Introduction à ElasticSearch pour la recherche textuelle",
    "section": "Créer un cluster Elastic sur le DataLab",
    "text": "Créer un cluster Elastic sur le DataLab\nPour lancer un service Elastic, il faut cliquer sur ce lien.\nUne fois créé, vous pouvez explorer l’interface graphique Kibana.\nCependant, grâce à l’API Elastic\nde Python, on se passera de celle-ci. Donc, en pratique,\nune fois lancé, pas besoin d’ouvrir ce service Elastic pour continuer à suivre1.\nDans un terminal, vous pouvez aussi vérifier que vous êtes en mesure de dialoguer avec votre cluster Elastic,\nqui est prêt à vous écouter:\nkubectl get statefulset\nPasser par la ligne de commande serait peu commode pour industrialiser notre\nrecherche.\nNous allons utiliser la librairie elasticsearch pour dialoguer avec notre moteur de recherche Elastic.\nLes instructions ci-dessous indiquent comment établir la connection.\n\nfrom elasticsearch import Elasticsearch\nHOST = 'elasticsearch-master'\n\ndef elastic():\n    \"\"\"Connection avec Elastic sur le data lab\"\"\"\n    es = Elasticsearch([{'host': HOST, 'port': 9200, 'scheme': 'http'}], http_compress=True, request_timeout=200)\n    return es\n\nes = elastic()\n\n&lt;Elasticsearch([{'host': 'elasticsearch-master', 'port': 9200}])&gt;\nMaintenant que la connection est établie, deux étapes nous attendent:\n\nIndexation Envoyer les documents parmi lesquels on veut chercher des echos pertinents dans notre elastic. Un index est une collection de document. Nous pourrions en créer deux: un pour les produits ciqual, un pour les produits openfood\nRequête Chercher les documents les plus pertinents suivant une recherche textuelle flexible. Nous allons rechercher les libellés de notre recette et de notre liste de course."
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#première-indexation",
    "href": "content/modern-ds/elastic_intro.html#première-indexation",
    "title": "Introduction à ElasticSearch pour la recherche textuelle",
    "section": "Première indexation",
    "text": "Première indexation\nOn crée donc nos deux index:\n\nif not es.indices.exists(index = 'openfood'):\n    es.indices.create(index = 'openfood')\nif not es.indices.exists(index = 'ciqual'):\n    es.indices.create(index = 'ciqual')\n\nPour l’instant, nos index sont vides! Ils contiennent 0 documents.\n\nes.count(index = 'openfood')\n\n{'count': 0, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}}\nNous allons en rajouter quelques-uns !\n\nes.create(index = 'openfood',  id = 1, body = {'product_name': 'Tarte noix de coco', 'product_name_clean': 'TARTE NOIX COCO'})\nes.create(index = 'openfood',  id = 2, body = {'product_name': 'Noix de coco', 'product_name_clean': 'NOIX COCO'})\nes.create(index = 'openfood',  id = 3, body = {'product_name': 'Beurre doux', 'product_name_clean': 'BEURRE DOUX'})\n\n\nes.count(index = 'openfood')\n\n{'count': 3, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}}\nDans l’interface graphique Kibana,\non peut vérifier que l’indexation\na bien eue lieu en allant dans Management &gt; Stack Management"
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#première-recherche",
    "href": "content/modern-ds/elastic_intro.html#première-recherche",
    "title": "Introduction à ElasticSearch pour la recherche textuelle",
    "section": "Première recherche",
    "text": "Première recherche\nFaisons notre première recherche: cherchons des noix de pécan!\n\nes.search(index = 'openfood', q = 'noix de pécan')\n\nObjectApiResponse({'took': 116, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 2, 'relation': 'eq'}, 'max_score': 0.9400072, 'hits': [{'_index': 'openfood', '_type': '_doc', '_id': '2', '_score': 0.9400072, '_source': {'product_name': 'Noix de coco', 'product_name_clean': 'NOIX COCO'}}, {'_index': 'openfood', '_type': '_doc', '_id': '1', '_score': 0.8272065, '_source': {'product_name': 'Tarte noix de coco', 'product_name_clean': 'TARTE NOIX COCO'}}]}})\nIntéressons nous aux hits (résultats pertinents, ou echos) : nous en avons 2.\nLe score maximal parmi les hits est mentionné dans max_score et correspond à celui du deuxième document indexé.\nElastic nous fournit ici un score de pertinence dans notre recherche d’information, et classe ainsi les documents renvoyés.\nIci nous utilisons la configuration par défaut. Mais comment est calculé ce score? Demandons à Elastic de nous expliquer le score du document 2 dans la requête \"noix de pécan\".\n\nes.explain(index = 'openfood', id = 2, q = 'noix de pécan')\n\nObjectApiResponse({'_index': 'openfood', '_type': '_doc', '_id': '2', 'matched': True, 'explanation': {'value': 0.9400072, 'description': 'max of:', 'details': [{'value': 0.49917626, 'description': 'sum of:', 'details': [{'value': 0.49917626, 'description': 'weight(product_name_clean:noix in 1) [PerFieldSimilarity], result of:', 'details': [{'value': 0.49917626, 'description': 'score(freq=1.0), computed as boost * idf * tf from:', 'details': [{'value': 2.2, 'description': 'boost', 'details': []}, {'value': 0.47000363, 'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:', 'details': [{'value': 2, 'description': 'n, number of documents containing term', 'details': []}, {'value': 3, 'description': 'N, total number of documents with field', 'details': []}]}, {'value': 0.48275858, 'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:', 'details': [{'value': 1.0, 'description': 'freq, occurrences of term within document', 'details': []}, {'value': 1.2, 'description': 'k1, term saturation parameter', 'details': []}, {'value': 0.75, 'description': 'b, length normalization parameter', 'details': []}, {'value': 2.0, 'description': 'dl, length of field', 'details': []}, {'value': 2.3333333, 'description': 'avgdl, average length of field', 'details': []}]}]}]}]}, {'value': 0.9400072, 'description': 'sum of:', 'details': [{'value': 0.4700036, 'description': 'weight(product_name:noix in 1) [PerFieldSimilarity], result of:', 'details': [{'value': 0.4700036, 'description': 'score(freq=1.0), computed as boost * idf * tf from:', 'details': [{'value': 2.2, 'description': 'boost', 'details': []}, {'value': 0.47000363, 'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:', 'details': [{'value': 2, 'description': 'n, number of documents containing term', 'details': []}, {'value': 3, 'description': 'N, total number of documents with field', 'details': []}]}, {'value': 0.45454544, 'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:', 'details': [{'value': 1.0, 'description': 'freq, occurrences of term within document', 'details': []}, {'value': 1.2, 'description': 'k1, term saturation parameter', 'details': []}, {'value': 0.75, 'description': 'b, length normalization parameter', 'details': []}, {'value': 3.0, 'description': 'dl, length of field', 'details': []}, {'value': 3.0, 'description': 'avgdl, average length of field', 'details': []}]}]}]}, {'value': 0.4700036, 'description': 'weight(product_name:de in 1) [PerFieldSimilarity], result of:', 'details': [{'value': 0.4700036, 'description': 'score(freq=1.0), computed as boost * idf * tf from:', 'details': [{'value': 2.2, 'description': 'boost', 'details': []}, {'value': 0.47000363, 'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:', 'details': [{'value': 2, 'description': 'n, number of documents containing term', 'details': []}, {'value': 3, 'description': 'N, total number of documents with field', 'details': []}]}, {'value': 0.45454544, 'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:', 'details': [{'value': 1.0, 'description': 'freq, occurrences of term within document', 'details': []}, {'value': 1.2, 'description': 'k1, term saturation parameter', 'details': []}, {'value': 0.75, 'description': 'b, length normalization parameter', 'details': []}, {'value': 3.0, 'description': 'dl, length of field', 'details': []}, {'value': 3.0, 'description': 'avgdl, average length of field', 'details': []}]}]}]}]}]}})\nElastic nous explique donc que le score 0.9400072 est le maximum entre deux sous-scores, 0.4991 et 0.9400072.\nPour chacun de ces sous-scores, le détail de son calcul est donné.\nLe premier sous-score n’a accordé un score que par rapport au premier mot (noix), tandis que le second a accordé un score sur la base des deux mots déjà connu dans les documents (“noix” et “de”). Il a ignoré pécan! Jusqu’à présent, ce terme n’est pas connu dans l’index.\nLa pertinence d’un mot pour notre recherche est construite sur une variante de la TF-IDF,\nconsidérant qu’un terme est pertinent s’il est souvent présent dans le document (Term Frequency)\nalors qu’il est peu fréquent dans les autres document (inverse document frequency).\nIci les notations des documents 1 et 2 sont très proches, la différence est dûe à des IDF plus faibles dans le document 1,\nqui est pénalisé pour être légérement plus long.\nBref, tout ça est un peu lourd, mais assez efficace,\nen tout cas moins rudimentaire que les distances caractères à caractères pour ramener des echos pertinents."
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#limite-de-cette-première-indexation",
    "href": "content/modern-ds/elastic_intro.html#limite-de-cette-première-indexation",
    "title": "Introduction à ElasticSearch pour la recherche textuelle",
    "section": "Limite de cette première indexation",
    "text": "Limite de cette première indexation\nPour l’instant, Elastic n’a pas l’air de gérer les fautes de frappes!\nPas le droit à l’erreur dans la requête:\n\nes.search(index = 'openfood',q = 'TART NOI')\n\nObjectApiResponse({'took': 38, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 0, 'relation': 'eq'}, 'max_score': None, 'hits': []}})\nCela s’explique par la représentation des champs (‘product_name’ par exemple) qu’Elastic a inféré,\npuisque nous n’avons rien spécifié.\nLa représentation d’une variable conditionne la façon dont les champs sont analysés pour calculer la pertinence.\nPar exemple, regardons la représentation du champ product_name\n\nes.indices.get_field_mapping(index = 'openfood', fields = 'product_name')\n\nObjectApiResponse({'openfood': {'mappings': {'product_name': {'full_name': 'product_name', 'mapping': {'product_name': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}}}}}})\nElastic a compris qu’il s’agissait d’un champ textuel.\nEn revanche, le type est keyword n’autorise pas des analyses approximatives donc\nne permet pas de tenir compte de fautes de frappes.\nPour qu’un echo remonte, un des termes doit matcher exactement. Dommage !\nMais c’est parce qu’on a utilisé le mapping par défaut.\nEn réalité, il est assez simple de préciser un mapping plus riche,\nautorisant une analyse “fuzzy” ou “flou”."
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#nos-premières-requêtes",
    "href": "content/modern-ds/elastic_intro.html#nos-premières-requêtes",
    "title": "Introduction à ElasticSearch pour la recherche textuelle",
    "section": "Nos premières requêtes",
    "text": "Nos premières requêtes\nVérifions qu’on recupère quelques tartes aux noix même si l’on fait plein de fautes:\n\nes.search(index = 'openfood', q = 'TART NOI', size = 3)\n\nObjectApiResponse({'took': 60, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 10000, 'relation': 'gte'}, 'max_score': 22.837925, 'hits': [{'_index': 'openfood', '_type': '_doc', '_id': '405332', '_score': 22.837925, '_source': {'product_name': 'Tarte noix', 'libel_clean': 'TARTE NOIX', 'energy_100g': 1833.0, 'nutriscore_score': 23.0}}, {'_index': 'openfood', '_type': '_doc', '_id': '1103594', '_score': 22.82367, '_source': {'product_name': 'Tarte aux noix', 'libel_clean': 'TARTE NOIX', 'energy_100g': 4.0, 'nutriscore_score': 4.0}}, {'_index': 'openfood', '_type': '_doc', '_id': '1150755', '_score': 22.82367, '_source': {'product_name': 'Tarte aux noix', 'libel_clean': 'TARTE NOIX', 'energy_100g': 1929.0, 'nutriscore_score': 21.0}}]}})\nSi on préfère sous une forme de DataFrame:\n\ndf = pd.json_normalize(\n    es.search(index = 'openfood', q = 'TART NOI', size = 3)['hits']['hits']\n)\ndf.columns = df.columns.str.replace(\"_source.\", \"\", regex = False)\ndf.head(2)\n\n\n\\n  \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n  \\n  \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n  \\n_index_type_id_scoreproduct_namelibel_cleanenergy_100gnutriscore_score0openfood_doc40533222.837925Tarte noixTARTE NOIX1833.023.01openfood_doc110359422.823670Tarte aux noixTARTE NOIX4.04.02openfood_doc115075522.823670Tarte aux noixTARTE NOIX1929.021.0\nPour automatiser l’envoi de requêtes et la récupération du meilleur\nécho, on peut définir la fonction suivante\n\ndef matchElastic(libelles):\n    start_time = time.time()\n    matches = {}\n    for l in libelles:\n        response = es.search(index = 'openfood', q = l, size = 1)\n        if len(response['hits']['hits'])&gt;0:\n            matches[l] = pd.json_normalize(\n              response['hits']['hits']\n            )\n    print(80*'-')\n    print(f\"Temps d'exécution total : {(time.time() - start_time):.2f} secondes ---\")\n    \n    return matches\n\n\nmatches = matchElastic(courses['libel_clean'])\nmatches = pd.concat(matches)\nmatches.sample(3)\n\n\n\\n  \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n  \\n  \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n  \\n_index_type_id_score_source.product_name_source.libel_clean_source.energy_100g_source.nutriscore_scoreGOUSSE AIL0openfood_doc198206257.93140Gousse d\\'ailGOUSSE AIL498.05.0IGP SAUVIGNON0openfood_doc180140696.55756vin blanc SauvignonVIN BLANC SAUVIGNON66.31.0POTIRON0openfood_doc104396175.96385PotironPOTIRON172.00.0\nEt voilà, on a un outil très rapide de requête !\nLa pertinence des résultats est encore douteuse.\nPour cela, il conviendrait de préciser des requêtes plus sophistiquées!2\n\nreq = {\n    \"bool\": {\n      \"should\": [\n        { \"match\": { \"libel_clean\":  { \"query\":  \"HUILE OLIVE\" , \"boost\" : 10}}},\n        { \"match\": { \"libel_clean.ngr\":   \"HUILE OLIVE\" }}\n        ],\n      \"minimum_should_match\": 1,\n      \"filter\": [\n      { \n            \"range\" : {\n                \"nutriscore_score\" : {\n                    \"gte\" : 10,\n                    \"lte\" : 20\n                    }\n                    }\n                    }\n      ]\n    }\n}\n\n\nout = es.search(index = 'openfood', query = req, size = 1)\npd.json_normalize(out['hits']['hits'])\n\n\n\\n  \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n  \\n  \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n  \\n_index_type_id_score_source.product_name_source.libel_clean_source.energy_100g_source.nutriscore_score0openfood_doc960041174.27896Huile d oliveHUILE OLIVE3761.011.0\nQu’a-t-on demandé ici?\n- De renvoyer 1 et 1 seul echo (\"size\":\"1\") et seulement si celui ci a:\n+ \"should\": Au moins un (\"minimum_should_match\":\"1\") des termes des deux champs libel_clean et libel_clean.ngr qui matche sur un terme de HUILE OLIVE, l’analyse (la définition du “terme”) étant réalisé soit en tant que text (“libel_clean”) soit en tant que n-gramme ngr (“libel_clean.ngr”, une analyse que nous avons spécifié dans le mapping)\n+ \"filter\": Le champ float nutriscore_score doit être compris entre 10 et 20 (“filter”).\nA noter :\n\nLes clauses (\"should\"+\"minimum_should_match\":\"1\") peuvent être remplacé par un \"must\". Auquel cas, l’écho doit obligatoirement matcher sur chaque clause.\nPréciser dans \"filter\" (plutôt que dans \"should\") une condition signifie que celle-ci ne participe pas au score de pertinence.\n\nOn n’a pas encore un appariemment très satisfaisant, en particulier sur les boissons. Comment faire ? La réponse est dans Galiana and Suarez Castillo (2022)\n\nA vous, de calculer le nombre de calories de notre recette de course !"
  },
  {
    "objectID": "content/modern-ds/elastic_intro.html#footnotes",
    "href": "content/modern-ds/elastic_intro.html#footnotes",
    "title": "Introduction à ElasticSearch pour la recherche textuelle",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLe lancement du service a créé dans votre NAMESPACE Kubernetes (l’ensemble de tout vos services) un cluster Elastic.\nVous n’avez droit qu’à un cluster par namespace (ou compte d’utilisateur).\nVotre service Jupyter, VSCode, RStudio, etc. est associé au même namespace.\nDe même qu’il n’est pas nécessaire de comprendre comment fonctionne le moteur d’une voiture pour conduire,\nil n’est pas nécessaire de comprendre la manière dont tout ce beau monde dialogue pour pouvoir utiliser le SSP Cloud.↩︎\nVous pouvez aussi explorer les possibilités de requêtes via la doc Elastic et vous entrainer à un écrire avec votre index tout neuf.↩︎"
  },
  {
    "objectID": "content/git/introgit.html",
    "href": "content/git/introgit.html",
    "title": "Git : un élément essentiel au quotidien",
    "section": "",
    "text": "Cette page reprend des éléments présents dans\nun cours dédié fait avec Romain Avouac."
  },
  {
    "objectID": "content/git/introgit.html#les-problèmes-classiques",
    "href": "content/git/introgit.html#les-problèmes-classiques",
    "title": "Git : un élément essentiel au quotidien",
    "section": "Les problèmes classiques",
    "text": "Les problèmes classiques\nDans un projet, il est commun de se demander (ou de demander à quelqu’un) :\n\nquelle était la bonne version d’un programme ?\nqui était l’auteur d’un bout de code en particulier ?\nsi un changement était important ou juste un essai ?\noù retrouver des traces d’un vieil essai abandonné mais potentiellement finalement prometteur ?\ncomment fusionner des programmes écrits par plusieurs personnes ?\netc."
  },
  {
    "objectID": "content/git/introgit.html#la-solution-le-contrôle-de-version",
    "href": "content/git/introgit.html#la-solution-le-contrôle-de-version",
    "title": "Git : un élément essentiel au quotidien",
    "section": "La solution: le contrôle de version",
    "text": "La solution: le contrôle de version\nIl existe un outil informatique puissant qui répond à tous ces besoins :\nla gestion de version (version control system (VCS) en anglais). Ses avantages sont incontestables et permettent de facilement :\n\nenregistrer l’historique des modifications d’un ensemble de fichiers ;\nrevenir à des versions précédentes d’un ou plusieurs fichiers ;\nrechercher les modifications qui ont pu créer des erreurs ;\npartager ses modifications et récupérer celles des autres ;\nproposer des modifications, les discuter, sans pour autant modifier d’emblée la dernière version existante ;\nidentifier les auteurs et la date des modifications.\n\nEn outre, ces outils fonctionnent avec tous les langages\ninformatiques (texte, R, Python, Markdown, LaTeX, Java, etc.)\ncar reposent sur la comparaison des lignes et des caractères des programmes."
  },
  {
    "objectID": "content/git/introgit.html#avantages-du-contrôle-de-version",
    "href": "content/git/introgit.html#avantages-du-contrôle-de-version",
    "title": "Git : un élément essentiel au quotidien",
    "section": "Avantages du contrôle de version",
    "text": "Avantages du contrôle de version\nOn peut ainsi résumer les principaux avantages du contrôle de version\nde la manière suivante :\n\nConserver et archiver l’ensemble des versions d’un code ou d’une documentation\nTravailler efficacement en équipe\nAméliorer la qualité des codes\nSimplifier la communication autour d’un projet\n\n\nConserver et archiver du code\nUne des principales fonctionnalités de la gestion de version est de conserver\nl’ensemble des fichiers de façon sécurisée et de proposer un archivage\nstructuré des codes. Les fichiers sont stockés dans un dépôt, qui constitue le projet.\nTout repose dans la gestion et la présentation de l’historique des modifications.\nChaque modification (ajout, suppression ou changement) sur un ou plusieurs fichiers est identifiée par son auteur,\nsa date et un bref descriptif1.\nChaque changement est donc unique et aisément identifiable quand les modifications sont classées par ordre chronologique. Les groupes de modifications transmis au dépôt sont appelées commit.\nAvec des outils graphiques, on peut vérifier l’\nensemble des évolutions d’un fichier (history),\nou l’histoire d’un dépôt.\nOn peut aussi\nse concentrer sur une modification particulière d’un fichier ou vérifier, pour un fichier, la\nmodification qui a entraîné l’apparition de telle ou telle ligne (blame)\nSur son poste de travail, les dizaines (voire centaines) de programmes organisés à la main n’existent plus. Tout est regroupé dans un seul dossier, rassemblant les éléments du dépôt. Au sein du dépôt, tout l’historique est stocké et accessible rapidement. Si on souhaite travailler sur la dernière version des programmes (ou sur une ancienne version spécifique), il n’y a plus besoin de conserver les autres fichiers car ils sont dans l’historique du projet. Il est alors possible de choisir sur quelle version on veut travailler (la dernière commune à tout le monde, la sienne en train d’être développée, celle de l’année dernière, etc.).\n\n\nTravailler efficacement en équipe\nLe deuxième avantage de la gestion de version représente une amélioration notable du travail en équipe sur des codes en commun.\nLa gestion de version permet de collaborer simplement et avec méthode. De façon organisée, elle permet de :\n\ntravailler en parallèle et fusionner facilement du code\npartager une documentation des programmes grâce :\n\naux commentaires des modifications\nà la possibilité d’une documentation commune et collaborative\n\ntrouver rapidement des erreurs et en diffuser rapidement la\ncorrection\n\nA ces avantages s’ajoutent les fonctionalités collaboratives des forges\nqui sont des plateformes où peuvent être stockés des dépôts.\nNéanmoins, ces forges proposent aujourd’hui beaucoup de fonctionalités\nqui vont au-delà de l’archivage de code :\ninteragir via\ndes issues,\nfaire des suggestions de modifications, exécuter du code dans des\nenvironnements normalisés, etc.\nIl faut vraiment les voir comme des réseaux sociaux du code.\nLes principales plateformes dans ce domaine étant Github et Gitlab.\nL’usage individuel, c’est-à-dire seul sur son projet,\npermet aussi de “travailler en équipe avec soi-même” car il permet de retrouver des mois plus tard le contenu et le contexte des modifications. Cela est notamment précieux lors des changements de poste ou des travaux réguliers mais espacés dans le temps (par exemple, un mois par an chaque année). Même lorsqu’on travaille tout seul, on collabore avec un moi futur qui peut ne plus se souvenir de la modification des fichiers.\n\n\nAméliorer la qualité des codes\nLe fonctionnement de la gestion de version, reposant sur l’archivage structuré des modifications et les commentaires les accompagnant, renforce la qualité des programmes informatiques. Ils sont plus documentés, plus riches et mieux structurés. C’est pour cette raison que le contrôle de version ne doit pas être considéré comme un outil réservé à des développeurs : toute personne travaillant sur des programmes informatiques gagne à utiliser du contrôle de version.\nLes services d’intégration continue permettent de faire des tests automatiques\nde programmes informatiques, notamment de packages, qui renforcent la\nréplicabilité des programmes. Mettre en place des méthodes de travail fondées\nsur l’intégration continue rend les programmes plus robustes en forçant\nceux-ci à tourner sur des machines autres que celles du développeur du code.\n\n\nSimplifier la communication autour d’un projet\nLes sites de dépôts Github et Gitlab permettent de faire beaucoup plus\nque seulement archiver des codes. Les fonctionalités de déploiement\nen continu permettent ainsi de :\n\ncréer des sites web pour valoriser des projets (par exemple les sites\nreadthedocs en python)\ndéployer de la documentation en continu\nrendre visible la qualité d’un projet avec des services de code coverage,\nde tests automatiques ou d’environnements intégrés de travail (binder, etc.)\nqu’on rend généralement visible au moyen de badges\n(exemple ici)"
  },
  {
    "objectID": "content/git/introgit.html#copies-de-travail-et-dépôt-collectif",
    "href": "content/git/introgit.html#copies-de-travail-et-dépôt-collectif",
    "title": "Git : un élément essentiel au quotidien",
    "section": "Copies de travail et dépôt collectif",
    "text": "Copies de travail et dépôt collectif\nGit est un système décentralisé et asynchrone de gestion de version.\nCela signifie que:\n\nChaque membre d’un projet travaille sur une copie locale du dépôt\n(système decentralisé). Cette copie de travail s’appelle un clone.\nCela signifie qu’on n’a pas une cohérence en continu de notre version\nde travail avec le dépôt ; on peut très bien ne jamais vouloir les\nmettre en cohérence (par exemple, si on teste une piste qui s’avère\ninfructueuse) ;\nC’est lorsqu’on propose la publication de modifications sur le dépôt\ncollectif qu’on doit s’assurer de la cohérence avec la version disponible\nen ligne (système asynchrone).\n\nLe dépôt distant est généralement stocké sur\nune forge logicielle (Github ou Gitlab) et sert à centraliser la version\ncollective d’un projet. Les copies locales sont des copies de travail\nqu’on utilise pour faire évoluer un projet :\nIl est tout à fait possible de faire du contrôle de version sans\nmettre en place de dépôt distant. Cependant,\n\nc’est dangereux puisque le dépôt distant fait office de sauvegarde\nd’un projet. Sans dépôt distant, on peut tout perdre en cas de problème\nsur la copie locale de travail ;\nc’est désirer être moins efficace car, comme nous allons le montrer, les\nfonctionalités des plateformes Github et Gitlab sont également très\nbénéfiques lorsqu’on travaille tout seul."
  },
  {
    "objectID": "content/git/introgit.html#principe",
    "href": "content/git/introgit.html#principe",
    "title": "Git : un élément essentiel au quotidien",
    "section": "Principe",
    "text": "Principe\nLes trois manipulations les plus courantes sont les suivantes et représentées sur le diagramme ci-après :\n\ncommit : je valide les modifications que j’ai faites en local avec un message qui les explique\npull : je récupère la dernière version des codes du dépôt distant\npush : je transmets mes modifications validées au dépôt distant\n\n\n\n\n\n\nLes deux dernières manipulations correspondent aux interactions (notamment\nla mise en cohérence) avec\nle dépôt commun alors que la première manipulation commit correspond à\nla modification des fichiers faite pour faire évoluer un projet.\nDe manière plus précise, il y a trois étapes avant d’envoyer les modifications validées (commit) au dépôt. Elles se définissent en fonction des commandes qui permettent de les appliquer quand Git est utilisé en lignes de commandes :\n\ndiff : inspection des modifications. Cela permet de comparer les fichiers modifiés et de distinguer les fichiers ajoutés ou supprimés.\nstaging area : sélection des modifications.\ncommit : validation des modifications sélectionnées (avec commentaire).\n\n\n\n\n\n\nLors des étapes de push et pull, des conflits peuvent apparaître, par exemple lorsque deux personnes ont modifié le même programme simultanément. Le terme conflit peut faire peur mais en fait c’est\nl’un des apports principaux de Git que de faciliter énormément la gestion\nde versions différentes. Les exercices du chapitre suivant l’illustreront."
  },
  {
    "objectID": "content/git/introgit.html#les-branches",
    "href": "content/git/introgit.html#les-branches",
    "title": "Git : un élément essentiel au quotidien",
    "section": "Les branches",
    "text": "Les branches\nC’est une des fonctionnalités les plus pratiques de la gestion de version.\nLa création de branches dans un projet (qui devient ainsi un arbre)\npermet de développer en parallèle des correctifs ou une nouvelle fonctionnalité\nsans modifier le dépôt commun.\nCela permet de séparer le nouveau développement et de faire cohabiter plusieurs versions, pouvant évoluer séparément ou pouvant être facilement rassemblées. Git est optimisé pour le travail sur les branches.\nDans un projet collaboratif, une branche dite master joue le rôle du tronc.\nC’est autour d’elle que vont pousser ou se greffer les branches.\nL’un des avantages de Git est qu’on peut toujours revenir en arrière. Ce\nfilet de sécurité permet d’oser des expérimentations, y compris au sein\nd’une branche. Il faut être prêt à aller dans la ligne de commande pour cela\nmais c’est extrêmement confortable.\n\n\n Note\nComment nommer les branches ? Là encore, il y a énormément de conventions différentes. Une fréquemment observée est :\n\npour les nouvelles fonctionnalités : feature/nouvelle-fonctionnalite où nouvelle-fontionnalite est un nom court résumant la fonctionnalité\npour les corrections de bug : issue-num où num est le numéro de l’issue\n\nN’hésitez pas à aller encore plus loin dans la normalisation !"
  },
  {
    "objectID": "content/git/introgit.html#footnotes",
    "href": "content/git/introgit.html#footnotes",
    "title": "Git : un élément essentiel au quotidien",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPlus précisément, chaque modification est identifiée de manière unique par un code SHA auquel est associé l’auteur, l’horodatage et des métadonnées (par exemple le message descriptif associé).↩︎"
  },
  {
    "objectID": "content/annexes/evaluation.html",
    "href": "content/annexes/evaluation.html",
    "title": "Evaluation",
    "section": "",
    "text": "Résumé :"
  },
  {
    "objectID": "content/annexes/evaluation.html#attentes-du-projet",
    "href": "content/annexes/evaluation.html#attentes-du-projet",
    "title": "Evaluation",
    "section": "Attentes du projet",
    "text": "Attentes du projet\nLe projet est une problématique à laquelle vous souhaitez répondre à\nl’aide d’un ou de plusieurs jeu(s) de données.\nIl faut donc dans un premier temps se pencher sur la recherche de problématisation et de contextualisation. Nous vous recommandons de prendre un sujet qui vous intéresse pour intéresser également le lecteur.\nTrois dimensions doivent être présentes dans le projet.\nPour chacune de ces parties, il est possible d’aller plus ou moins loin. Il est recommandé d’aller loin sur au moins une des 3 dimensions.\n\nLa récupération et le traitement des données\nCes données peuvent être directement disponibles sous la forme de fichiers txt, csv … ou provenir de sites internet (scraping, API). Plus le travail sur la récupération de données est important (par exemple scraping sur plusieurs sites), plus la partie obtiendra de points. Si le jeu de données utilisé est un téléchargement d’un jeu propre existant, il faudra chercher à le compléter d’une manière ou d’une autre pour obtenir des points sur cette partie.\nVous obtiendrez vraisemblablement des données qui ne sont pas « propres » du premier coup : mettez en place des protocoles de nettoyage pour obtenir à la fin de cette étape un ou des jeux de données fiable et robuste pour mener ensuite votre analyse. C’est également le moment de créer des variables plus appréhendables, mieux identifiées etc.\n\n\nL’analyse descriptive et la représentation graphique\nLa présence de statistiques descriptives est indispensable dans le projet. De la description de la base aux premières grandes tendances des données, cette partie permet d’avoir une vision globale des données : le lien avec la problématique, comment elle permet d’y répondre, quels sont les premiers éléments de réponse… Chaque résultat doit être interprété : pas la peine de faire un describe et de ne pas le commenter.\nEn termes de représentation graphique, plusieurs niveaux sont envisageables. Vous pouvez simplement représenter vos données en utilisant matplotlib, aller plus loin avec seaborn ou scikit-plot, (voire D3.js pour les plus motivés). La base d’une bonne visualisation est de trouver le type de graphique adéquat pour ce que vous voulez montrer (faut-il un scatter ou un line pour représenter une évolution ?) et de le rendre visible : une légende qui a du sens, des axes avec des noms etc. Encore une fois, il faudra commenter votre graphique, qu’est ce qu’il montre, en quoi cela valide / contredit votre argumentaire ?\n\n\nLa modélisation\nVient ensuite la phase de modélisation : un modèle peut être le bienvenu quand des statistiques descriptives ne suffisent pas à apporter une solution complète à votre problématique ou pour compléter / renforcer l’analyse descriptive. Le modèle importe peu (régression linéaire, random forest ou autre) : il doit être approprié (répondre à votre problématique) et justifié.\nVous pouvez aussi confronter plusieurs modèles qui n’ont pas la même vocation : par exemple une CAH pour catégoriser et créer des nouvelles variables / faire des groupes puis une régression.\nMême si le projet n’est pas celui du cours de stats, il faut que la démarche soit scientifique et que les résultats soient interprétés."
  },
  {
    "objectID": "content/annexes/evaluation.html#format-du-rendu",
    "href": "content/annexes/evaluation.html#format-du-rendu",
    "title": "Evaluation",
    "section": "Format du rendu",
    "text": "Format du rendu\nSur le format du rendu, vous devrez :\n\nÉcrire un rapport sous forme de Notebook (quelques exceptions à cette règle peuvent exister, par exemple si vous développer une appli Dash)\nAvoir un répertoire Github avec le rapport. Les données utilisées doivent être accessibles également, dans le dépôt ou sur internet.\nLes dépôts Github où seul un upload du projet a été réalisé seront pénalisés. A l’inverse, les dépôts dans lequels le contrôle de version et le travail collaboratif ont été activement pratiqués (commits fréquents, pull requests, ..) seront valorisés.\nLe code contenu dans le rapport devra être un maximum propre (pas de copier coller de cellule, préférez des fonctions)\n\nCe post donne\nquelques conseils pour avoir des notebooks agréables à lire. N’oubliez pas cette règle :\n\ncode is read much more often than written\n\nLors de l’évaluation, une attention particulière sera donnée à la reproductibilité de votre projet.\nChaque étape (récupération et traitement des données, analyses descriptives, modélisation) doit pouvoir être reproduite à partir du notebook final. Pour les opérations qui prennent du temps (ex : web scraping massif, requêtage d’API avec des limites de nombre de requêtes, entraînement de modèle, etc.), vous devez inclure l’output (base de données, modèle entraîné..) dans le dépôt, afin que les étapes suivantes puissent s’éxecuter sans problème.\nLe test à réaliser : faire tourner toutes les cellules de votre notebook et ne pas avoir d’erreur est une condition sine qua non pour avoir la moyenne."
  },
  {
    "objectID": "content/annexes/evaluation.html#barême-approximatif",
    "href": "content/annexes/evaluation.html#barême-approximatif",
    "title": "Evaluation",
    "section": "Barême approximatif",
    "text": "Barême approximatif\n\nDonnées (collecte et nettoyage) : 4 points\nAnalyse descriptive : 4 points\nModélisation : 2 points\nDémarche scientifique et reproductibilité du projet : 4 points\nFormat du code (code propre et github) : 2 points\nSoutenance : 4 points\n\nLe projet doit être réalisé en groupe de trois, voire deux."
  },
  {
    "objectID": "content/annexes/evaluation.html#projets-menés-par-les-étudiants",
    "href": "content/annexes/evaluation.html#projets-menés-par-les-étudiants",
    "title": "Evaluation",
    "section": "Projets menés par les étudiants",
    "text": "Projets menés par les étudiants\n\n\n\n\n\n\n\n\n\nProjet\nAuteurs\nURL projet \nTags\n\n\n\n\nGPS vélo intégrant les bornes Vélib, les accidents, la congestion et la météo\nVinciane Desbois ; Imane Fares ; Romane Gajdos\nhttps://github.com/ImaneFa/Projet_Python\nVélib ; Pistes cyclables ; Accidents ; Folium\n\n\nQuiz Generator\nAdrien Servière ; Mélissa Tamine\nhttps://github.com/taminemelissa/quiz-generator\nMachine Learning ; Natural Language Processing ; Question Generation ; Word2Vec\n\n\nAnalyse de sentiments sur les vaccins COVID administrés en France\nKOAGNE FONGUIENG Florette ; KONKOBO Idrissa\nhttps://github.com/kidrissa/projetpy\nAPI ; NLP ; Wordcloud ; Modélisation prédictive\n\n\nEstimation de l’empreinte carbone d’une recette de cuisine\nJean-Baptiste Laval ; Hadrien Lolivier ; Sirine Louati\nhttps://github.com/sirinelouati/Plat_CO2\nscraping ; Dashboard ; Empreinte carbone ; Alimentation\n\n\nLe “bon sens du boucher-charcutier de Tourcoing vaut-il mieux que les enquêtes de victimation ?”\nConrad Thiounn ; Gaston Vermersch\nhttps://github.com/cthiounn/python-datascience-ENSAE-2A\nAPI ; Open-data ; ACP ; CAH ; LASSO\n\n\nPrédiction du revenu généré par un film en fonction de ses caractéristiques\nDmitri Lebrun ; Corentin Pernot ; Nina Stizi\nhttps://github.com/NinaStizi/Python_ENSAE_2A\nScrapping ; Cinéma ; Machine Learning\n\n\nAnalyse du réseau ferré de la SNCF: Comment expliquer les retards permanents de la compagnie française ?\nDiego Renaud ; Victor Parent ; Marion Chabrol\nhttps://github.com/NinaStizi/Python_ENSAE_2A\nAPI ; SNCF ; LASSO\n\n\nLe “bon sens du boucher-charcutier de Tourcoing vaut-il mieux que les enquêtes de victimation ?”\nConrad Thiounn ; Gaston Vermersch\nhttps://github.com/cthiounn/python-datascience-ENSAE-2A\nAPI ; Open-data ; ACP ; CAH ; LASSO\n\n\nPrédiction du revenu généré par un film en fonction de ses caractéristiques\nDmitri Lebrun ; Corentin Pernot ; Nina Stizi\nhttps://github.com/NinaStizi/Python_ENSAE_2A\nScrapping ; Cinéma ; Machine Learning\n\n\nAnalyse du réseau ferré de la SNCF: Comment expliquer les retards permanents de la compagnie française ?\nDiego Renaud ; Victor Parent ; Marion Chabrol\nhttps://github.com/NinaStizi/Python_ENSAE_2A\nAPI ; SNCF ; LASSO"
  }
]