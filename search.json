[
  {
    "objectID": "index.html#th√®mes-en-vrac",
    "href": "index.html#th√®mes-en-vrac",
    "title": "Python pour la data science",
    "section": "Th√®mes en vrac",
    "text": "Th√®mes en vrac\nPour d√©couvrir Python  de mani√®re th√©matique\n\n\n\n\n\n\n\n\n\n\nQuelques √©l√©ments pour comprendre les enjeux du NLP\n\n\n\nNLP\n\n\nTutoriel\n\n\n\nLes corpus textuels √©tant des objets de tr√®s grande dimension\no√π le ratio signal/bruit est faible, il est n√©cessaire de mettre\nen oeuvre une s√©rie d‚Äô√©tapes de nettoyage de‚Ä¶\n\n\n\n\n\n\nJul 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNettoyer un texte: des exercices pour d√©couvrir l‚Äôapproche bag-of-words\n\n\n\nNLP\n\n\nExercice\n\n\n\nCe chapitre continue de pr√©senter l‚Äôapproche de nettoyage de donn√©es\ndu NLP en s‚Äôappuyant sur le corpus de trois auteurs\nanglo-saxons : Mary Shelley, Edgar Allan Poe‚Ä¶\n\n\n\n\n\n\nOct 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLatent Dirichlet Allocation (LDA)\n\n\n\nTutoriel\n\n\nNLP\n\n\n\nLa Latent Dirichlet Allocation (LDA)\nest un mod√®le probabiliste g√©n√©ratif qui permet\nde d√©crire des‚Ä¶\n\n\n\n\n\n\nOct 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nM√©thodes de vectorisation : comptages et word embeddings\n\n\n\nTutoriel\n\n\nNLP\n\n\n\nPour pouvoir utiliser des donn√©es textuelles dans des algorithmes\nde machine learning, il faut les vectoriser, c‚Äôest √† dire transformer\nle texte en donn√©es num√©riques.‚Ä¶\n\n\n\n\n\n\nOct 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercices suppl√©mentaires\n\n\n\nExercice\n\n\nNLP\n\n\n\nDes exercices suppl√©mentaires pour pratiquer les concepts du NLP\n\n\n\n\n\n\nApr 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nPartie 4 : Natural Language Processing (NLP)\n\n\n\n\n\n\n\n\n\nOct 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nCorrections\n\n\n\n\n\n\n\n\n\nSep 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfiguration de Python\n\n\n\nTutoriel\n\n\nRappels\n\n\n\nL‚Äôenvironnement que propose Python pour la data-science\nest tr√®s riche. Afin de b√©n√©ficier du meilleur environnement\npour tirer parti du langage, ce chapitre‚Ä¶\n\n\n\n\n\n\nJul 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL‚Äôenvironnement Python pour la data-science\n\n\n\nTutoriel\n\n\nRappels\n\n\n\nPython propose un √©cosyst√®me tr√®s riche pour la\ndata-science. Ce chapitre fait un tour\nd‚Äôhorizon de celui-ci en pr√©sentant les principaux\npackages qui seront pr√©sent√©s‚Ä¶\n\n\n\n\n\n\nJul 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComment aborder un jeu de donn√©es\n\n\n\nTutoriel\n\n\nRappels\n\n\n\nQuelques √©l√©ments pour adopter une d√©marche\nscientifique et √©thique face √† un\njeu de donn√©es.\n\n\n\n\n\n\nJul 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBonne pratique de Python\n\n\n\nTutoriel\n\n\nRappels\n\n\n\nLes normes communautaires du monde de\nl‚Äôopen-source ont permis une\nharmonisation de la structure des projets\nPython et des scripts. Ce chapitre\n√©voque quelques unes de‚Ä¶\n\n\n\n\n\n\nJul 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuelques rappels sur les principes de base de Python\n\n\n\nTutoriel\n\n\nRappels\n\n\n\nRappels d‚Äô√©l√©ments essentiels en Python: les r√®gles de syntaxes, les classes,\nles m√©thodes, etc.\n\n\n\n\n\n\nJul 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModules, tests, boucles, fonctions\n\n\n\nTutoriel\n\n\nRappels\n\n\n\nLes fonctions permettent de g√©n√©raliser des\ninstructions. Il s‚Äôagit ainsi d‚Äôun outil privil√©gi√©\npour automatiser des t√¢ches r√©p√©titives ou r√©duire\nla complexit√© d‚Äôune cha√Æne‚Ä¶\n\n\n\n\n\n\nJul 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLes classes en Python\n\n\n\nTutoriel\n\n\nRappels\n\n\n\nLa programmation orient√©e objet (POO) est\nl‚Äôun des atouts de Python. Elle permet\nd‚Äôadapter des‚Ä¶\n\n\n\n\n\n\nJul 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\nCette introduction propose quelques √©l√©ments de\nr√©vision des concepts de base en Python et\npr√©sente l‚Äô√©cosyst√®me Python que nous allons\nd√©couvrir tout au long de ce‚Ä¶\n\n\n\n\n\n\nJun 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUn cadavre exquis pour d√©couvrir Git\n\n\n\nExercice\n\n\nGit\n\n\n\nCe chapitre propose une mise en application de quelques principes\ncentraux du langage Git vus pr√©c√©demment\n\n\n\n\n\n\nSep 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGit: un outil n√©cessaire pour les data-scientists\n\n\n\nGit\n\n\n\nUne partie annexe au cours pour d√©couvrir Git,\nun outil\ndevenu indispensable pour les data-scientists\nafin de mener des projets impliquant\ndu code Python.\n\n\n\n\n\n\nJul 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGit : un √©l√©ment essentiel au quotidien\n\n\n\nTutoriel\n\n\nGit\n\n\n\nGit est un syst√®me de contr√¥le de version qui facilite la\nsauvegarde, la gestion des √©volutions et le partage\nd‚Äôun projet informatique. Il s‚Äôagit d‚Äôun √©l√©ment‚Ä¶\n\n\n\n\n\n\nSep 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nContenu du cours\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumpy, la brique de base de la data science\n\n\n\nTutoriel\n\n\nManipulation\n\n\n\nNumpy constitue la brique de base de l‚Äô√©cosyst√®me de la data-science en\nPython. Toutes les librairies de manipulation de donn√©es, de mod√©lisation\net de visualisation‚Ä¶\n\n\n\n\n\n\nJul 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction √† Pandas\n\n\n\nTutoriel\n\n\nManipulation\n\n\n\nPandas est l‚Äô√©l√©ment central de l‚Äô√©cosyst√®me Python pour la data-science.\nLe succ√®s r√©cent de Python dans l‚Äôanalyse de donn√©es tient beaucoup √† pandas qui a permis‚Ä¶\n\n\n\n\n\n\nJul 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPratique de pandas: un exemple complet\n\n\n\nManipulation\n\n\nExercice\n\n\n\nApr√®s avoir pr√©sent√© la logique de pandas dans le chapitre pr√©c√©dent,\nce chapitre vise √† illustrer les fonctionalit√©s du package\n√† partir de donn√©es d‚Äô√©missions de gaz √†‚Ä¶\n\n\n\n\n\n\nJul 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPratique de geopandas avec les donn√©es v√©lib\n\n\n\nManipulation\n\n\nExercice\n\n\n\nCe chapitre illustre les fonctionalit√©s de GeoPandas √† partir des\nd√©comptes de v√©lo fournis par la ville de Paris\nen‚Ä¶\n\n\n\n\n\n\nJul 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDonn√©es spatiales: d√©couverte de geopandas\n\n\n\nTutoriel\n\n\nManipulation\n\n\n\nLes donn√©es g√©olocalis√©es se sont multipli√©es depuis quelques ann√©es, qu‚Äôil\ns‚Äôagisse de donn√©es open-data ou de traces num√©riques g√©olocalis√©es de\ntype big-data. Pour les‚Ä¶\n\n\n\n\n\n\nJul 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWebscraping avec python\n\n\n\nExercice\n\n\nManipulation\n\n\n\nPython permet de facilement r√©cup√©rer une page web pour en extraire des\ndonn√©es √† restructurer. Le webscraping, que les Canadiens nomment\n‚Äúmoissonnage du web‚Äù, est une‚Ä¶\n\n\n\n\n\n\nSep 8, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMa√Ætriser les expressions r√©guli√®res\n\n\n\nTutoriel\n\n\nManipulation\n\n\n\nLes expressions r√©guli√®res fournissent un cadre tr√®s pratique pour manipuler\nde mani√®re flexible des donn√©es textuelles. Elles sont tr√®s utiles\nnotamment pour les t√¢ches de‚Ä¶\n\n\n\n\n\n\nJul 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR√©cup√©rer des donn√©es avec des API depuis Python\n\n\n\nExercice\n\n\nManipulation\n\n\n\nLes API (Application Programming Interface) sont un mode d‚Äôacc√®s aux\ndonn√©es en expansion. Gr√¢ce aux API, l‚Äôautomatisation de scripts\nest facilit√©e puisqu‚Äôil n‚Äôest‚Ä¶\n\n\n\n\n\n\nJul 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercices suppl√©mentaires de webscraping\n\n\n\nExercice\n\n\nManipulation\n\n\n\n\n\n\n\n\n\n\nJul 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction √† dask gr√¢ce aux donn√©es DVF\n\n\n\nTutoriel\n\n\nManipulation\n\n\n\n\n\n\n\n\n\n\nAug 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPartie 1: manipuler des donn√©es\n\n\n\n\n\n\n\n\n\nJul 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPr√©paration des donn√©es pour construire un mod√®le\n\n\n\nMod√©lisation\n\n\nExercice\n\n\n\nAfin d‚Äôavoir des donn√©es coh√©rentes avec les hypoth√®ses de mod√©lisation,\nil est absolument fondamental de prendre le temps de\npr√©parer les donn√©es √† fournir √† un mod√®le. La‚Ä¶\n\n\n\n\n\n\nOct 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluer la qualit√© d‚Äôun mod√®le\n\n\n\nMod√©lisation\n\n\nExercice\n\n\n\nFaire preuve de m√©thode pour √©valuer la qualit√© d‚Äôun mod√®le\npermet de proposer des pr√©dictions plus robustes, ayant\nde meilleures performances sur un nouveau jeu de‚Ä¶\n\n\n\n\n\n\nOct 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification: premier mod√®le avec les SVM\n\n\n\nMod√©lisation\n\n\nExercice\n\n\n\nLa classification permet d‚Äôattribuer une classe d‚Äôappartenance (label\ndans la terminologie du machine learning)\ndiscr√®te √† des donn√©es √† partir de certaines variables‚Ä¶\n\n\n\n\n\n\nOct 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR√©gression : une introduction\n\n\n\nMod√©lisation\n\n\nExercice\n\n\n\nLa r√©gression lin√©aire est la premi√®re mod√©lisation statistique\nqu‚Äôon d√©couvre dans un cursus quantitatif. Il s‚Äôagit en effet d‚Äôune\nm√©thode tr√®s intuitive et tr√®s riche. Le‚Ä¶\n\n\n\n\n\n\nOct 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nS√©lection de variables : une introduction\n\n\n\nMod√©lisation\n\n\nExercice\n\n\n\nL‚Äôacc√®s √† des bases de donn√©es de plus en plus riches permet\ndes mod√©lisations de plus en plus raffin√©es. Cependant,\nles mod√®les parcimonieux sont g√©n√©ralement‚Ä¶\n\n\n\n\n\n\nOct 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustering\n\n\n\nMod√©lisation\n\n\nExercice\n\n\n\nLe clustering consiste √† r√©partir des observations dans des groupes,\ng√©n√©ralement non observ√©s,\nen fonction de caract√©ristiques observables. Il s‚Äôagit d‚Äôune\napplication‚Ä¶\n\n\n\n\n\n\nOct 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPremier pas vers l‚Äôindustrialisation avec les pipelines scikit\n\n\n\nMod√©lisation\n\n\nTutoriel\n\n\n\nLes pipelines scikit permettent d‚Äôint√©grer de mani√®re tr√®s flexible\nun ensemble d‚Äôop√©rations de pre-processing et d‚Äôentra√Ænement de mod√®les\ndans une cha√Æne d‚Äôop√©rations.‚Ä¶\n\n\n\n\n\n\nOct 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nPartie 3: mod√©liser\n\n\n\n\n\n\n\n\n\nOct 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nInt√©gration continue avec Python\n\n\n\n\n\n\n\n\n\nJul 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nG√©n√©ration d‚Äôimages avec Python, DALL-E et StableDiffusion\n\n\n\nTutoriel\n\n\nAvanc√©\n\n\n\nLa hype autour du\nmod√®le de g√©n√©ration d‚Äôimage Dall-E a amen√©\nune grande attention sur les mod√®les\nautog√©n√©ratifs de contenu. Dall-E est, √† l‚Äôheure\nactuelle, le mod√®le‚Ä¶\n\n\n\n\n\n\nAug 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nApprofondissement ElasticSearch pour des recherches de proximit√© g√©ographique\n\n\n\nTutoriel\n\n\nAvanc√©\n\n\n\nUn chapitre plus approfondi sur ElasticSearch\n\n\n\n\n\n\nSep 3, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction √† ElasticSearch pour la recherche textuelle\n\n\n\nTutoriel\n\n\nAvanc√©\n\n\n\nElasticSearch est un moteur de recherche extr√™mement rapide et flexible.\nCette technologie s‚Äôest impos√©e dans le domaine du traitement des\ndonn√©es textuelles. L‚ÄôAPI‚Ä¶\n\n\n\n\n\n\nSep 3, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nPartie 5: Introduction aux outils et m√©thodes √† l‚Äô√©tat de l‚Äôart\n\n\n\n\n\n\n\n\n\nOct 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nLes nouveaux modes d‚Äôacc√®s aux donn√©es: le format parquet et les donn√©es sur le cloud\n\n\n\nTutoriel\n\n\n\n\n\n\n\n\n\n\nAug 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPartie 2: visualiser les donn√©es\n\n\n\n\n\n\n\n\n\nSep 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDe belles cartes avec python: mise en pratique\n\n\n\nVisualisation\n\n\nExercice\n\n\n\nLa cartographie est un excellent moyen de diffuser\nune connaissance, y compris √† des publics peu\nfamiliers de la statistique. Ce chapitre permet\nde d√©couvrir la mani√®re dont‚Ä¶\n\n\n\n\n\n\nOct 6, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDe beaux graphiques avec python: mise en pratique\n\n\n\nVisualisation\n\n\nExercice\n\n\n\nUne partie essentielle du travail du\ndata-scientist est d‚Äô√™tre en mesure\nde synth√©tiser une information dans des\nrepr√©sentations graphiques percutantes. Ce\nchapitre permet‚Ä¶\n\n\n\n\n\n\nSep 24, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/slides/intro/index.html#pr√©sentation",
    "href": "content/slides/intro/index.html#pr√©sentation",
    "title": "1¬† Python pour la data-science",
    "section": "1.1 Pr√©sentation",
    "text": "1.1 Pr√©sentation\n\nData-scientist √† l‚ÄôInsee\nCoordinateur d‚Äôun r√©seau des data-scientists dans l‚Äôadministration:\n\nSite web: https://ssphub.netlify.app/ ;\n\n\n. . .\n\nExp√©rience traitement big-data et NLP:\n\nCours opinionated ;\nDes conseils et des recommandations issues de l‚Äôexp√©rience\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n Envoyez-moi un mail :\n\npour suivre l‚Äôactualit√© du r√©seau des data-scientists ;\nsi vous √™tes int√©ress√©s par travailler dans l‚Äôadministration ;"
  },
  {
    "objectID": "content/slides/intro/index.html#objectifs-du-cours",
    "href": "content/slides/intro/index.html#objectifs-du-cours",
    "title": "1¬† Python pour la data-science",
    "section": "1.2 Objectifs du cours",
    "text": "1.2 Objectifs du cours\n\nPouvoir utiliser Python  pour l‚Äôensemble du processus de valorisation des donn√©es :\n\nR√©cup√©rer et structurer des donn√©es ;\nManipuler ;\nVisualiser ;\nMod√©liser.\n\n\n. . .\n\nComprendre comment Python  sert dans une d√©marche de recherche ou de production ;\n\n. . .\n\nS‚Äôinitier √† la question de la reproductibilit√©  (open-source, open-data‚Ä¶) ;\n\n. . .\n\nD√©couvrir la pratique moderne de Python  dans le monde de la data-science ;\n\n. . .\n\nD√©couvrir qu‚Äôon peut faire des choses sympa ‚ú® avec Python ."
  },
  {
    "objectID": "content/slides/intro/index.html#organisation-du-cours",
    "href": "content/slides/intro/index.html#organisation-du-cours",
    "title": "1¬† Python pour la data-science",
    "section": "1.3 Organisation du cours",
    "text": "1.3 Organisation du cours\n\n\nCours dure 21h avec des s√©ances de 3h ;\n\n. . .\n\nValidation avec un projet de fin de semestre\n\nPlus de d√©tails dans quelques instants\n\n\n. . .\n\nUn canal d‚Äôinformation (non obligatoire) sur Slack"
  },
  {
    "objectID": "content/slides/intro/index.html#contenu-mis-√†-disposition",
    "href": "content/slides/intro/index.html#contenu-mis-√†-disposition",
    "title": "1¬† Python pour la data-science",
    "section": "1.4 Contenu mis √† disposition",
    "text": "1.4 Contenu mis √† disposition\n\n\nPoints d‚Äôentr√©e principaux:\n\nSite web : https://pythonds.linogaliana.fr/ ;\nD√©p√¥t Github  : https://github.com/linogaliana/python-datascientist\n\n¬†Star this website on Github43\n\n\n\n. . .\n\nDes tutoriels et exercices sous format Jupyter Notebook:\n\nPossibilit√© de les tester sur des environnements temporaires d‚Äôex√©cution\n\n\n\n\nPossibilit√© de les visualiser et t√©l√©charger au format Jupyter Notebook\n\n\n\n\n\n\n\n\n\n\n\n\nRecommandation\n\n\n\nPrivil√©gier le SSPCloud https://datalab.sspcloud.fr"
  },
  {
    "objectID": "content/slides/intro/index.html#examen",
    "href": "content/slides/intro/index.html#examen",
    "title": "1¬† Python pour la data-science",
    "section": "1.5 Examen",
    "text": "1.5 Examen\n\nProjet collaboratif s‚Äôappuyant sur Python \nr√©pondant √† une d√©marche scientifique et ouverte:\n\nProjet disponible sur Github \nLe projet doit √™tre document√©\n\n\n. . .\n\nSujet libre :\n\nDiscutez avec votre charg√© de TD pour avoir un avis ;\n\n\n. . .\n\n\n\n\n\n\nImportant\n\n\n\n\nExigence de reproductibilit√©\n\nProjet utilisant des sources open-data ou scrappant des sites publics ;\nCode doit pouvoir √™tre r√©pliqu√© par charg√© TD ;\nPas un projet Kaggle !"
  },
  {
    "objectID": "content/slides/intro/index.html#examen-1",
    "href": "content/slides/intro/index.html#examen-1",
    "title": "1¬† Python pour la data-science",
    "section": "1.6 Examen",
    "text": "1.6 Examen\n\nApprofondir des aspects du cours:\n\nTrois dimensions doivent √™tre pr√©sentes dans le projet: manipuler, visualiser ou mod√©liser ;\nPlus ou moins loin selon les projets.\n\n\n. . .\n\nExemples de sujets l‚Äôan dernier:\n\nSuggestions de trajets pour cyclistes ;\nCr√©ation d‚Äôun syst√®me de r√©ponse automatique aux tweets mentionnant la RATP ;\nComparer la performance de mod√®les de gestion de portefeuille ;\n\n\n\n\n\n\n\n\nImportant\n\n\n\nDeadline en janvier 2023, date pr√©cis√©e ult√©rieurement"
  },
  {
    "objectID": "content/course/modern-ds/index.html#contenu-de-la-partie",
    "href": "content/course/modern-ds/index.html#contenu-de-la-partie",
    "title": "3¬† Partie 5: Introduction aux outils et m√©thodes √† l‚Äô√©tat de l‚Äôart",
    "section": "3.1 Contenu de la partie",
    "text": "3.1 Contenu de la partie"
  },
  {
    "objectID": "content/course/modern-ds/elastic_approfondissement/index.html",
    "href": "content/course/modern-ds/elastic_approfondissement/index.html",
    "title": "4¬† Approfondissement ElasticSearch pour des recherches de proximit√© g√©ographique",
    "section": "",
    "text": "5 Introduction\nDans le cadre particulier de l‚Äôidentification des entreprises, Elasticsearch fait partie de la solution retenue par\nl‚ÄôAPI ‚ÄúSirene donn√©es ouvertes‚Äù (DINUM) (cf https://annuaire-entreprises.data.gouv.fr/) l‚ÄôAPI de recherche d‚Äôentreprises Fran√ßaise de la Fabrique num√©rique des Minist√®res Sociaux (cf https://api.recherche-entreprises.fabrique.social.gouv.fr/)le projet de l‚ÄôInsee ‚ÄúAm√©lioration de l‚Äôidentification de l‚Äôemployeur dans le recensement‚Äù, pour faire une premi√®re s√©lection des √©tablissements pertinents pour un bulletin de recensement donn√©.\nDans le cadre de l‚Äôidentification des individus, Elasticsearch fait partie de la solution envisag√©e pour l‚Äôidentification des individus au RNIPP (R√©pertoire national des personnes physiques) pour le projet CSNS (Code statistique non signifiant), et est la solution technique sous-jacente au projet matchID du minist√®re de l‚Äôint√©rieur.\nAu del√† du secteur public, on peut citer qu‚ÄôAmazon AWS fait partie des utilisateurs historiques d‚ÄôElasticsearch.\nLe jeu de donn√©es pr√©sente d√©j√†\nl‚Äôidentifiant\nde l‚Äô√©tablissement, dit num√©ro siret.\nNous allons faire comme si nous √©tions\nen amont de cet appariement et que nous\nd√©sirons trouver ce num√©ro. La pr√©sence\ndans la base de ce num√©ro nous permettra d‚Äô√©valuer la qualit√©\nde notre m√©thode de recherche avec\nElastic.\nimport requests\nimport zipfile\nimport pandas as pd\n\nurl = \"https://www.data.gouv.fr/fr/datasets/r/9af639b9-e2b6-4d7d-8c5f-0c4953c48663\"\nreq = requests.get(url)\n\nwith open(\"irep.zip\",'wb') as f:\n  f.write(req.content)\n\nwith zipfile.ZipFile(\"irep.zip\", 'r') as zip_ref:\n  zip_ref.extractall(\"irep\")\n\netablissements = pd.read_csv(\"irep/2019/etablissements.csv\", sep = \";\")\nDans un premier temps, on va combiner ensemble les diff√©rentes sources\nopen-data pour cr√©er un r√©f√©rentiel fiable d‚Äôentreprises\ng√©olocalis√©es.\nOn va supposer que l‚Äôutilisateur dispose d√©j√† d‚Äôun serveur Elastic\nfonctionnel mais d√©sire cr√©er un nouvel index. Si vous utilisez\nle SSPCloud, la d√©marche de cr√©ation d‚Äôun service\nElastic est disponible dans le chapitre pr√©c√©dent.\nfrom elasticsearch import Elasticsearch\nHOST = 'elasticsearch-master'\n\ndef elastic():\n    \"\"\"Connection avec Elastic sur le data lab\"\"\"\n    es = Elasticsearch([{'host': HOST, 'port': 9200, 'scheme': 'http'}], http_compress=True, request_timeout=200)\n    return es\n\nes = elastic()\nes\nPour se faciliter la cr√©ation de cartes\nr√©actives, nous allons r√©guli√®rement\nutiliser la fonction suivante qui s‚Äôappuie\nsur un code d√©j√† pr√©sent√© dans un autre\nchapitre.\ndef plot_folium_sirene(df, yvar, xvar):\n\n    center = df[[yvar, xvar]].mean().values.tolist()\n    sw = df[[yvar, xvar]].min().values.tolist()\n    ne = df[[yvar, xvar]].max().values.tolist()\n\n    m = folium.Map(location = center, tiles='Stamen Toner')\n\n    # I can add marker one by one on the map\n    for i in range(0,len(df)):\n        folium.Marker(\n            [df.iloc[i][yvar], df.iloc[i][xvar]],\n            popup = df.iloc[i][\"_source.denom\"] + f'&lt;br&gt;(Score: {df.iloc[i][\"_score\"]:.2f})',\n            icon=folium.Icon(icon=\"home\")\n        ).add_to(m)\n\n    m.fit_bounds([sw, ne])\n\n    return m"
  },
  {
    "objectID": "content/course/modern-ds/elastic_approfondissement/index.html#objectif",
    "href": "content/course/modern-ds/elastic_approfondissement/index.html#objectif",
    "title": "4¬† Approfondissement ElasticSearch pour des recherches de proximit√© g√©ographique",
    "section": "5.1 Objectif",
    "text": "5.1 Objectif\nCe chapitre vise √† approfondir les √©l√©ments pr√©sent√©s sur Elastic pr√©c√©demment. L‚Äôid√©e\nest de se placer dans un contexte op√©rationnel o√π on re√ßoit des informations\nsur des entreprises telles que l‚Äôadresse et la localisation et qu‚Äôon\nd√©sire associer √† des donn√©es administratives consid√©r√©es plus fliables."
  },
  {
    "objectID": "content/course/modern-ds/elastic_approfondissement/index.html#r√©plication-de-ce-chapitre",
    "href": "content/course/modern-ds/elastic_approfondissement/index.html#r√©plication-de-ce-chapitre",
    "title": "4¬† Approfondissement ElasticSearch pour des recherches de proximit√© g√©ographique",
    "section": "5.2 R√©plication de ce chapitre",
    "text": "5.2 R√©plication de ce chapitre\nComme le pr√©c√©dent, ce chapitre est plus exigeant en termes d‚Äôinfrastructures que les pr√©c√©dents.\nIl n√©cessite un serveur Elastic. Les utilisateurs du\nSSP Cloud pourront r√©pliquer les exemples de ce cours\ncar cette technologie est disponible (que ce soit pour indexer une base ou\npour requ√™ter une base existante).\nLa premi√®re partie de ce tutoriel, qui consiste √† cr√©er une base Sirene g√©olocalis√©e\n√† partir des donn√©es open-data ne n√©cessite pas d‚Äôarchitecture particuli√®re et\npeut ainsi √™tre ex√©cut√©e en utilisant les packages suivants:\n\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "content/course/modern-ds/elastic_approfondissement/index.html#sources",
    "href": "content/course/modern-ds/elastic_approfondissement/index.html#sources",
    "title": "4¬† Approfondissement ElasticSearch pour des recherches de proximit√© g√©ographique",
    "section": "5.3 Sources",
    "text": "5.3 Sources\nCe chapitre va utiliser plusieurs sources de diffusion de\nl‚ÄôInsee:\n\nLe stock des √©tablissements pr√©sents dans les donn√©es de diffusion Sirene ;\nLes donn√©es Sirene g√©olocalis√©es\n\nLes donn√©es √† siretiser sont celles du registre Fran√ßais des √©missions polluantes\n√©tabli par le Minist√®re de la Transition Energ√©tique. Le jeu de donn√©es\nest disponible sur data.gouv"
  },
  {
    "objectID": "content/course/modern-ds/elastic_approfondissement/index.html#importer-la-base-d√©j√†-cr√©√©e",
    "href": "content/course/modern-ds/elastic_approfondissement/index.html#importer-la-base-d√©j√†-cr√©√©e",
    "title": "4¬† Approfondissement ElasticSearch pour des recherches de proximit√© g√©ographique",
    "section": "7.1 Importer la base d√©j√† cr√©√©e",
    "text": "7.1 Importer la base d√©j√† cr√©√©e\nLes donn√©es √† utiliser pour constuire une base Sirene g√©olocalis√©e\nsont trop volumineuses pour les serveurs mis √† disposition\ngratuitement par Github pour la compilation de ce site web.\nNous proposons ainsi une version d√©j√† construite, stock√©e\ndans l‚Äôespace de mise √† disposition du SSP Cloud. Ce fichier est\nau format parquet et est ouvert √†\ntous, m√™me pour les personnes ne disposant pas d‚Äôun compte.\nLe code ayant construit cette base est pr√©sent√© ci-dessous.\nPour importer cette base, on utilise les fonctionalit√©s\nde pyarrow qui permettent d‚Äôimporter un fichier sur\nun syst√®me de stockage cloud comme s‚Äôil √©tait\npr√©sent sur le disque :\n\nfrom pyarrow import fs\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\nbucket = \"lgaliana\"\npath = \"diffusion/sirene_geolocalized.parquet\"\n\ns3 = fs.S3FileSystem(endpoint_override=\"http://\"+\"minio.lab.sspcloud.fr\")\n\ndf_geolocalized = pq.ParquetDataset(f'{bucket}/{path}', filesystem=s3).read_pandas().to_pandas()\ndf_geolocalized.head(3)"
  },
  {
    "objectID": "content/course/modern-ds/elastic_approfondissement/index.html#reproduire-la-construction-de-la-base",
    "href": "content/course/modern-ds/elastic_approfondissement/index.html#reproduire-la-construction-de-la-base",
    "title": "4¬† Approfondissement ElasticSearch pour des recherches de proximit√© g√©ographique",
    "section": "7.2 Reproduire la construction de la base",
    "text": "7.2 Reproduire la construction de la base\nLa premi√®re base d‚Äôentr√©e √† utiliser est disponible sur\ndata.gouv\n\nimport requests\nimport zipfile\n\nurl_download = \"https://www.data.gouv.fr/fr/datasets/r/0651fb76-bcf3-4f6a-a38d-bc04fa708576\"\nreq = requests.get(url_download)\n\nwith open(\"sirene.zip\",'wb') as f:\n  f.write(req.content)\n\nwith zipfile.ZipFile(\"sirene.zip\", 'r') as zip_ref:\n  zip_ref.extractall(\"sirene\")\n\nOn va importer seulement les colonnes utiles et simplifier la structure\npour √™tre en mesure de ne garder que les informations qui nous\nint√©ressent (nom de l‚Äôentreprise, adresse, commune, code postal‚Ä¶)\n\nimport pandas as pd\nimport numpy as np\n\nlist_cols = [\n  'siren', 'siret',\n  'activitePrincipaleRegistreMetiersEtablissement',\n  'complementAdresseEtablissement',\n  'numeroVoieEtablissement',\n  'typeVoieEtablissement',\n  'libelleVoieEtablissement',\n  'codePostalEtablissement',\n  'libelleCommuneEtablissement',\n  'codeCommuneEtablissement',\n  'etatAdministratifEtablissement',\n  'denominationUsuelleEtablissement',\n  'activitePrincipaleEtablissement'\n]\n\ndf = pd.read_csv(\n  \"sirene/StockEtablissement_utf8.csv\",\n  usecols = list_cols)\n\ndf['numero'] = df['numeroVoieEtablissement']\\\n  .replace('-', np.NaN).str.split().str[0]\\\n  .str.extract('(\\d+)', expand=False)\\\n  .fillna(\"0\").astype(int)\n\ndf['numero'] = df['numero'].astype(str).replace(\"0\",\"\")\n\ndf['adresse'] = df['numero'] + \" \" + \\\n  df['typeVoieEtablissement'] + \" \" + \\\n  df['libelleVoieEtablissement']\n\ndf['adresse'] = df['adresse'].replace(np.nan, \"\")\n\ndf = df.loc[df['etatAdministratifEtablissement'] == \"A\"]\n\ndf.rename(\n  {\"denominationUsuelleEtablissement\": \"denom\",\n  \"libelleCommuneEtablissement\": \"commune\",\n  \"codeCommuneEtablissement\" : \"code_commune\",\n  \"codePostalEtablissement\": \"code_postal\"},\n  axis = \"columns\", inplace = True)\n\ndf['ape'] = df['activitePrincipaleEtablissement'].str.replace(\"\\.\", \"\", regex = True)\ndf['denom'] = df[\"denom\"].replace(np.nan, \"\")\n\ndf_siret = df.loc[:, ['siren', 'siret','adresse', 'ape', 'denom', 'commune', 'code_commune','code_postal']]\ndf_siret['code_postal'] = df_siret['code_postal'].replace(np.nan, \"0\").astype(int).astype(str).replace(\"0\",\"\")\n\nOn importe ensuite les donn√©es g√©olocalis√©es\n\nimport zipfile\nimport shutil\nimport os\n\n#os.remove(\"sirene.zip\")\n#shutil.rmtree('sirene/')\n\nurl_geoloc = \"https://files.data.gouv.fr/insee-sirene-geo/GeolocalisationEtablissement_Sirene_pour_etudes_statistiques_utf8.zip\"\nr = requests.get(url_geoloc)  \n\nwith open('geoloc.zip', 'wb') as f:\n    f.write(r.content)\n\nwith zipfile.ZipFile(\"geoloc.zip\", 'r') as zip_ref:\n  zip_ref.extractall(\"geoloc\")\n\ndf_geoloc = pd.read_csv(\n  \"geoloc/GeolocalisationEtablissement_Sirene_pour_etudes_statistiques_utf8.csv\",\n  usecols = [\"siret\", \"epsg\", \"x_longitude\", \"y_latitude\"] , sep = \";\")\n\nIl ne reste plus qu‚Äô√† associer les deux jeux de donn√©es\n\ndf_geolocalized = df_siret.merge(df_geoloc, on = \"siret\") \ndf_geolocalized['code_commune'] = df_geolocalized['code_commune'].astype(str) \n\nSi vous avez acc√®s √† un espace de stockage cloud de type\nS3, il est possible d‚Äôutiliser pyarrow pour enregister\ncette base. Afin de l‚Äôenregistrer dans un espace de stockage\npublic, nous allons l‚Äôenregistrer dans un dossier diffusion1\n\nfrom pyarrow import fs\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\nbucket = \"lgaliana\"\npath = \"diffusion/sirene_geolocalized.parquet\"\n\ns3 = fs.S3FileSystem(endpoint_override=\"http://\"+\"minio.lab.sspcloud.fr\")\n\ntable = pa.Table.from_pandas(df_geolocalized)"
  },
  {
    "objectID": "content/course/modern-ds/elastic_approfondissement/index.html#d√©finition-du-mapping",
    "href": "content/course/modern-ds/elastic_approfondissement/index.html#d√©finition-du-mapping",
    "title": "4¬† Approfondissement ElasticSearch pour des recherches de proximit√© g√©ographique",
    "section": "9.1 D√©finition du mapping",
    "text": "9.1 D√©finition du mapping\nOn va proc√©der par √©tape en essayant d‚Äôutiliser la structure la plus simple\npossible.\n:one: On s‚Äôoccupe d‚Äôabord de d√©finir le mapping\npour les variables textuelles.\n\nstring_var = [\"adresse\", \"denom\", \"ape\", \"commune\"]\nmap_string = {\"type\": \"text\", \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}}}\nmapping_string = {l: map_string for l in string_var}\n\n:two: Les variables cat√©gorielles sont utilis√©es\npar le biais du type keyword:\n\n# keywords\nkeyword_var = [\"siren\",\"siret\",\"code_commune\",\"code_postal\"]\nmap_keywords = {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}\nmapping_keywords = {l: map_keywords for l in keyword_var}\n\n:three: La nouveaut√© par rapport √† la partie\npr√©c√©dente est l‚Äôutilisation de la\ndimension g√©ographique. Elastic propose\nle type geo_point pour cela.\n\n# geoloc\nmapping_geoloc = {\n  \"location\": {\n    \"type\": \"geo_point\"\n    }\n}    \n\nOn collecte tout cela ensemble dans un\ndictionnaire:\n\n# mapping\nmapping_elastic = {\"mappings\":\n  {\"properties\":\n    {**mapping_string, **mapping_geoloc, **mapping_keywords}\n  }\n}\n\nIl est tout √† fait possible de d√©finir un mapping\nplus raffin√©. Ici, on va privil√©gier\nl‚Äôutilisation d‚Äôun mapping simple pour\nillustrer la recherche par distance\ng√©ographique en priorit√©."
  },
  {
    "objectID": "content/course/modern-ds/elastic_approfondissement/index.html#cr√©ation-de-lindex",
    "href": "content/course/modern-ds/elastic_approfondissement/index.html#cr√©ation-de-lindex",
    "title": "4¬† Approfondissement ElasticSearch pour des recherches de proximit√© g√©ographique",
    "section": "9.2 Cr√©ation de l‚Äôindex",
    "text": "9.2 Cr√©ation de l‚Äôindex\nPour cr√©er le nouvel index, on s‚Äôassure d‚Äôabord de ne pas\nd√©j√† l‚Äôavoir cr√©√© et on passe le mapping d√©fini\npr√©c√©demment.\n\nif es.indices.exists('sirene'):\n    es.indices.delete('sirene')\n\nes.indices.create(index = \"sirene\", body = mapping_elastic)"
  },
  {
    "objectID": "content/course/modern-ds/elastic_approfondissement/index.html#indexation-de-la-base-g√©olocalis√©e",
    "href": "content/course/modern-ds/elastic_approfondissement/index.html#indexation-de-la-base-g√©olocalis√©e",
    "title": "4¬† Approfondissement ElasticSearch pour des recherches de proximit√© g√©ographique",
    "section": "9.3 Indexation de la base g√©olocalis√©e",
    "text": "9.3 Indexation de la base g√©olocalis√©e\nPour le moment, l‚Äôindex est vide. Il convient de\nle peupler.\nIl est n√©anmoins n√©cessaire de cr√©er le champ location\nau format attendu par elastic: lat, lon √† partir\nde nos colonnes.\n\ndf_geolocalized['location'] = df_geolocalized['y_latitude'].astype(str) + \", \" + df_geolocalized['x_longitude'].astype(str)\n\nLa fonction suivante permet de structurer chaque\nligne du DataFrame telle qu‚ÄôElastic l‚Äôattend:\n\ndef gen_dict_from_pandas(index_name, df):\n    '''\n    Lit un dataframe pandas Open Food Facts, renvoi un it√©rable = dictionnaire des donn√©es √† indexer, sous l'index fourni\n    '''\n    for i, row in df.iterrows():\n        header= {\"_op_type\": \"index\",\"_index\": index_name,\"_id\": i}\n        yield {**header,**row}\n\nEnfin, on peut industrialiser l‚Äôindexation\nde notre DataFrame en faisant tourner de\nmani√®re successive cette fonction:\n\nfrom elasticsearch.helpers import bulk, parallel_bulk\nfrom collections import deque\ndeque(parallel_bulk(client=es, actions=gen_dict_from_pandas(\"sirene\", df_geolocalized), chunk_size = 1000, thread_count = 4))\n\n\nes.count(index = 'sirene')\n\nObjectApiResponse({'count': 13059694, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}})"
  },
  {
    "objectID": "content/course/modern-ds/elastic_approfondissement/index.html#premier-exemple-de-requ√™te-g√©ographique",
    "href": "content/course/modern-ds/elastic_approfondissement/index.html#premier-exemple-de-requ√™te-g√©ographique",
    "title": "4¬† Approfondissement ElasticSearch pour des recherches de proximit√© g√©ographique",
    "section": "10.1 Premier exemple de requ√™te g√©ographique",
    "text": "10.1 Premier exemple de requ√™te g√©ographique\n\nex1 = es.search(index = 'sirene', body = '''{\n  \"query\": {\n    \"bool\": {\n      \"must\":\n      { \"match\": { \"denom\":   \"institut national de la statistique\"}}\n      }\n  }\n}\n''')['hits']['hits']\n\necho_insee = pd.json_normalize(ex1)\necho_insee\n\nOn remarque d√©j√† que les intitul√©s ne sont\npas bons. Quand est-il de leurs localisations ?\n\nplot_folium_sirene(\n  echo_insee, yvar = \"_source.y_latitude\",\n  xvar = \"_source.x_longitude\")\n\nCe premier essai nous sugg√®re qu‚Äôil est\nn√©cessaire d‚Äôam√©liorer notre recherche.\nPlusieurs voies sont possibles:\n\nAm√©liorer le preprocessing de nos champs\ntextuels en excluant, par exemple, les\nstopwords ;\nEffectuer une restriction g√©ographique\npour mieux cibler l‚Äôensemble de recherche\nTrouver une variable cat√©gorielle jouant\nle r√¥le de variable de blocage2 pour\nmieux cibler les paires pertinentes\n\nConcernant la restriction\ng√©ographique, Elastic fournit une approche\ntr√®s efficace de ciblage g√©ographique.\nEn connaissant une position approximative\nde l‚Äôentreprise √† rechercher,\nil est ainsi possible de\nrechercher dans un rayon\nd‚Äôune taille plus ou moins grande.\nEn supposant qu‚Äôon connaisse pr√©cis√©ment\nla localisation de l‚ÄôInsee, on peut\nchercher dans un rayon relativement\nrestreint. Si notre position √©tait plus\napproximative, on pourrait rechercher\ndans un rayon plus large.\n\nex2 = es.search(index = 'sirene', body = '''{\n  \"query\": {\n    \"bool\": {\n      \"must\":\n      { \"match\": { \"denom\":   \"institut national de la statistique\"}}\n      ,\n      \"filter\":\n        {\"geo_distance\": {\n          \"distance\": \"1km\",\n          \"location\": {\n            \"lat\": \"48.8168\",\n            \"lon\": \"2.3099\"\n          }\n        }\n      }\n    }\n  }\n}\n''')['hits']['hits']\n\necho_insee = pd.json_normalize(ex2)\necho_insee\n\n{{% box status=‚Äúhint‚Äù title=‚ÄúConseil‚Äù icon=‚Äúfa fa-lightbulb‚Äù %}}\nConna√Ætre la pr√©cision pr√©cise d‚Äôune\nentreprise\nn√©cessite d√©j√† une bonne remont√©e\nd‚Äôinformation sur celle-ci.\nIl est plus plausible de supposer\nqu‚Äôon dispose, dans une phase amont\nde la chaine de production,\nde l‚Äôadresse de celle-ci.\nDans ce cas, il est utile\nd‚Äôutiliser un service de g√©ocodage,\ncomme l‚ÄôAPI Adresse\nd√©velopp√©e par Etalab.\n{{% /box %}}\nLes r√©sultats sont par construction mieux\ncibl√©s. N√©anmoins ils sont toujours d√©cevants\npuisqu‚Äôon ne parvient pas √† identifier l‚ÄôInsee\ndans les dix meilleurs √©chos.\n\nspecificsearch = es.search(index = 'sirus_2020', body = \n'''{\n  \"query\": {\n    \"bool\": {\n      \"should\":\n          { \"match\": { \"rs_denom\":   \"CPCU - CENTRALE DE BERCY\"}},\n      \"filter\": [\n          {\"geo_distance\": {\n                  \"distance\": \"0.5km\",\n                  \"location\": {\n                        \"lat\": \"48.84329\", \n                        \"lon\": \"2.37396\"\n                              }\n                            }\n            }, \n            { \"prefix\":  { \"apet\": \"3530\" }}\n                ]\n            }\n          }\n}'''\n)"
  },
  {
    "objectID": "content/course/modern-ds/dallE/index.html",
    "href": "content/course/modern-ds/dallE/index.html",
    "title": "5¬† G√©n√©ration d‚Äôimages avec Python, DALL-E et StableDiffusion",
    "section": "",
    "text": "6 Contexte\nLa publication en avril 2022 par l‚Äôorganisation Open AI de\nson mod√®le de g√©n√©ration de contenu cr√©atif Dall-E-2\n(un jeu de mot m√©langeant Dali et Wall-E) a cr√©√© un bruit in√©dit dans\nle monde de la data-science.\nA propos de Dall-E, le bloggeur tech Casey Newton a pu parler d‚Äôune\nr√©volution cr√©ative dans le monde de l‚ÄôIA.\nThe Economist a par consacr√©\nun num√©ro au sujet de l‚Äô√©mergence de ces intelligences artificielles\ncr√©atrices de contenu.\nCe bruit sur la capacit√© des\nintelligences artificielle √† g√©n√©rer du contenu cr√©atif\na d‚Äôailleurs √©t√© amplifi√© plus r√©cemment\navec la publication du chatbot chatGPT\n(voir cet √©ditorial du Guardian).\nL‚Äôinconv√©nient principal de Dall-E\npour g√©n√©rer facilement du contenu\nest que le nombre de contenu pouvant √™tre cr√©√©\navec un acc√®s gratuit est limit√© (50 cr√©dits gratuits par mois).\nDepuis le 22 Ao√ªt 2022, un g√©n√©rateur de contenu\nsimilaire est disponible gratuitement,\navec une licence plus permissive1. Ce g√©n√©rateur, d√©velopp√©\npar une √©quipe de chercheurs (Rombach et al. 2022),\ns‚Äôappelle Stable Diffusion (d√©p√¥t Github pour le code source et\nd√©p√¥t HuggingFace pour le mod√®le mis √† disposition2).\nUn excellent article de blog d√©crit la d√©marche de Stable Diffusion. La plupart des exemples originaux\ndans cette partie seront bas√©s sur Stable Diffusion.\nHuggingFace\nHuggingface est une plateforme de partage de mod√®les de type r√©seau de neurone. Les utilisateurs de r√©seaux de neurone peuvent\nainsi mettre √† disposition le r√©sultat de leurs travaux sous forme d‚ÄôAPI pour faciliter la r√©utilisation de leurs\nmod√®les ou r√©utiliser facilement des mod√®les, ce qui √©vite de les r√©-entra√Æner (ce qui aurait un co√ªt √©cologique non\nn√©gligeable comme expliqu√© dans le chapitre introductif).\nDall-E-2 et StableDiffusion\nsont des mod√®les g√©n√©ralistes.\nD‚Äôautres mod√®les, plus sp√©cialis√©s,\nexistent √©galement.\nLe mod√®le Midjourney\n(produit propri√©taire de la soci√©t√© du m√™me nom)\npermet la production de contenu\nartistique, DreamBooth (d√©velopp√© par Google)\nest sp√©cialis√© dans la g√©n√©ration de contenu dans un nouveau\ncontexte.\nLe principe de tous ces mod√®les est le m√™me: un utilisateur\ndonne une instruction (une ou plusieurs phrases) et l‚Äôintelligence\nartificielle l‚Äôinterpr√®te et g√©n√®re une image cens√©e √™tre\ncoh√©rente avec l‚Äôinstruction.\nVoici par exemple l‚Äôune des productions possibles de DALL-E-2\n‚ÄúA Shiba Inu dog wearing a beret and black turtleneck‚Äù\nMidjourney, sp√©cialis√© dans le contenu esth√©tique,\ng√©n√®rera l‚Äôimage suivante avec l‚Äôinstruction ‚Äúmechanical dove‚Äù:\nStableDiffusion, mod√®le g√©n√©raliste comme Dall-E,\ncr√©√©ra le contenu suivant avec\nl‚Äôinstruction ‚ÄúA photograph of an astronaut riding a horse‚Äù:\nEnfin, DreamBooth pourra lui introduire un chien dans une grande vari√©t√©\nde contextes:\nUn compte Twitter (Weird AI Generations)\npropose de nombreuses g√©n√©rations de contenu dr√¥les ou incongrues.\nVoici un premier exemple de production humoristique faite √† partir de Mini Dall-E, la version\npublique:\npic.twitter.com/DIerJPtXGE‚Äî Weird Ai Generations (@weirddalle) August 6, 2022\nAinsi qu‚Äôun deuxi√®me:\npic.twitter.com/Ju0Pdcokth‚Äî Weird Ai Generations (@weirddalle) August 8, 2022\nLes mod√®les Dall-E-2 et Stable Diffusion\ns‚Äôappuient sur des r√©seaux de neurone √† diff√©rents niveaux :\nIllustration du fonctionnement de ce type de g√©n√©rateur d‚Äôimage (ici √† partir de Stable Diffusion)\nWarning\nLes services d‚ÄôOpenAI ne sont gratuits que dans une certaine\nlimite. Votre cl√© d‚ÄôAPI est donc assez pr√©cieuse car si elle\nest usurp√©e, elle peut permettre √† certaines personnes\nd‚Äô√©puiser vos cr√©dits gratuits voire d‚Äôutiliser des cr√©dits\npayants √† votre place.\nSi vous √™tes enregistr√©s r√©cemment dans le service d‚ÄôAPI\nd‚ÄôOpenAI, vous avez acc√®s √† des cr√©dits gratuits. Ne les\nutilisez n√©anmoins pas avec trop de l√©g√®ret√© en ne contr√¥lant\npas les param√®tres de vos appels aux API car ces cr√©dits\nsont pour l‚Äôensemble des services d‚ÄôOpenAI(chatGPT,\nDall-E, DaVinci‚Ä¶)\nLe contenu de cette partie s‚Äôappuie sur\nle tutoriel du site realpython\nL‚Äôutilisation de Dall-E sera faite via le package openai qui donne\nacc√®s √† l‚ÄôAPI d‚ÄôOpenAI.\nPour l‚Äôinstaller depuis la cellule d‚Äôun Notebook:\n!pip install openai\nApr√®s avoir obtenu votre cl√© d‚ÄôAPI, on va supposer que celle-ci\nest stock√©e dans une variable key:\nkey = \"sk-XXXXXXXXXX\" #remplacer avec votre cl√©\nEnsuite, l‚Äôutilisation de l‚ÄôAPI est assez directe:\nopenai.api_key = key\nopenai.Image.create(\n  prompt=\"Teddy bears working on new AI research underwater with 1990s technology\",\n  n=2,\n  size=\"1024x1024\"\n)\nL‚Äôoutput est un JSON avec les URL des images g√©n√©r√©es.\nVoici les deux images g√©n√©r√©es:\nPour aller plus loin, vous pouvez consulter\nle tutoriel de realpython\nStable Diffusion est\nune intelligence artificielle cr√©atrice de contenu qui permet de\ng√©n√©rer du contenu √† partir d‚Äôune phrase - ce pour quoi nous allons\nl‚Äôutiliser - mais aussi modifier des images √† partir d‚Äôinstructions.\nStable Diffusion est un mod√®le plus pratique √† utiliser depuis Python\nque Dall-E. Celui-ci\nest open source et peut √™tre t√©l√©charg√© et r√©utilis√© directement depuis Python.\nLa m√©thode la plus pratique est d‚Äôutiliser le mod√®le mis\n√† disposition sur HuggingFace. Le mod√®le est impl√©ment√©\n√† travers le framework PyTorch.\nPyTorch, librairie d√©velopp√©e\npar Meta, n‚Äôest pas implement√© directement en Python\npour des raisons de performance mais en C++ - Python √©tant un\nlangage lent, le revers de la m√©daille de sa facilit√©\nd‚Äôusage. A travers Python, on va utiliser une API haut niveau\nqui va contr√¥ler la structure des r√©seaux de neurone ou\ncr√©er une interface entre des\ndonn√©es (sous forme d‚Äôarray Numpy) et le mod√®le.\nPour ce type de packages qui utilisent un langage compil√©,\nl‚Äôinstallation via Pandas\nConfiguration sp√©cifique √† Colab üëá\nSur Colab, conda n‚Äôest pas disponible par d√©faut.\nPour pouvoir\ninstaller un package en utilisant conda sur Colab,\non utilise donc l‚Äôastuce\nsuivante:\n!pip install -q condacolab\nimport condacolab\ncondacolab.install()\nOn va cr√©er l‚Äôimage suivante:\nPas mal comme sc√©nario, non ?!\nNote\nPour que les r√©sultats soient reproductibles entre diff√©rentes\nsessions,\nnous allons fixer\nla racine du g√©n√©rateur al√©atoire.\nimport torch\ngenerator = torch.Generator(\"cuda\").manual_seed(123)\nSi vous voulez vous amuser √† explorer diff√©rents r√©sultats\npour un m√™me texte, vous pouvez ne pas fixer de racine al√©atoire.\nDans ce cas, retirer l‚Äôargument generator des codes pr√©sent√©s\nult√©rieurement.\nNous allons donc utiliser l‚Äôinstruction suivante :\nprompt = \"Chuck Norris fighting against Zeus on Mount Olympus in an epic Mortal Kombat scene\"\nL‚Äôinitialisation du mod√®le se fait de la mani√®re\nsuivante:\nimport torch\nfrom torch import autocast\nfrom diffusers import StableDiffusionPipeline\n\nmodel_id = \"CompVis/stable-diffusion-v1-4\"\ndevice = \"cuda\"\n\ngenerator = torch.Generator(\"cuda\").manual_seed(1024)\nEnfin, pour g√©n√©rer l‚Äôimage:\npipe = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=HF_TOKEN, generator=generator)\npipe = pipe.to(device)\n\nwith autocast(\"cuda\"):\n    image = pipe(prompt, guidance_scale=7.5, generator = generator)[\"images\"][0]  \n\n   \nimage.save(\"featured.png\")\nQui peut √™tre visualis√© avec le code suivant, dans un notebook:\nfrom IPython.display import Image \npil_img = Image(filename=\"featured.png\")\ndisplay(pil_img)\nC‚Äôest une repr√©sentation assez fid√®le du\npitch ‚ÄúChuck Norris fighting against Zeus on Mount Olympus in an epic Mortal Kombat scene‚Äù :boom:.\nY a un petit c√¥t√© Les Dix Commandements que j‚Äôaime bien.\nEn voici une autre que j‚Äôaime bien (mais malheureusement je ne peux la reproduire car je n‚Äôai pas\ngard√© en m√©moire la racine l‚Äôayant g√©n√©r√© :sob:)\nIl est √©galement possible de g√©n√©rer plusieurs images du m√™me texte (voir\nla note de blog de l‚Äô√©quipe\n√† l‚Äôorigine de Stable Diffusion). Cependant, c‚Äôest assez exigeant en\nm√©moire et cela risque d‚Äô√™tre impossible sur Colab (y compris\nen r√©duisant le poids des vecteurs num√©riques comme propos√© dans le post)\nPour le plaisir, voici PuppyMan, le dernier n√© du Marvel Universe:\nprompt = \"In a new Marvel film we discover puppyman a new super hero that is half man half bulldog\"\nimport torch\nfrom torch import autocast\nfrom diffusers import StableDiffusionPipeline\n\nmodel_id = \"CompVis/stable-diffusion-v1-4\"\ndevice = \"cuda\"\n\ngenerator = torch.Generator(\"cuda\").manual_seed(1024)\n\npipe = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=HF_TOKEN, generator=generator)\npipe = pipe.to(device)\n\nwith autocast(\"cuda\"):\n    image = pipe(prompt, guidance_scale=7.5, generator = generator)[\"images\"][0]  \n\n   \nimage.save(\"puppyman.png\")\nLa moiti√© humain semble √™tre son costume de super-h√©ros, pas la bip√©die.\nMais le rendu\nest quand m√™me √©patant !\nA vous de jouer :hugging_face:"
  },
  {
    "objectID": "content/course/modern-ds/dallE/index.html#installation-de-pytorch",
    "href": "content/course/modern-ds/dallE/index.html#installation-de-pytorch",
    "title": "5¬† G√©n√©ration d‚Äôimages avec Python, DALL-E et StableDiffusion",
    "section": "8.1 Installation de PyTorch",
    "text": "8.1 Installation de PyTorch\nPour installer PyTorch, la librairie de Deep Learning\nd√©velopp√©e par Meta, il suffit de suivre les recommandations\nsur le site web officiel.\nDans un Notebook, cela prendra la forme suivante:\n\n!conda install mamba\n!mamba install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch\n\n\n\n Note\nJe propose ici d‚Äôutiliser mamba pour acc√©l√©rer l‚Äôinstallation.\nDes √©l√©ments sur mamba sont disponibles dans l‚Äôintroduction de ce cours."
  },
  {
    "objectID": "content/course/modern-ds/dallE/index.html#acc√®s-√†-huggingface",
    "href": "content/course/modern-ds/dallE/index.html#acc√®s-√†-huggingface",
    "title": "5¬† G√©n√©ration d‚Äôimages avec Python, DALL-E et StableDiffusion",
    "section": "8.2 Acc√®s √† HuggingFace",
    "text": "8.2 Acc√®s √† HuggingFace\nLa question - non n√©gligeable - de l‚Äôacc√®s √†\nde la GPU mise √† part,\nla r√©utilisation des mod√®les de Stable Diffusion est\ntr√®s facile car la documentation mise √† disposition sur\nHuggingFace est tr√®s bien faite.\nLa premi√®re √©tape est de se cr√©er un compte sur HuggingFace\net se cr√©er un token3. Ce token sera donn√© √† l‚ÄôAPI\nde HuggingFace pour s‚Äôauthentifier.\nL‚ÄôAPI d‚ÄôHuggingFace n√©cessite l‚Äôinstallation du\npackage diffusers.\nDans un Notebook, le code suivant permet d‚Äôinstaller la librairie\nrequise:\n\n!pip install --upgrade diffusers transformers scipy accelerate\n\n\n\n Note\nOn va supposer que le token est stock√© dans une variable\nd‚Äôenvironnement HF_PAT. Cela √©vite d‚Äô√©crire le token\ndans un Notebook qu‚Äôon va\npotentiellement partager, alors que le token\nest un √©l√©ment √† garder secret. Pour l‚Äôimporter\ndans la session Python:\nSi vous n‚Äôavez pas la possibilit√© de rentrer le token dans les variables\nd‚Äôenvironnement, cr√©ez une cellule qui cr√©e la variable\nHF_TOKEN et supprimez l√† de suite pour ne pas l‚Äôoublier avant\nde partager votre token.\n\n\n\nimport os\nHF_TOKEN = os.getenv('HF_PAT')"
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html",
    "href": "content/course/modern-ds/elastic_intro/index.html",
    "title": "6¬† Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "",
    "text": "7 Introduction\nPour √©valuer la similarit√© entre deux donn√©es textuelles, il est\nn√©cessaire de transformer l‚Äôinformation qualitative qu‚Äôest le nom\ndu produit en information quantitative qui permettra de rapprocher\ndiff√©rents types de produits.\nLes ordinateurs ont en effet besoin de transformer les informations\ntextuelles en information num√©rique pour √™tre en mesure\nde les exploiter.\nOn appelle distance de Levenshtein entre deux cha√Ænes de caract√®res\nle co√ªt minimal (en nombre d‚Äôop√©rations)\npour transformer la premi√®re en la seconde par:\nLa distance de Levenshtein est une mesure tr√®s utilis√©e pour comparer la similarit√© entre deux\ncha√Ænes de caract√®res. Il existe plusieurs packages pour calculer cette derni√®re.\nfuzzywuzzy est le plus connu mais ce dernier est assez lent (impl√©mentation en pur Python).\nLe package rapidfuzz, pr√©sent√© ici, propose les m√™mes fonctionalit√©s mais est plus rapide car impl√©ment√©\nen C++ qui est plus efficace.\nCependant, nous allons le voir, ce package ne nous\noffrira pas des performances\nassez bonnes pour que nous puissions\npasser √† l‚Äô√©chelle.\nVoici trois exemples pour √©valuer le co√ªt de chaque\nop√©ration:\nimport rapidfuzz\n\n[\n  rapidfuzz.distance.Levenshtein.distance('salut','slut', weights =(1,1,1)), # Suppression \n  rapidfuzz.distance.Levenshtein.distance('salut','saalut', weights =(1,1,1)), # Addition \n  rapidfuzz.distance.Levenshtein.distance('salut','selut', weights =(1,1,1)) # Substitution\n]\nA partir de maintenant, commence, √† proprement parler, la d√©monstration Elastic.\nCette\npartie d√©veloppe les √©l√©ments les plus techniques, √† savoir l‚Äôindexation d‚Äôune base.\nTous les utilisateurs d‚ÄôElastic n‚Äôont pas n√©cessairement √† passer par l√†, ils peuvent\ntrouver une base d√©j√† index√©e, id√©alement par un data engineer qui aura optimis√©\nles traitements.\nLes utilisateurs du SSP Cloud, architecture qui\nrepose sur la technologie Kubernetes peuvent\nr√©pliquer les √©l√©ments de la suite du document.\nOn peut sp√©cifier la fa√ßon dont l‚Äôon souhaite analyser le texte.\nPar exemple, on peut pr√©ciser que l‚Äôon souhaite enlever des stopwords, raciniser, analyser les termes via des n-grammes\npour rendre la recherche plus robuste aux fautes de frappes‚Ä¶\nCes concepts sont pr√©sent√©s dans la partie NLP.\nPour une pr√©sentation plus compl√®te, voir\nla documentation officielle d‚ÄôElastic\nOn propose les analyseurs stock√©s dans un fichier schema.json\nLes n-grammes sont des s√©quences de n caract√®res ou plus g√©n√©ralement n √©l√©ments qui s‚Äôencha√Ænent s√©quentiellement.\nPar exemple, NOI et OIX sont des tri-grammes de caract√®res dans NOIX.\nComparer les n-grammes composant des libell√©s peut permettre d‚Äôavoir dans des comparaisons √† fautes de frappe/abbr√©viations pr√®s.\nCela fait aussi plus de comparaisons √† op√©rer ! D‚Äôo√π √©galement, l‚Äôint√©r√™t d‚ÄôElastic, qui int√©gre facilement et efficacement ces comparaisons.\nOn va pr√©ciser un peu le sch√©ma de donn√©es qu‚Äôon souhaite indexer, et aussi pr√©ciser comment les diff√©rents champs seront analys√©s."
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#r√©plication-de-ce-chapitre",
    "href": "content/course/modern-ds/elastic_intro/index.html#r√©plication-de-ce-chapitre",
    "title": "6¬† Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "7.1 R√©plication de ce chapitre",
    "text": "7.1 R√©plication de ce chapitre\nCe chapitre est plus exigeant en termes d‚Äôinfrastructures que les pr√©c√©dents.\nSi la premi√®re partie de ce chapitre peut √™tre men√©e avec une\ninstallation standard de Python, ce n‚Äôest pas le cas de la\ndeuxi√®me qui n√©cessite un serveur ElasticSearch. Les utilisateurs du\nSSP Cloud pourront r√©pliquer les exemples de ce cours\ncar cette technologie est disponible (que ce soit pour indexer une base ou\npour requ√™ter une base existante).\n:warning: Ce\nchapitre n√©cessite une version particuli√®re du\npackage ElasticSearch pour tenir compte de l‚Äôh√©ritage de la version 7 du moteur Elastic.\nPour cela, faire\n\n!pip install elasticsearch==8.2.0\n!pip install unidecode\n!pip install rapidfuzz\n!pip install xlrd\n\nLa premi√®re partie de ce tutoriel ne n√©cessite pas d‚Äôarchitecture particuli√®re et\npeut ainsi √™tre ex√©cut√©e en utilisant les packages suivants:\n\nimport time\nimport pandas as pd\n\nLe script functions.py, disponible sur Github,\nregroupe un certain nombre de fonctions utiles permettant\nd‚Äôautomatiser certaines t√¢ches de nettoyage classiques\nen NLP.\n{{% box status=‚Äúhint‚Äù title=‚ÄúHint‚Äù icon=‚Äúfa fa-lightbulb‚Äù %}}\nPlusieurs m√©thodes peuvent √™tre mises en oeuvre pour r√©cup√©rer\nle script d‚Äôutilitaires. Vous pouvez trouver en dessous\nde cet encadr√© une m√©thode qui va chercher la derni√®re\nversion sur le d√©p√¥t Github du cours\n{{% /box %}}\n\nimport requests\nbaseurl = \"https://raw.githubusercontent.com/linogaliana/python-datascientist\"\nbranch = \"master\"\npath = \"content/course/modern-ds/elastic_intro/functions.py\"\n\nurl = f\"{baseurl}/{branch}/{path}\"\nr = requests.get(url, allow_redirects=True)\n\nopen('functions.py', 'wb').write(r.content)\n\nApr√®s l‚Äôavoir r√©cup√©r√© (cf.¬†encadr√© d√©di√©),\nil convient d‚Äôimporter les fonctions sous forme de module:\n\nimport functions as fc"
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#cas-dusage",
    "href": "content/course/modern-ds/elastic_intro/index.html#cas-dusage",
    "title": "6¬† Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "7.2 Cas d‚Äôusage",
    "text": "7.2 Cas d‚Äôusage\nCe notebook recense et propose d‚Äôappr√©hender quelques outils utilis√©s\npour le papier pr√©sent√© aux\nJourn√©es de M√©thodologie Statistiques 2022: Galiana and Suarez-Castillo, ‚ÄúFuzzy matching on big-data: an illustration with scanner data and crowd-sourced nutritional data‚Äù\n(travail en cours!)\nOn va partir du cas d‚Äôusage suivant:\n\nCombien de calories dans ma recette de cuisine de ce soir? Combien de calories dans mes courses de la semaine?\n\nL‚Äôobjectif est de reconstituer, √† partir de libell√©s de produits, les caract√©ristiques nutritionnelles d‚Äôune recette.\nLe probl√®me est que les libell√©s des tickets de caisse ne sont pas des champs textuels tr√®s propres, ils contiennent,\npar exemple, beaucoup d‚Äôabbr√©viations, toutes n‚Äô√©tant pas √©videntes.\nVoici par exemple une s√©rie de noms de produits qu‚Äôon va utiliser par la suite:\n\nticket = ['CROISSANTS X6 400G',\n          'MAQUEREAUX MOUTAR.',\n          'IGP OC SAUVIGNON B',\n          'LAIT 1/2 ECRM UHT',\n          '6 OEUFS FRAIS LOCA',\n          'ANANAS C2',\n          'L POMME FUDJI X6 CAL 75/80 1KG ENV',\n          'PLT MIEL',\n          'STELLA ARTOIS X6',\n          'COTES DU LUBERON AIGUEBRUN 75C']\n\nA ces produits, s‚Äôajoutent les ingr√©dients suivants, issus de la\nrecette du velout√© de potiron et carottes de Marmiton\nqui sera notre plat principal :\n\ningredients = ['500 g de carottes',\n '2 pommes de terre',\n \"1 gousse d'ail\",\n '1/2 l de lait',\n '1/2 l de bouillon de volaille',\n \"1 cuill√®re √† soupe de huile d'olive\",\n '1 kg de potiron',\n '1 oignon',\n '10 cl de cr√®me liquide (facultatif)']\n\nEssayer de r√©cup√©rer par webscraping cette liste est un bon exercice pour r√©viser\nles concepts vus pr√©cedemment\nOn va donc cr√©er une liste de course compilant\nces deux\nlistes h√©t√©rog√®nes de noms de produits:\n\nlibelles = ticket + ingredients\n\nOn part avec cette liste dans notre supermarch√© virtuel. L‚Äôobjectif sera de trouver\nune m√©thode permettant de passer √† l‚Äô√©chelle:\nautomatiser les traitements, effectuer des recherches efficaces, garder une certaine g√©n√©ralit√© et flexibilit√©.\nCe chapitre montrera par l‚Äôexemple l‚Äôint√©r√™t d‚ÄôElastic par rapport √† une solution\nqui n‚Äôutiliserait que du Python."
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#les-bases-offrant-des-informations-nutritionnelles",
    "href": "content/course/modern-ds/elastic_intro/index.html#les-bases-offrant-des-informations-nutritionnelles",
    "title": "6¬† Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "8.1 Les bases offrant des informations nutritionnelles",
    "text": "8.1 Les bases offrant des informations nutritionnelles\nPour un nombre restreint de produits, on pourrait bien-s√ªr chercher √†\nla main les caract√©ristiques des produits en utilisant les\nfonctionalit√©s d‚Äôun moteur de recherche:\n\nCependant, cette approche serait tr√®s fastidieuse et\nn√©cessiterait de r√©cuperer, √† la main, chaque caract√©ristique\npour chaque produit. Ce n‚Äôest donc pas envisageable.\nLes donn√©es disponibles sur Google viennent de l‚ÄôUSDA,\nl‚Äô√©quivalent am√©ricain de notre Minist√®re de l‚ÄôAgriculture.\nCependant, pour des recettes comportant des noms de produits fran√ßais, ainsi que\ndes produits potentiellement transform√©s, ce n‚Äôest pas tr√®s pratique d‚Äôutiliser\nune base de donn√©es de produits agricoles en Fran√ßais. Pour cette raison,\nnous proposons d‚Äôutiliser les deux bases suivantes,\nqui servent de base au travail de\nGaliana and Suarez Castillo (2022)\n\nL‚ÄôOpenFoodFacts database qui est une base\ncollaborative fran√ßaise de produits alimentaires. Issue d‚Äôun projet Data4Good, il s‚Äôagit d‚Äôune\nalternative opensource et opendata √† la base de donn√©es de l‚Äôapplication Yuka.\nLa table de composition nutritionnelle Ciqual produite par l‚ÄôAnses. Celle-ci\npropose la composition nutritionnelle moyenne des aliments les plus consomm√©s en France. Il s‚Äôagit d‚Äôune base de donn√©es\nenrichie par rapport √† celle de l‚ÄôUSDA puisqu‚Äôelle ne se cantonne pas aux produits agricoles non transform√©s.\nAvec cette base, il ne s‚Äôagit pas de trouver un produit exact mais essayer de trouver un produit type proche du produit\ndont on d√©sire conna√Ætre les caract√©ristiques."
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#import",
    "href": "content/course/modern-ds/elastic_intro/index.html#import",
    "title": "6¬† Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "8.2 Import",
    "text": "8.2 Import\nQuelques fonctions utiles sont regroup√©es dans le script functions.py et import√©es dans le notebook.\nLa base OpenFood peut √™tre r√©cup√©r√©e en ligne\nvia la fonction fc.import_openfood. N√©anmoins, cette op√©ration n√©cessitant\nun certain temps (les donn√©es brutes faisant autour de 2Go), nous proposons une m√©thode\npour les utilisateurs du SSP-Cloud o√π une version est disponible sur\nl‚Äôespace de stockage.\nLa base Ciqual, qui plus l√©g√®re, est r√©cup√©r√©e elle directement en ligne\nvia la fonction fc.import_ciqual.\n\n# Pour les utilisateurs du SSP-Cloud\nopenfood = fc.import_openfood_s3()\n# Pour les utilisateurs hors du SSP-Cloud\n# openfood = fc.import_openfood()\nciqual = fc.import_ciqual()\n\n\nopenfood.head()\n\n\nciqual.head()"
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#quest-ce-quelastic",
    "href": "content/course/modern-ds/elastic_intro/index.html#quest-ce-quelastic",
    "title": "6¬† Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "9.1 Qu‚Äôest-ce qu‚ÄôElastic ?",
    "text": "9.1 Qu‚Äôest-ce qu‚ÄôElastic ?\nElasticSearch c‚Äôest un logiciel qui fournit un moteur de recherche install√© sur\nun serveur (ou une machine personnelle) qu‚Äôil est possible de requ√™ter depuis un client\n(une session Python par exemple).\nC‚Äôest un moteur de recherche\ntr√®s performant, puissant et flexible, extr√™mement utilis√© dans le domaine de la datascience\nsur donn√©es textuelles.\nUn cas d‚Äôusage est par exemple de trouver,\ndans un corpus de grande dimension\n(plusieurs sites web, livres‚Ä¶), un certain texte en s‚Äôautorisant des termes voisins\n(verbes conjugu√©s, fautes de frappes‚Ä¶).\nUn index est une collection de documents dans lesquels on souhaite chercher, pr√©alablement ing√©r√©s dans un moteur de recherche les documents sont les √©tablissements.\nL‚Äôindexation consiste √† pr√©-r√©aliser les traitements des termes des documents pour gagner en efficacit√© lors de la phase de recherche.\nL‚Äôindexation est faite une fois pour de nombreuses recherches potentielles, pour lesquelles la rapidit√© de r√©ponse peut √™tre cruciale.\nApr√®s avoir index√© une base, on effectuera des requ√™tes qui sont des recherches\nd‚Äôun document dans la base index√© (√©quivalent de notre web) √† partir de\ntermes de recherche normalis√©s.\nLe principe est le m√™me que celui d‚Äôun moteur de recherche du web comme Google.\nD‚Äôun c√¥t√©, l‚Äôensemble √† parcourir est index√© pour √™tre en\nmesure de parcourir de mani√®re efficace l‚Äôensemble du corpus.\nDe l‚Äôautre c√¥t√©, la phase de recherche permet de retrouver l‚Äô√©l√©ment du corpus le\nplus coh√©rent avec la requ√™te de recherche.\nL‚Äôindexation consiste, par exemple,\n√† pr√©-d√©finir des traitements des termes du corpus pour gagner en efficacit√©\nlors de la phase de recherche. En effet, l‚Äôindexation est une op√©ration peu fr√©quente\npar rapport √† la recherche. Pour cette derni√®re, l‚Äôefficacit√© est cruciale (un site web\nqui prend plusieurs secondes √† interpr√©ter une requ√™te simple ne sera pas utilis√©). Mais, pour\nl‚Äôindexation, ceci est moins crucial.\nLes documents sont constitu√©s de variables, les champs (‚Äòfields‚Äô),\ndont le type est sp√©cifi√© (‚Äútext‚Äù, ‚Äúkeywoard‚Äù, ‚Äúgeo_point‚Äù, ‚Äúnumeric‚Äù‚Ä¶) √† l‚Äôindexation.\nElasticSearch propose une interface graphique nomm√©e Kibana.\nCelle-ci est pratique\npour tester des requ√™tes et pour superviser le serveur Elastic. Cependant,\npour le passage √† l‚Äô√©chelle, notamment pour mettre en lien une base index√©e dans\nElastic avec une autre source de donn√©es, les API propos√©es par ElasticSearch\nsont beaucoup plus pratiques. Ces API permettent de connecter une session Python (idem pour R)\n√† un serveur Elastic afin de communiquer avec lui\n(√©changer des flux via une API REST)."
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#elasticsearch-et-python",
    "href": "content/course/modern-ds/elastic_intro/index.html#elasticsearch-et-python",
    "title": "6¬† Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "9.2 ElasticSearch et Python",
    "text": "9.2 ElasticSearch et Python\nEn Python, le package officiel est elasticsearch.\nCe dernier permet de configurer les param√®tres pour interagir avec un serveur, indexer\nune ou plusieurs bases, envoyer de mani√®re automatis√©e un ensemble de requ√™tes\nau serveur, r√©cup√©rer les r√©sultats directement dans une session Python‚Ä¶"
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#premier-essai-les-produits-ciqual-les-plus-similaires-aux-produits-de-la-recette",
    "href": "content/course/modern-ds/elastic_intro/index.html#premier-essai-les-produits-ciqual-les-plus-similaires-aux-produits-de-la-recette",
    "title": "6¬† Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "10.1 Premier essai: les produits Ciqual les plus similaires aux produits de la recette",
    "text": "10.1 Premier essai: les produits Ciqual les plus similaires aux produits de la recette\nOn pourrait √©crire une fonction qui prend en argument\nune liste de libell√©s d‚Äôint√©r√™t et une liste de candidat au match et\nrenvoie le libell√© le plus proche.\nCependant, le risque est que cet algorithme soit relativement lent s‚Äôil n‚Äôest pas cod√©\nparfaitement.\nIl est, √† mon avis, plus simple, quand\non est habitu√© √† la logique Pandas,\nde faire un produit cart√©sien pour obtenir un vecteur mettant en miroir\nchaque produit de notre recette avec l‚Äôensembles des produits Ciqual et ensuite comparer les deux vecteurs pour prendre,\npour chaque produit, le meilleur match.\nLes bases √©tant de taille limit√©e, le produit cart√©sien n‚Äôest pas probl√©matique.\nAvec des bases plus cons√©quentes, une strat√©gie plus parcimonieuse en m√©moire devrait √™tre envisag√©e.\nPour faire cette op√©ration, on va utiliser la fonction match_product de\nnote script d‚Äôutilitaires.\n\ndist_leven = fc.match_product(libelles, ciqual)\ndist_leven\n\nCette premi√®re √©tape na√Øve est d√©cevante √† plusieurs √©gards:\n\nCertes, on a des matches coh√©rent (par exemple ‚ÄúOignon rouge, cru‚Äù et ‚Äú1 oignon‚Äù)\nmais on a plus de couples incoh√©rents ;\nLe temps de calcul peut appara√Ætre faible mais le passage √† l‚Äô√©chelle risque d‚Äô√™tre compliqu√© ;\nLes besoins m√©moires sont potentiellement importants lors de l‚Äôappel √†\nrapidfuzz.process.extract ce qui peut bloquer le passage √† l‚Äô√©chelle ;\nLa distance textuelle n‚Äôest pas n√©cessairement la plus pertinente.\n\nOn a, en fait, n√©glig√© une √©tape importante: la normalisation (ou nettoyage des textes) pr√©sent√©e dans la\npartie NLP, notamment:\n\nharmonisation de la casse, suppression des accents‚Ä¶\nsuppressions des mots outils (e.g.¬†ici on va d‚Äôabord n√©gliger les quantit√©s pour trouver la nature de l‚Äôaliment, en particulier pour Ciqual)\n\n\n\n\n\n\n\nScanner-data avant nettoyage\n\n\n\n\n\n\n\nOpenFood data avant nettoyage\n\n\n\n\n\n\n\n\n\nScanner-data apr√®s nettoyage\n\n\n\n\n\n\n\nOpenFood data apr√®s nettoyage\n\n\n\n\n\nFaisons donc en apparence un retour en arri√®re qui sera\nn√©anmoins salvateur pour am√©liorer\nla pertinence des liens faits entre nos\nbases de donn√©es."
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#objectif",
    "href": "content/course/modern-ds/elastic_intro/index.html#objectif",
    "title": "6¬† Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "11.1 Objectif",
    "text": "11.1 Objectif\nLe preprocessing correspond √† l‚Äôensemble des op√©rations\nayant lieu avant l‚Äôanalyse √† proprement parler.\nIci, ce preprocessing est int√©ressant √† plusieurs\n√©gards:\n\nIl r√©duit le bruit dans nos jeux de donn√©es (par exemple des mots de liaisons) ;\nIl permet de normaliser et harmoniser les syntaxes dans nos diff√©rentes sources.\n\nL‚Äôobjectif est ainsi de r√©duire nos noms de produits √† la substantifique moelle\npour am√©liorer la pertinence de la recherche.\nPour √™tre pertinent, le preprocessing comporte g√©n√©ralement deux types de\ntraitements. En premier lieu, ceux qui sont g√©n√©raux et applicables\n√† tous types de corpus textuels: retrait des stopwords, de la ponctuation, etc.\nles m√©thodes disponibles dans la partie NLP.\nEnsuite, il est n√©cessaire de mettre en oeuvre des nettoyages plus sp√©cifiques √† chaque corpus.\nPar exemple dans la source Ciqual,\nla cuisson est souvent renseign√©e et bruite les appariemments."
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#d√©marche",
    "href": "content/course/modern-ds/elastic_intro/index.html#d√©marche",
    "title": "6¬† Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "11.2 D√©marche",
    "text": "11.2 D√©marche\n{{% box status=‚Äúexercise‚Äù title=‚ÄúExercice 1‚Äù icon=‚Äúfas fa-pencil-alt‚Äù %}}\nExercice 1: preprocessing\n\nPour transformer les lettres avec accents en leur √©quivalent\nsans accent, la fonction unidecode\n(du package du m√™me nom) est pratique.\nLa tester sur le jeu de donn√©es ciqual en cr√©ant une nouvelle\ncolonne nomm√©e libel_clean\nLa casse diff√©rente selon les jeux de donn√©es peut √™tre p√©nalisante\npour trouver des produits similaires. Pour √©viter ces probl√®mes,\nmettre tout en majuscule.\nLes informations sur les quantit√©s ou le packaging peuvent apporter\ndu bruit dans notre comparaison. Nous allons retirer ces mots,\n√† travers la liste ['KG','CL','G','L','CRUE?S?', 'PREEMBALLEE?S?'],\nqu‚Äôon peut consid√©rer comme un dictionnaire de stop-words m√©tier.\nPour cela, il convient d‚Äôutiliser une expression r√©guli√®re dans la m√©thode\nstr.replace de Pandas.\nAvec ceux-ci, on va utiliser la liste des stop-words de\nla librairie nltk pour retirer les stop-words classiques (_‚Äúle‚Äù,‚Äúla‚Äù, etc.).\nLa librairie SpaCy, plus riche, pourrait √™tre utilis√©e ; nous laissons\ncela sous la forme d‚Äôexercice suppl√©mentaire.\nOn a encore des signes de ponctuation ou des chiffres qui peuvent\npoluer la comparaison. Les retirer gr√¢ce √† la m√©thode replace et\nune regex [^a-zA-Z]\nEnfin, par s√©curit√©, on peut supprimer les espaces multiples.\nUtiliser la regex '([ ]{2,})' pour cela. Observer le r√©sultat\nfinal.\n(Optionnel). Comme exercice suppl√©mentaire, faire la m√™me chose avec les\npipelines SpaCy.\n\n{{% /box %}}\nA l‚Äôissue de la question 1, le jeu de donn√©es ciqual devrait\nressembler √† celui-ci:\nApr√®s avoir mis en majuscule, on se retrouve avec le jeu de donn√©es\nsuivant:\nApr√®s retrait des stop-words, nos libell√©s prennent\nla forme suivante:\nLa regex pour √©liminer les caract√®res de ponctuation permet ainsi d‚Äôobtenir:\nEnfin, √† l‚Äôissue de la question 5, le DataFrame obtenu est le suivant:\nCes √©tapes de nettoyage ont ainsi permis de concentrer l‚Äôinformation\ndans les noms de produits sur ce qui l‚Äôidentifie vraiment."
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#approche-syst√©matique",
    "href": "content/course/modern-ds/elastic_intro/index.html#approche-syst√©matique",
    "title": "6¬† Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "11.3 Approche syst√©matique",
    "text": "11.3 Approche syst√©matique\nPour syst√©matiser cette approche √† nos diff√©rents DataFrame, rien de mieux\nqu‚Äôune fonction. Celle-ci est pr√©sente dans le module functions\nsous le nom clean_libelle.\n\nfrom functions import clean_libelle\n\nPour r√©sumer l‚Äôexercice pr√©c√©dent, cette fonction va :\n\nHarmoniser la casse et retirer les accents (voir functions.py) ;\nRetirer tout les caract√®res qui ne sont pas des lettres (chiffres, ponctuations) ;\nRetirer les caract√®res isol√©s.\n\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\n\nstop_words = ['KG','CL','G','L','CRUE?S?', 'PREEMBALLEE?S?']\nstop_words += [l.upper() for l in stopwords.words('french')]\n\nreplace_regex = {r'[^A-Z]': ' ', r'\\b[A-Z0-9]{1,2}?\\b':' '} # \n\nCela permet d‚Äôobtenir les bases nettoy√©es suivantes:\n\nciqual = clean_libelle(ciqual, yvar = 'alim_nom_fr', replace_regex = replace_regex, stopWords = stop_words)\nciqual.sample(10)\n\n\nopenfood = clean_libelle(openfood, yvar = 'product_name', replace_regex = replace_regex, stopWords = stop_words)\nopenfood.sample(10)\n\n\ncourses = pd.DataFrame(libelles, columns = ['libel'])\ncourses = clean_libelle(courses, yvar = 'libel', replace_regex = replace_regex, stopWords = stop_words)\ncourses.sample(10)\n\nLes noms de produits sont d√©j√† plus harmonis√©s.\nVoyons voir si cela permet de trouver un\nmatch dans l‚ÄôOpenfood database:\n\ndist_leven_openfood = fc.match_product(courses[\"libel_clean\"], openfood, \"libel_clean\")\ndist_leven_openfood.sample(10)\n\nPas encore parfait, mais on progresse sur les produits appari√©s!\nConcernant le temps de calcul, les quelques secondes n√©cessaires √†\nce calcul peuvent appara√Ætre un faible prix √† payer. Cependant,\nil convient de rappeler que le nombre de produits dans l‚Äôensemble\nde recherche est faible. Cette solution n‚Äôest donc pas g√©n√©ralisable."
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#r√©duire-les-temps-de-recherche",
    "href": "content/course/modern-ds/elastic_intro/index.html#r√©duire-les-temps-de-recherche",
    "title": "6¬† Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "11.4 R√©duire les temps de recherche",
    "text": "11.4 R√©duire les temps de recherche\nFinalement, l‚Äôid√©al serait de disposer d‚Äôun moteur de recherche adapt√© √† notre besoin,\ncontenant les produits candidats, que l‚Äôon pourrait interroger, rapide en lecture, capable de classer les √©chos renvoy√©s par pertinence, que l‚Äôon pourrait requ√™ter de mani√®re flexible.\nPar exemple, on pourrait vouloir signaler qu‚Äôun\n√©cho nous int√©resse seulement si la donn√©e calorique n‚Äôest pas manquante.\nOn pourrait m√™me vouloir qu‚Äôil effectue pour nous des pr√©traitements sur les donn√©es.\nCela para√Æt beaucoup demander. Mais c‚Äôest exactement ce que fait ElasticSearch."
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#cr√©er-un-cluster-elastic-sur-le-datalab",
    "href": "content/course/modern-ds/elastic_intro/index.html#cr√©er-un-cluster-elastic-sur-le-datalab",
    "title": "6¬† Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "12.1 Cr√©er un cluster Elastic sur le DataLab",
    "text": "12.1 Cr√©er un cluster Elastic sur le DataLab\nPour lancer un service Elastic, il faut cliquer sur ce lien.\nUne fois cr√©√©, vous pouvez explorer l‚Äôinterface graphique Kibana.\nCependant, gr√¢ce √† l‚ÄôAPI Elastic\nde Python, on se passera de celle-ci. Donc, en pratique,\nune fois lanc√©, pas besoin d‚Äôouvrir ce service Elastic pour continuer √† suivre1.\nDans un terminal, vous pouvez aussi v√©rifier que vous √™tes en mesure de dialoguer avec votre cluster Elastic,\nqui est pr√™t √† vous √©couter:\nkubectl get statefulset\nPasser par la ligne de commande serait peu commode pour industrialiser notre\nrecherche.\nNous allons utiliser la librairie elasticsearch pour dialoguer avec notre moteur de recherche Elastic.\nLes instructions ci-dessous indiquent comment √©tablir la connection.\n\nfrom elasticsearch import Elasticsearch\nHOST = 'elasticsearch-master'\n\ndef elastic():\n    \"\"\"Connection avec Elastic sur le data lab\"\"\"\n    es = Elasticsearch([{'host': HOST, 'port': 9200, 'scheme': 'http'}], http_compress=True, request_timeout=200)\n    return es\n\nes = elastic()\n\n&lt;Elasticsearch([{'host': 'elasticsearch-master', 'port': 9200}])&gt;\nMaintenant que la connection est √©tablie, deux √©tapes nous attendent:\n\nIndexation Envoyer les documents parmi lesquels on veut chercher des echos pertinents dans notre elastic. Un index est une collection de document. Nous pourrions en cr√©er deux: un pour les produits ciqual, un pour les produits openfood\nRequ√™te Chercher les documents les plus pertinents suivant une recherche textuelle flexible. Nous allons rechercher les libell√©s de notre recette et de notre liste de course."
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#premi√®re-indexation",
    "href": "content/course/modern-ds/elastic_intro/index.html#premi√®re-indexation",
    "title": "6¬† Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "12.2 Premi√®re indexation",
    "text": "12.2 Premi√®re indexation\nOn cr√©e donc nos deux index:\n\nif not es.indices.exists(index = 'openfood'):\n    es.indices.create(index = 'openfood')\nif not es.indices.exists(index = 'ciqual'):\n    es.indices.create(index = 'ciqual')\n\nPour l‚Äôinstant, nos index sont vides! Ils contiennent 0 documents.\n\nes.count(index = 'openfood')\n\n{'count': 0, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}}\nNous allons en rajouter quelques uns !\n\nes.create(index = 'openfood',  id = 1, body = {'product_name': 'Tarte noix de coco', 'product_name_clean': 'TARTE NOIX COCO'})\nes.create(index = 'openfood',  id = 2, body = {'product_name': 'Noix de coco', 'product_name_clean': 'NOIX COCO'})\nes.create(index = 'openfood',  id = 3, body = {'product_name': 'Beurre doux', 'product_name_clean': 'BEURRE DOUX'})\n\n\nes.count(index = 'openfood')\n\n{'count': 3, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}}\nDans l‚Äôinterface graphique Kibana,\non peut v√©rifier que l‚Äôindexation\na bien eue lieu en allant dans Management &gt; Stack Management"
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#premi√®re-recherche",
    "href": "content/course/modern-ds/elastic_intro/index.html#premi√®re-recherche",
    "title": "6¬† Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "12.3 Premi√®re recherche",
    "text": "12.3 Premi√®re recherche\nFaisons notre premi√®re recherche: cherchons des noix de p√©can!\n\nes.search(index = 'openfood', q = 'noix de p√©can')\n\nObjectApiResponse({'took': 116, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 2, 'relation': 'eq'}, 'max_score': 0.9400072, 'hits': [{'_index': 'openfood', '_type': '_doc', '_id': '2', '_score': 0.9400072, '_source': {'product_name': 'Noix de coco', 'product_name_clean': 'NOIX COCO'}}, {'_index': 'openfood', '_type': '_doc', '_id': '1', '_score': 0.8272065, '_source': {'product_name': 'Tarte noix de coco', 'product_name_clean': 'TARTE NOIX COCO'}}]}})\nInt√©ressons nous aux hits (r√©sultats pertinents, ou echos) : nous en avons 2.\nLe score maximal parmi les hits est mentionn√© dans max_score et correspond √† celui du deuxi√®me document index√©.\nElastic nous fournit ici un score de pertinence dans notre recherche d‚Äôinformation, et classe ainsi les documents renvoy√©s.\nIci nous utilisons la configuration par d√©faut. Mais comment est calcul√© ce score? Demandons √† Elastic de nous expliquer le score du document 2 dans la requ√™te \"noix de p√©can\".\n\nes.explain(index = 'openfood', id = 2, q = 'noix de p√©can')\n\nObjectApiResponse({'_index': 'openfood', '_type': '_doc', '_id': '2', 'matched': True, 'explanation': {'value': 0.9400072, 'description': 'max of:', 'details': [{'value': 0.49917626, 'description': 'sum of:', 'details': [{'value': 0.49917626, 'description': 'weight(product_name_clean:noix in 1) [PerFieldSimilarity], result of:', 'details': [{'value': 0.49917626, 'description': 'score(freq=1.0), computed as boost * idf * tf from:', 'details': [{'value': 2.2, 'description': 'boost', 'details': []}, {'value': 0.47000363, 'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:', 'details': [{'value': 2, 'description': 'n, number of documents containing term', 'details': []}, {'value': 3, 'description': 'N, total number of documents with field', 'details': []}]}, {'value': 0.48275858, 'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:', 'details': [{'value': 1.0, 'description': 'freq, occurrences of term within document', 'details': []}, {'value': 1.2, 'description': 'k1, term saturation parameter', 'details': []}, {'value': 0.75, 'description': 'b, length normalization parameter', 'details': []}, {'value': 2.0, 'description': 'dl, length of field', 'details': []}, {'value': 2.3333333, 'description': 'avgdl, average length of field', 'details': []}]}]}]}]}, {'value': 0.9400072, 'description': 'sum of:', 'details': [{'value': 0.4700036, 'description': 'weight(product_name:noix in 1) [PerFieldSimilarity], result of:', 'details': [{'value': 0.4700036, 'description': 'score(freq=1.0), computed as boost * idf * tf from:', 'details': [{'value': 2.2, 'description': 'boost', 'details': []}, {'value': 0.47000363, 'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:', 'details': [{'value': 2, 'description': 'n, number of documents containing term', 'details': []}, {'value': 3, 'description': 'N, total number of documents with field', 'details': []}]}, {'value': 0.45454544, 'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:', 'details': [{'value': 1.0, 'description': 'freq, occurrences of term within document', 'details': []}, {'value': 1.2, 'description': 'k1, term saturation parameter', 'details': []}, {'value': 0.75, 'description': 'b, length normalization parameter', 'details': []}, {'value': 3.0, 'description': 'dl, length of field', 'details': []}, {'value': 3.0, 'description': 'avgdl, average length of field', 'details': []}]}]}]}, {'value': 0.4700036, 'description': 'weight(product_name:de in 1) [PerFieldSimilarity], result of:', 'details': [{'value': 0.4700036, 'description': 'score(freq=1.0), computed as boost * idf * tf from:', 'details': [{'value': 2.2, 'description': 'boost', 'details': []}, {'value': 0.47000363, 'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:', 'details': [{'value': 2, 'description': 'n, number of documents containing term', 'details': []}, {'value': 3, 'description': 'N, total number of documents with field', 'details': []}]}, {'value': 0.45454544, 'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:', 'details': [{'value': 1.0, 'description': 'freq, occurrences of term within document', 'details': []}, {'value': 1.2, 'description': 'k1, term saturation parameter', 'details': []}, {'value': 0.75, 'description': 'b, length normalization parameter', 'details': []}, {'value': 3.0, 'description': 'dl, length of field', 'details': []}, {'value': 3.0, 'description': 'avgdl, average length of field', 'details': []}]}]}]}]}]}})\nElastic nous explique donc que le score 0.9400072 est le maximum entre deux sous-scores, 0.4991 et 0.9400072.\nPour chacun de ces sous-scores, le d√©tail de son calcul est donn√©.\nLe premier sous-score n‚Äôa accord√© un score que par rapport au premier mot (noix), tandis que le second a accord√© un score sur la base des deux mots d√©j√† connu dans les documents (‚Äúnoix‚Äù et ‚Äúde‚Äù). Il a ignor√© p√©can! Jusqu‚Äô√† pr√©sent, ce terme n‚Äôest pas connu dans l‚Äôindex.\nLa pertinence d‚Äôun mot pour notre recherche est construite sur une variante de la TF-IDF,\nconsid√©rant qu‚Äôun terme est pertinent s‚Äôil est souvent pr√©sent dans le document (Term Frequency)\nalors qu‚Äôil est peu fr√©quent dans les autres document (inverse document frequency).\nIci les notations des documents 1 et 2 sont tr√®s proches, la diff√©rence est d√ªe √† des IDF plus faibles dans le document 1,\nqui est p√©nalis√© pour √™tre l√©g√©rement plus long.\nBref, tout √ßa est un peu lourd, mais assez efficace,\nen tout cas moins rudimentaire que les distances caract√®res √† caract√®res pour ramener des echos pertinents."
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#limite-de-cette-premi√®re-indexation",
    "href": "content/course/modern-ds/elastic_intro/index.html#limite-de-cette-premi√®re-indexation",
    "title": "6¬† Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "12.4 Limite de cette premi√®re indexation",
    "text": "12.4 Limite de cette premi√®re indexation\nPour l‚Äôinstant, Elastic n‚Äôa pas l‚Äôair de g√©rer les fautes de frappes!\nPas le droit √† l‚Äôerreur dans la requ√™te:\n\nes.search(index = 'openfood',q = 'TART NOI')\n\nObjectApiResponse({'took': 38, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 0, 'relation': 'eq'}, 'max_score': None, 'hits': []}})\nCela s‚Äôexplique par la repr√©sentation des champs (‚Äòproduct_name‚Äô par exemple) qu‚ÄôElastic a inf√©r√©,\npuisque nous n‚Äôavons rien sp√©cifi√©.\nLa repr√©sentation d‚Äôune variable conditionne la fa√ßon dont les champs sont analys√©s pour calculer la pertinence.\nPar exemple, regardons la repr√©sentation du champ product_name\n\nes.indices.get_field_mapping(index = 'openfood', fields = 'product_name')\n\nObjectApiResponse({'openfood': {'mappings': {'product_name': {'full_name': 'product_name', 'mapping': {'product_name': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}}}}}})\nElastic a compris qu‚Äôil s‚Äôagissait d‚Äôun champ textuel.\nEn revanche, le type est keyword n‚Äôautorise pas des analyses approximatives donc\nne permet pas de tenir compte de fautes de frappes.\nPour qu‚Äôun echo remonte, un des termes doit matcher exactement. Dommage !\nMais c‚Äôest parce qu‚Äôon a utilis√© le mapping par d√©faut.\nEn r√©alit√©, il est assez simple de pr√©ciser un mapping plus riche,\nautorisant une analyse ‚Äúfuzzy‚Äù ou ‚Äúflou‚Äù."
  },
  {
    "objectID": "content/course/modern-ds/elastic_intro/index.html#nos-premi√®res-requ√™tes",
    "href": "content/course/modern-ds/elastic_intro/index.html#nos-premi√®res-requ√™tes",
    "title": "6¬† Introduction √† ElasticSearch pour la recherche textuelle",
    "section": "13.1 Nos premi√®res requ√™tes",
    "text": "13.1 Nos premi√®res requ√™tes\nV√©rifions qu‚Äôon recup√®re quelques tartes aux noix m√™me si l‚Äôon fait plein de fautes:\n\nes.search(index = 'openfood', q = 'TART NOI', size = 3)\n\nObjectApiResponse({'took': 60, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 10000, 'relation': 'gte'}, 'max_score': 22.837925, 'hits': [{'_index': 'openfood', '_type': '_doc', '_id': '405332', '_score': 22.837925, '_source': {'product_name': 'Tarte noix', 'libel_clean': 'TARTE NOIX', 'energy_100g': 1833.0, 'nutriscore_score': 23.0}}, {'_index': 'openfood', '_type': '_doc', '_id': '1103594', '_score': 22.82367, '_source': {'product_name': 'Tarte aux noix', 'libel_clean': 'TARTE NOIX', 'energy_100g': 4.0, 'nutriscore_score': 4.0}}, {'_index': 'openfood', '_type': '_doc', '_id': '1150755', '_score': 22.82367, '_source': {'product_name': 'Tarte aux noix', 'libel_clean': 'TARTE NOIX', 'energy_100g': 1929.0, 'nutriscore_score': 21.0}}]}})\nSi on pr√©f√®re sous une forme de DataFrame:\n\ndf = pd.json_normalize(\n    es.search(index = 'openfood', q = 'TART NOI', size = 3)['hits']['hits']\n)\ndf.columns = df.columns.str.replace(\"_source.\", \"\", regex = False)\ndf.head(2)\n\n\\n  \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n  \\n  \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n  \\n_index_type_id_scoreproduct_namelibel_cleanenergy_100gnutriscore_score0openfood_doc40533222.837925Tarte noixTARTE NOIX1833.023.01openfood_doc110359422.823670Tarte aux noixTARTE NOIX4.04.02openfood_doc115075522.823670Tarte aux noixTARTE NOIX1929.021.0\nPour automatiser l‚Äôenvoi de requ√™tes et la r√©cup√©ration du meilleur\n√©cho, on peut d√©finir la fonction suivante\n\ndef matchElastic(libelles):\n    start_time = time.time()\n    matches = {}\n    for l in libelles:\n        response = es.search(index = 'openfood', q = l, size = 1)\n        if len(response['hits']['hits'])&gt;0:\n            matches[l] = pd.json_normalize(\n              response['hits']['hits']\n            )\n    print(80*'-')\n    print(f\"Temps d'ex√©cution total : {(time.time() - start_time):.2f} secondes ---\")\n    \n    return matches\n\n\nmatches = matchElastic(courses['libel_clean'])\nmatches = pd.concat(matches)\nmatches.sample(3)\n\n\\n  \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n  \\n  \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n  \\n_index_type_id_score_source.product_name_source.libel_clean_source.energy_100g_source.nutriscore_scoreGOUSSE AIL0openfood_doc198206257.93140Gousse d\\'ailGOUSSE AIL498.05.0IGP SAUVIGNON0openfood_doc180140696.55756vin blanc SauvignonVIN BLANC SAUVIGNON66.31.0POTIRON0openfood_doc104396175.96385PotironPOTIRON172.00.0\nEt voil√†, on a un outil tr√®s rapide de requ√™te !\nLa pertinence des r√©sultats est encore douteuse.\nPour cela, il conviendrait de pr√©ciser des requ√™tes plus sophistiqu√©es!2\n\nreq = {\n    \"bool\": {\n      \"should\": [\n        { \"match\": { \"libel_clean\":  { \"query\":  \"HUILE OLIVE\" , \"boost\" : 10}}},\n        { \"match\": { \"libel_clean.ngr\":   \"HUILE OLIVE\" }}\n        ],\n      \"minimum_should_match\": 1,\n      \"filter\": [\n      { \n            \"range\" : {\n                \"nutriscore_score\" : {\n                    \"gte\" : 10,\n                    \"lte\" : 20\n                    }\n                    }\n                    }\n      ]\n    }\n}\n\n\nout = es.search(index = 'openfood', query = req, size = 1)\npd.json_normalize(out['hits']['hits'])\n\n\\n  \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n  \\n  \\n    \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n    \\n  \\n_index_type_id_score_source.product_name_source.libel_clean_source.energy_100g_source.nutriscore_score0openfood_doc960041174.27896Huile d oliveHUILE OLIVE3761.011.0\nQu‚Äôa-t-on demand√© ici?\n- De renvoyer 1 et 1 seul echo (\"size\":\"1\") et seulement si celui ci a:\n+ \"should\": Au moins un (\"minimum_should_match\":\"1\") des termes des deux champs libel_clean et libel_clean.ngr qui matche sur un terme de HUILE OLIVE, l‚Äôanalyse (la d√©finition du ‚Äúterme‚Äù) √©tant r√©alis√© soit en tant que text (‚Äúlibel_clean‚Äù) soit en tant que n-gramme ngr (‚Äúlibel_clean.ngr‚Äù, une analyse que nous avons sp√©cifi√© dans le mapping)\n+ \"filter\": Le champ float nutriscore_score doit √™tre compris entre 10 et 20 (‚Äúfilter‚Äù).\nA noter :\n\nLes clauses (\"should\"+\"minimum_should_match\":\"1\") peuvent √™tre remplac√© par un \"must\". Auquel cas, l‚Äô√©cho doit obligatoirement matcher sur chaque clause.\nPr√©ciser dans \"filter\" (plut√¥t que dans \"should\") une condition signifie que celle-ci ne participe pas au score de pertinence.\n\nOn n‚Äôa pas encore un appariemment tr√®s satisfaisant, en particulier sur les boissons. Comment faire ? La r√©ponse est dans Galiana and Suarez Castillo (2022)\n\nA vous, de calculer le nombre de calories de notre recette de course !\n\n\n\n\n\nGaliana, Lino, and Milena Suarez Castillo. 2022. ‚ÄúFuzzy Matching on Big-Data: An Illustration with Scanner and Crowd-Sourced Nutritional Datasets.‚Äù In Proceedings of the 2022 ACM Conference on Information Technology for Social Good, 331‚Äì37. GoodIT ‚Äô22. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3524458.3547244."
  },
  {
    "objectID": "content/course/modern-ds/continuous_integration/index.html",
    "href": "content/course/modern-ds/continuous_integration/index.html",
    "title": "7¬† Int√©gration continue avec Python",
    "section": "",
    "text": "8 L‚Äôint√©gration continue: une opportunit√© pour les data-scientists\nOn retrouve r√©guli√®rement l‚Äôacronyme CI/CD\npour illustrer cette\nnouvelle m√©thode de travail dans le\nmonde du d√©veloppement logiciel :\nCette pratique permet ainsi de d√©tecter de mani√®re pr√©coce des possibilit√©s\nde bug ou l‚Äôintroduction d‚Äôun changement non anticip√©. Tout comme Git,\ncette pratique devient un standard dans les domaines collaboratifs.\nL‚Äôint√©gration continue permet de s√©curiser le travail, puisqu‚Äôelle offre un\nfilet de s√©curit√© (par exemple un test sur une machine √† la configuration\narbitraire), mais permet aussi de d√©ployer en temps r√©el certaines\n√©volutions. On parle parfois de d√©ploiement en continu, compl√©mentaire de\nl‚Äôint√©gration continue. Cette approche r√©duit ainsi\nla muraille de Chine entre un\nanalyste de donn√©es et une √©quipe de d√©veloppeurs d‚Äôapplication. Elle offre donc\nplus de contr√¥le, pour le producteur d‚Äôune analyse statistique, sur la\nvalorisation de celle-ci.\nCette approche consiste une excellente opportunit√©\npour les data-scientists d‚Äô√™tre en mesure\nde valoriser leurs projets aupr√®s de publics aux\nexigences diff√©rentes. Pour des d√©veloppeurs, le\ndata-scientist pourra fournir une image Docker\n(environnement portable o√π l‚Äôensemble des d√©pendances\net des configurations syst√®mes pour faire tourner un code\nsont contr√¥l√©s) permettant √† d‚Äôautres d‚Äôex√©cuter\nfacilement le code d‚Äôun projet. Pour faciliter\nla r√©utilisation d‚Äôun mod√®le par d‚Äôautres data-scientists,\nil devient de plus en plus fr√©quent d‚Äôexposer\nun mod√®le sous forme d‚ÄôAPI: les personnes d√©sirant\nr√©utiliser le mod√®le peuvent directement l‚Äôappliquer\nen acc√©dant √† une pr√©diction par le biais d‚Äôune API\nce qui √©vite d‚Äôavoir √† fournir le jeu d‚Äôentra√Ænement\nsi ce dernier est sensible. Pour toucher\ndes publics moins\nfamiliers du code, la mise √† disposition de sites web\ninteractifs valorisant certains r√©sultats d‚Äôun projet\npeut √™tre int√©ressante. Cette approche tr√®s exigeante\nd‚Äôutiliser un m√™me projet pour toucher des cibles\ntr√®s diff√©rentes est grandement facilit√©e par le\nd√©ploiement en continu et la mise √† disposition\nde librairies ou d‚Äôinfrastructures\nd√©di√©es dans le monde de l‚Äôopen-source.\nTout en restant √©co-responsable (voir partie XXX), cela\npermet de mieux valoriser des projets pour r√©duire\nles co√ªts √† le maintenir et le faire √©voluer.\nLe cours de derni√®re ann√©e de l‚ÄôENSAE que je d√©veloppe\navec Romain Avouac (https://ensae-reproductibilite.netlify.app/)\npr√©sente beaucoup plus de d√©tails sur cette question.\nL‚Äôint√©gration continue fonctionne tr√®s bien sur Gitlab et sur Github.\nA chaque interaction avec le d√©p√¥t distant (push), une s√©rie d‚Äôinstruction\nd√©finie par l‚Äôutilisateur est ex√©cut√©e. Python et R s‚Äôint√®grent tr√®s bien dans ce paradigme gr√¢ce\n√† un certain nombre d‚Äôimages de base (concept sur lequel nous allons revenir)\nqui peuvent √™tre customis√©es pour r√©pondre √† une certaine configuration\nn√©cessaire pour ex√©cuter des codes\n(voir ici pour quelques √©l√©ments sur R.\nC‚Äôest une m√©thode id√©ale pour am√©liorer la reproductibilit√© d‚Äôun projet: les\ninstructions ex√©cut√©es le sont dans un environnement isol√© et contr√¥l√©, ce qui\ndiff√®re d‚Äôune machine personnelle.\nL‚Äôint√©gration continue repose sur le syst√®me de la dockerisation ou conteneurisation.\nLa technologie sous jacente s‚Äôappelle Docker.\nIl s‚Äôagit d‚Äôune technologie qui permet la construction\nde machines autosuffisantes\n(que l‚Äôon nomme containeurs) r√©pliquant un environnement\ncontr√¥l√© (que l‚Äôon nomme image).\nOn parle de pipelines pour d√©signer une suite de t√¢ches pour partir de 0\n(g√©n√©ralement une machine Linux √† la configuration minimale) et aboutir\n√† l‚Äôissue d‚Äôune s√©rie d‚Äôinstructions d√©finies par l‚Äôutilisateur.\nL‚Äôobjectif est de trouver une image la plus\nparcimonieuse possible, c‚Äôest-√†-dire √† la configuration minimale, qui permet\nde faire tourner le code voulu.\nLes Actions Github\nconsistuent un mod√®le sur lequel il est facile\nde s‚Äôappuyer lorsqu‚Äôon a des connaissances limit√©es\nconcernant `Docker.\nIl est √©galement tr√®s simple de construire son image\nde rien, ce qui est la d√©marche choisie dans\nl‚Äôautre cours de l‚ÄôENSAE que nous donnons avec Romain\nAvouac (https://ensae-reproductibilite.netlify.app/).\nQuand on utilise un d√©p√¥t Github \nou Gitlab ,\ndes services automatiques\nd‚Äôint√©gration continue peuvent √™tre utilis√©s:\nHistoriquement, il existait d‚Äôautres services d‚Äôint√©gration continue, notamment\nTravis CI ou AppVeyor1\nLes projets de valorisation de donn√©es prennent des formes\ntr√®s vari√©es et s‚Äôadressent √† des publics multiples dont\nles attentes peuvent √™tre tr√®s diverses.\nNe pas attendre la finalisation d‚Äôun projet pour mettre\nen oeuvre certains livrables est une m√©thode efficace\npour ne pas se retrouver noy√©, au dernier moment,\nsous des demandes et de nouvelles contraintes.\nLa production en continu de livrables est donc une\nm√©thode tr√®s pris√©e dans le monde de la donn√©e.\nLes principaux fournisseurs de services\nd‚Äôint√©gration continue, √† commencer par\nGithub et Gitlab proposent des services\npour le d√©ploiement en continu. Cependant,\nceux-ci ne sont adapt√©s qu‚Äô√† certains types\nde livrables, principalement la mise √† disposition\nde sites internet, et il peut √™tre int√©ressant\nd‚Äôutiliser des services externes ou une\ninfrastructures Kubernetes selon les\nmoyens √† dispositon et les besoins des utilisateurs."
  },
  {
    "objectID": "content/course/modern-ds/continuous_integration/index.html#fonctionnement-des-actions-github",
    "href": "content/course/modern-ds/continuous_integration/index.html#fonctionnement-des-actions-github",
    "title": "7¬† Int√©gration continue avec Python",
    "section": "10.1 Fonctionnement des actions Github",
    "text": "10.1 Fonctionnement des actions Github\nLes actions Github fonctionnent par couches successives au sein desquelles\non effectue un certain nombre d‚Äôinstructions.\nLa meilleure mani√®re d‚Äôapprendre les actions Github est, certes, de lire la\ndocumentation officielle mais surtout,\n√† mon avis, de regarder quelques pipelines pour comprendre la d√©marche.\nL‚Äôun des int√©r√™ts des Github Actions est la possibilit√© d‚Äôavoir un pipeline\nproposant une intrication de langages diff√©rents pour avoir une chaine de\nproduction qui propose les outils les plus efficaces pour r√©pondre √† un\nobjectif en limitant les verrous techniques.\nPar exemple, le pipeline de ce cours, disponible\nsur Github propose une intrication des langages\nPython et R avec des technologies Anaconda (pour contr√¥ler\nl‚Äôenvironnement Python comme expliqu√© dans les chapitres pr√©c√©dents)\net Javascript (pour le d√©ploiement d‚Äôun site web avec le service tiers\nNetlify)2. Cette cha√Æne de production multi-langage permet que\nles m√™mes fichiers sources g√©n√®rent un site web et des notebooks disponibles\nsur plusieurs environnements.\n\n\nname: Production deployment\n\non:\n  push:\n    branches:\n      - main\n      - master\n\njobs:\n  pages:\n    name: Render-Blog\n    runs-on: ubuntu-latest\n    container: linogaliana/python-datascientist:latest\n    steps:\n      - uses: actions/checkout@v2\n        with:\n          fetch-depth: 0\n          ref: ${{ github.event.pull_request.head.ref }}\n          repository: ${{github.event.pull_request.head.repo.full_name}}\n      - name: Configure safe.directory  # Workaround for actions/checkout#760\n        run: git config --global --add safe.directory /__w/python-datascientist/python-datascientist\n      - shell: bash\n        run: |\n          conda info\n          conda list\n      - name: Render website\n        run: |\n          python build/tweak_render.py\n          python build/wc_website.py\n          quarto render --to html\n      - name: Publish to Pages\n        run: |\n          git config --global user.email quarto-github-actions-publish@example.com\n          git config --global user.name \"Quarto GHA Workflow Runner\"\n          quarto publish gh-pages . --no-render --no-browser\n      - uses: actions/upload-artifact@v2\n        with:\n          name: Website\n          path: public/\n\n\n\nLes couches qui constituent les √©tapes du pipeline\nportent ainsi le nom de steps. Un step peut comporter un certain\nnombre d‚Äôinstructions ou ex√©cuter des instructions pr√©-d√©finies.\nL‚Äôune de ces instructions pr√©d√©finies est, par exemple,\nl‚Äôinstallation de Python\nou l‚Äôinitialisation d‚Äôun environnement conda.\nLa documentation officielle de Github propose un\nfichier qui peut servir de mod√®le\npour tester un script Python voire l‚Äôuploader de mani√®re automatique\nsur Pypi."
  },
  {
    "objectID": "content/course/modern-ds/continuous_integration/index.html#int√©gration-continue-avec-python-tester-un-notebook",
    "href": "content/course/modern-ds/continuous_integration/index.html#int√©gration-continue-avec-python-tester-un-notebook",
    "title": "7¬† Int√©gration continue avec Python",
    "section": "10.2 Int√©gration continue avec Python: tester un notebook",
    "text": "10.2 Int√©gration continue avec Python: tester un notebook\nCette section n‚Äôest absolument pas exhaustive. Au contraire, elle ne fournit\nqu‚Äôun exemple minimal pour expliquer la logique de l‚Äôint√©gration continue. Il\nne s‚Äôagit ainsi pas d‚Äôune garantie absolue de reproductibilit√© d‚Äôun notebook.\nGithub propose une action officielle pour utiliser Python dans un\npipeline d‚Äôint√©gration continue. Elle est disponible sur le\nMarketPlace Github.\nIl s‚Äôagit d‚Äôun bon point de d√©part, √† enrichir.\nLe fichier qui contr√¥le les instructions ex√©cut√©es dans l‚Äôenvironnement Actions\ndoit se trouver dans le dossier .github/workflows/\n(:warning: ne pas oublier le point au d√©but du\nnom du dossier). Il doit √™tre au format YAML avec une extension .yml\nou .yaml.\nIl peut avoir n‚Äôimporte quel nom n√©anmoins il\nvaut mieux lui donner un nom signifiant,\npar exemple prod.yml pour un fichier contr√¥lant une cha√Æne de production.\n\n10.2.1 Lister les d√©pendances\nAvant d‚Äô√©crire les instructions √† ex√©cuter par Github, il faut d√©finir un\nenvironnement d‚Äôex√©cution car Github ne conna√Æt pas la configuration Python\ndont vous avez besoin.\nIl convient ainsi de lister les d√©pendances n√©cessaires dans un fichier\nrequirements.txt (si on utilise un environnement virtuel)\nou un fichier environment.yml (si on pr√©f√®re\nutiliser un environnement conda).\nBien que le principe sous-jacent soit l√©g√®rement diff√©rent,\nces fichiers ont la m√™me fonction:\npermettre la cr√©ation d‚Äôun environnement ex-nihilo\navec un certain nombre de d√©pendances pr√©-install√©es3.\nSi on fait le choix de l‚Äôoption environment.yml,\nle fichier prendra ainsi la forme\nsuivante, √† enrichir en fonction de la\nrichesse de l‚Äôenvironnement souhait√©. :\nchannels:\n  - conda-forge\n\ndependencies:\n  - python&gt;=3.10\n  - jupyter\n  - jupytext\n  - matplotlib\n  - nbconvert\n  - numpy\n  - pandas\n  - scipy\n  - seaborn\nLe m√™me fichier sous le format requirements.txt aura\nla forme suivante:\njupyter\njupytext\nmatplotlib\nnbconvert\nnumpy\npandas\nscipy\nseaborn\nSous leur apparente √©quivalence, au-del√† de\nla question du formatage, ces fichiers ont\ndeux diff√©rences principales :\n\nla version minimale de Python est d√©finie dans\nle fichier environment.yml alors qu‚Äôelle ne l‚Äôest\npas dans un fichier requirements.txt. C‚Äôest\nparce que le second installe les d√©pendances dans\nun environnement d√©j√† existant par ailleurs alors\nque le premier peut servir √† cr√©er l‚Äôenvironnement\navec une certaine configuration de Python ;\nle mode d‚Äôinstallation des packages n‚Äôest pas le\nm√™me. Avec un environment.yml on installera des\npackages via conda alors qu‚Äôavec un requirements.txt\non privil√©giera plut√¥t pip4.\n\nDans le cas de l‚Äôenvironnement conda,\nle choix du channel conda-forge vise √† contr√¥ler le d√©p√¥t utilis√© par\nAnaconda.\n{{% box status=‚Äúhint‚Äù title=‚ÄúConseil‚Äù icon=‚Äúfa fa-lightbulb‚Äù %}}\nLa conda forge est un d√©p√¥t de package alternatif\nau canal par d√©faut d‚ÄôAnaconda qui est maintenu par\nl‚Äô√©quipe de d√©veloppeurs officiels d‚ÄôAnaconda.\nComme cette derni√®re cherche en priorit√© √†\nassurer la stabilit√© de l‚Äô√©cosyst√®me Anaconda,\nles versions de package √©voluent moins vite\nque le rythme voulu par les d√©veloppeurs de\npackages. Pour cette raison, un d√©p√¥t\nalternatif, o√π les mont√©es de version sont\nplus simples parce qu‚Äôelles d√©pendent des\nd√©veloppeurs de chaque package, a √©merg√©.\nIl s‚Äôagit de la conda forge. Lorsqu‚Äôon\nd√©sire utiliser des fonctionalit√©s r√©centes\nde l‚Äô√©cosyst√®me de la data-science,\nil est conseill√© de l‚Äôutiliser.\n{{% /box %}}\nNe pas oublier de mettre ce fichier sous contr√¥le de version et de l‚Äôenvoyer\nsur le d√©p√¥t par un push.\n\n\n10.2.2 Cr√©er un environnement reproductible dans Github Actions\nDeux approches sont possibles √† ce niveau, selon le degr√©\nde reproductibilit√© d√©sir√©5:\n\nCr√©er l‚Äôenvironnement via une action existante. L‚Äôaction\nconda-incubator/setup-miniconda@v2\nest un bon point de d√©part.\nCr√©er l‚Äôenvironnement dans une image Docker.\n\nLa deuxi√®me solution permet de contr√¥ler de mani√®re\nbeaucoup plus fine l‚Äôenvironnement dans lequel\nPython s‚Äô√©x√©cutera ainsi que la mani√®re dont\nl‚Äôenvironnement sera cr√©√©6. N√©anmoins, elle n√©cessite\ndes connaissances plus pouss√©es dans la principe\nde la conteneurisation qui peuvent √™tre co√ªteuses\n√† acqu√©rir. Selon l‚Äôambition du projet, notamment\nles r√©utilisation qu‚Äôil d√©sire,\nun data-scientist pourra privil√©gier\ntelle ou telle option. Les deux solutions sont pr√©sent√©es\ndans l‚Äôexemple fil-rouge du cours que nous\ndonnons avec Romain Avouac\n(https://ensae-reproductibilite.netlify.app/application/).\n\n\n10.2.3 Tester un notebook myfile.ipynb\nDans cette partie, on va supposer que le notebook √† tester s‚Äôappelle myfile.ipynb\net se trouve √† la racine du d√©p√¥t. Les\nd√©pendances pour l‚Äôex√©cuter sont\nlist√©es dans un fichier requirements.txt.\nLe mod√®le suivant, expliqu√© en dessous, fournit un mod√®le de recette pour\ntester un notebook. Supposons que ce fichier soit pr√©sent\ndans un chemin .github/workflows/test-notebook.yml\n\n\nEnvironnement virtuel\n\nname: Test notebook execution using Github Actions\n\non: [push]\n\njobs:\n  build-linux:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up Python 3.10\n      uses: actions/setup-python@v3\n      with:\n        python-version: '3.10'\n      - shell: bash\n      run: |\n        python --version\n    - name: Install dependencies\n      run:\n        pip install -r requirements.txt\n        pip install jupyter nbconvert\n    - name: Test jupyter from command line\n      run:\n        jupyter nbconvert --execute --to notebook --inplace myfile.ipynb\n    - uses: actions/upload-artifact@v3\n      with:\n        name: Notebook\n        path: myfile.ipynb\n        retention-days: 5\n\n\n\nEnvironnement conda\n\nname: Test notebook execution using Github Actions\n\non: [push]\n\njobs:\n  build-linux:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up Python 3.10\n      uses: actions/setup-python@v3\n      with:\n        python-version: '3.10'\n    - name: Add conda to system path\n      run: |\n        # $CONDA is an environment variable pointing to the root of the miniconda directory\n        echo $CONDA/bin &gt;&gt; $GITHUB_PATH\n    - name: Install dependencies\n      run: |\n        conda env update --file environment.yml --name base\n        conda install jupyter nbconvert\n    - name: Test jupyter from command line\n      run:\n        jupyter nbconvert --execute --to notebook --inplace myfile.ipynb\n    - uses: actions/upload-artifact@v3\n      with:\n        name: Notebook\n        path: myfile.ipynb\n        retention-days: 5\n\nDans les deux cas, la d√©marche est la m√™me:\n\non r√©cup√®re les fichiers pr√©sents dans le d√©p√¥t\n(action checkout) ;\non installe Python ;\non installe les d√©pendances pour ex√©cuter le code.\nDans l‚Äôapproche conda, il est √©galement n√©cessaire\nde faire quelques configurations suppl√©mentaires (notamment\najouter conda aux logiciels reconnus par la ligne\nde commande) ;\non teste le notebook en ligne de commande et remplace\ncelui existant, sur la machine temporaire, par la version\nproduite sur cet environnement neutre.\non rend possible le t√©l√©chargement du\nnotebook produit automatiquement pendant 5 jours7. Ceci\nrepose sur les artefacts qui sont un √©l√©ment r√©cup√©r√©\ndes machines temporaires qui n‚Äôexistent plus d√®s que le\ncode a fini d‚Äô√™tre ex√©cut√©.\n\nCes actions sont ex√©cut√©es √† chaque interaction avec\nle d√©p√¥t distant (push), quelle que soit la\nbranche. A partir de ce mod√®le, il est possible de\nraffiner pour, par exemple, automatiquement\nfaire un commit du notebook valid√© et le pusher\nvia le robot Github8"
  },
  {
    "objectID": "content/course/modern-ds/continuous_integration/index.html#les-services-de-mise-√†-disposition-de-github-et-gitlab",
    "href": "content/course/modern-ds/continuous_integration/index.html#les-services-de-mise-√†-disposition-de-github-et-gitlab",
    "title": "7¬† Int√©gration continue avec Python",
    "section": "11.1 Les services de mise √† disposition de Github et Gitlab",
    "text": "11.1 Les services de mise √† disposition de Github et Gitlab\nGithub et Gitlab, les deux plateformes de partage\nde code, proposent non seulement des services\ngratuits d‚Äôint√©gration continue mais aussi des services\nde mise √† disposition de sites web pleinement int√©gr√©s\naux services de stockage de code.\nCes services, Gitlab Pages et Github Pages, auxquels\non peut associer le service externe Netlify qui r√©pond\nau m√™me principe9 permettent, √† chaque modification\ndu code source d‚Äôun projet, de reconstruire le site web (le livrable)\nqui peut √™tre directement produit √† partir de certains fichiers\n(des slides revealJS par exemple) ou qui\nsert d‚Äôoutput √† l‚Äôint√©gration continue apr√®s compilation\nde fichiers plus complexes (des fichiers quarto par exemple).\nChaque d√©p√¥t sur Github ou Gitlab peut ainsi √™tre associ√©\n√† un URL de d√©ploiement disponible sur internet. A chaque\ncommit sur le d√©p√¥t, le site web qui sert de livrable\nest ainsi mis √† jour. La version d√©ploy√©e √† partir de la\nbranche principale peut ainsi √™tre consid√©r√©e\ncomme la version de production alors que les branches\nsecondaires peuvent servir d‚Äôespace bac √† sable pour\nv√©rifier que des changements dans le code source\nne mettent pas en p√©ril le livrable. Cette m√©thode,\nqui s√©curise la production d‚Äôun livrable sous forme\nde site web, est ainsi particuli√®rement appr√©ciable."
  },
  {
    "objectID": "content/course/modern-ds/continuous_integration/index.html#les-services-externes-disponibles-sans-infrastructure-sp√©ciale",
    "href": "content/course/modern-ds/continuous_integration/index.html#les-services-externes-disponibles-sans-infrastructure-sp√©ciale",
    "title": "7¬† Int√©gration continue avec Python",
    "section": "11.2 Les services externes disponibles sans infrastructure sp√©ciale",
    "text": "11.2 Les services externes disponibles sans infrastructure sp√©ciale\nPour fonctionner, l‚Äôint√©gration continue\nn√©cessite de mettre en oeuvre des environnements normalis√©s.\nComme √©voqu√© pr√©c√©demment,\nla technologie sous-jacente est celle de la conteneurisation.\nLes images qui servent de point de d√©part au lancement\nd‚Äôun conteneur sont elles-m√™mes mises √† disposition\ndans des espaces communautaires (des registres d‚Äôimages).\nIl en existe plusieurs, les plus connus √©tant\nle dockerhub ou le registry de Gitlab.\nCes registres servent d‚Äôespaces de stockage pour des images,\nqui sont des objets volumineux (potentiellement plusieurs\nGigas) mais aussi d‚Äôespace de mutualisation en permettant\n√† d‚Äôautres de r√©utiliser une image pr√™te √† l‚Äôemploi ou,\nau contraire, √† partir de\nlaquelle on peut ajouter un certain nombre de couches\npour obtenir l‚Äôenvironnement minimal\nde reproductibilit√©.\nIl est possible d‚Äôutiliser certaines actions Github\npr√™te √† l‚Äôemploi pour constuire une image Docker\n√† partir d‚Äôun fichier Dockerfile. Apr√®s avoir\ncr√©e une connexion entre un compte sur la\nplateforme Github et l‚Äôautre sur DockerHub,\nune mise √† disposition automatis√©e d‚Äôun livrable\nsous forme d‚Äôimage Docker est ainsi possible.\nUne image Docker peut offrir une grande vari√©t√©\nd‚Äôoutput. Elle peut servir uniquement √†\nmettre √† disposition un environnement de\nreproductibilit√© mais elle peut servir √† mettre\n√† disposition, pour les personnes ma√Ætrisant\nDocker, des output plus raffin√©s. Par exemple,\ndans le cours que nous donnons √† l‚ÄôENSAE, nous\nmontrons comment docker peut servir √†\nmettre √† disposition √† un utilisateur tiers\nune application minimaliste (construite avec flask)\nqu‚Äôil fera tourner\nsur son ordinateur.\nSi une image Docker peut √™tre tr√®s utile pour la mise\n√† disposition, elle n√©cessite pour sa r√©utilisation\nun niveau avanc√© d‚Äôexpertise en programmation.\nCela ne conviendra pas √† tous les publics. Certains\nne d√©sireront que b√©n√©ficier d‚Äôune application interactive\no√π ils pourrons visualiser certains r√©sultats en fonction\nd‚Äôactions comme des filtres sur des sous-champs ou le choix\nde certaines plages de donn√©es. D‚Äôautres publics seront\nplut√¥t int√©ress√© par la r√©utilisation d‚Äôun programme\nou des r√©sultats d‚Äôun mod√®le sous forme d‚ÄôAPI mais n‚Äôauront\npas l‚Äôinfrastructure interne pour faire tourner le code\nd‚Äôorigine ou une image Docker. C‚Äôest pour r√©pondre √† ces\nlimites qu‚Äôil peut devenir int√©ressant, pour une √©quipe\nde data-science de d√©velopper une architecture\nkubernetes interne, si l‚Äôorganisation en a les moyens, ou\nde payer un fournisseur de service, comme AWS, qui permet\ncela."
  },
  {
    "objectID": "content/course/modern-ds/continuous_integration/index.html#kubernetes-le-sommet-de-la-pente-du-d√©ploiement",
    "href": "content/course/modern-ds/continuous_integration/index.html#kubernetes-le-sommet-de-la-pente-du-d√©ploiement",
    "title": "7¬† Int√©gration continue avec Python",
    "section": "11.3 Kubernetes: le sommet de la pente du d√©ploiement",
    "text": "11.3 Kubernetes: le sommet de la pente du d√©ploiement\nKubernetes est une technologie qui pousse la logique\nde la conteneurisation √† son paroxysme.\nIl s‚Äôagit d‚Äôun syst√®me open-source, d√©velopp√©\npar Google, permettant\nd‚Äôautomatiser le d√©ploiement, la mise √† l‚Äô√©chelle\net la gestion d‚Äôapplications conteneuris√©es.\nGr√¢ce √† Kubernetes, une application, par exemple\nun site web proposant de la r√©activit√©,\npeut √™tre mise √† disposition et reporter les calculs,\nlorsqu‚Äôils sont n√©cessaires, sur\nun serveur. L‚Äôutilisation de Kubernetes dans\nun projet de data-science permet ainsi\nd‚Äôanticiper √† la fois l‚Äôinterface d‚Äôune application\nvalorisant un projet mais aussi le fonctionnement\ndu back-office, par exemple en testant la capacit√©\nde charge de cette application. Une introduction\n√† Kubernetes orient√© donn√©e peut √™tre trouv√©e dans\nle cours d√©di√© √† la mise en production\nque nous donnons avec Romain Avouac et dans ce\npost de blog tr√®s bien fait.\nDans les grandes organisations, o√π les r√¥les sont\nplus sp√©cialis√©s que dans les petites structures,\nce ne sont pas n√©cessairement les data-scientists\nqui devront ma√Ætriser Kubernetes mais plut√¥t\nles data-architect ou les data-engineer. N√©anmoins,\nles data-scientists devront √™tre capable de\ndialoguer avec eux et mettre en oeuvre une m√©thode\nde travail adapt√©e (celle-ci reposera en principe sur\nl‚Äôapproche CI/CD). Dans les petites structures, les\ndata-scientist peuvent √™tre en mesure\nde mettre en oeuvre le d√©ploiement en continu. En\nrevanche, il est plus rare, dans ces structures,\no√π les moyens humains de maintenance sont limit√©s,\nque les serveurs sur lesquels fonctionnent Kubernetes\nsoient d√©tenus en propres. En g√©n√©ral, ils sont lou√©s\ndans des services de paiement √† la demande de type\nAWS."
  },
  {
    "objectID": "content/course/modern-ds/s3/index.html",
    "href": "content/course/modern-ds/s3/index.html",
    "title": "8¬† Les nouveaux modes d‚Äôacc√®s aux donn√©es: le format parquet et les donn√©es sur le cloud",
    "section": "",
    "text": "9 Elements de contexte\nLe format CSV a rencontr√© un grand succ√®s par sa simplicit√©: il\nest lisible par un humain (un bloc-note suffit pour l‚Äôouvrir et\napercevoir les premi√®res lignes), sa nature plate lui permet\nde bien correspondre au concept de donn√©es tabul√©es sans hi√©rarchie\nqui peuvent √™tre rapidement valoris√©es, il est universel (il n‚Äôest\npas adh√©rent √† un logiciel). Cependant, le CSV pr√©sente\nplusieurs inconv√©nients qui justifient l‚Äô√©mergence d‚Äôun format\nconcurrent:\nNote\nLa plupart des logiciels d‚Äôanalyse de donn√©es proposent\nun format de fichier pour sauvegarder des bases de donn√©es. On\npeut citer le .pickle (Python), le .rda ou .RData (R),\nle .dta (Stata) ou le .sas7bdat (SAS). L‚Äôutilisation\nde ces formats est probl√©matique car cela revient √† se lier\nles mains pour l‚Äôanalyse ult√©rieure des donn√©es, surtout\nlorsqu‚Äôil s‚Äôagit d‚Äôun format propri√©taire (comme avec\nSAS ou Stata). Par exemple, Python ne\nsait pas nativement lire un .sas7bdat. Il existe des librairies\npour le faire (notamment Pandas) mais le format\n√©tant propri√©taire, les d√©veloppeurs de la librairie ont d√ª t√¢tonner et\non n‚Äôest ainsi jamais assur√© qu‚Äôil n‚Äôy ait pas d‚Äôalt√©ration de la donn√©e.\nMalgr√© tous les inconv√©nients du .csv list√©s plus haut, il pr√©sente\nl‚Äôimmense avantage, par rapport √† ces formats, de l‚Äôuniversalit√©.\nIl vaut ainsi mieux privil√©gier un .csv √† ces formats pour le stockage\nde la donn√©e. Ceci dit, comme vise √† le montrer ce chapitre, il vaut\nmieux privil√©gier le format parquet au CSV.\nPour r√©pondre √† ces limites du CSV, le format parquet,\nqui est un projet open-source Apache, a √©merg√©.\nLa premi√®re diff√©rence entre le format parquet et le CSV est\nque le premier repose sur un stockage orient√© colonne l√† o√π\nle second est orient√© ligne. Pour comprendre la diff√©rence, voici un\nexemple issu du blog d‚Äôupsolver:\nDans notre exemple pr√©c√©dent, cela donnera une information prenant\nla forme suivante (ignorez l‚Äô√©l√©ment pyarrow.Table, nous\nreviendrons dessus) :\npyarrow.Table\nnom : string\nprofession: string\n----\nnom : [[\"Ast√©rix \",\"Ob√©lix \",\"Assurancetourix \"]]\nprofession: [[\"\",\"Tailleur de menhir\",\"Barde\"]]\nPour reprendre l‚Äôexemple fil rouge :point_up:, il sera ainsi beaucoup plus\nfacile de r√©cup√©rer la deuxi√®me ligne de la colonne profession:\non ne consid√®re que le vecteur profession et on r√©cup√®re la deuxi√®me\nvaleur.\nLe requ√™tage d‚Äô√©chantillon de donn√©es ne n√©cessite donc pas l‚Äôimport de\nl‚Äôensemble des donn√©es. A cela s‚Äôajoute des fonctionnalit√©s suppl√©mentaires\ndes librairies d‚Äôimport de donn√©es parquet (par exemple pyarrow ou spark)\nqui vont faciliter des recherches complexes bas√©es, par exemple, sur des\nrequ√™tes de type SQL, ou permettant l‚Äôutilisation de donn√©es plus volumineuses que la RAM.\nLe format parquet pr√©sente d‚Äôautres avantages par rapport au\nCSV:\nQu‚Äôon lise un ou plusieurs fichiers, on finira avec le sch√©ma suivant:\nCes diff√©rents avantages expliquent le succ√®s du format parquet dans le monde du\nbig-data. Le paragraphe suivant, extrait du post d‚Äôupsolver d√©j√† cit√©,\nr√©sume bien l‚Äôint√©r√™t:\nCependant, Parquet ne devrait pas int√©resser que les producteurs ou utilisateurs de donn√©es big-data.\nC‚Äôest l‚Äôensemble\ndes producteurs de donn√©es qui b√©n√©ficient des fonctionalit√©s\nde Parquet.\nPour en savoir plus sur Arrow,\ndes √©l√©ments suppl√©mentaires sur Parquet sont disponibles sur ce tr√®s bon\npost de blog d‚Äôupsolver\net sur la page officielle du projet Parquet.\nSi les fichiers parquet sont une\nsolution avantageuse pour\nles data-scientists, ils ne r√©solvent\npas tous les inconv√©nients de\nl‚Äôapproche fichier.\nEn particulier, la question de la\nduplication des donn√©es pour la mise\n√† disposition s√©curis√©e des sources\nn‚Äôest pas r√©solue. Pour que\nl‚Äôutilisateur B n‚Äôalt√®re pas les\ndonn√©es de l‚Äôutilisateur A, il est n√©cessaire\nqu‚Äôils travaillent sur deux fichiers\ndiff√©rents, dont l‚Äôun peut √™tre une copie\nde l‚Äôautre.\nLa mise √† disposition de donn√©es dans\nles syst√®mes de stockage cloud est\nune r√©ponse √† ce probl√®me.\nLes data lake qui se sont d√©velopp√©s dans les\ninstitutions et entreprises utilisatrices de donn√©es\nLe principe d‚Äôun stockage cloud\nest le m√™me que celui d‚Äôune\nDropbox ou d‚Äôun Drive mais adapt√© √†\nl‚Äôanalyse de donn√©es. Un utilisateur de donn√©es\nacc√®de √† un fichier stock√© sur un serveur distant\ncomme s‚Äôil √©tait dans son file system local2.\nDonc, du point de vue de l‚Äôutilisateur Python,\nil n‚Äôy a pas de diff√©rence fondamentale. Cependant,\nles donn√©es ne sont pas heberg√©es dans un dossier\nlocal (par exemple Mes Documents/monsuperfichier)\nmais sur un serveur distant auquel l‚Äôutilisateur\nde Python acc√®de √† travers un √©change r√©seau.\nDans l‚Äôunivers du cloud, la hi√©rarchisation des donn√©es\ndans des dossiers et des fichiers bien rang√©s\nest d‚Äôailleurs moins\nimportante que dans le monde du file system local.\nLorsque vous essayez de retrouver un fichier dans\nvotre arborescence de fichiers, vous utilisez parfois\nla barre de recherche de votre explorateur de fichiers,\navec des r√©sultats mitig√©s3. Dans le monde du cloud,\nles fichiers sont parfois accumul√©s de mani√®re plus\nchaotique car les outils de recherche sont plus\nefficaces4.\nEn ce qui concerne la s√©curit√© des donn√©es,\nla gestion des droits de lecture et √©criture peut √™tre\nfine: on peut autoriser certains utilisateurs uniquement\n√† la lecture, d‚Äôautres peuvent avoir les droits\nd‚Äô√©criture pour modifier les donn√©es. Cela permet\nde concilier les avantages des bases de donn√©es (la s√©curisation\ndes donn√©es) avec ceux des fichiers."
  },
  {
    "objectID": "content/course/modern-ds/s3/index.html#principe-du-stockage-de-la-donn√©e",
    "href": "content/course/modern-ds/s3/index.html#principe-du-stockage-de-la-donn√©e",
    "title": "8¬† Les nouveaux modes d‚Äôacc√®s aux donn√©es: le format parquet et les donn√©es sur le cloud",
    "section": "9.1 Principe du stockage de la donn√©e",
    "text": "9.1 Principe du stockage de la donn√©e\nPour comprendre les apports du format Parquet, il est n√©cessaire\nde faire un d√©tour pour comprendre la mani√®re dont une information\nest stock√©e et accessible √† un langage de traitement de la donn√©e.\nIl existe deux approches dans le monde du stockage de la donn√©e.\nLa premi√®re est celle de la base de donn√©es relationnelle. La seconde est le\nprincipe du fichier.\nLa diff√©rence entre les deux est dans la mani√®re dont l‚Äôacc√®s aux\ndonn√©es est organis√©."
  },
  {
    "objectID": "content/course/modern-ds/s3/index.html#les-fichiers",
    "href": "content/course/modern-ds/s3/index.html#les-fichiers",
    "title": "8¬† Les nouveaux modes d‚Äôacc√®s aux donn√©es: le format parquet et les donn√©es sur le cloud",
    "section": "9.2 Les fichiers",
    "text": "9.2 Les fichiers\nDans un fichier, les donn√©es sont organis√©es selon un certain format et\nle logiciel de traitement de la donn√©e va aller chercher et structurer\nl‚Äôinformation en fonction de ce format. Par exemple, dans un fichier\n.csv, les diff√©rentes informations seront stock√©es au m√™me niveau\navec un caract√®re pour les s√©parer (la virgule , dans les .csv anglosaxons, le point virgule dans les .csv fran√ßais, la tabulation dans les .tsv). Le fichier suivant\nnom ; profession \nAst√©rix ; \nOb√©lix ; Tailleur de menhir ;\nAssurancetourix ; Barde\nsera ainsi organis√© naturellement sous forme tabul√©e par Python\n\n\n\n\n\n\n\n\n\nnom\nprofession\n\n\n\n\n0\nAst√©rix\n\n\n\n1\nOb√©lix\nTailleur de menhir\n\n\n2\nAssurancetourix\nBarde\n\n\n\n\n\n\n\nA propos des fichiers de ce type, on parle de fichiers plats car\nles enregistrements relatifs √† une observation sont stock√©s ensemble,\nsans hi√©rarchie.\nCertains formats de donn√©es vont permettre d‚Äôorganiser les informations\nde mani√®re diff√©rente. Par exemple, le format JSON va\nhi√©rarchiser diff√©remment la m√™me information [^1]:\n[\n  {\n    \"nom\": \"Ast√©rix\"\n  },\n  {\n    \"nom\": \"Ob√©lix\",\n    \"profession\": \"Tailleur de menhir\"\n  },\n  {\n    \"nom\": \"Assurancetourix\",\n    \"profession\": \"Barde\"\n  }\n]\n\n\n Hint \nLa diff√©rence entre le CSV et le format JSON va au-del√† d‚Äôun simple ‚Äúformattage‚Äù des donn√©es.\nPar sa nature non tabulaire, le format JSON permet des mises √† jour beaucoup plus facile de la donn√©e dans les entrep√¥ts de donn√©es.\nPar exemple, un site web qui collecte de nouvelles donn√©es n‚Äôaura pas √† mettre √† jour l‚Äôensemble de ses enregistrements ant√©rieurs\npour stocker la nouvelle donn√©e (par exemple pour indiquer que pour tel ou tel client cette donn√©e n‚Äôa pas √©t√© collect√©e)\nmais pourra la stocker dans\nun nouvel item. Ce sera √† l‚Äôoutil de requ√™te (Python ou un autre outil)\nde cr√©er une relation entre les enregistrements stock√©s √† des endroits\ndiff√©rents.\nCe type d‚Äôapproche flexible est l‚Äôun des fondements de l‚Äôapproche NoSQL,\nsur laquelle nous allons revenir, qui a permis l‚Äô√©mergence de technologies au coeur de l‚Äô√©cosyst√®me actuel du big-data comme Hadoop ou ElasticSearch.\n\n\nCette fois, quand on n‚Äôa pas d‚Äôinformation, on ne se retrouve pas avec nos deux s√©parateurs accol√©s (cf.¬†la ligne ‚ÄúAst√©rix‚Äù) mais l‚Äôinformation\nn‚Äôest tout simplement pas collect√©e.\n\n\n Note\nIl se peut tr√®s bien que l‚Äôinformation sur une observation soit diss√©min√©e\ndans plusieurs fichiers dont les formats diff√®rent.\nPar exemple, dans le domaine des donn√©es g√©ographiques,\nlorsqu‚Äôune donn√©e est disponible sous format de fichier(s), elle peut l‚Äô√™tre de deux mani√®res!\n\nSoit la donn√©e est stock√©e dans un seul fichier qui m√©lange contours g√©ographiques et valeurs attributaires\n(la valeur associ√©e √† cette observation g√©ographique, par exemple le taux d‚Äôabstention). Ce principe est celui du geojson.\nSoit la donn√©e est stock√©e dans plusieurs fichiers qui sont sp√©cialis√©s: un fichier va stocker les contours g√©ographiques,\nl‚Äôautre les donn√©es attributaires et d‚Äôautres fichiers des informations annexes (comme le syst√®me de projection). Ce principe est celui du shapefile.\nC‚Äôest alors le logiciel qui requ√™te\nles donn√©es (Python par exemple) qui saura o√π aller chercher l‚Äôinformation\ndans les diff√©rents fichiers et associer celle-ci de mani√®re coh√©rente.\n\n\n\nUn concept suppl√©mentaire dans le monde du fichier est celui du file system. Le file system est\nle syst√®me de localisation et de nommage des fichiers.\nPour simplifier, le file system est la mani√®re dont votre ordinateur saura\nretrouver, dans son syst√®me de stockage, les bits pr√©sents dans tel ou tel fichier\nappartenant √† tel ou tel dossier."
  },
  {
    "objectID": "content/course/modern-ds/s3/index.html#les-bases-de-donn√©es",
    "href": "content/course/modern-ds/s3/index.html#les-bases-de-donn√©es",
    "title": "8¬† Les nouveaux modes d‚Äôacc√®s aux donn√©es: le format parquet et les donn√©es sur le cloud",
    "section": "9.3 Les bases de donn√©es",
    "text": "9.3 Les bases de donn√©es\nLa logique des bases de donn√©es est diff√©rente. Elle est plus syst√©mique.\nUn syst√®me de gestion de base de donn√©es (Database Management System)\nest un logiciel qui g√®re √† la fois le stockage d‚Äôun ensemble de donn√©es reli√©e,\npermet de mettre √† jour celle-ci (ajout ou suppression d‚Äôinformations, modification\ndes caract√©ristiques d‚Äôune table‚Ä¶)\net qui g√®re √©galement\nles modalit√©s d‚Äôacc√®s √† la donn√©e (type de requ√™te, utilisateurs\nayant les droits en lecture ou en √©criture‚Ä¶).\nLa relation entre les entit√©s pr√©sentes dans une base de donn√©es\nprend g√©n√©ralement la forme d‚Äôun sch√©ma en √©toile. Une base va centraliser\nles informations disponibles qui seront ensuite d√©taill√©es dans des tables\nd√©di√©es.\n\nSource: La documentation Databricks sur le sch√©ma en √©toile\nLe logiciel associ√© √† la base de donn√©es fera ensuite le lien\nentre ces tables √† partir de requ√™tes SQL. L‚Äôun des logiciels les plus efficaces dans ce domaine\nest PostgreSQL. Python est tout √† fait\nutilisable pour passer une requ√™te SQL √† un gestionnaire de base de donn√©es.\nLes packages sqlalchemy et psycopg2\npeuvent servir √† utiliser PostgreSQL pour requ√™ter une\nbase de donn√©e ou la mettre √† jour.\nLa logique de la base de donn√©es est donc tr√®s diff√©rente de celle du fichier.\nCes derniers sont beaucoup plus l√©gers pour plusieurs raisons.\nD‚Äôabord, parce qu‚Äôils sont moins adh√©rents √†\nun logiciel gestionnaire. L√† o√π le fichier ne n√©cessite, pour la gestion,\nqu‚Äôun file system, install√© par d√©faut sur\ntout syst√®me d‚Äôexploitation, une base de donn√©es va n√©cessiter un\nlogiciel sp√©cialis√©. L‚Äôinconv√©nient de l‚Äôapproche fichier, sous sa forme\nstandard, est qu‚Äôelle\nne permet pas une gestion fine des droits d‚Äôacc√®s et am√®ne g√©n√©ralement √† une\nduplication de la donn√©e pour √©viter que la source initiale soit\nr√©-√©crite (involontairement ou de mani√®re intentionnelle par un utilisateur malveillant).\nR√©soudre ce probl√®me est l‚Äôune des\ninnovations des syst√®mes cloud, sur lesquelles nous reviendrons en √©voquant le\nsyst√®me S3.\nUn deuxi√®me inconv√©nient de l‚Äôapproche base de donn√©es par\nrapport √† l‚Äôapproche fichier, pour un utilisateur de Python,\nest que les premiers n√©cessitent l‚Äôinterm√©diation du logiciel de gestion\nde base de donn√©es l√† o√π, dans le second cas, on va se contenter d‚Äôune\nlibrairie, donc un syst√®me beaucoup plus l√©ger,\nqui sait comment transformer la donn√©e brute en DataFrame.\nPour ces raisons, entre autres, les bases de donn√©es sont donc moins √† la\nmode dans l‚Äô√©cosyst√®me r√©cent de la data-science que les fichiers."
  },
  {
    "objectID": "content/course/modern-ds/s3/index.html#lire-un-parquet-en-python-la-librairie-pyarrow",
    "href": "content/course/modern-ds/s3/index.html#lire-un-parquet-en-python-la-librairie-pyarrow",
    "title": "8¬† Les nouveaux modes d‚Äôacc√®s aux donn√©es: le format parquet et les donn√©es sur le cloud",
    "section": "10.1 Lire un parquet en Python: la librairie pyarrow",
    "text": "10.1 Lire un parquet en Python: la librairie pyarrow\nLa librairie pyarrow permet la lecture et l‚Äô√©criture\nde fichiers parquet avec Python1. Elle repose\nsur un type particulier de dataframe, le pyarrow.Table\nqui peut √™tre utilis√© en substitut ou en compl√©ment\ndu DataFrame\nde pandas. Il est recommand√© de r√©guli√®rement\nconsulter la documentation officielle de pyarrow\nconcernant la lecture et √©criture de fichiers et celle relative\naux manipulations de donn√©es.\nPour illustrer les fonctionalit√©s de pyarrow,\nrepartons de notre CSV initial que nous allons\nenrichir d‚Äôune nouvelle variable num√©rique\net que nous\nallons\nconvertir en objet pyarrow avant de l‚Äô√©crire au format parquet:\n\nimport pandas as pd\nfrom io import StringIO \nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\ns = \"\"\"\nnom;cheveux;profession\nAst√©rix;blond;\nOb√©lix;roux;Tailleur de menhir\nAssurancetourix;blond;Barde\n\"\"\"\n\nsource = StringIO(s)\n\ndf = pd.read_csv(source, sep = \";\", index_col=False)\ndf[\"taille\"] = [155, 190, 175]\ntable = pa.Table.from_pandas(df)\n\ntable\n\npq.write_table(table, 'example.parquet')\n\n\n\n Hint \nL‚Äôutilisation des noms pa pour pyarrow et pq pour\npyarrow.parquet est une convention communautaire\nqu‚Äôil est recommand√© de suivre.\n\n\nPour importer et traiter ces donn√©es, on peut conserver\nles donn√©es sous le format pyarrow.Table\nou transformer en pandas.DataFrame. La deuxi√®me\noption est plus lente mais pr√©sente l‚Äôavantage\nde permettre ensuite d‚Äôappliquer toutes les\nmanipulations offertes par l‚Äô√©cosyst√®me\npandas qui est g√©n√©ralement mieux connu que\ncelui d‚ÄôArrow.\nSupposons qu‚Äôon ne s‚Äôint√©resse qu‚Äô√† la taille et √† la couleur\nde cheveux de nos gaulois.\nIl n‚Äôest pas n√©cessaire d‚Äôimporter l‚Äôensemble de la base, cela\nferait perdre du temps pour rien. On appelle\ncette approche le column pruning qui consiste √†\nne parcourir, dans le fichier, que les colonnes qui nous\nint√©ressent. Du fait du stockage orient√© colonne du parquet,\nil suffit de ne consid√©rer que les blocs qui nous\nint√©ressent (alors qu‚Äôavec un CSV il faudrait scanner tout\nle fichier avant de pouvoir √©liminer certaines colonnes).\nCe principe du column pruning se mat√©rialise avec\nl‚Äôargument columns dans parquet.\nEnsuite, avec pyarrow, on pourra utiliser pyarrow.compute pour\neffectuer des op√©rations directement sur une table\nArrow :\n\nimport pyarrow.compute as pc\n\ntable = pq.read_table('example.parquet', columns=['taille', 'cheveux'])\n\ntable.group_by(\"cheveux\").aggregate([(\"taille\", \"mean\")])\n\nLa mani√®re √©quivalente de proc√©der en passant\npar l‚Äôinterm√©diaire de pandas est\n\ntable = pq.read_table('example.parquet', columns=['taille', 'cheveux'])\n\ntable.to_pandas().groupby(\"cheveux\")[\"taille\"].mean()\n\ncheveux\nblond    165.0\nroux     190.0\nName: taille, dtype: float64\n\n\nIci, comme les donn√©es sont peu volumineuses, deux des\navantages du parquet par rapport\nau CSV (donn√©es moins\nvolumineuses et vitesse de l‚Äôimport)\nne s‚Äôappliquent pas vraiment.\n\n\n Note\nUn autre principe d‚Äôoptimisation de la performance qui est\nau coeur de la librairie Arrow est le filter pushdown\n(ou predicate pushdown).\nQuand on ex√©cute un filtre de s√©lection de ligne\njuste apr√®s avoir charg√© un jeu de donn√©es,\nArrow va essayer de le mettre en oeuvre lors de l‚Äô√©tape de lecture\net non apr√®s. Autrement dit, Arrow va modifier le plan\nd‚Äôex√©cution pour pousser le filtre en amont de la s√©quence d‚Äôex√©cution\nafin de ne pas essayer de lire les lignes inutiles."
  },
  {
    "objectID": "content/course/modern-ds/s3/index.html#quest-ce-que-le-syst√®me-de-stockage-s3",
    "href": "content/course/modern-ds/s3/index.html#quest-ce-que-le-syst√®me-de-stockage-s3",
    "title": "8¬† Les nouveaux modes d‚Äôacc√®s aux donn√©es: le format parquet et les donn√©es sur le cloud",
    "section": "12.1 Qu‚Äôest-ce que le syst√®me de stockage S3 ?",
    "text": "12.1 Qu‚Äôest-ce que le syst√®me de stockage S3 ?\nDans les entreprises et administrations,\nun nombre croissant de donn√©es sont\ndisponibles depuis un syst√®me de stockage\nnomm√© S3.\nLe syst√®me S3 (Simple Storage System) est un syst√®me de stockage d√©velopp√©\npar Amazon et qui est maintenant devenu une r√©f√©rence pour le stockage en ligne.\nIl s‚Äôagit d‚Äôune architecture √† la fois\ns√©curis√©e (donn√©es crypt√©es, acc√®s restreints) et performante.\nLe concept central du syst√®me S3 est le bucket.\nUn bucket est un espace (priv√© ou partag√©) o√π on peut stocker une\narborescence de fichiers. Pour acc√©der aux fichiers figurant\ndans un bucket priv√©, il faut des jetons d‚Äôacc√®s (l‚Äô√©quivalent d‚Äôun mot de passe)\nreconnus par le serveur de stockage. On peut alors lire et √©crire dans le bucket.\n\n\n Note\nLes exemples suivants seront r√©plicables pour les utilisateurs de la plateforme\nSSP-cloud\n\nIls peuvent √©galement l‚Äô√™tre pour des utilisateurs ayant un\nacc√®s √† AWS, il suffit de changer l‚ÄôURL du endpoint\npr√©sent√© ci-dessous."
  },
  {
    "objectID": "content/course/modern-ds/s3/index.html#comment-faire-avec-python",
    "href": "content/course/modern-ds/s3/index.html#comment-faire-avec-python",
    "title": "8¬† Les nouveaux modes d‚Äôacc√®s aux donn√©es: le format parquet et les donn√©es sur le cloud",
    "section": "12.2 Comment faire avec Python ?",
    "text": "12.2 Comment faire avec Python ?\n\n12.2.1 Les librairies principales\nL‚Äôinteraction entre ce syst√®me distant de fichiers et une session locale de Python\nest possible gr√¢ce √† des API. Les deux principales librairies sont les suivantes:\n\nboto3, une librairie cr√©√©e et maintenue par Amazon ;\ns3fs, une librairie qui permet d‚Äôinteragir avec les fichiers stock√©s √† l‚Äôinstar d‚Äôun filesystem classique.\n\nLa librairie pyarrow que nous avons d√©j√† pr√©sent√© permet √©galement\nde traiter des donn√©es stock√©es sur le cloud comme si elles\n√©taient sur le serveur local. C‚Äôest extr√™mement pratique\net permet de fiabiliser la lecture ou l‚Äô√©criture de fichiers\ndans une architecture cloud.\nUn exemple, assez court, est disponible\ndans la documentation officielle\nIl existe √©galement d‚Äôautres librairies permettant de g√©rer\ndes pipelines de donn√©es (chapitre √† venir) de mani√®re\nquasi indiff√©rente entre une architecture locale et une architecture\ncloud. Parmi celles-ci, nous pr√©senterons quelques exemples\navec snakemake.\nEn arri√®re-plan, snakemake\nva utiliser boto3 pour communiquer avec le syst√®me\nde stockage.\nEnfin, selon le m√™me principe du comme si les donn√©es\n√©taient en local, il existe l‚Äôoutil en ligne de commande\nmc (Minio Client) qui permet de g√©rer par des lignes\nde commande Linux les d√©p√¥ts distants comme s‚Äôils √©taient\nlocaux.\nToutes ces librairies offrent la possibilit√© de se connecter depuis Python,\n√† un d√©p√¥t de fichiers distant, de lister les fichiers disponibles dans un\nbucket, d‚Äôen t√©l√©charger un ou plusieurs ou de faire de l‚Äôupload\nNous allons pr√©senter quelques unes des op√©rations les plus fr√©quentes,\nen mode cheatsheet."
  },
  {
    "objectID": "content/course/modern-ds/s3/index.html#connexion-√†-un-bucket",
    "href": "content/course/modern-ds/s3/index.html#connexion-√†-un-bucket",
    "title": "8¬† Les nouveaux modes d‚Äôacc√®s aux donn√©es: le format parquet et les donn√©es sur le cloud",
    "section": "12.3 Connexion √† un bucket",
    "text": "12.3 Connexion √† un bucket\nPar la suite, on va utiliser des alias pour les trois valeurs suivantes, qui servent\n√† s‚Äôauthentifier.\nkey_id = 'MY_KEY_ID'\naccess_key = 'MY_ACCESS_KEY'\ntoken = \"MY_TOKEN\"\nCes valeurs peuvent √™tre √©galement disponibles dans\nles variables d‚Äôenvironnement de Python. Comme il s‚Äôagit d‚Äôune information\nd‚Äôauthentification personnelle, il ne faut pas stocker les vraies valeurs de ces\nvariables dans un projet, sous peine de partager des traits d‚Äôidentit√© sans le\nvouloir lors d‚Äôun partage de code.\n\nboto3 üëá\nAvec boto3, on cr√©√© d‚Äôabord un client puis on ex√©cute des requ√™tes dessus.\nPour initialiser un client, il suffit, en supposant que l‚Äôurl du d√©p√¥t S3 est\n\"https://minio.lab.sspcloud.fr\", de faire:\nimport boto3\ns3 = boto3.client(\"s3\",endpoint_url = \"https://minio.lab.sspcloud.fr\")\n\n\n\nS3FS üëá\nLa logique est identique avec s3fs.\nSi on a des jetons d‚Äôacc√®s √† jour et dans les variables d‚Äôenvironnement\nad√©quates:\nimport s3fs\nfs = s3fs.S3FileSystem(\n  client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})\n\n\n\nArrow üëá\nLa logique d‚ÄôArrow est proche de celle de s3fs. Seuls les noms\nd‚Äôarguments changent\nSi on a des jetons d‚Äôacc√®s √† jour et dans les variables d‚Äôenvironnement\nad√©quates:\nfrom pyarrow import fs\ns3 = fs.S3FileSystem(endpoint_override=\"http://\"+\"minio.lab.sspcloud.fr\")\n\n\n\nSnakemake üëá\nLa logique de Snakemake est, quant √† elle,\nplus proche de celle de boto3. Seuls les noms\nd‚Äôarguments changent\nSi on a des jetons d‚Äôacc√®s √† jour et dans les variables d‚Äôenvironnement\nad√©quates:\nfrom snakemake.remote.S3 import RemoteProvider as S3RemoteProvider\nS3 = S3RemoteProvider(host = \"https://\" + os.getenv('AWS_S3_ENDPOINT'))\n\n\nIl se peut que la connexion √† ce stade soit refus√©e (HTTP error 403).\nCela peut provenir\nd‚Äôune erreur dans l‚ÄôURL utilis√©. Cependant, cela refl√®te plus g√©n√©ralement\ndes param√®tres d‚Äôauthentification erron√©s.\n\nboto3 üëá\nLes param√®tres d‚Äôauthentification sont des arguments suppl√©mentaires:\nimport boto3\ns3 = boto3.client(\"s3\",endpoint_url = \"https://minio.lab.sspcloud.fr\",\n                  aws_access_key_id=key_id, \n                  aws_secret_access_key=access_key, \n                  aws_session_token = token)\n\n\n\nS3FS üëá\nLa logique est la m√™me, seuls les noms d‚Äôarguments diff√®rent\nimport s3fs\nfs = s3fs.S3FileSystem(\n  client_kwargs={'endpoint_url': 'https://'+'minio.lab.sspcloud.fr'},\n  key = key_id, secret = access_key,\n  token = token)\n\n\n\nArrow üëá\nTout est en argument cette fois:\nfrom pyarrow import fs\n\ns3 = fs.S3FileSystem(\n    access_key = key_id,\n    secret_key = access_key,\n    session_token = token,\n    endpoint_override = 'https://'+'minio.lab.sspcloud.fr',\n    scheme = \"https\"\n    )\n\n\n\nSnakemake üëá\nLa logique est la m√™me, seuls les noms d‚Äôarguments diff√®rent\nfrom snakemake.remote.S3 import RemoteProvider as S3RemoteProvider\nS3 = S3RemoteProvider(host = \"https://\" + os.getenv('AWS_S3_ENDPOINT'), access_key_id=key_id, secret_access_key=access_key)\n\n\n\n\n Note\nDans le SSP-cloud,\nlorsque l‚Äôinitialisation du service Jupyter du SSP-cloud est r√©cente\n(moins de 12 heures), il est possible d‚Äôutiliser\nautomatiquement les jetons stock√©s automatiquement √† la cr√©ation du d√©p√¥t.\nSi on d√©sire acc√©der aux donn√©es du SSP-cloud depuis une session python du\ndatalab (service VSCode, Jupyter‚Ä¶),\nil faut remplacer l‚Äôurl par http://minio.lab.sspcloud.fr"
  },
  {
    "objectID": "content/course/modern-ds/s3/index.html#lister-les-fichiers",
    "href": "content/course/modern-ds/s3/index.html#lister-les-fichiers",
    "title": "8¬† Les nouveaux modes d‚Äôacc√®s aux donn√©es: le format parquet et les donn√©es sur le cloud",
    "section": "12.4 Lister les fichiers",
    "text": "12.4 Lister les fichiers\nS‚Äôil n‚Äôy a pas d‚Äôerreur √† ce stade, c‚Äôest que la connexion est bien effective.\nPour le v√©rifier, on peut essayer de faire la liste des fichiers disponibles\ndans un bucket auquel on d√©sire acc√©der.\nPar exemple, on peut vouloir\ntester l‚Äôacc√®s aux bases FILOSOFI (donn√©es de revenu localis√©es disponibles\nsur https://www.insee.fr) au sein du bucket donnees-insee.\n\nboto3 üëá\nPour cela,\nla m√©thode list_objects offre toutes les options n√©cessaires:\nimport boto3\ns3 = boto3.client(\"s3\",endpoint_url = \"https://minio.lab.sspcloud.fr\")\nfor key in s3.list_objects(Bucket='donnees-insee', Prefix='diffusion/FILOSOFI')['Contents']:\n    print(key['Key'])\n\n\n\nS3FS üëá\nPour lister les fichiers, c‚Äôest la m√©thode ls (celle-ci ne liste pas par\nd√©faut les fichiers de mani√®re r√©cursive comme boto3):\nimport s3fs\nfs = s3fs.S3FileSystem(\n  client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})\nfs.ls(\"donnees-insee/diffusion\")\n\n\n\nArrow üëá\nfrom pyarrow import fs\ns3 = fs.S3FileSystem(endpoint_override='https://'+'minio.lab.sspcloud.fr')\ns3.get_file_info(fs.FileSelector('donnees-insee/diffusion', recursive=True))\n\n\n\nmc üëá\nmc ls -r"
  },
  {
    "objectID": "content/course/modern-ds/s3/index.html#t√©l√©charger-un-fichier-depuis-s3-pour-lenregistrer-en-local",
    "href": "content/course/modern-ds/s3/index.html#t√©l√©charger-un-fichier-depuis-s3-pour-lenregistrer-en-local",
    "title": "8¬† Les nouveaux modes d‚Äôacc√®s aux donn√©es: le format parquet et les donn√©es sur le cloud",
    "section": "12.5 T√©l√©charger un fichier depuis S3 pour l‚Äôenregistrer en local",
    "text": "12.5 T√©l√©charger un fichier depuis S3 pour l‚Äôenregistrer en local\nCette m√©thode n‚Äôest en g√©n√©ral pas recommand√©e car, comme on va le voir\npar la suite, il est possible de lire √† la vol√©e des fichiers. Cependant,\nt√©l√©charger un fichier depuis le cloud pour l‚Äô√©crire sur le disque\nlocal peut parfois √™tre utile (par exemple, lorsqu‚Äôil est n√©cessaire\nde d√©zipper un fichier).\n\nboto3 üëá\nOn utilise cette fois la m√©thode download_file\nimport boto3\ns3 = boto3.client(\"s3\",endpoint_url = \"https://minio.lab.sspcloud.fr\")\ns3.download_file('donnees-insee', \"diffusion/FILOSOFI/2014/FILOSOFI_COM.csv\", 'data.csv')\n\n\n\nS3FS üëá\nimport s3fs\nfs = s3fs.S3FileSystem(\n  client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})\nfs.download('donnees-insee/diffusion/FILOSOFI/2014/FILOSOFI_COM.csv','test.csv')\n\n\n\nSnakemake üëá\nfrom snakemake.remote.S3 import RemoteProvider as S3RemoteProvider\nS3 = S3RemoteProvider(host = \"https://\" + os.getenv('AWS_S3_ENDPOINT'))\nbucket = \"mon-bucket\"\n\nrule ma_super_regle_s3:\n    input:\n        fichier = S3.remote(f'{bucket}/moninput.csv')\n    output:\n        fichier='mon_dossier_local/monoutput.csv'\n    run:\n        shell(\"cp {input[0]} {output[0]}\")\n\n\n\nmc üëá\nmc cp \"donnees-insee/FILOSOFI/2014/FILOSOFI_COM.csv\" 'data.csv'"
  },
  {
    "objectID": "content/course/modern-ds/s3/index.html#lire-un-fichier-directement",
    "href": "content/course/modern-ds/s3/index.html#lire-un-fichier-directement",
    "title": "8¬† Les nouveaux modes d‚Äôacc√®s aux donn√©es: le format parquet et les donn√©es sur le cloud",
    "section": "12.6 Lire un fichier directement",
    "text": "12.6 Lire un fichier directement\nLa m√©thode pr√©c√©dente n‚Äôest pas optimale. En effet, l‚Äôun des int√©r√™ts des API\nest qu‚Äôon peut traiter un fichier sur S3 comme s‚Äôil s‚Äôagissait d‚Äôun fichier\nsur son PC. Cela est d‚Äôailleurs une mani√®re plus s√©curis√©e de proc√©der puisqu‚Äôon\nlit les donn√©es √† la vol√©e, sans les √©crire dans un filesystem local.\n\nboto3 üëá\nimport boto3\ns3 = boto3.client(\"s3\",endpoint_url = \"https://minio.lab.sspcloud.fr\")\nobj = s3.get_object(Bucket='donnees-insee', Key=\"diffusion/FILOSOFI/2014/FILOSOFI_COM.csv\")\ndf = pd.read_csv(obj['Body'], sep = \";\")\ndf.head(2)\n\n\n\nS3FS üëá\nLe code suivant devrait permettre d‚Äôeffectuer la m√™me op√©ration avec s3fs\nimport pandas as pd\nimport s3fs\nfs = s3fs.S3FileSystem(\n  client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})\ndf = pd.read_csv(fs.open('{}/{}'.format('donnees-insee', \"diffusion/FILOSOFI/2014/FILOSOFI_COM.csv\"),\n                         mode='rb'), sep = \";\"\n                 )\n\ndf.head(2)\n\n\n\nSnakemake üëá\nfrom snakemake.remote.S3 import RemoteProvider as S3RemoteProvider\nS3 = S3RemoteProvider(host = \"https://\" + os.getenv('AWS_S3_ENDPOINT'))\nbucket = \"mon-bucket\"\n\nrule ma_super_regle_s3:\n    input:\n        fichier = S3.remote(f'{bucket}/moninput.csv')\n    run:\n        import pandas as pd\n        df = pd.read_csv(input.fichier)\n        # PLUS D'OPERATIONS\n\n\n\nArrow üëá\nArrow est une librairie qui permet de lire des CSV.\nIl est n√©anmoins\nbeaucoup plus pratique d‚Äôutiliser le format parquet avec arrow.\nDans un premier temps, on configure le filesystem avec les\nfonctionalit√©s d‚ÄôArrow (cf.¬†pr√©c√©demment).\n\nfrom pyarrow import fs\n\ns3 = fs.S3FileSystem(endpoint_override='http://'+'minio.lab.sspcloud.fr')\n\nPour lire un csv, on fera:\nfrom pyarrow import fs\nfrom pyarrow import csv\n\ns3 = fs.S3FileSystem(endpoint_override='https://'+'minio.lab.sspcloud.fr')\n\nwith s3.open_input_file(\"donnees-insee/diffusion/FILOSOFI/2014/FILOSOFI_COM.csv\") as file:\n    df = csv.read_csv(file, parse_options=csv.ParseOptions(delimiter=\";\")).to_pandas()\nPour un fichier au format parquet, la d√©marche est plus simple gr√¢ce √† l‚Äôargument\nfilesystem dans pyarrow.parquet.ParquetDataset :\nimport pyarrow.parquet as pq\n\n#bucket = \"\"\n#parquet_file=\"\"\ndf = pq.ParquetDataset(f'{bucket}/{parquet_file}', filesystem=s3).read_pandas().to_pandas()"
  },
  {
    "objectID": "content/course/modern-ds/s3/index.html#uploader-un-fichier",
    "href": "content/course/modern-ds/s3/index.html#uploader-un-fichier",
    "title": "8¬† Les nouveaux modes d‚Äôacc√®s aux donn√©es: le format parquet et les donn√©es sur le cloud",
    "section": "12.7 Uploader un fichier",
    "text": "12.7 Uploader un fichier\n\nboto3 üëá\ns3.upload_file(file_name, bucket, object_name)\n\n\n\nS3FS üëá\nfs.put(filepath, f\"{bucket}/{object_name}\", recursive=True)\n\n\n\nArrow üëá\nSupposons que df soit un pd.DataFrame\nDans un syst√®me local, on convertirait\nen table Arrow puis on √©crirait en parquet\n(voir la documentation officielle).\nQuand on est sur un syst√®me S3, il s‚Äôagit seulement d‚Äôajouter\nnotre connexion √† S3 dans l‚Äôargument filesystem\n(voir la page sur ce sujet dans la documentation Arrow)\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\ntable = pa.Table.from_pandas(df)\npq.write_table(table, f\"{bucket}/{path}\", filesystem=s3)\n\n\n\nSnakemake üëá\nfrom snakemake.remote.S3 import RemoteProvider as S3RemoteProvider\nS3 = S3RemoteProvider(host = \"https://\" + os.getenv('AWS_S3_ENDPOINT'))\nbucket = \"mon-bucket\"\n\nrule ma_super_regle_s3:\n    input:\n        fichier='mon_dossier_local/moninput.csv'\n    output:\n        fichier=S3.remote(f'{bucket}/monoutput.csv')\n    run:\n        shell(\"cp output.fichier input.fichier\")\n\n\n\nmc üëá\nmc cp 'data.csv' \"MONBUCKET/monoutput.csv\""
  },
  {
    "objectID": "content/course/modern-ds/s3/index.html#pour-aller-plus-loin",
    "href": "content/course/modern-ds/s3/index.html#pour-aller-plus-loin",
    "title": "8¬† Les nouveaux modes d‚Äôacc√®s aux donn√©es: le format parquet et les donn√©es sur le cloud",
    "section": "12.8 Pour aller plus loin",
    "text": "12.8 Pour aller plus loin\n\nLa documentation sur MinIO du SSPCloud"
  },
  {
    "objectID": "content/course/modelisation/index.html#principes",
    "href": "content/course/modelisation/index.html#principes",
    "title": "9¬† Partie 3: mod√©liser",
    "section": "9.1 Principes",
    "text": "9.1 Principes\n\n9.1.1 Machine Learning ou Econom√©trie ?\nUn mod√®le statistique est une construction math√©matique qui formalise une loi\nayant g√©n√©r√© les donn√©es. La diff√©rence principale entre machine learning (ML) et √©conom√©trie\nest dans le degr√© de structure impos√© par le mod√©lisateur :\n\nEn ML,\nla structure impos√©e par le data scientist est minimale et ce sont plut√¥t\nles algorithmes qui, sur des crit√®res de performance statistique, vont\nd√©terminer une loi math√©matique qui correspond aux donn√©es.\nEn √©conom√©trie,\nles hypoth√®ses de structure des lois sont plus fortes (m√™me dans un cadre semi ou non-param√©trique) et sont plus souvent impos√©es\npar le mod√©lisateur.\n\nL‚Äôadoption du Machine Learning dans la litt√©rature √©conomique a √©t√© longue\ncar la structuration des donn√©es est souvent le\npendant empirique d‚Äôhypoth√®ses th√©oriques sur le comportement des acteurs ou des march√©s (Athey and Imbens 2019).\nPour caricaturer, l‚Äô√©conom√©trie s‚Äôattacherait √† comprendre la causalit√© de certaines variables sur une autre.\nCela implique que ce qui int√©resse l‚Äô√©conom√®tre\nest principalement de l‚Äôestimation des param√®tres (et l‚Äôincertitude\nsur l‚Äôestimation de ceux-ci) qui permettent de quantifier l‚Äôeffet d‚Äôune\nvariation d‚Äôune variable sur une autre.\nToujours pour caricaturer,\nle Machine Learning se focaliserait\nsur un simple objectif pr√©dictif en exploitant les relations de corr√©lations entre les variables.\nDans cette perspective, l‚Äôimportant n‚Äôest pas la causalit√© mais le fait qu‚Äôune variation\nde \\(x\\)% d‚Äôune variable permette d‚Äôanticiper un changement de \\(\\beta x\\) de la variable\nd‚Äôint√©r√™t ; peu importe la raison.\nCette approche est n√©anmoins caricaturale: la recherche est tr√®s dynamique\nsur la question de l‚Äôexplicabilit√© et de l‚Äôinterpr√©tabilit√©\ndes mod√®les de Machine Learning. Certaines approches sont reli√©es\n√† des notions th√©oriques\ncomme les valeurs de Shapley.\n\n\n9.1.2 Apprentissage supervis√© ou non supervis√© ?\nOn distingue g√©n√©ralement deux types de m√©thodes, selon qu‚Äôon dispose d‚Äôinformation, dans l‚Äô√©chantillon\nd‚Äôapprentissage, sur les valeurs cibles y (on utilisera parfois le terme label) :\n\napprentissage supervis√© : la valeur cible est connue et peut-√™tre utilis√©e pour √©valuer la qualit√© d‚Äôun mod√®le\n\nEx : mod√®les de pr√©diction du type r√©gression / classification : SVM, kNN, arbres de classification‚Ä¶\n\napprentissage non supervis√© : la valeur cible est inconnue et ce sont des crit√®res statistiques qui vont amener\n√† s√©lectionner la structure de donn√©es la plus plausible.\n\nEx : mod√®les de r√©duction de dimension ou de clustering (PCA, kmeans‚Ä¶)"
  },
  {
    "objectID": "content/course/modelisation/index.html#panorama-dun-√©co-syst√®me-vaste",
    "href": "content/course/modelisation/index.html#panorama-dun-√©co-syst√®me-vaste",
    "title": "9¬† Partie 3: mod√©liser",
    "section": "9.2 Panorama d‚Äôun √©co-syst√®me vaste",
    "text": "9.2 Panorama d‚Äôun √©co-syst√®me vaste\nGr√¢ce aux principaux packages de Machine Learning (scikit), Deep Learning (keras, pytorch, TensorFlow‚Ä¶) et √©conom√©trie (statsmodels), la mod√©lisation est extr√™mement simplifi√©e. Cela ne doit pas faire oublier l‚Äôimportance de la structuration et de la pr√©paration des donn√©es. Souvent, l‚Äô√©tape la plus cruciale est le choix du mod√®le le plus adapt√© √† la structure des donn√©es.\nL‚Äôaide-m√©moire suivante, issue de l‚Äôaide de scikit-learn, concernant les mod√®les de Machine Learning peut d√©j√† donner de premiers enseignements sur les diff√©rentes familles de mod√®les:"
  },
  {
    "objectID": "content/course/modelisation/index.html#donn√©es",
    "href": "content/course/modelisation/index.html#donn√©es",
    "title": "9¬† Partie 3: mod√©liser",
    "section": "9.3 Donn√©es",
    "text": "9.3 Donn√©es\nLa plupart des exemples de cette partie s‚Äôappuient sur les r√©sultats des\n√©lections US 2020 au niveau comt√©s. Plusieurs bases sont utilis√©es pour\ncela:\n\nLes donn√©es √©lectorales sont une reconstruction √† partir des donn√©es du MIT election lab\npropos√©es sur Github par (tonmcg?)\nou directement disponibles sur le site du MIT Election Lab\nLes donn√©es socio√©conomiques (population, donn√©es de revenu et de pauvret√©,\ntaux de ch√¥mage, variables d‚Äô√©ducation) proviennent de l‚ÄôUSDA (source)\nLe shapefile vient des donn√©es du Census Bureau. Le fichier peut\n√™tre t√©l√©charg√© directement depuis cet url:\nhttps://www2.census.gov/geo/tiger/GENZ2019/shp/cb_2019_us_county_20m.zip\n\nLe code pour construire une base unique √† partir de ces sources diverses\nest disponible ci-dessous :\n\n\nimport urllib\nimport urllib.request\nimport os\nimport zipfile\nfrom urllib.request import Request, urlopen\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\n\ndef download_url(url, save_path):\n    with urllib.request.urlopen(url) as dl_file:\n        with open(save_path, 'wb') as out_file:\n            out_file.write(dl_file.read())\n\n\ndef create_votes_dataframes():\n    \n  Path(\"data\").mkdir(parents=True, exist_ok=True)\n  \n  \n  download_url(\"https://www2.census.gov/geo/tiger/GENZ2019/shp/cb_2019_us_county_20m.zip\", \"data/shapefile\")\n  with zipfile.ZipFile(\"data/shapefile\", 'r') as zip_ref:\n      zip_ref.extractall(\"data/counties\")\n  \n  shp = gpd.read_file(\"data/counties/cb_2019_us_county_20m.shp\")\n  shp = shp[~shp[\"STATEFP\"].isin([\"02\", \"69\", \"66\", \"78\", \"60\", \"72\", \"15\"])]\n  shp\n  \n  df_election = pd.read_csv(\"https://raw.githubusercontent.com/tonmcg/US_County_Level_Election_Results_08-20/master/2020_US_County_Level_Presidential_Results.csv\")\n  df_election.head(2)\n  population = pd.read_excel(\"https://www.ers.usda.gov/webdocs/DataFiles/48747/PopulationEstimates.xls?v=290.4\", header = 2).rename(columns = {\"FIPStxt\": \"FIPS\"})\n  education = pd.read_excel(\"https://www.ers.usda.gov/webdocs/DataFiles/48747/Education.xls?v=290.4\", header = 4).rename(columns = {\"FIPS Code\": \"FIPS\", \"Area name\": \"Area_Name\"})\n  unemployment = pd.read_excel(\"https://www.ers.usda.gov/webdocs/DataFiles/48747/Unemployment.xls?v=290.4\", header = 4).rename(columns = {\"fips_txt\": \"FIPS\", \"area_name\": \"Area_Name\", \"Stabr\": \"State\"})\n  income = pd.read_excel(\"https://www.ers.usda.gov/webdocs/DataFiles/48747/PovertyEstimates.xls?v=290.4\", header = 4).rename(columns = {\"FIPStxt\": \"FIPS\", \"Stabr\": \"State\", \"Area_name\": \"Area_Name\"})\n  \n  \n  dfs = [df.set_index(['FIPS', 'State']) for df in [population, education, unemployment, income]]\n  data_county = pd.concat(dfs, axis=1)\n  df_election = df_election.merge(data_county.reset_index(), left_on = \"county_fips\", right_on = \"FIPS\")\n  df_election['county_fips'] = df_election['county_fips'].astype(str).str.lstrip('0')\n  shp['FIPS'] = shp['GEOID'].astype(str).str.lstrip('0')\n  votes = shp.merge(df_election, left_on = \"FIPS\", right_on = \"county_fips\")\n  \n  req = Request('https://dataverse.harvard.edu/api/access/datafile/3641280?gbrecs=false')\n  req.add_header('User-Agent', 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:77.0) Gecko/20100101 Firefox/77.0')\n  content = urlopen(req)\n  df_historical = pd.read_csv(content, sep = \"\\t\")\n  #df_historical = pd.read_csv('https://dataverse.harvard.edu/api/access/datafile/3641280?gbrecs=false', sep = \"\\t\")\n  \n  df_historical = df_historical.dropna(subset = [\"FIPS\"])\n  df_historical[\"FIPS\"] = df_historical[\"FIPS\"].astype(int)\n  df_historical['share'] = df_historical['candidatevotes']/df_historical['totalvotes']\n  df_historical = df_historical[[\"year\", \"FIPS\", \"party\", \"candidatevotes\", \"share\"]]\n  df_historical['party'] = df_historical['party'].fillna(\"other\")\n  \n  df_historical_wide = df_historical.pivot_table(index = \"FIPS\", values=['candidatevotes',\"share\"], columns = [\"year\",\"party\"])\n  df_historical_wide.columns = [\"_\".join(map(str, s)) for s in df_historical_wide.columns.values]\n  df_historical_wide = df_historical_wide.reset_index()\n  df_historical_wide['FIPS'] = df_historical_wide['FIPS'].astype(str).str.lstrip('0')\n  votes['FIPS'] = votes['GEOID'].astype(str).str.lstrip('0')\n  votes = votes.merge(df_historical_wide, on = \"FIPS\")\n  votes[\"winner\"] =  np.where(votes['votes_gop'] &gt; votes['votes_dem'], 'republican', 'democrats') \n\n  return votes"
  },
  {
    "objectID": "content/course/modelisation/index.html#contenu-de-la-partie",
    "href": "content/course/modelisation/index.html#contenu-de-la-partie",
    "title": "9¬† Partie 3: mod√©liser",
    "section": "9.4 Contenu de la partie",
    "text": "9.4 Contenu de la partie\nAutres champs:\n* maximum vraisemblance\n* stats bay√©siennes\n* semi et non param√©trique: m√©thodes noyaux, GAM"
  },
  {
    "objectID": "content/course/modelisation/index.html#r√©f√©rences",
    "href": "content/course/modelisation/index.html#r√©f√©rences",
    "title": "9¬† Partie 3: mod√©liser",
    "section": "9.5 R√©f√©rences",
    "text": "9.5 R√©f√©rences\n\n\nAthey, Susan, and Guido W Imbens. 2019. ‚ÄúMachine Learning Methods That Economists Should Know About.‚Äù Annual Review of Economics 11: 685‚Äì725."
  },
  {
    "objectID": "content/course/modelisation/2_SVM/index.html#la-m√©thode-des-svm-support-vector-machines",
    "href": "content/course/modelisation/2_SVM/index.html#la-m√©thode-des-svm-support-vector-machines",
    "title": "10¬† Classification: premier mod√®le avec les SVM",
    "section": "10.1 La m√©thode des SVM (Support Vector Machines)",
    "text": "10.1 La m√©thode des SVM (Support Vector Machines)\nL‚Äôune des m√©thodes de Machine Learning les plus utilis√©es en classification est les SVM.\nIl s‚Äôagit de trouver, dans un syst√®me de projection ad√©quat (noyau ou kernel),\nles param√®tres de l‚Äôhyperplan (en fait d‚Äôun hyperplan √† marges maximales)\ns√©parant les classes de donn√©es:\n\n\n\n Formalisation math√©matique\nOn peut, sans perdre de g√©n√©ralit√©, supposer que le probl√®me consiste √† supposer l‚Äôexistence d‚Äôune loi de probabilit√© \\(\\mathbb{P}(x,y)\\) (\\(\\mathbb{P} \\to \\{-1,1\\}\\)) qui est inconnue. Le probl√®me de discrimination\nvise √† construire un estimateur de la fonction de d√©cision id√©ale qui minimise la probabilit√© d‚Äôerreur, autrement dit \\(\\theta = \\arg\\min_\\Theta \\mathbb{P}(h_\\theta(X) \\neq y |x)\\)\nLes SVM les plus simples sont les SVM lin√©aires. Dans ce cas, on suppose qu‚Äôil existe un s√©parateur lin√©aire qui permet d‚Äôassocier chaque classe √† son signe:\n\\[\nh_\\theta(x) = \\text{signe}(f_\\theta(x)) ; \\text{ avec } f_\\theta(x) = \\theta^T x + b\n\\]\navec \\(\\theta \\in \\mathbb{R}^p\\) et \\(w \\in \\mathbb{R}\\).\n\nLorsque des observations sont lin√©airement s√©parables, il existe une infinit√© de fronti√®res de d√©cision lin√©aire s√©parant les deux classes. Le ‚Äúmeilleur‚Äù choix est de prendre la marge maximale permettant de s√©parer les donn√©es. La distance entre les deux marges est \\(\\frac{2}{||\\theta||}\\). Donc maximiser cette distance entre deux hyperplans revient √† minimiser \\(||\\theta||^2\\) sous la contrainte \\(y_i(\\theta^Tx_i + b) \\geq 1\\).\nDans le cas non lin√©airement s√©parable, la hinge loss \\(\\max\\big(0,y_i(\\theta^Tx_i + b)\\big)\\) permet de lin√©ariser la fonction de perte:\n\nce qui donne le programme d‚Äôoptimisation suivant:\n\\[\n\\frac{1}{n} \\sum_{i=1}^n \\max\\big(0,y_i(\\theta^Tx_i + b)\\big) + \\lambda ||\\theta||^2\n\\]\nLa g√©n√©ralisation au cas non lin√©aire implique d‚Äôintroduire des noyaux transformant l‚Äôespace de coordonn√©es des observations."
  },
  {
    "objectID": "content/course/modelisation/2_SVM/index.html#exercice",
    "href": "content/course/modelisation/2_SVM/index.html#exercice",
    "title": "10¬† Classification: premier mod√®le avec les SVM",
    "section": "10.2 Exercice",
    "text": "10.2 Exercice\n\n# packages utiles\nfrom sklearn import svm\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\n\n\n Exercice 1 : Premier algorithme de classification\n\nCr√©er une variable dummy appel√©e y dont la valeur vaut 1 quand les r√©publicains l‚Äôemportent.\nEn utilisant la fonction pr√™te √† l‚Äôemploi nomm√©e train_test_split de la librairie sklearn.model_selection,\ncr√©er des √©chantillons de test (20 % des observations) et d‚Äôestimation (80 %) avec comme features: 'Unemployment_rate_2019', 'Median_Household_Income_2019', 'Percent of adults with less than a high school diploma, 2015-19', \"Percent of adults with a bachelor's degree or higher, 2015-19\" et comme label la variable y.\n\n\nA column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel()\n\nNote : Pour √©viter ce warning √† chaque fois que vous estimez votre mod√®le, vous pouvez utiliser DataFrame[['y']].values.ravel() plut√¥t que DataFrame[['y']] lorsque vous constituez vos √©chantillons.\n\nEntra√Æner un classifieur SVM avec comme param√®tre de r√©gularisation C = 1. Regarder les mesures de performance suivante : accuracy, f1, recall et precision.\nV√©rifier la matrice de confusion : vous devriez voir que malgr√© des scores en apparence pas si mauvais, il y a un probl√®me notable.\nRefaire les questions pr√©c√©dentes avec des variables normalis√©es. Le r√©sultat est-il diff√©rent ?\nChanger de variables x. Utiliser uniquement le r√©sultat pass√© du vote d√©mocrate et le revenu (votes_gop et Median_Household_Income_2019). Regarder les r√©sultats, notamment la matrice de confusion.\n[OPTIONNEL] Faire une 5-fold validation crois√©e pour d√©terminer le param√®tre C id√©al.\n\n\n\nA l‚Äôissue de la question 3,\nle classifieur avec C = 1\ndevrait avoir les performances suivantes:\n\n\n\n\nScore\n\n\n\n\nAccuracy\n0.868167\n\n\nRecall\n0.878307\n\n\nPrecision\n0.97456\n\n\nF1\n0.923933\n\n\n\nLa matrice de confusion associ√©e\nprend cette forme:\n\n\nA l‚Äôissue de la question 6,\nle nouveau classifieur avec devrait avoir les performances suivantes :\n\n\n\n\nScore\n\n\n\n\nAccuracy\n0.868167\n\n\nRecall\n0.878307\n\n\nPrecision\n0.97456\n\n\nF1\n0.923933\n\n\n\nEt la matrice de confusion associ√©e:"
  },
  {
    "objectID": "content/course/modelisation/1_modelevaluation/index.html",
    "href": "content/course/modelisation/1_modelevaluation/index.html",
    "title": "11¬† Evaluer la qualit√© d‚Äôun mod√®le",
    "section": "",
    "text": "12 D√©couper l‚Äô√©chantillon\nLe chapitre pr√©c√©dent pr√©sentait le pipeline simple ci-dessous\npour introduire √† la notion d‚Äôentra√Ænement d‚Äôun mod√®le:\nCe pipeline fait abstraction d‚Äôhypoth√®ses exog√®nes √† l‚Äôestimation\nmais qui sont √† faire sur des param√®tres\ncar elles affectent la performance de la pr√©diction.\nPar exemple, de nombreux mod√®les proposent une p√©nalisation des mod√®les\nnon parcimonieux pour √©viter le sur-apprentissage. Le choix de la p√©nalisation\nid√©ale d√©pend de la structure des donn√©es et n‚Äôest jamais connue, ex-ante\npar le mod√©lisateur. Faut-il p√©naliser fortement ou non le mod√®le ? En l‚Äôabsence\nd‚Äôargument th√©orique, on aura tendance √† tester plusieurs param√®tres de\np√©nalisation et choisir celui qui permet la meilleure pr√©diction.\nLa notion de validation crois√©e permettra de g√©n√©raliser cette approche. Ces param√®tres\nqui affectent la pr√©diction seront pas la suite appel√©s des\nhyperparam√®tres. Comme nous allons le voir, nous allons aboutir √† un\nraffinement de l‚Äôapproche pour obtenir un pipeline ayant plut√¥t cet aspect:\nLe but du Machine Learning est de calibrer l‚Äôalgorithme sur des exemples\nconnus (donn√©es labellis√©es) afin de g√©n√©raliser √† des\nexemples nouveaux (√©ventuellement non labellis√©s).\nOn vise donc de bonnes qualit√©s\npr√©dictives et non un ajustement parfait\naux donn√©es historiques.\nIl existe un arbitrage biais-variance dans la qualit√© d‚Äôestimation[^1]. Soit \\(h(X,\\theta)\\) un mod√®le statistique. On\npeut d√©composer l‚Äôerreur d‚Äôestimation en deux parties :\n\\[\n\\mathbb{E}\\bigg[(y - h(\\theta,X))^2 \\bigg] = \\underbrace{ \\bigg( y - \\mathbb{E}(h_\\theta(X)) \\bigg)^2}_{\\text{biais}^2} + \\underbrace{\\mathbb{V}\\big(h(\\theta,X)\\big)}_{\\text{variance}}\n\\]\nIl y a ainsi un compromis √† faire entre biais et variance. Un mod√®le peu parcimonieux, c‚Äôest-√†-dire proposant un grand nombre de param√®tres, va, en g√©n√©ral, avoir un faible biais mais une grande variance. En effet, le mod√®le va tendre √† se souvenir d‚Äôune combinaison de param√®tres √† partir d‚Äôun grand nombre d‚Äôexemples sans √™tre capable d‚Äôapprendre la r√®gle qui permette de structurer les donn√©es.\n[^1]! Cette formule permet de bien comprendre la th√©orie statistique asymptotique, notamment le th√©or√®me de Cramer-Rao. Dans la classe des estimateurs sans biais, c‚Äôest-√†-dire dont le premier terme est nul, trouver l‚Äôestimateur √† variance minimale revient √† trouver l‚Äôestimateur qui minimise \\(\\mathbb{E}\\bigg[(y - h_\\theta(X))^2 \\bigg]\\). C‚Äôest la d√©finition m√™me de la r√©gression, ce qui, quand on fait des hypoth√®ses suppl√©mentaires sur le mod√®le statistique, explique le th√©or√®me de Cramer-Rao.\nPar exemple, la ligne verte ci-dessous est trop d√©pendante des donn√©es et risque de produire une erreur plus importante que la ligne noire (qui moyennise plus) sur de nouvelles donn√©es.\nPour renforcer la validit√© externe d‚Äôun mod√®le, il est ainsi commun, en Machine Learning:\nPour d√©composer un mod√®le en jeu d‚Äôestimation et de test,\nla meilleure m√©thode est d‚Äôutiliser les fonctionnalit√©s de scikit de la mani√®re suivante :\nfrom sklearn.model_selection import train_test_split\nxTrain, xTest, yTrain, yTest = train_test_split(\n  x, y,\n  test_size = 0.2,\n  random_state = 0\n  )\nLa proportion d‚Äôobservations dans le jeu de test est contr√¥l√©e par l‚Äôargument test_size.\nLa proportion optimale n‚Äôexiste pas.\nLa r√®gle du pouce habituelle est d‚Äôassigner al√©atoirement 20 % des observations\ndans l‚Äô√©chantillon de test pour garder suffisamment d‚Äôobservations\ndans l‚Äô√©chantillon d‚Äôestimation.\nHint \nLorsqu‚Äôon travaille avec des s√©ries temporelles, l‚Äô√©chantillonnage al√©atoire des observations n‚Äôa pas vraiment de sens. Il vaut mieux tester la qualit√© de l‚Äôobservation sur des p√©riodes distingu√©es.\nNote\nAvec des donn√©es multi-niveaux,\ncomme c‚Äôest le cas de donn√©es g√©ographiques ou de donn√©es individuelles avec des variables de classe,\nil peut √™tre int√©ressant d‚Äôutiliser un √©chantillonnage stratifi√©.\nCela permet de garder une proportion √©quivalente de chaque groupe dans les deux jeux de donn√©es de test ou d‚Äôapprentissage.\nCe type d‚Äô√©chantillonnage stratifi√© est √©galement possible avec scikit.\nL‚Äôexercice sur les SVM illustre cette construction et la mani√®re\ndont elle facilite l‚Äô√©valuation de la qualit√© d‚Äôun mod√®le.\nJusqu‚Äô√† pr√©sent, nous avons pass√© sous silence la question du support de \\(y\\), c‚Äôest-√†-dire\nde l‚Äô√©tendue des valeurs de notre variable d‚Äôint√©r√™t.\nEn pratique, la distribution des \\(y\\)\nva n√©anmoins d√©terminer deux questions cruciales : la m√©thode et l‚Äôindicateur de performance.\nEn apprentissage supervis√©, on distingue en g√©n√©ral les probl√®mes de:\nLes deux approches ne sont pas sans lien. On peut par exemple voir le mod√®le √©conom√©trique de choix d‚Äôoffre de travail comme un probl√®me de classification (participation ou non au march√© du travail) ou de r√©gression (r√©gression sur un mod√®le √† variable latente)"
  },
  {
    "objectID": "content/course/modelisation/1_modelevaluation/index.html#validation-crois√©e",
    "href": "content/course/modelisation/1_modelevaluation/index.html#validation-crois√©e",
    "title": "11¬† Evaluer la qualit√© d‚Äôun mod√®le",
    "section": "13.1 Validation crois√©e",
    "text": "13.1 Validation crois√©e\nCertains algorithmes font intervenir des hyperparam√®tres,\nc‚Äôest-√†-dire des param√®tres exog√®nes qui d√©terminent la pr√©diction mais ne sont pas estim√©s.\nLa validation crois√©e est une m√©thode permettant de choisir la valeur du param√®tre\nqui optimise la qualit√© de la pr√©diction en agr√©geant\ndes scores de performance sur des d√©coupages diff√©rents de l‚Äô√©chantillon d‚Äôapprentissage.\nLa validation crois√©e permet d‚Äô√©valuer les performances de mod√®les diff√©rents (SVM, random forest, etc.) ou, coupl√© √† une strat√©gie de grid search de trouver les valeurs des hyperparam√®tres qui aboutissent √† la meilleure pr√©diction.\n\n\n Note\nL‚Äô√©tape de d√©coupage de l‚Äô√©chantillon de validation crois√©e est √† distinguer de l‚Äô√©tape split_sample_test. A ce stade, on a d√©j√† partitionn√© les donn√©es en √©chantillon d‚Äôapprentissage et test. C‚Äôest l‚Äô√©chantillon d‚Äôapprentissage qu‚Äôon d√©coupe en sous-morceaux.\n\n\nLa m√©thode la plus commune est la validation crois√©e k-fold.\nOn partitionne les donn√©es en \\(K\\) morceaux et on consid√®re chaque pli, tour √† tour, comme un √©chantillon\nde test en apprenant sur les \\(K-1\\) √©chantillons restants. Les \\(K\\) indicateurs ainsi calcul√©s sur les \\(K\\) √©chantillons de test peuvent √™tre moyenn√©s et\ncompar√©s pour plusieurs valeurs des hyperparam√®tres.\n\nIl existe d‚Äôautres types de validation crois√©e, notamment la leave one out qui consiste √† consid√©rer une fois\nexactement chaque observation comme l‚Äô√©chantillon de test (une n-fold cross validation)."
  },
  {
    "objectID": "content/course/modelisation/4_featureselection/index.html",
    "href": "content/course/modelisation/4_featureselection/index.html",
    "title": "12¬† S√©lection de variables : une introduction",
    "section": "",
    "text": "13 Principe du LASSO"
  },
  {
    "objectID": "content/course/modelisation/4_featureselection/index.html#principe-g√©n√©ral",
    "href": "content/course/modelisation/4_featureselection/index.html#principe-g√©n√©ral",
    "title": "12¬† S√©lection de variables : une introduction",
    "section": "13.1 Principe g√©n√©ral",
    "text": "13.1 Principe g√©n√©ral\nLa classe des mod√®les de feature selection est ainsi tr√®s vaste et regroupe\nun ensemble tr√®s diverse de mod√®les. Nous allons nous focaliser sur le LASSO\n(Least Absolute Shrinkage and Selection Operator)\nqui est une extension de la r√©gression lin√©aire qui vise √† s√©lectionner des\nmod√®les sparses. Ce type de mod√®le est central dans le champ du\nCompressed sensing (o√π on emploie plut√¥t le terme\nde L1-regularization que de LASSO). Le LASSO est un cas particulier des\nr√©gressions elastic-net dont un autre cas fameux est la r√©gression ridge.\nContrairement √† la r√©gression lin√©aire classique, elles fonctionnent √©galement\ndans un cadre o√π \\(p&gt;N\\), c‚Äôest √† dire o√π le nombre de r√©gresseurs est tr√®s grand puisque sup√©rieur\nau nombre d‚Äôobservations."
  },
  {
    "objectID": "content/course/modelisation/4_featureselection/index.html#p√©nalisation",
    "href": "content/course/modelisation/4_featureselection/index.html#p√©nalisation",
    "title": "12¬† S√©lection de variables : une introduction",
    "section": "13.2 P√©nalisation",
    "text": "13.2 P√©nalisation\nEn adoptant le principe d‚Äôune fonction objectif p√©nalis√©e,\nle LASSO permet de fixer un certain nombre de coefficients √† 0.\nLes variables dont la norme est non nulle passent ainsi le test de s√©lection.\n\n\n Hint\nLe LASSO est un programme d‚Äôoptimisation sous contrainte. On cherche √† trouver l‚Äôestimateur \\(\\beta\\) qui minimise l‚Äôerreur quadratique (r√©gression lin√©aire) sous une contrainte additionnelle r√©gularisant les param√®tres:\n\\[\n\\min_{\\beta} \\frac{1}{2}\\mathbb{E}\\bigg( \\big( X\\beta - y  \\big)^2 \\bigg) \\\\\n\\text{s.t. } \\sum_{j=1}^p |\\beta_j| \\leq t\n\\]\nCe programme se reformule gr√¢ce au Lagrangien est permet ainsi d‚Äôobtenir un programme de minimisation plus maniable :\n\\[\n\\beta^{\\text{LASSO}} = \\arg \\min_{\\beta} \\frac{1}{2}\\mathbb{E}\\bigg( \\big( X\\beta - y  \\big)^2 \\bigg) + \\alpha \\sum_{j=1}^p |\\beta_j| = \\arg \\min_{\\beta} ||y-X\\beta||_{2}^{2} + \\alpha ||\\beta||_1\n\\]\no√π \\(\\lambda\\) est une r√©√©criture de la r√©gularisation pr√©c√©dente."
  },
  {
    "objectID": "content/course/modelisation/4_featureselection/index.html#premi√®re-r√©gression-lasso",
    "href": "content/course/modelisation/4_featureselection/index.html#premi√®re-r√©gression-lasso",
    "title": "12¬† S√©lection de variables : une introduction",
    "section": "13.3 Premi√®re r√©gression LASSO",
    "text": "13.3 Premi√®re r√©gression LASSO\nAvant de se lancer dans les exercices, on va √©liminer quelques colonnes redondantes,\ncelles qui concernent les votes des partis concurrents (forc√©ment tr√®s\ncorr√©l√©s au vote R√©publicain‚Ä¶) :\n\ndf2 = votes.loc[:,~votes.columns.str.endswith(\n  ('_democrat','_green','_other', 'per_point_diff', 'per_dem')\n  )]\n\nNous allons utiliser par la suite les fonctions ou\npackages suivants:\n\n# packages utiles\nimport numpy as np\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Lasso\nimport sklearn.metrics\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import lasso_path\nimport seaborn as sns\n\n\n\n Exercice 1 : Premier LASSO\nOn cherche toujours √† pr√©dire la variable per_gop.\n\nPr√©parez les variables √† utiliser.\n\n\nNe garder que les colonnes num√©riques (id√©alement on transformerait\nles variables non num√©riques en num√©riques)\nRemplacer les valeurs infinies par des NaN et les valeurs manquantes par 0.\nStandardiser les features (c‚Äôest-√†-dire les variables autres que la variable per_gop) avec StandardScaler\n\n\nOn cherche toujours √† pr√©dire la variable per_gop. Cr√©ez un √©chantillon d‚Äôentra√Ænement et un √©chantillon test.\nEstimer un mod√®le LASSO p√©nalis√© avec \\(alpha = 0.1\\). Afficher les valeurs des coefficients. Quelles variables ont une valeur non nulle ?\nMontrer que les variables s√©lectionn√©es sont parfois tr√®s corr√©l√©es.\nComparer la performance de ce mod√®le parcimonieux avec celle d‚Äôun mod√®le avec plus de variables\nUtiliser la fonction lasso_path pour √©valuer le nombre de param√®tres s√©lectionn√©s par LASSO lorsque \\(\\alpha\\)\nvarie (parcourir \\(\\alpha \\in [0.001,0.01,0.02,0.025,0.05,0.1,0.25,0.5,0.8,1.0]\\) ).\n\n\n\nA l‚Äôissue de la question 3,\nles variables s√©lectionn√©es sont :\n\n\n['ALAND',\n 'FIPS_y',\n 'INTERNATIONAL_MIG_2017',\n 'DOMESTIC_MIG_2014',\n 'DOMESTIC_MIG_2017',\n 'RESIDUAL_2010',\n 'RESIDUAL_2019',\n 'R_death_2012',\n 'R_death_2019',\n 'R_NATURAL_INC_2019',\n 'R_INTERNATIONAL_MIG_2011',\n 'R_DOMESTIC_MIG_2012',\n \"Percent of adults with a bachelor's degree or higher, 1990\",\n 'Percent of adults with a high school diploma only, 2000',\n \"Percent of adults with a bachelor's degree or higher, 2000\",\n \"Percent of adults with a bachelor's degree or higher, 2015-19\",\n 'Rural_urban_continuum_code_2013',\n 'Metro_2013',\n 'Unemployment_rate_2002',\n 'Unemployment_rate_2003',\n 'Unemployment_rate_2012',\n 'Rural-urban_Continuum_Code_2003',\n 'Rural-urban_Continuum_Code_2013',\n 'CI90LB517P_2019',\n 'candidatevotes_2016_republican',\n 'share_2012_republican',\n 'share_2016_republican']\n\n\nCertaines variables font sens, comme les variables d‚Äô√©ducation par exemple. Notamment, un des meilleurs pr√©dicteurs pour le score des R√©publicains en 2020 est‚Ä¶ le score des R√©publicains (et m√©caniquement des d√©mocrates) en 2016.\nPar ailleurs, on s√©lectionne des variables redondantes. Une phase plus approfondie de nettoyage des donn√©es serait en r√©alit√© n√©cessaire.\n\n\n\n\n\nOn voit que plus \\(\\alpha\\) est √©lev√©, moins le mod√®le s√©lectionne de variables."
  },
  {
    "objectID": "content/course/modelisation/4_featureselection/index.html#validation-crois√©e-pour-s√©lectionner-le-mod√®le",
    "href": "content/course/modelisation/4_featureselection/index.html#validation-crois√©e-pour-s√©lectionner-le-mod√®le",
    "title": "12¬† S√©lection de variables : une introduction",
    "section": "13.4 Validation crois√©e pour s√©lectionner le mod√®le",
    "text": "13.4 Validation crois√©e pour s√©lectionner le mod√®le\nQuel \\(\\alpha\\) faut-il privil√©gier ? Pour cela,\nil convient d‚Äôeffectuer une validation crois√©e afin de choisir le mod√®le pour\nlequel les variables qui passent la phase de s√©lection permettent de mieux\npr√©dire le r√©sultat R√©publicain :\n\nfrom sklearn.linear_model import LassoCV\n\ndf3 = df2.select_dtypes(include=np.number)\ndf3.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf3 = df3.fillna(0)\nscaler = StandardScaler()\nyindex = df3.columns.get_loc(\"per_gop\")\ndf3_scale = scaler.fit(df3).transform(df3)\n# X_train, X_test , y_train, y_test = train_test_split(np.delete(data, yindex, axis = 1),data[:,yindex], test_size=0.2, random_state=0)\n\nlcv = LassoCV(alphas=my_alphas, fit_intercept=False,random_state=0,cv=5).fit(np.delete(df3_scale, yindex, axis = 1), df3_scale[:,yindex])\n\n\nprint(\"alpha optimal :\", lcv.alpha_)\n\nalpha optimal : 0.001\n\n\n\nlasso2 = Lasso(fit_intercept=True, alpha = lcv.alpha_).fit(X_train,y_train)\nfeatures_selec2 = df2.select_dtypes(include=np.number).drop(\"per_gop\", axis = 1).columns[np.abs(lasso2.coef_)&gt;0].tolist()\n\n/miniconda/envs/python-ENSAE/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning:\n\nObjective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.486e+03, tolerance: 6.352e+01\n\n\n\nLes variables s√©lectionn√©es sont :\n\nprint(features_selec2)\n\n['ALAND', 'AWATER', 'votes_gop', 'diff', 'Rural-urban_Continuum Code_2003', 'Rural-urban_Continuum Code_2013', 'Urban_Influence_Code_2013', 'Economic_typology_2015', 'CENSUS_2010_POP', 'N_POP_CHG_2013', 'N_POP_CHG_2016', 'N_POP_CHG_2017', 'N_POP_CHG_2018', 'N_POP_CHG_2019', 'Births_2011', 'Births_2015', 'Deaths_2015', 'Deaths_2017', 'Deaths_2018', 'NATURAL_INC_2012', 'NATURAL_INC_2013', 'NATURAL_INC_2014', 'NATURAL_INC_2016', 'NATURAL_INC_2018', 'INTERNATIONAL_MIG_2010', 'INTERNATIONAL_MIG_2011', 'INTERNATIONAL_MIG_2012', 'INTERNATIONAL_MIG_2013', 'INTERNATIONAL_MIG_2014', 'INTERNATIONAL_MIG_2015', 'INTERNATIONAL_MIG_2016', 'INTERNATIONAL_MIG_2017', 'INTERNATIONAL_MIG_2018', 'INTERNATIONAL_MIG_2019', 'DOMESTIC_MIG_2010', 'DOMESTIC_MIG_2012', 'DOMESTIC_MIG_2013', 'DOMESTIC_MIG_2015', 'DOMESTIC_MIG_2016', 'DOMESTIC_MIG_2018', 'NET_MIG_2011', 'NET_MIG_2014', 'NET_MIG_2018', 'NET_MIG_2019', 'RESIDUAL_2010', 'RESIDUAL_2011', 'RESIDUAL_2012', 'RESIDUAL_2013', 'RESIDUAL_2014', 'RESIDUAL_2015', 'RESIDUAL_2016', 'RESIDUAL_2017', 'RESIDUAL_2018', 'RESIDUAL_2019', 'GQ_ESTIMATES_BASE_2010', 'GQ_ESTIMATES_2013', 'GQ_ESTIMATES_2015', 'GQ_ESTIMATES_2017', 'R_birth_2011', 'R_birth_2013', 'R_birth_2014', 'R_birth_2016', 'R_birth_2017', 'R_birth_2019', 'R_death_2011', 'R_death_2012', 'R_death_2013', 'R_death_2014', 'R_death_2015', 'R_death_2016', 'R_death_2017', 'R_death_2018', 'R_death_2019', 'R_NATURAL_INC_2012', 'R_NATURAL_INC_2015', 'R_NATURAL_INC_2017', 'R_NATURAL_INC_2018', 'R_NATURAL_INC_2019', 'R_INTERNATIONAL_MIG_2011', 'R_INTERNATIONAL_MIG_2012', 'R_INTERNATIONAL_MIG_2013', 'R_INTERNATIONAL_MIG_2014', 'R_INTERNATIONAL_MIG_2015', 'R_INTERNATIONAL_MIG_2016', 'R_INTERNATIONAL_MIG_2017', 'R_INTERNATIONAL_MIG_2018', 'R_INTERNATIONAL_MIG_2019', 'R_DOMESTIC_MIG_2011', 'R_DOMESTIC_MIG_2012', 'R_DOMESTIC_MIG_2013', 'R_DOMESTIC_MIG_2015', 'R_DOMESTIC_MIG_2016', 'R_DOMESTIC_MIG_2018', 'R_NET_MIG_2014', 'R_NET_MIG_2017', 'R_NET_MIG_2019', '2003 Rural-urban Continuum Code', 'Less than a high school diploma, 1970', 'High school diploma only, 1970', 'Some college (1-3 years), 1970', 'Four years of college or higher, 1970', 'Percent of adults with less than a high school diploma, 1970', 'Percent of adults with a high school diploma only, 1970', 'Percent of adults completing some college (1-3 years), 1970', 'Percent of adults completing four years of college or higher, 1970', 'Less than a high school diploma, 1980', 'High school diploma only, 1980', 'Some college (1-3 years), 1980', 'Four years of college or higher, 1980', 'Percent of adults with less than a high school diploma, 1980', 'Percent of adults with a high school diploma only, 1980', 'Percent of adults completing some college (1-3 years), 1980', 'Percent of adults completing four years of college or higher, 1980', 'Less than a high school diploma, 1990', 'Percent of adults with less than a high school diploma, 1990', 'Percent of adults with a high school diploma only, 1990', 'Less than a high school diploma, 2000', 'High school diploma only, 2000', \"Some college or associate's degree, 2000\", \"Bachelor's degree or higher, 2000\", 'Percent of adults with less than a high school diploma, 2000', 'Percent of adults with a high school diploma only, 2000', \"Percent of adults completing some college or associate's degree, 2000\", \"Percent of adults with a bachelor's degree or higher, 2000\", 'Less than a high school diploma, 2015-19', 'High school diploma only, 2015-19', \"Some college or associate's degree, 2015-19\", \"Bachelor's degree or higher, 2015-19\", 'Percent of adults with less than a high school diploma, 2015-19', 'Percent of adults with a high school diploma only, 2015-19', \"Percent of adults completing some college or associate's degree, 2015-19\", \"Percent of adults with a bachelor's degree or higher, 2015-19\", 'Metro_2013', 'Unemployed_2000', 'Unemployment_rate_2000', 'Unemployment_rate_2001', 'Unemployed_2002', 'Unemployment_rate_2002', 'Unemployed_2003', 'Unemployment_rate_2003', 'Civilian_labor_force_2004', 'Employed_2004', 'Unemployment_rate_2004', 'Civilian_labor_force_2005', 'Unemployed_2005', 'Unemployment_rate_2005', 'Civilian_labor_force_2006', 'Unemployed_2006', 'Unemployment_rate_2006', 'Unemployed_2007', 'Unemployment_rate_2007', 'Unemployed_2008', 'Unemployment_rate_2008', 'Employed_2009', 'Unemployment_rate_2009', 'Employed_2010', 'Unemployment_rate_2010', 'Civilian_labor_force_2011', 'Employed_2011', 'Unemployed_2011', 'Civilian_labor_force_2012', 'Employed_2012', 'Unemployed_2012', 'Unemployment_rate_2012', 'Unemployed_2013', 'Unemployment_rate_2013', 'Unemployed_2014', 'Unemployment_rate_2014', 'Civilian_labor_force_2015', 'Employed_2015', 'Unemployment_rate_2015', 'Unemployed_2016', 'Unemployment_rate_2016', 'Unemployed_2017', 'Unemployment_rate_2017', 'Unemployed_2018', 'Unemployment_rate_2018', 'Unemployment_rate_2019', 'Med_HH_Income_Percent_of_State_Total_2019', 'Rural-urban_Continuum_Code_2003', 'Urban_Influence_Code_2003', 'Rural-urban_Continuum_Code_2013', 'POVALL_2019', 'CI90LBALL_2019', 'CI90UBALL_2019', 'CI90LBALLP_2019', 'CI90UBALLP_2019', 'POV017_2019', 'CI90LB017_2019', 'CI90UB017_2019', 'CI90LB017P_2019', 'CI90LB517_2019', 'CI90UB517_2019', 'PCTPOV517_2019', 'CI90LB517P_2019', 'CI90LBINC_2019', 'CI90UBINC_2019', 'candidatevotes_2000_republican', 'candidatevotes_2004_republican', 'candidatevotes_2008_republican', 'candidatevotes_2012_republican', 'candidatevotes_2016_republican', 'share_2000_republican', 'share_2008_republican', 'share_2012_republican', 'share_2016_republican']\n\n\n\ndf2.select_dtypes(include=np.number).drop(\"per_gop\", axis = 1).columns[np.abs(lasso2.coef_)&gt;0]\nnlasso = sum(np.abs(lasso2.coef_)&gt;0)\n\nCela correspond √† un mod√®le avec 206 variables s√©lectionn√©es.\n\n\n Hint\nDans le cas o√π le mod√®le para√Ætrait trop peu parcimonieux, il faudrait revoir la phase de d√©finition des variables pertinentes pour comprendre si des √©chelles diff√©rentes de certaines variables ne seraient pas plus appropri√©es (par exemple du log)."
  },
  {
    "objectID": "content/course/modelisation/3_regression/index.html",
    "href": "content/course/modelisation/3_regression/index.html",
    "title": "13¬† R√©gression : une introduction",
    "section": "",
    "text": "14 Principe g√©n√©ral\nLe principe g√©n√©ral de la r√©gression consiste √† trouver une loi \\(h_\\theta(X)\\)\ntelle que\n\\[\nh_\\theta(X) = \\mathbb{E}_\\theta(Y|X)\n\\]\nCette formalisation est extr√™mement g√©n√©raliste et ne se restreint d‚Äôailleurs\npar √† la r√©gression lin√©aire.\nEn √©conom√©trie, la r√©gression offre une alternative aux m√©thodes de maximum\nde vraisemblance et aux m√©thodes des moments. La r√©gression est un ensemble\ntr√®s vaste de m√©thodes, selon la famille de mod√®les\n(param√©triques, non param√©triques, etc.) et la structure de mod√®les."
  },
  {
    "objectID": "content/course/modelisation/3_regression/index.html#la-r√©gression-lin√©aire",
    "href": "content/course/modelisation/3_regression/index.html#la-r√©gression-lin√©aire",
    "title": "13¬† R√©gression : une introduction",
    "section": "14.1 La r√©gression lin√©aire",
    "text": "14.1 La r√©gression lin√©aire\nC‚Äôest la mani√®re la plus simple de repr√©senter la loi \\(h_\\theta(X)\\) comme\ncombinaison lin√©aire de variables \\(X\\) et de param√®tres \\(\\theta\\). Dans ce\ncas,\n\\[\n\\mathbb{E}_\\theta(Y|X) = X\\beta\n\\]\nCette relation est encore, sous cette formulation, th√©orique. Il convient\nde l‚Äôestimer √† partir des donn√©es observ√©es \\(y\\). La m√©thode des moindres\ncarr√©s consiste √† minimiser l‚Äôerreur quadratique entre la pr√©diction et\nles donn√©es observ√©es (ce qui explique qu‚Äôon puisse voir la r√©gression comme\nun probl√®me de Machine Learning). En toute g√©n√©ralit√©, la m√©thode des\nmoindres carr√©s consiste √† trouver l‚Äôensemble de param√®tres \\(\\theta\\)\ntel que\n\\[\n\\theta = \\arg \\min_{\\theta \\in \\Theta} \\mathbb{E}\\bigg[ \\left( y - h_\\theta(X) \\right)^2 \\bigg]\n\\]\nCe qui, dans le cadre de la r√©gression lin√©aire, s‚Äôexprime de la mani√®re suivante:\n\\[\n\\beta = \\arg\\min \\mathbb{E}\\bigg[ \\left( y - X\\beta \\right)^2 \\bigg]\n\\]\nLorsqu‚Äôon am√®ne le mod√®le th√©orique (\\(\\mathbb{E}_\\theta(Y|X) = X\\beta\\)) aux donn√©es,\non formalise le mod√®le de la mani√®re suivante:\n\\[\nY = X\\beta + \\epsilon\n\\]\nAvec une certaine distribution du bruit \\(\\epsilon\\) qui d√©pend\ndes hypoth√®ses faites. Par exemple, avec des\n\\(\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)\\) i.i.d., l‚Äôestimateur \\(\\beta\\) obtenu\nest √©quivalent √† celui du Maximum de Vraisemblance dont la th√©orie asymptotique\nnous assure l‚Äôabsence de biais, la variance minimale (borne de Cramer-Rao).\n\n# packages utiles\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n\n\n Exercice 1a : R√©gression lin√©aire avec scikit\nCet exercice vise √† illustrer la mani√®re d‚Äôeffectuer une r√©gression lin√©aire avec scikit.\nDans ce domaine,\nstatsmodels est nettement plus complet, ce que montrera l‚Äôexercice suivant.\nL‚Äôint√©r√™t principal de faire\ndes r√©gressions avec scikit est de pouvoir comparer les r√©sultats d‚Äôune r√©gression lin√©aire\navec d‚Äôautres mod√®les de r√©gression. Cependant, le chapitre sur les\npipelines montrera qu‚Äôon peut tr√®s bien ins√©rer, avec quelques efforts\nde programmation orient√©e objet, une r√©gression statsmodels dans\nun pipeline scikit.\nL‚Äôobjectif est d‚Äôexpliquer le score des R√©publicains en fonction de quelques\nvariables. Contrairement au chapitre pr√©c√©dent, o√π on se focalisait sur\nun r√©sultat binaire (victoire/d√©faite des R√©publicains), cette\nfois on va chercher √† mod√©liser directement le score des R√©publicains.\n\nA partir de quelques variables, par exemple, ‚ÄòUnemployment_rate_2019‚Äô, ‚ÄòMedian_Household_Income_2019‚Äô, ‚ÄòPercent of adults with less than a high school diploma, 2015-19‚Äô, ‚ÄúPercent of adults with a bachelor‚Äôs degree or higher, 2015-19‚Äù, expliquer la variable per_gop √† l‚Äôaide d‚Äôun √©chantillon d‚Äôentra√Ænement X_train constitu√© au pr√©alable.\n\n:warning: utiliser la variable Median_Household_Income_2019\nen log sinon son √©chelle risque d‚Äô√©craser tout effet.\n\nAfficher les valeurs des coefficients, constante comprise\nEvaluer la pertinence du mod√®le avec le \\(R^2\\) et la qualit√© du fit avec le MSE.\nRepr√©senter un nuage de points des valeurs observ√©es\net des erreurs de pr√©diction. Observez-vous\nun probl√®me de sp√©cification ?\n\n\n\n\n# packages utiles\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n\n\n Exercice 1b : R√©gression lin√©aire avec statsmodels\nCet exercice vise √† illustrer la mani√®re d‚Äôeffectuer une r√©gression lin√©aire avec statsmodels qui offre des fonctionnalit√©s plus proches de celles de R, et moins orient√©es Machine Learning.\nL‚Äôobjectif est toujours d‚Äôexpliquer le score des R√©publicains en fonction de quelques\nvariables.\n\nA partir de quelques variables, par exemple, ‚ÄòUnemployment_rate_2019‚Äô, ‚ÄòMedian_Household_Income_2019‚Äô, ‚ÄòPercent of adults with less than a high school diploma, 2015-19‚Äô, ‚ÄúPercent of adults with a bachelor‚Äôs degree or higher, 2015-19‚Äù, expliquer la variable per_gop. :warning: utiliser la variable Median_Household_Income_2019\nen log sinon son √©chelle risque d‚Äô√©craser tout effet.\nAfficher un tableau de r√©gression.\nEvaluer la pertinence du mod√®le avec le R^2.\nUtiliser l‚ÄôAPI formula pour r√©gresser le score des r√©publicains en fonction de la variable Unemployment_rate_2019, de Unemployment_rate_2019 au carr√© et du log de\nMedian_Household_Income_2019.\n\n\n\n\n\n Hint\nPour sortir une belle table pour un rapport sous \\(\\LaTeX\\), il est possible d‚Äôutiliser\nla m√©thode Summary.as_latex. Pour un rapport HTML, on utilisera Summary.as_html\n\n\n\n\n Note\nLes utilisateurs de R retrouveront des √©l√©ments tr√®s familiers avec statsmodels,\nnotamment la possibilit√© d‚Äôutiliser une formule pour d√©finir une r√©gression.\nLa philosophie de statsmodels est similaire √† celle qui a influ√© sur la construction\ndes packages stats et MASS de R: offrir une librairie g√©n√©raliste, proposant\nune large gamme de mod√®les. N√©anmoins, statsmodels b√©n√©ficie de sa jeunesse\npar rapport aux packages R. Depuis les ann√©es 1990, les packages R visant\n√† proposer des fonctionalit√©s manquantes dans stats et MASS se sont\nmultipli√©s alors que statsmodels, enfant des ann√©es 2010, n‚Äôa eu qu‚Äô√†\nproposer un cadre g√©n√©ral (les generalized estimating equations) pour\nenglober ces mod√®les."
  },
  {
    "objectID": "content/course/modelisation/3_regression/index.html#la-r√©gression-logistique",
    "href": "content/course/modelisation/3_regression/index.html#la-r√©gression-logistique",
    "title": "13¬† R√©gression : une introduction",
    "section": "14.2 La r√©gression logistique",
    "text": "14.2 La r√©gression logistique\nCe mod√®le s‚Äôapplique √† une distribution binaire.\nDans ce cas, \\(\\mathbb{E}\\_{\\theta}(Y|X) = \\mathbb{P}\\_{\\theta}(Y = 1|X)\\).\nLa r√©gression logistique peut √™tre vue comme un mod√®le lin√©aire en probabilit√©:\n\\[\n\\text{logit}\\bigg(\\mathbb{E}\\_{\\theta}(Y|X)\\bigg) = \\text{logit}\\bigg(\\mathbb{P}\\_{\\theta}(Y = 1|X)\\bigg) = X\\beta\n\\]\nLa fonction \\(\\text{logit}\\) est \\(]0,1[ \\to \\mathbb{R}: p \\mapsto \\log(\\frac{p}{1-p})\\).\nElle permet ainsi de transformer une probabilit√© dans \\(\\mathbb{R}\\).\nSa fonction r√©ciproque est la sigmo√Øde (\\(\\frac{1}{1 + e^{-x}}\\)),\nobjet central du Deep Learning.\nIl convient de noter que les probabilit√©s ne sont pas observ√©es, c‚Äôest l‚Äôoutcome\nbinaire (0/1) qui l‚Äôest. Cela am√®ne √† voir la r√©gression logistique de deux\nmani√®res diff√©rentes :\n\nEn √©conom√©trie, on s‚Äôint√©resse au mod√®le latent qui d√©termine le choix de\nl‚Äôoutcome. Par exemple, si on observe les choix de participer ou non au march√©\ndu travail, on va mod√©liser les facteurs d√©terminant ce choix ;\nEn Machine Learning, le mod√®le latent n‚Äôest n√©cessaire que pour classifier\ndans la bonne cat√©gorie les observations\n\nL‚Äôestimation des param√®tres \\(\\beta\\) peut se faire par maximum de vraisemblance\nou par r√©gression, les deux solutions sont √©quivalentes sous certaines\nhypoth√®ses.\n\n\n Note\nPar d√©faut, scikit applique une r√©gularisation pour p√©naliser les mod√®les\npeu parcimonieux (comportement diff√©rent\nde celui de statsmodels). Ce comportement par d√©faut est √† garder √† l‚Äôesprit\nsi l‚Äôobjectif n‚Äôest pas de faire de la pr√©diction.\n\n\n\n# packages utiles\nfrom sklearn.linear_model import LogisticRegression\nimport sklearn.metrics\n\n\n\n Exercice 2a : R√©gression logistique avec scikit\nAvec scikit, en utilisant √©chantillons d‚Äôapprentissage et d‚Äôestimation :\n\nEvaluer l‚Äôeffet des variables d√©j√† utilis√©es sur la probabilit√© des R√©publicains\nde gagner. Affichez la valeur des coefficients.\nD√©duire une matrice de confusion et\nune mesure de qualit√© du mod√®le.\nSupprimer la r√©gularisation gr√¢ce au param√®tre penalty. Quel effet sur les param√®tres estim√©s ?\n\n\n\n\n# packages utiles\nfrom scipy import stats\n\n\n\n Exercice 2b : R√©gression logistique avec statmodels\nEn utilisant √©chantillons d‚Äôapprentissage et d‚Äôestimation :\n\nEvaluer l‚Äôeffet des variables d√©j√† utilis√©es sur la probabilit√© des R√©publicains\nde gagner.\nFaire un test de ratio de vraisemblance concernant l‚Äôinclusion de la variable de (log)-revenu.\n\n\n\n\n\n Hint\nLa statistique du test est:\n\\[\nLR = -2\\log\\bigg(\\frac{\\mathcal{L}_{\\theta}}{\\mathcal{L}_{\\theta_0}}\\bigg) = -2(\\mathcal{l}_{\\theta} - \\mathcal{l}_{\\theta_0})\n\\]"
  },
  {
    "objectID": "content/course/modelisation/0_preprocessing/index.html",
    "href": "content/course/modelisation/0_preprocessing/index.html",
    "title": "14¬† Pr√©paration des donn√©es pour construire un mod√®le",
    "section": "",
    "text": "15 Construction de la base de donn√©es\nLes sources de donn√©es √©tant diverses, le code qui construit la base finale est directement fourni. Le travail de construction d‚Äôune base unique\nest un peu fastidieux mais il s‚Äôagit d‚Äôun bon exercice, que vous pouvez tenter,\npour r√©viser pandas :\nExercice 1 : Importer les donn√©es des √©lections US\nCet exercice est OPTIONNEL\n\nT√©l√©charger et importer le shapefile depuis ce lien\nExclure les Etats suivants: ‚Äú02‚Äù, ‚Äú69‚Äù, ‚Äú66‚Äù, ‚Äú78‚Äù, ‚Äú60‚Äù, ‚Äú72‚Äù, ‚Äú15‚Äù\nImporter les r√©sultats des √©lections depuis ce lien\nImporter les bases disponibles sur le site de l‚ÄôUSDA en faisant attention √† renommer les variables de code FIPS de mani√®re identique\ndans les 4 bases\nMerger ces 4 bases dans une base unique de caract√©ristiques socio√©conomiques\nMerger aux donn√©es √©lectorales √† partir du code FIPS\nMerger au shapefile √† partir du code FIPS. Faire attention aux 0 √† gauche dans certains codes. Il est\nrecommand√© d‚Äôutiliser la m√©thode str.lstrip pour les retirer\nImporter les donn√©es des √©lections 2000 √† 2016 √† partir du MIT Election Lab?\nLes donn√©es peuvent √™tre directement requ√™t√©es depuis l‚Äôurl\nhttps://dataverse.harvard.edu/api/access/datafile/3641280?gbrecs=false\nCr√©er une variable share comptabilisant la part des votes pour chaque candidat.\nNe garder que les colonnes \"year\", \"FIPS\", \"party\", \"candidatevotes\", \"share\"\nFaire une conversion long to wide avec la m√©thode pivot_table pour garder une ligne\npar comt√© x ann√©e avec en colonnes les r√©sultats de chaque candidat dans cet √©tat.\nMerger √† partir du code FIPS au reste de la base.\nSi vous ne faites pas l‚Äôexercice 1, pensez √† charger les donn√©es en executant la fonction get_data.py :\n#!pip install --upgrade xlrd #colab bug verson xlrd\n#!pip install geopandas\n\nimport requests\n\nurl = 'https://raw.githubusercontent.com/linogaliana/python-datascientist/master/content/course/modelisation/get_data.py'\nr = requests.get(url, allow_redirects=True)\nopen('getdata.py', 'wb').write(r.content)\n\nimport getdata\nvotes = getdata.create_votes_dataframes()\nCe code introduit une base nomm√©e votes dans l‚Äôenvironnement. Il s‚Äôagit d‚Äôune\nbase rassemblant les diff√©rentes sources. Elle a l‚Äôaspect\nsuivant:\nSTATEFP\nCOUNTYFP\nCOUNTYNS\nAFFGEOID\nGEOID\nNAME\nLSAD\nALAND\nAWATER\ngeometry\n...\nshare_2008_democrat\nshare_2008_other\nshare_2008_republican\nshare_2012_democrat\nshare_2012_other\nshare_2012_republican\nshare_2016_democrat\nshare_2016_other\nshare_2016_republican\nwinner\n\n\n\n\n0\n29\n227\n00758566\n0500000US29227\n29227\nWorth\n06\n690564983\n493903\nPOLYGON ((-94.63203 40.57176, -94.53388 40.570...\n...\n0.363714\n0.034072\n0.602215\n0.325382\n0.041031\n0.633588\n0.186424\n0.041109\n0.772467\nrepublican\n\n\n1\n31\n061\n00835852\n0500000US31061\n31061\nFranklin\n06\n1491355860\n487899\nPOLYGON ((-99.17940 40.35068, -98.72683 40.350...\n...\n0.284794\n0.019974\n0.695232\n0.250000\n0.026042\n0.723958\n0.149432\n0.045427\n0.805140\nrepublican\n\n\n2\n36\n013\n00974105\n0500000US36013\n36013\nChautauqua\n06\n2746047476\n1139407865\nPOLYGON ((-79.76195 42.26986, -79.62748 42.324...\n...\n0.495627\n0.018104\n0.486269\n0.425017\n0.115852\n0.459131\n0.352012\n0.065439\n0.582550\nrepublican\n\n\n\n\n3 rows √ó 383 columns\nLa carte choropl√®the suivante permet de visualiser rapidement les r√©sultats\n(l‚ÄôAlaska et Hawa√Ø ont √©t√© exclus).\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# republican : red, democrat : blue\ncolor_dict = {'republican': '#FF0000', 'democrats': '#0000FF'}\n\nfig, ax = plt.subplots(figsize = (12,12))\ngrouped = votes.groupby('winner')\nfor key, group in grouped:\n    group.plot(ax=ax, column='winner', label=key, color=color_dict[key])\nplt.axis('off')\n\n(-127.6146362, -64.0610978, 23.253819649999997, 50.628669349999996)\nLes cartes choropl√®thes peuvent donner une impression fallacieuse\nce qui exiplique que\nce type de carte a servi\nde justification pour contester les r√©sultats du vote.\nEn effet, un biais\nconnu des repr√©sentations choropl√®thes est qu‚Äôelles donnent une importance\nvisuelle excessive aux grands espaces. Or, ceux-ci sont souvent des espaces\npeu denses et influencent donc moins la variable d‚Äôint√©r√™t (en l‚Äôoccurrence\nle taux de vote en faveur des r√©publicains/d√©mocrates). Une repr√©sentation √†\nprivil√©gier pour ce type de ph√©nom√®nes est les\nronds proportionnels (voir Insee (2018), ‚ÄúLe pi√®ge territorial en cartographie‚Äù).\nLe GIF ‚ÄúLand does not vote, people do‚Äù\nqui avait eu un certain succ√®s en 2020 propose un autre mode de visualisation.\nLa carte originale a probablement √©t√© construite avec JavaScript. Cependant,\non dispose avec Python de plusieurs outils\npour r√©pliquer, √† faible co√ªt, cette carte\ngr√¢ce √†\nl‚Äôune des surcouches √† JavaScript vue dans la partie visualisation.\nEn l‚Äôoccurrence, on peut utiliser plotly pour tenir compte de la population:\nLa Figure a √©t√© obtenue avec le code suivant:\nimport plotly\nimport plotly.graph_objects as go\nimport pandas as pd\nimport geopandas as gpd\n\n\ncentroids = votes.copy()\ncentroids.geometry = centroids.centroid\ncentroids['size'] = centroids['CENSUS_2010_POP'] / 10000  # to get reasonable plotable number\n\ncolor_dict = {\"republican\": '#FF0000', 'democrats': '#0000FF'}\ncentroids[\"winner\"] =  np.where(centroids['votes_gop'] &gt; centroids['votes_dem'], 'republican', 'democrats') \n\n\ncentroids['lon'] = centroids['geometry'].x\ncentroids['lat'] = centroids['geometry'].y\ncentroids = pd.DataFrame(centroids[[\"county_name\",'lon','lat','winner', 'CENSUS_2010_POP',\"state_name\"]])\ngroups = centroids.groupby('winner')\n\ndf = centroids.copy()\n\ndf['color'] = df['winner'].replace(color_dict)\ndf['size'] = df['CENSUS_2010_POP']/6000\ndf['text'] = df['CENSUS_2010_POP'].astype(int).apply(lambda x: '&lt;br&gt;Population: {:,} people'.format(x))\ndf['hover'] = df['county_name'].astype(str) +  df['state_name'].apply(lambda x: ' ({}) '.format(x)) + df['text']\n\nfig_plotly = go.Figure(data=go.Scattergeo(\n    locationmode = 'USA-states',\n    lon=df[\"lon\"], lat=df[\"lat\"],\n    text = df[\"hover\"],\n    mode = 'markers',\n    marker_color = df[\"color\"],\n    marker_size = df['size'],\n    hoverinfo=\"text\"\n    ))\n\nfig_plotly.update_traces(\n  marker = {'opacity': 0.5, 'line_color': 'rgb(40,40,40)', 'line_width': 0.5, 'sizemode': 'area'}\n)\n\nfig_plotly.update_layout(\n        title_text = \"Reproduction of the \\\"Acres don't vote, people do\\\" map &lt;br&gt;(Click legend to toggle traces)\",\n        showlegend = True,\n        geo = {\"scope\": 'usa', \"landcolor\": 'rgb(217, 217, 217)'}\n    )\nLes cercles proportionnels permettent ainsi √† l‚Äôoeil de se concentrer sur les\nzones les plus denses et non sur les grands espaces.\nLa premi√®re √©tape n√©cessaire √† suivre avant de se lancer dans la mod√©lisation\nest de d√©terminer les variables √† inclure dans le mod√®le.\nLes fonctionnalit√©s de pandas sont, √† ce niveau, suffisantes pour explorer des structures simples.\nN√©anmoins, lorsqu‚Äôon est face √† un jeu de donn√©es pr√©sentant de\nnombreuses variables explicatives (features en machine learning, covariates en √©conom√©trie),\nil est souvent judicieux d‚Äôavoir une premi√®re √©tape de s√©lection de variables,\nce que nous verrons par la suite dans la partie d√©di√©e.\nAvant d‚Äô√™tre en mesure de s√©lectionner le meilleur ensemble de variables explicatives,\nnous allons en prendre un nombre restreint et arbitraire.\nLa premi√®re t√¢che est de repr√©senter les relations entre les donn√©es,\nnotamment la relation des variables explicatives\n√† la variable d√©pendante (le score du parti r√©publicain)\nainsi que les relations entre les variables explicatives.\nExercice 2 : Regarder les corr√©lations entre les variables\n\nCr√©er un DataFrame df2 plus petit avec les variables winner, votes_gop, Unemployment_rate_2019,\nMedian_Household_Income_2019,\nPercent of adults with less than a high school diploma, 2015-19,\nPercent of adults with a bachelor's degree or higher, 2015-19\nRepr√©senter gr√¢ce √† un graphique la matrice de corr√©lation avec heatmap de seaborn.\nRepr√©senter une matrice de nuages de points des variables de la base df2 avec pd.plotting.scatter_matrix\n(optionnel) Refaire ces figures avec plotly qui offre √©galement la possibilit√© de faire une matrice de corr√©lation.\nLa matrice construite avec seaborn (question 2) aura l‚Äôaspect suivant:\nAlors que celle construite directement avec corr de pandas\nressemblera plut√¥t √† ce tableau :\nvotes_gop\nUnemployment_rate_2019\nMedian_Household_Income_2019\nPercent of adults with less than a high school diploma, 2015-19\nPercent of adults with a bachelor's degree or higher, 2015-19\n\n\n\n\nvotes_gop\n1.00\n-0.08\n0.35\n-0.11\n0.37\n\n\nUnemployment_rate_2019\n-0.08\n1.00\n-0.43\n0.36\n-0.36\n\n\nMedian_Household_Income_2019\n0.35\n-0.43\n1.00\n-0.51\n0.71\n\n\nPercent of adults with less than a high school diploma, 2015-19\n-0.11\n0.36\n-0.51\n1.00\n-0.59\n\n\nPercent of adults with a bachelor's degree or higher, 2015-19\n0.37\n-0.36\n0.71\n-0.59\n1.00\nLe nuage de point obtenu √† l‚Äôissue de la question 3 ressemblera √† :\narray([[&lt;Axes: xlabel='votes_gop', ylabel='votes_gop'&gt;,\n        &lt;Axes: xlabel='Unemployment_rate_2019', ylabel='votes_gop'&gt;,\n        &lt;Axes: xlabel='Median_Household_Income_2019', ylabel='votes_gop'&gt;,\n        &lt;Axes: xlabel='Percent of adults with less than a high school diploma, 2015-19', ylabel='votes_gop'&gt;,\n        &lt;Axes: xlabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\", ylabel='votes_gop'&gt;],\n       [&lt;Axes: xlabel='votes_gop', ylabel='Unemployment_rate_2019'&gt;,\n        &lt;Axes: xlabel='Unemployment_rate_2019', ylabel='Unemployment_rate_2019'&gt;,\n        &lt;Axes: xlabel='Median_Household_Income_2019', ylabel='Unemployment_rate_2019'&gt;,\n        &lt;Axes: xlabel='Percent of adults with less than a high school diploma, 2015-19', ylabel='Unemployment_rate_2019'&gt;,\n        &lt;Axes: xlabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\", ylabel='Unemployment_rate_2019'&gt;],\n       [&lt;Axes: xlabel='votes_gop', ylabel='Median_Household_Income_2019'&gt;,\n        &lt;Axes: xlabel='Unemployment_rate_2019', ylabel='Median_Household_Income_2019'&gt;,\n        &lt;Axes: xlabel='Median_Household_Income_2019', ylabel='Median_Household_Income_2019'&gt;,\n        &lt;Axes: xlabel='Percent of adults with less than a high school diploma, 2015-19', ylabel='Median_Household_Income_2019'&gt;,\n        &lt;Axes: xlabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\", ylabel='Median_Household_Income_2019'&gt;],\n       [&lt;Axes: xlabel='votes_gop', ylabel='Percent of adults with less than a high school diploma, 2015-19'&gt;,\n        &lt;Axes: xlabel='Unemployment_rate_2019', ylabel='Percent of adults with less than a high school diploma, 2015-19'&gt;,\n        &lt;Axes: xlabel='Median_Household_Income_2019', ylabel='Percent of adults with less than a high school diploma, 2015-19'&gt;,\n        &lt;Axes: xlabel='Percent of adults with less than a high school diploma, 2015-19', ylabel='Percent of adults with less than a high school diploma, 2015-19'&gt;,\n        &lt;Axes: xlabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\", ylabel='Percent of adults with less than a high school diploma, 2015-19'&gt;],\n       [&lt;Axes: xlabel='votes_gop', ylabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\"&gt;,\n        &lt;Axes: xlabel='Unemployment_rate_2019', ylabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\"&gt;,\n        &lt;Axes: xlabel='Median_Household_Income_2019', ylabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\"&gt;,\n        &lt;Axes: xlabel='Percent of adults with less than a high school diploma, 2015-19', ylabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\"&gt;,\n        &lt;Axes: xlabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\", ylabel=\"Percent of adults with a bachelor's degree or higher, 2015-19\"&gt;]],\n      dtype=object)\nLe r√©sultat de la question 4 devrait, quant √† lui,\nressembler au graphique suivant :\n# Pour inclusion dans le site web\nhtmlsnip2.write_json(\"scatter.json\")\nLes diff√©rences d‚Äô√©chelle ou de distribution entre les variables peuvent\ndiverger des hypoth√®ses sous-jacentes dans les mod√®les.\nPar exemple, dans le cadre\nde la r√©gression lin√©aire, les variables cat√©gorielles ne sont pas trait√©es √† la m√™me\nenseigne que les variables ayant valeur dans \\(\\mathbb{R}\\). Une variable\ndiscr√®te (prenant un nombre fini de valeurs) devra √™tre transform√©es en suite de\nvariables 0/1 par rapport √† une modalit√© de r√©f√©rence pour √™tre en ad√©quation\navec les hypoth√®ses de la r√©gression lin√©aire.\nOn appelle ce type de transformation\none-hot encoding, sur lequel nous reviendrons. Il s‚Äôagit d‚Äôune transformation,\nparmi d‚Äôautres, disponibles dans scikit pour mettre en ad√©quation un jeu de\ndonn√©es et des hypoth√®ses math√©matiques.\nL‚Äôensemble de ces t√¢ches s‚Äôappelle le preprocessing. L‚Äôun des int√©r√™ts\nd‚Äôutiliser scikit est qu‚Äôon peut consid√©rer qu‚Äôune t√¢che de preprocessing\nest une t√¢che d‚Äôapprentissage (on apprend des param√®tres d‚Äôune structure\nde donn√©es) qui est r√©utilisable pour un jeu de donn√©es √† la structure\nsimilaire:\nNous allons voir deux processus tr√®s classiques de preprocessing :\nWarning\nPour un statisticien,\nle terme normalization dans le vocable scikit peut avoir un sens contre-intuitif.\nOn s‚Äôattendrait √† ce que la normalisation consiste √† transformer une variable de mani√®re √† ce que \\(X \\sim \\mathcal{N}(0,1)\\).\nC‚Äôest, en fait, la standardisation en scikit qui fait cela."
  },
  {
    "objectID": "content/course/modelisation/0_preprocessing/index.html#standardisation",
    "href": "content/course/modelisation/0_preprocessing/index.html#standardisation",
    "title": "14¬† Pr√©paration des donn√©es pour construire un mod√®le",
    "section": "17.1 Standardisation",
    "text": "17.1 Standardisation\nLa standardisation consiste √† transformer des donn√©es pour que la distribution empirique suive une loi \\(\\mathcal{N}(0,1)\\). Pour √™tre performants, la plupart des mod√®les de machine learning n√©cessitent souvent d‚Äôavoir des donn√©es dans cette distribution.\n\n\n Exercice 3: Standardisation\n\nStandardiser la variable Median_Household_Income_2019 (ne pas √©craser les valeurs !) et regarder l‚Äôhistogramme avant/apr√®s normalisation.\n\nNote : On obtient bien une distribution centr√©e √† z√©ro et on pourrait v√©rifier que la variance empirique soit bien √©gale √† 1. On pourrait aussi v√©rifier que ceci est vrai √©galement quand on transforme plusieurs colonnes √† la fois.\n\nCr√©er scaler, un Transformer que vous construisez sur les 1000 premi√®res lignes de votre DataFrame df2 √† l‚Äôexception de la variable √† expliquer winner. V√©rifier la moyenne et l‚Äô√©cart-type de chaque colonne sur ces m√™mes observations.\n\nNote : Les param√®tres qui seront utilis√©s pour une standardisation ult√©rieure sont stock√©s dans les attributs .mean_ et .scale_\nOn peut voir ces attributs comme des param√®tres entra√Æn√©s sur un certain jeu de\ndonn√©es et qu‚Äôon peut r√©utiliser sur un autre, √† condition que les\ndimensions co√Øncident.\n\nAppliquer scaler sur les autres lignes du DataFrame et comparer les distributions obtenues de la variable Median_Household_Income_2019.\n\nNote : Une fois appliqu√©s √† un autre DataFrame, on peut remarquer que la distribution n‚Äôest pas exactement centr√©e-r√©duite dans le DataFrame sur lequel les param√®tres n‚Äôont pas √©t√© estim√©s. C‚Äôest normal, l‚Äô√©chantillon initial n‚Äô√©tait pas al√©atoire, les moyennes et variances de cet √©chantillon n‚Äôont pas de raison de co√Øncider avec les moments de l‚Äô√©chantillon complet."
  },
  {
    "objectID": "content/course/modelisation/0_preprocessing/index.html#normalisation",
    "href": "content/course/modelisation/0_preprocessing/index.html#normalisation",
    "title": "14¬† Pr√©paration des donn√©es pour construire un mod√®le",
    "section": "17.2 Normalisation",
    "text": "17.2 Normalisation\nLa normalisation est l‚Äôaction de transformer les donn√©es de mani√®re\n√† obtenir une norme (\\(\\mathcal{l}_1\\) ou \\(\\mathcal{l}_2\\)) unitaire.\nAutrement dit, avec la norme ad√©quate, la somme des √©l√©ments est √©gale √† 1.\nPar d√©faut, la norme est dans \\(\\mathcal{l}_2\\).\nCette transformation est particuli√®rement utilis√©e en classification de texte ou pour effectuer du clustering.\n\n\n Exercice 4 : Normalisation\n\nNormaliser la variable Median_Household_Income_2019 (ne pas √©craser les valeurs !) et regarder l‚Äôhistogramme avant/apr√®s normalisation.\nV√©rifier que la norme \\(\\mathcal{l}_2\\) est bien √©gale √† 1.\n\n\n\n\n\n Warning\npreprocessing.Normalizer n‚Äôaccepte pas les valeurs manquantes, alors que preprocessing.StandardScaler() s‚Äôen accomode (dans la version 0.22 de scikit). Pour pouvoir ais√©ment appliquer le normalizer, il faut\n\nretirer les valeurs manquantes du DataFrame avec la m√©thode dropna: df.dropna(how = \"any\");\nou les imputer avec un mod√®le ad√©quat. scikit permet de le faire."
  },
  {
    "objectID": "content/course/modelisation/0_preprocessing/index.html#encodage-des-valeurs-cat√©gorielles",
    "href": "content/course/modelisation/0_preprocessing/index.html#encodage-des-valeurs-cat√©gorielles",
    "title": "14¬† Pr√©paration des donn√©es pour construire un mod√®le",
    "section": "17.3 Encodage des valeurs cat√©gorielles",
    "text": "17.3 Encodage des valeurs cat√©gorielles\nLes donn√©es cat√©gorielles doivent √™tre recod√©es\nsous forme de valeurs num√©riques pour √™tre int√©gr√©s aux mod√®les de machine learning.\nCela peut √™tre fait de plusieurs mani√®res :\n\nLabelEncoder: transforme un vecteur [\"a\",\"b\",\"c\"] en vecteur num√©rique [0,1,2].\nCette approche a l‚Äôinconv√©nient d‚Äôintroduire un ordre dans les modalit√©s, ce qui n‚Äôest pas toujours souhaitable\nOrdinalEncoder: une version g√©n√©ralis√©e du LabelEncoder qui a vocation √† s‚Äôappliquer sur des matrices (\\(X\\)),\nalors que LabelEncoder s‚Äôapplique plut√¥t √† un vecteur (\\(y\\))\npandas.get_dummies effectue une op√©ration de dummy expansion.\nUn vecteur de taille n avec K cat√©gories sera transform√© en matrice de taille \\(n \\times K\\)\npour lequel chaque colonne sera une variable dummy pour la modalit√© k.\nIl y a ici \\(K\\) modalit√©s et il y a donc multicolin√©arit√©.\nAvec une r√©gression lin√©aire avec constante,\nil convient de retirer une modalit√© avant l‚Äôestimation.\nOneHotEncoder est une version g√©n√©ralis√©e (et optimis√©e) de la dummy expansion.\nIl a plut√¥t vocation √† s‚Äôappliquer sur les features (\\(X\\)) du mod√®le\n\n\n\n Exercice 5 : Encoder des variables cat√©gorielles\n\nCr√©er df qui conserve uniquement les variables state_name et county_name dans votes.\nAppliquer √† state_name un LabelEncoder\nNote : Le r√©sultat du label encoding est relativement intuitif, notamment quand on le met en relation avec le vecteur initial.\nRegarder la dummy expansion de state_name\nAppliquer un OrdinalEncoder √† df[['state_name', 'county_name']]\nNote : Le r√©sultat du ordinal encoding est coh√©rent avec celui du label encoding\nAppliquer un OneHotEncoder √† df[['state_name', 'county_name']]\n\nNote : scikit optimise l‚Äôobjet n√©cessaire pour stocker le r√©sultat d‚Äôun mod√®le de transformation. Par exemple, le r√©sultat de l‚Äôencoding One Hot est un objet tr√®s volumineux. Dans ce cas, scikit utilise une matrice Sparse."
  },
  {
    "objectID": "content/course/modelisation/0_preprocessing/index.html#r√©f√©rences",
    "href": "content/course/modelisation/0_preprocessing/index.html#r√©f√©rences",
    "title": "14¬† Pr√©paration des donn√©es pour construire un mod√®le",
    "section": "17.4 R√©f√©rences",
    "text": "17.4 R√©f√©rences\n\n\nInsee. 2018. ‚ÄúGuide de S√©miologie Cartographique.‚Äù"
  },
  {
    "objectID": "content/course/modelisation/6_pipeline/index.html",
    "href": "content/course/modelisation/6_pipeline/index.html",
    "title": "15¬† Premier pas vers l‚Äôindustrialisation avec les pipelines scikit",
    "section": "",
    "text": "16 Pourquoi utiliser les pipelines ?\nLes chapitres pr√©c√©dents ont permis de montrer des bouts de code\n√©pars pour entra√Æner des mod√®les ou faire du preprocessing.\nCette d√©marche est int√©ressante pour t√¢tonner mais risque d‚Äô√™tre co√ªteuse\nult√©rieurement s‚Äôil est n√©cessaire d‚Äôajouter une √©tape de preprocessing\nou de changer d‚Äôalgorithme.\nHeureusement, scikit propose un excellent outil pour proposer un cadre\ng√©n√©ral pour cr√©er une cha√Æne de production machine learning. Il\ns‚Äôagit des\npipelines.\nIls pr√©sentent de nombreux int√©r√™ts, parmi lesquels:\n{{% box status=‚Äúhint‚Äù title=‚ÄúHint‚Äù icon=‚Äúfa fa-lightbulb‚Äù %}}\nUn des int√©r√™ts des pipelines scikit est qu‚Äôils fonctionnent aussi avec\ndes m√©thodes qui ne sont pas issues de scikit.\nIl est tr√®s\nfacile d‚Äôintroduire un mod√®le de r√©seau de neurone Keras dans\nun pipeline scikit.\nPour introduire un mod√®le √©conom√©trique statsmodels\nc‚Äôest un peu plus co√ªteux mais nous allons proposer des exemples\nqui peuvent servir de mod√®le et qui montrent que c‚Äôest faisable\nsans trop de difficult√©.\n{{% /box %}}\n{{% box status=‚Äúwarning‚Äù title=‚ÄúWarning‚Äù icon=‚Äúfa fa-exclamation-triangle‚Äù %}}\nLes √©l√©ments pr√©sents dans ce chapitre n√©cessitent une version assez r√©cente\nde scikit (au moins la version 1.0). Pour v√©rifier la version, faire:\nimport sklearn\nprint(sklearn.__version__)\nIl faut √©galement une version r√©cente de yellowbrick pour √©viter l‚Äôerreur\nsuivante quand on utilise une version r√©cente de scikit (ce que\nje recommande):\n{{% /box %}}\nUn pipeline est un encha√Ænement d‚Äôop√©rations qu‚Äôon code en enchainant\ndes pairs (cl√©, valeur):\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\n\nestimators = [('reduce_dim', PCA()), ('clf', SVC())]\npipe = Pipeline(estimators)\n{{% box status=‚Äúhint‚Äù title=‚ÄúHint‚Äù icon=‚Äúfa fa-lightbulb‚Äù %}}\nIl est pratique de visualiser un pipeline sous forme de DAG.\nPour cela, dans un notebook, on utilise la configuration\nsuivante:\n{{% /box %}}\nAu sein d‚Äôune √©tape de pipeline, les param√®tres d‚Äôun estimateur\nsont accessibles avec la notation &lt;estimator&gt;__&lt;parameter&gt;.\nCela permet de fixer des valeurs pour les arguments des fonctions scikit\nqui sont appel√©es au sein d‚Äôun pipeline.\nC‚Äôest cela qui rendra l‚Äôapproche des pipelines particuli√®rement utile\npour la grid search:\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {\"reduce_dim__n_components\":[2, 5, 10], \"clf__C\":[0.1, 10, 100]}\ngrid_search = GridSearchCV(pipe, param_grid=param_grid)\nNotre premier pipeline va nous permettre d‚Äôint√©grer ensemble:\nPour le moment, on va prendre comme acquis un certain nombre de variables\nexplicatives (les features) et les hyperparam√®tres du mod√®le\nOn d√©tecte que le premier mod√®le n‚Äôest pas tr√®s bon et ne nous aidera\npas vraiment √† √©valuer de mani√®re fiable la maison de nos r√™ves.\nOn va essayer de voir si notre mod√®le ne serait pas meilleur avec des\nhyperparam√®tres plus adapt√©s. Apr√®s tout, nous avons choisi par d√©faut\nla profondeur de l‚Äôarbre mais c‚Äô√©tait un choix au doigt mouill√©.\nQuels sont les hyperparam√®tres qu‚Äôon peut essayer d‚Äôoptimiser ?\npipe['randomforest'].get_params()\nUn d√©tour par la documentation\nnous aide √† comprendre ceux sur lesquels on va jouer. Par exemple, il serait\nabsurde de jouer sur le param√®tre random_state qui est la racine du g√©n√©rateur\npseudo-al√©atoire.\nComme l‚Äôobjectif est de se concentrer sur la d√©marche plus qu‚Äôessayer de\ntrouver un bon mod√®le,\nnous allons √©galement r√©duire la taille des donn√©es pour acc√©l√©rer\nles calculs\nmutations2 = mutations2.groupby('dep').sample(frac = 0.5, random_state = 123)\n\nX_train, X_test, y_train, y_test = train_test_split(mutations2[xvars2], mutations2[[\"Valeur_fonciere\"]].values.ravel(), test_size = 0.2, random_state = 0, stratify=mutations2[['dep']])\n\nX = pd.concat((X_train, X_test), axis=0)\nY = np.concatenate([y_train,y_test])\nNous allons nous contenter de jouer sur les param√®tres:\nIl existe plusieurs mani√®res de faire de la validation crois√©e. Nous allons ici\nutiliser la grid search qui consiste √† estimer et tester le mod√®le sur chaque\ncombinaison d‚Äôune grille de param√®tres et s√©lectionner le couple de valeurs\ndes hyperparam√®tres amenant √† la meilleure pr√©diction. Par d√©faut, scikit\neffectue une 5-fold cross validation. Nous n‚Äôallons pas changer\nce comportement.\nComme expliqu√© pr√©c√©demment, les param√®tres s‚Äôappelent sous la forme\n&lt;step&gt;__&lt;parameter_name&gt;\nLa validation crois√©e pouvant √™tre tr√®s consommatrice de temps, nous\nn‚Äôallons l‚Äôeffectuer que sur un nombre r√©duit de valeurs de notre grille.\nIl est possible de passer la liste des valeurs √† passer au crible sous\nforme de liste (comme pour l‚Äôargument max_depth ci-dessous) ou\nsous forme d‚Äôarray (comme pour l‚Äôargument n_estimators) ce qui est\nsouvent pratique pour g√©n√©rer un criblage d‚Äôun intervalle avec np.linspace.\nOn peut r√©cup√©rer les param√®tres optimaux avec la m√©thode best_params_:\ngrid_search.best_params_\nOn pourra aussi r√©-utiliser le mod√®le optimal de la mani√®re suivante:\nToutes les performances sur les ensembles d‚Äô√©chantillons et de test sur la grille\nd‚Äôhyperparam√®tres sont disponibles dans l‚Äôattribut:\nperf_random_forest = pd.DataFrame(grid_search.cv_results_)\nRegardons les r√©sultats moyens pour chaque valeur des hyperparam√®tres:\nGlobalement, √† profondeur d‚Äôarbre donn√©e, le nombre d‚Äôarbres change\nmarginalement la performance (cela d√©t√©riore\nla performance quand la profondeur est de 4, cela am√©liore quand\non fixe la profondeur de 2).\nEn revanche, changer la profondeur de l‚Äôarbre am√©liore la\nperformance de mani√®re plus marqu√©e.\nMaintenant, il nous reste √† re-entra√Æner le mod√®le avec ces nouveaux\nparam√®tres sur l‚Äôensemble du jeu de train et l‚Äô√©valuer sur l‚Äôensemble\ndu jeu de test:\nOn obtient le RMSE suivant:\nprint(\"Le RMSE sur le jeu de test est {:,}\".format(\n   int(np.sqrt(mean_squared_error(y_test, pipe_optimal.predict(X_test))))\n))\nEt si on regarde la qualit√© en pr√©diction:\ng.figure.get_figure()\nOn obtient plus de variance dans la pr√©diction, c‚Äôest d√©j√† un peu mieux.\nCependant, cela reste d√©cevant pour plusieurs raisons:\nfeatures_names=pipe_optimal['preprocessor'].get_feature_names_out()\nimportances = pipe_optimal['randomforest'].feature_importances_\nstd = np.std([tree.feature_importances_ for tree in pipe_optimal['randomforest'].estimators_], axis=0)\n\nforest_importances = pd.Series(importances[importances&gt;0], index=features_names[importances&gt;0])\n\nfig, ax = plt.subplots()\nforest_importances.plot.bar(yerr=std[importances&gt;0], ax=ax)\nax.set_title(\"Feature importances using MDI\")\nax.set_ylabel(\"Mean decrease in impurity\")\nfig.tight_layout()\nCe chapitre est amen√© √† √™tre enrichi des √©l√©ments suivants\n(cf.¬†#207)"
  },
  {
    "objectID": "content/course/modelisation/6_pipeline/index.html#donn√©es-utilis√©es",
    "href": "content/course/modelisation/6_pipeline/index.html#donn√©es-utilis√©es",
    "title": "15¬† Premier pas vers l‚Äôindustrialisation avec les pipelines scikit",
    "section": "17.1 Donn√©es utilis√©es",
    "text": "17.1 Donn√©es utilis√©es\nNous allons utiliser les donn√©es de transactions immobili√®res DVF pour chercher\nla meilleure mani√®re de pr√©dire, sachant les caract√©ristiques d‚Äôun bien, son\nprix.\nCes donn√©es peuvent √™tre import√©es directement depuis data.gouv:\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nmutations = pd.read_csv('https://www.data.gouv.fr/fr/datasets/r/3004168d-bec4-44d9-a781-ef16f41856a2', sep = \"|\", decimal=\",\")\n\nOn propose d‚Äôenrichir la base de quelques variables qui pourraient servir\nult√©rieurement:\n\nmutations['Date mutation'] = pd.to_datetime(mutations['Date mutation'], format = \"%d/%m/%Y\")\nmutations['year'] = mutations['Date mutation'].dt.year\nmutations['month'] = mutations['Date mutation'].dt.month\nmutations['dep'] = mutations['Code postal'].astype(str).str[:2]\nmutations['lprix'] = np.log(mutations[\"Valeur fonciere\"])\n\nSi vous travaillez avec les donn√©es de 2020, n‚Äôoubliez pas\nd‚Äôint√©grer l‚Äôeffet\nconfinement strict dans vos mod√®les. Pour cela, vous pouvez cr√©er une variable\nindicatrice entre les dates en question:\n\nmutations['confinement'] = mutations['Date mutation'].between(pd.to_datetime(\"2020-03-17\"), pd.to_datetime(\"2020-05-03\")).astype(int)\n\nLes donn√©es DVF proposent une observation par transaction. Ces transactions\npeuvent concerner plusieurs lots.\nPour simplifier,\non va cr√©er une variable de surface qui agr√®ge les diff√©rentes informations\nde surface disponibles dans le jeu de donn√©es. En effet, les variables\nen question sont tr√®s corr√©l√©es les unes entre elles :\n\ng.figure.get_figure()\n\nLes agr√©ger revient √† supposer que le mod√®le de fixation des prix est le m√™me\nentre chaque lot. C‚Äôest une hypoth√®se simplificatrice qu‚Äôune personne plus\nexperte du march√© immobilier, ou qu‚Äôune approche propre de s√©lection\nde variable pourrait amener √† nier\n\nmutations['surface'] = mutations.loc[:, mutations.columns[mutations.columns.str.startswith('Surface Carrez')].tolist()].sum(axis = 1)"
  },
  {
    "objectID": "content/course/modelisation/6_pipeline/index.html#d√©finition-des-ensembles-traintest",
    "href": "content/course/modelisation/6_pipeline/index.html#d√©finition-des-ensembles-traintest",
    "title": "15¬† Premier pas vers l‚Äôindustrialisation avec les pipelines scikit",
    "section": "18.1 D√©finition des ensembles train/test",
    "text": "18.1 D√©finition des ensembles train/test\nNous allons donc nous restreindre √† un sous-ensemble de colonnes dans un\npremier temps :\n\nxvars = ['dep', 'Nombre de lots', 'Code type local', 'surface', 'Nombre pieces principales']\nxvars2 = pd.Series(xvars).str.replace(\" \",\"_\").tolist()\n\nmutations2 = mutations.loc[:, xvars + [\"Valeur fonciere\"]]\n\nNous allons √©galement ne conserver que les transactions inf√©rieures √† 5 millions\nd‚Äôeuros (on anticipe que celles ayant un montant sup√©rieur sont des transactions\nexceptionnelles dont le m√©canisme de fixation du prix diff√®re)\n\nmutations2  = mutations2.dropna()\nmutations2 = mutations2.loc[mutations2['Valeur fonciere'] &lt; 5e6] #keep only values below 10 millions\n\nmutations2.columns = mutations2.columns.str.replace(\" \", \"_\")\nnumeric_features = mutations2.columns[~mutations2.columns.isin(['dep','Code_type_local', 'confinement'])].tolist()\ncategorical_features = ['dep','Code_type_local']\n\nAu passage, nous avons abandonn√© la variable de code postal pour privil√©gier\nla commune afin de r√©duire la dimension de notre jeu de donn√©es. Si on voulait\nvraiment avoir un bon mod√®le, il faudrait faire autrement car le code postal\nest probablement un tr√®s bon pr√©dicteur du prix d‚Äôun bien, une fois que\nles caract√©ristiques du bien sont contr√¥l√©es.\nNous allons stratifier notre √©chantillonage de train/test par d√©partement\nafin de tenir compte, de mani√®re minimale, de la g√©ographie.\nPour acc√©l√©rer les calculs pour ce tutoriel, nous n‚Äôallons consid√©rer que\n20% des transactions observ√©es sur chaque d√©partement.\n\nfrom sklearn.model_selection import train_test_split\n\nmutations2 = mutations2.groupby('dep').sample(frac = 0.2, random_state = 123)\n\nX_train, X_test, y_train, y_test = train_test_split(mutations2[xvars2], mutations2[[\"Valeur_fonciere\"]].values.ravel(), test_size = 0.2, random_state = 0, stratify=mutations2[['dep']])"
  },
  {
    "objectID": "content/course/modelisation/6_pipeline/index.html#d√©finition-du-premier-pipeline",
    "href": "content/course/modelisation/6_pipeline/index.html#d√©finition-du-premier-pipeline",
    "title": "15¬† Premier pas vers l‚Äôindustrialisation avec les pipelines scikit",
    "section": "18.2 D√©finition du premier pipeline",
    "text": "18.2 D√©finition du premier pipeline\nNous allons donc partir d‚Äôun random forest avec des valeurs d‚Äôhyperparam√®tres\ndonn√©es.\nLes random forest sont une m√©thode d‚Äôaggr√©gation1 d‚Äôarbres de d√©cision.\nOn calcule \\(K\\) arbres de d√©cision et en tire, par une m√©thode d‚Äôagr√©gation,\nune r√®gle de d√©cision moyenne qu‚Äôon va appliquer pour tirer une\npr√©diction de nos donn√©es.\n\nC‚Äôest un article de L√©o Breiman (2001)2, statisticien √† Berkeley, qui\nest √† l‚Äôorigine du succ√®s des random forests. L‚Äôun des int√©r√™ts\ndes random forest est qu‚Äôil existe des m√©thodes pour d√©terminer\nl‚Äôimportance relative de chaque variable dans la pr√©diction.\nPour commencer, nous allons fixer la taille des arbres de d√©cision avec\nl‚Äôhyperparam√®tre max_depth = 2.\n\nfrom sklearn.ensemble import RandomForestRegressor\nregr = RandomForestRegressor(max_depth=2, random_state=123)\n\nNotre pipeline va int√©grer les √©tapes suivantes:\n\nPreprocessing:\n\nLes variables num√©riques vont √™tre standardis√©es avec un StandardScaler.\nPour cela, nous allons utiliser la liste numeric_features d√©finie pr√©c√©demment.\nLes variables cat√©gorielles vont √™tre explos√©es avec un one hot encoding\n(m√©thode OneHotEncoder de scikit)\nPour cela, nous allons utiliser la liste categorical_features\n\nRandom forest: nous allons appliquer l‚Äôestimateur regr d√©fini plus haut\n\nJ‚Äôajoute en commentaire un exemple de comment s‚Äôintroduirait une imputation\nde valeurs manquantes. La version 1.0 de scikit facilite l‚Äôint√©gration\nd‚Äô√©tapes complexes dans les pipelines3. Si vous utilisez une\nversion ant√©rieure √† la 1.0 de scikit, vous pouvez vous rendre dans\nla section Annexe pour avoir des exemples de d√©finition alternative\n(attention cependant, vous ne pourrez r√©cup√©rer le nom des features\ntransform√©es comme ici, ce qui peut p√©naliser l‚Äôanalyse d‚Äôimportance\nde variables)\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import make_column_transformer\n\nnumeric_pipeline = make_pipeline(\n  #SimpleImputer(),\n  StandardScaler()\n)\ntransformer = make_column_transformer(\n    (numeric_pipeline, numeric_features[:-1]),\n    (OneHotEncoder(sparse = False, handle_unknown = \"ignore\"), categorical_features))\npipe = Pipeline(steps=[('preprocessor', transformer),\n                      ('randomforest', regr)])\n\nNous avons construit ce pipeline sous forme de couches successives. La couche\nrandomforest prendra automatiquement le r√©sultat de la couche preprocessor\nen input. La couche features permet d‚Äôintroduire de mani√®re relativement\nsimple (quand on a les bonnes m√©thodes) la complexit√© du preprocessing\nsur donn√©es r√©elles dont les types divergent.\nOn peut visualiser le graphe et ainsi se repr√©senter la mani√®re dont\nce pipeline op√®re:\npipe\nMaintenant, il ne reste plus qu‚Äô√† estimer notre mod√®le sur l‚Äôensemble\nd‚Äôentra√Ænement. C‚Äôest tr√®s simple avec un pipeline : il suffit d‚Äôutiliser\nde mettre √† jour le pipeline avec la m√©thode fit\nOn peut utiliser le nom du pipeline en conjonction de certaines m√©thodes\npour appliquer cette √©tape sur un jeu de donn√©es pour visualiser\nl‚Äôeffet de la transformation.\nPar exemple, pour visualiser le jeu de donn√©es transform√© avant l‚Äô√©tape\nd‚Äôestimation, on peut\nfaire\n\npipe[:-1].fit_transform(X_train)\n\nDe m√™me, si on veut r√©cup√©rer le nom des features en sortie du preprocessing,\non utilisera la m√©thode .get_feature_names_out qui est bien pratique\n(c‚Äôest cette m√©thode qui est plus complexe √† appeler dans les versions scikit\nancienne qui nous a fait privil√©gier le pipeline ci-dessous)\n\nfeatures_names=pipe['preprocessor'].get_feature_names_out()\nfeatures_names"
  },
  {
    "objectID": "content/course/modelisation/6_pipeline/index.html#variable-importance",
    "href": "content/course/modelisation/6_pipeline/index.html#variable-importance",
    "title": "15¬† Premier pas vers l‚Äôindustrialisation avec les pipelines scikit",
    "section": "18.3 Variable importance",
    "text": "18.3 Variable importance\nOn ne va repr√©senter, parmi notre ensemble important de colonnes, que celles\nqui ont une importance non nulle. Gr√¢ce √† notre vecteur features_names,\non va pouvoir facilement afficher le nom des colonnes en question (et donc\ncomprendre les features d√©terminantes)\nL‚Äôimportance va √™tre d√©finie √† partir\nde la mesure d‚Äôimpuret√©4\nOn voit donc que deux variables d√©terminantes sont des effets fixes\ng√©ographiques (qui servent √† ajuster de la diff√©rence de prix entre\nParis et les Hauts de Seine et le reste de la France), une autre variable\nest un effet fixe type de bien. Les deux variables qui pourraient introduire\nde la variabilit√©, √† savoir la surface et, dans une moindre mesure, le\nnombre de lots, ont une importance moindre.\n{{% box status=‚Äúnote‚Äù title=‚ÄúNote‚Äù icon=‚Äúfa fa-comment‚Äù %}}\nId√©alement, on utiliserait yellowbrick pour repr√©senter l‚Äôimportance des variables\nMais en l‚Äô√©tat actuel du pipeline on a beaucoup de variables dont le poids\nest nul qui viennent polluer la visualisation. Vous pouvez\nconsulter la\ndocumentation de yellowbrick sur ce sujet\n{{% /box %}}"
  },
  {
    "objectID": "content/course/modelisation/6_pipeline/index.html#pr√©diction",
    "href": "content/course/modelisation/6_pipeline/index.html#pr√©diction",
    "title": "15¬† Premier pas vers l‚Äôindustrialisation avec les pipelines scikit",
    "section": "18.4 Pr√©diction",
    "text": "18.4 Pr√©diction\nL‚Äôanalyse de l‚Äôimportance de variables permet de mieux comprendre\nle fonctionnement interne des random forests.\nOn obtient un mod√®le dont les performances sont les suivantes:\n\nfrom sklearn.metrics import mean_squared_error\n\n\ncompar = pd.DataFrame([y_test, pipe.predict(X_test)]).T\ncompar.columns = ['obs','pred']\ncompar['diff'] = compar.obs - compar.pred\n\nprint(\"Le RMSE sur le jeu de test est {:,}\".format(\n   int(np.sqrt(mean_squared_error(y_test, pipe.predict(X_test))))\n))\n\nLe RMSE n‚Äôest pas tr√®s bon. Pour comprendre pourquoi, repr√©sentons\nnotre nuage de point des valeurs observ√©es et pr√©dites:\nC‚Äôest tr√®s d√©cevant. La pr√©diction a trop peu de variabilit√© pour capturer\nla variance des prix observ√©e. Cela vient du fait que les variables\nayant de l‚Äôimportance dans la pr√©diction sont principalement des effets fixes,\nqui ne permettent donc qu‚Äôune variabilit√© limit√©e.\n\ng.figure.get_figure()"
  },
  {
    "objectID": "content/course/modelisation/6_pipeline/index.html#remarque-sur-la-performance",
    "href": "content/course/modelisation/6_pipeline/index.html#remarque-sur-la-performance",
    "title": "15¬† Premier pas vers l‚Äôindustrialisation avec les pipelines scikit",
    "section": "19.1 Remarque sur la performance",
    "text": "19.1 Remarque sur la performance\nLes estimations sont, par d√©faut, men√©es de mani√®re s√©quentielle (l‚Äôune apr√®s\nl‚Äôautre). Nous sommes cependant face √† un probl√®me\nembarassingly parallel.\nPour gagner en performance, il est recommand√© d‚Äôutiliser l‚Äôargument\nn_jobs=-1."
  },
  {
    "objectID": "content/course/modelisation/6_pipeline/index.html#remarque-sur-l√©chantillonnage",
    "href": "content/course/modelisation/6_pipeline/index.html#remarque-sur-l√©chantillonnage",
    "title": "15¬† Premier pas vers l‚Äôindustrialisation avec les pipelines scikit",
    "section": "19.2 Remarque sur l‚Äô√©chantillonnage",
    "text": "19.2 Remarque sur l‚Äô√©chantillonnage\nEn l‚Äô√©tat actuel de l‚Äô√©chantillonnage entre train et test au sein de la\ngrid search,\non est face √† un probl√®me de data leaking car l‚Äô√©chantillon\nn‚Äôest pas balanc√© entre nos classes (les d√©partements).\nCertaines classes se\nretrouvent hors de l‚Äô√©chantillon d‚Äôestimation mais dans l‚Äô√©chantillon de pr√©diction.\nAutrement dit, notre pipeline de preprocessing se retrouve √† devoir\nnettoyer des valeurs qu‚Äôil ne conna√Æt pas.\nNous avons choisi une option, dans notre pipeline pour se faciliter la vie\n√† ce propos. Nous ne rencontrons pas d‚Äôerreur car nous avons utilis√© l‚Äôoption\nhandle_unknown = \"ignore\" plut√¥t que\nhandle_unknown = \"error\" (d√©faut) dans le one hot encoding.\nCette option est dangereuse et n‚Äôest pas recommand√©e pour un vrai pipeline.\nDe mani√®re g√©n√©rale, il vaut mieux adopter une approche de\nprogrammation d√©fensive en n‚Äôh√©sitant pas √† renvoyer une erreur si la\nstructure du DataFrame de pr√©diction diff√®re vraiment de celle du DataFrame\nd‚Äôentra√Ænement.\nPour √©viter cette erreur, il serait mieux de d√©finir explicitement le sch√©ma de\nvalidation crois√©e √† mettre en oeuvre.\nPr√©c√©demment, nous avions utilis√© un √©chantillonnage stratifi√©.\nCela pourrait √™tre fait ici avec\nla m√©thode StratifiedGroupKFold (plus d‚Äô√©l√©ments √† venir)\nfrom sklearn.model_selection import StratifiedGroupKFold\ncv = StratifiedGroupKFold(n_splits=5)\n#grid_search.fit(pd.concat((X_train, X_test), axis=0), np.concatenate([y_train,y_test]), cv = cv, groups = pd.concat((X_train, X_test), axis=0)['dep'])"
  },
  {
    "objectID": "content/course/modelisation/6_pipeline/index.html#pr√©alable-quelques-m√©thodes-pour-gagner-en-flexibilit√©-dans-le-preprocessing",
    "href": "content/course/modelisation/6_pipeline/index.html#pr√©alable-quelques-m√©thodes-pour-gagner-en-flexibilit√©-dans-le-preprocessing",
    "title": "15¬† Premier pas vers l‚Äôindustrialisation avec les pipelines scikit",
    "section": "21.1 Pr√©alable : quelques m√©thodes pour gagner en flexibilit√© dans le preprocessing",
    "text": "21.1 Pr√©alable : quelques m√©thodes pour gagner en flexibilit√© dans le preprocessing\nNotre DataFrame comporte des types h√©t√©rog√®nes de variables:\n\nDes variables num√©riques dont les variances sont tr√®s h√©t√©rog√®nes\nDes variables textuelles qui m√©riteraient un recodage sous forme num√©rique\nDes variables discr√®tes dont les modalit√©s devraient √™tre √©clat√©es (one hot encoding)\n\nPour gagner en flexibilit√©, nous allons proposer certaines m√©thodes qui permettent\nd‚Äôappliquer les √©tapes de preprocessing ad√©quates √† un sous-ensemble de\nvariables5. Ces m√©thodes ne sont plus n√©cessaires dans les versions\nr√©centes de scikit.\nPour cela, il convient d‚Äôadopter l‚Äôapproche de la programmation orient√©e objet.\nOn va cr√©er des classes avec des m√©thodes transform et fit_transform\nqui pourront ainsi √™tre int√©gr√©es directement dans les pipelines, comme s‚Äôil\ns‚Äôagissait de m√©thodes issues de scikit.\nLa premi√®re g√©n√©ralise LabelEncoder √† un sous-ensemble de colonnes:\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nclass MultiColumnLabelEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)\n\nLa seconde g√©n√©ralise cette fois le one hot encoding √† un sous ensemble de\nfonctions\n\nclass MultiColumnOneHotEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                output[col] = OneHotEncoder(sparse=False).fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = OneHotEncoder(sparse=False).fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)\n\nLes m√©thodes suivantes vont nous permettre de passer en arguments les noms\nde colonnes pour int√©grer la r√©cup√©ration des bonnes colonnes de nos\ndataframes dans le pipeline:\n\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n\nclass Columns(BaseEstimator, TransformerMixin):\n    def __init__(self, names=None):\n        self.names = names\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X):\n        return X[self.names]\n\nclass Normalize(BaseEstimator, TransformerMixin):\n    def __init__(self, func=None, func_param={}):\n        self.func = func\n        self.func_param = func_param\n\n    def transform(self, X):\n        if self.func != None:\n            return self.func(X, **self.func_param)\n        else:\n            return X\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\nEnfin, on va cr√©er une m√©thode interm√©diaire sous forme de hack\n(elle prend une matrice en entr√©e et renvoie la m√™me matrice)\npour\npouvoir facilement r√©cup√©rer notre matrice de feature afin de v√©rifier\nses caract√©ristiques (notamment le nombre de colonnes disponibles):\n\nclass Collect(BaseEstimator, TransformerMixin):\n\n    def transform(self, X):\n        #print(X.shape)\n        #self.shape = shape\n        # what other output you want\n        return X\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n\nfrom sklearn.pipeline import make_pipeline, FeatureUnion\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\n\npipe2 = Pipeline([\n    (\"features\", FeatureUnion([\n        ('numeric', make_pipeline(Columns(names=numeric_features[:-1]),StandardScaler())),\n        ('categorical', make_pipeline(Columns(names=categorical_features),OneHotEncoder(sparse=False)))\n    ])),\n    ('identity', Collect()),\n    ('randomforest', regr)\n])\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('numeric', StandardScaler(), numeric_features[:-1]),\n        ('categorical', OneHotEncoder(sparse=False, handle_unknown = \"ignore\"), categorical_features)])\n\npipe3 = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('randomforest', regr)])"
  },
  {
    "objectID": "content/course/modelisation/5_clustering/index.html",
    "href": "content/course/modelisation/5_clustering/index.html",
    "title": "16¬† Clustering",
    "section": "",
    "text": "17 Introduction sur le clustering\nJusqu‚Äô√† pr√©sent, nous avons fait de l‚Äôapprentissage supervis√© puisque nous\nconnaissions la vraie valeur de la variable √† expliquer/pr√©dire (y). Ce n‚Äôest plus le cas avec\nl‚Äôapprentissage non supervis√©.\nLe clustering est un champ d‚Äôapplication de l‚Äôapprentissage non-supervis√©.\nIl s‚Äôagit d‚Äôexploiter l‚Äôinformation disponible pour regrouper des observations\nqui se ressemblent.\nL‚Äôobjectif est de cr√©er des clusters d‚Äôobservations pour lesquels :\nEn Machine Learning, les m√©thodes de clustering sont tr√®s utilis√©es pour\nfaire de la recommandation. En faisant, par exemple, des classes homog√®nes de\nconsommateurs, il est plus facile d‚Äôidentifier et cibler des comportements\npropres √† chaque classe de consommateurs.\nCes m√©thodes ont √©galement un int√©r√™t en √©conomie et sciences sociales parce qu‚Äôelles permettent\nde regrouper des observations sans a priori et ainsi interpr√©ter une variable\nd‚Äôint√©r√™t √† l‚Äôaune de ces r√©sultats. Cette publication sur la s√©gr√©gation spatiale utilisant des donn√©es de t√©l√©phonie mobile\nutilise par exemple cette approche.\nLes m√©thodes de clustering peuvent aussi intervenir en amont d‚Äôun probl√®me de classification (dans des\nprobl√®mes d‚Äôapprentissage semi-supervis√©).\nLe manuel Hands-on machine learning with scikit-learn, Keras et TensorFlow pr√©sente dans le\nchapitre d√©di√© √† l‚Äôapprentissage non supervis√© quelques exemples.\nDans certaines bases de donn√©es, on peut se retrouver avec quelques exemples labellis√©s mais la plupart sont\nnon labellis√©s. Les labels ont par exemple √©t√© faits manuellement par des experts.\nPar exemple, supposons que dans la base MNIST des chiffres manuscrits, les chiffres ne soient pas labellis√©s\net que l‚Äôon se demande quelle est la meilleure strat√©gie pour labelliser cette base.\nOn pourrait regarder des images de chiffres manuscrits au hasard de la base et les labelliser.\nLes auteurs du livre montrent qu‚Äôil existe toutefois une meilleure strat√©gie.\nIl vaut mieux appliquer un algorithme de clustering en amont pour regrouper les images ensemble et avoir une\nimage repr√©sentative par groupe, et labelliser ces images repr√©sentatives au lieu de labelliser au hasard.\nLes m√©thodes de clustering sont nombreuses.\nNous allons nous pencher sur la plus intuitive : les k-means."
  },
  {
    "objectID": "content/course/modelisation/5_clustering/index.html#principe",
    "href": "content/course/modelisation/5_clustering/index.html#principe",
    "title": "16¬† Clustering",
    "section": "18.1 Principe",
    "text": "18.1 Principe\nL‚Äôobjectif des k-means est de partitionner l‚Äôespace des observations en trouvant des points (centroids) jouant le r√¥le de centres de gravit√© pour lesquels les observations proches peuvent √™tre regroup√©es dans une classe homog√®ne.\nL‚Äôalgorithme k-means fonctionne par it√©ration, en initialisant les centro√Ødes puis en les mettant √† jour √† chaque\nit√©ration, jusqu‚Äô√† ce que les centro√Ødes se stabilisent. Quelques exemples de clusters issus de la m√©thode k-means :\n\n\n\n Hint\nL‚Äôobjectif des k-means est de trouver une partition des donn√©es \\(S=\\{S_1,...,S_K\\}\\) telle que\n\\[\n\\arg\\min_{S} \\sum_{i=1}^K \\sum_{x \\in S_i} ||x - \\mu_i||^2\n\\]\navec \\(\\mu_i\\) la moyenne des \\(x_i\\) dans l‚Äôensemble de points \\(S_i\\)\n\n\n\n# packages utiles\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nfrom sklearn.cluster import KMeans #pour kmeans\nimport seaborn as sns #pour scatterplots\n\n\n\n Exercice 1 : Principe des k-means\n\nImporter les donn√©es et se restreindre aux variables 'Unemployment_rate_2019', 'Median_Household_Income_2019', 'Percent of adults with less than a high school diploma, 2015-19', \"Percent of adults with a bachelor's degree or higher, 2015-19\" et bien-s√ªr 'per_gop'. Appelez cette base restreinte df2 et enlevez les valeurs manquantes.\nFaire un k-means avec \\(k=4\\).\nCr√©er une variable label dans votes stockant le r√©sultat de la typologie\nAfficher cette typologie sur une carte.\nChoisir les variables Median_Household_Incomme_2019 et Unemployment_rate_2019 et repr√©senter le nuage de points en colorant diff√©remment\nen fonction du label obtenu.\nRepr√©senter la distribution du vote pour chaque cluster\n\n\n\nLa carte obtenue √† la question 4, qui permet de\nrepr√©senter spatialement nos groupes, est\nla suivante:\n\n\n\n\n\nLe nuage de point de la question 5, permettant de repr√©senter\nla relation entre Median_Household_Incomme_2019\net Unemployment_rate_2019, aura l‚Äôaspect suivant:\n\n\n\n\n\nEnfin, l‚Äôhistogramme des votes pour chaque cluster est :\n\n\n\n\n\n\n\n Hint\nIl faut noter plusieurs points sur l‚Äôalgorithme impl√©ment√© par d√©faut par scikit-learn, que l‚Äôon peut lire dans\nla documentation :\n- l‚Äôalgorithme impl√©ment√© par d√©faut est kmeans ++ (cf.¬†param√®tre init). Cela signifie que\nl‚Äôinitialisation des centro√Ødes est faite de mani√®re intelligente pour que les centro√Ødes initiaux soient choisis\nafin de ne pas √™tre trop proches.\n- l‚Äôalgorithme va √™tre d√©marr√© avec n_init centro√Ødes diff√©rents et le mod√®le va choisir la meilleure initialisation\nen fonction de l‚Äôinertia du mod√®le, par d√©faut √©gale √† 10.\nLe mod√®le renvoie les cluster_centers_, les labels labels_, l‚Äôinertia inertia_ et le nombre d‚Äôit√©rations\nn_iter_."
  },
  {
    "objectID": "content/course/modelisation/5_clustering/index.html#choisir-le-nombre-de-clusters",
    "href": "content/course/modelisation/5_clustering/index.html#choisir-le-nombre-de-clusters",
    "title": "16¬† Clustering",
    "section": "18.2 Choisir le nombre de clusters",
    "text": "18.2 Choisir le nombre de clusters\nLe nombre de clusters est fix√© par le mod√©lisateur.\nIl existe plusieurs fa√ßons de fixer ce nombre :\n\nconnaissance a priori du probl√®me ;\nanalyse d‚Äôune m√©trique sp√©cifique pour d√©finir le nombre de clusters √† choisir ;\netc.\n\nIl y a un arbitrage √† faire\nentre biais et variance :\nun trop grand nombre de clusters implique une variance\nintra-cluster tr√®s faible (sur-apprentissage, m√™me s‚Äôil n‚Äôest jamais possible de d√©terminer\nle vrai type d‚Äôune observation puisqu‚Äôon est en apprentissage non supervis√©).\nSans connaissance a priori du nombre de clusters, on peut recourir √† deux familles de m√©thodes :\n\nLa m√©thode du coude (elbow method): On prend le point d‚Äôinflexion de la courbe\nde performance du mod√®le. Cela repr√©sente le moment o√π ajouter un cluster\n(complexit√© croissante du mod√®le) n‚Äôapporte que des gains mod√©r√©s dans la\nmod√©lisation des donn√©es.\nLe score de silhouette : On mesure la similarit√© entre un point et les autres points\ndu cluster par rapport aux autres clusters. Plus sp√©cifiquement :\n\n\nSilhouette value is a measure of how similar an object is to its own cluster\n(cohesion) compared to other clusters (separation). The silhouette ranges\nfrom ‚àí1 to +1, where a high value indicates that the object is\nwell matched to its own cluster and poorly matched to neighboring\nclusters. If most objects have a high value, then the clustering\nconfiguration is appropriate. If many points have a low or negative\nvalue, then the clustering configuration may have too many or too few clusters\nSource: Wikipedia\n\nLe score de silhouette d‚Äôune observation est donc √©gal √†\n(m_nearest_cluster - m_intra_cluster)/max( m_nearest_cluster,m_intra_cluster)\no√π m_intra_cluster est la moyenne des distances de l‚Äôobservation aux observations du m√™me cluster\net m_nearest_cluster est la moyenne des distances de l‚Äôobservation aux observations du cluster le plus proche.\nLe package yellowbrick fournit une extension utile √† scikit pour repr√©senter\nfacilement la performance en clustering.\n\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\n\nvisualizer = KElbowVisualizer(model, k=(2,12))\nvisualizer.fit(df2[xvars])        # Fit the data to the visualizer\n\n/miniconda/envs/python-ENSAE/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/miniconda/envs/python-ENSAE/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/miniconda/envs/python-ENSAE/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/miniconda/envs/python-ENSAE/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/miniconda/envs/python-ENSAE/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/miniconda/envs/python-ENSAE/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/miniconda/envs/python-ENSAE/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/miniconda/envs/python-ENSAE/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/miniconda/envs/python-ENSAE/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/miniconda/envs/python-ENSAE/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\nKElbowVisualizer(ax=&lt;Axes: &gt;, estimator=KMeans(n_clusters=11), k=(2, 12))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KElbowVisualizerKElbowVisualizer(ax=&lt;Axes: &gt;, estimator=KMeans(n_clusters=11), k=(2, 12))estimator: KMeansKMeans(n_clusters=11)KMeansKMeans(n_clusters=11)\n\n\n\n\n\n\n\n&lt;Axes: title={'center': 'Distortion Score Elbow for KMeans Clustering'}, xlabel='k', ylabel='distortion score'&gt;\n\n\n&lt;Figure size 768x528 with 0 Axes&gt;\n\n\n\nPour la m√©thode du coude, la courbe\nde performance du mod√®le marque un coude l√©ger √† \\(k=4\\). Le mod√®le initial\nsemblait donc appropri√©.\nyellowbrick permet √©galement de repr√©senter des silhouettes mais\nl‚Äôinterpr√©tation est moins ais√©e et le co√ªt computationnel plus √©lev√© :\n\nfrom yellowbrick.cluster import SilhouetteVisualizer\n\nfig, ax = plt.subplots(2, 2, figsize=(15,8))\nj=0\nfor i in [3, 4, 6, 10]:\n    j += 1\n    '''\n    Create KMeans instance for different number of clusters\n    '''\n    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)\n    q, mod = divmod(j, 2)\n    '''\n    Create SilhouetteVisualizer instance with KMeans instance\n    Fit the visualizer\n    '''\n    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])\n    ax[q-1][mod].set_title(\"k = \" + str(i))\n    visualizer.fit(df2[xvars])\n\n\nLe score de silhouette offre une repr√©sentation plus riche que la courbe coud√©e.\nSur ce graphique, les barres verticales en rouge et en pointill√© repr√©sentent le score de silhouette\nglobal pour chaque k choisi. On voit par exemple que pour tous les k repr√©sent√©s ici, le\nscore de silhouette se situe entre 0.5 et 0.6 et varie peu.\nEnsuite, pour un k donn√©, on va avoir la repr√©sentation des scores de silhouette de chaque\nobservation, regroup√©es par cluster.\nPar exemple, pour k = 4, ici, on voit bien quatre couleurs diff√©rentes qui sont les 4 clusters mod√©lis√©s.\nLes ordonn√©es sont toutes les observations clusteris√©es et en abscisses on a le score de silhouette de\nchaque observation. Si au sein d‚Äôun cluster, les observations ont un score de silhouette plus faible que le\nscore de silhouette global (ligne verticale en rouge), cela signifie que les observations du clusters sont\ntrop proches des autres clusters.\nGr√¢ce √† cette repr√©sentation, on peut aussi se rendre compte de la taille relative des clusters. Par exemple,\navec k = 3, on voit qu‚Äôon a deux clusters cons√©quents et un plus ‚Äúpetit‚Äù cluster relativement aux deux autres.\nCela peut nous permettre de choisir des clusters de tailles homog√®nes ou non.\nEnfin, quand le score de silhouette est n√©gatif, cela signifie que la moyenne des distances de l‚Äôobservation\naux observations du cluster le plus proche est inf√©rieure √† la moyenne des distances de l‚Äôobservation aux\nobservations de son cluster. Cela signifie que l‚Äôobservation est mal class√©e."
  },
  {
    "objectID": "content/course/modelisation/5_clustering/index.html#autres-m√©thodes-de-clustering",
    "href": "content/course/modelisation/5_clustering/index.html#autres-m√©thodes-de-clustering",
    "title": "16¬† Clustering",
    "section": "18.3 Autres m√©thodes de clustering",
    "text": "18.3 Autres m√©thodes de clustering\nIl existe de nombreuses autres m√©thodes de clustering. Parmi les plus connues, on peut citer deux exemples en particulier :\n\nDBSCAN\nles m√©langes de Gaussiennes\n\n\n18.3.1 DBSCAN\nL‚Äôalgorithme DBSCAN est impl√©ment√© dans sklearn.cluster.\nIl peut √™tre utilis√© pour faire de la d√©tection d‚Äôanomalies\nnotamment.\nEn effet, cette m√©thode repose sur le clustering en r√©gions o√π la densit√©\ndes observations est continue, gr√¢ce √† la notion de voisinage selon une certaine distance epsilon.\nPour chaque observation, on va regarder si dans son voisinage selon une distance epsilon, il y a des voisins. S‚Äôil y a au\nmoins min_samples voisins, alors l‚Äôobservation sera une core instance.\nLes observations qui ne sont pas des core instances et qui n‚Äôen ont pas dans leur voisinage selon une distance espilon\nvont √™tre d√©tect√©es comme des anomalies.\n\n\n18.3.2 Les m√©langes de gaussiennes\nEn ce qui concerne la th√©orie, voir le cours Probabilit√©s num√©riques et statistiques computationnelles, M1 Jussieu, V.Lemaire et T.Rebafka\nSe r√©f√©rer notamment aux notebooks pour l‚Äôalgorithme EM pour m√©lange gaussien.\nDans sklearn, les m√©langes gaussiens sont impl√©ment√©s dans sklearn.mixture comme GaussianMixture.\nLes param√®tres importants sont alors le nombre de gaussiennes n_components et le nombre d‚Äôinitiatisations n_init.\nIl est possible de faire de la d√©tection d‚Äôanomalie savec les m√©langes de gaussiennes.\n\n\n Pour aller plus loin\nIl existe de nombreuses autres m√©thodes de clustering :\n\nLocal outlier factor ;\nbayesian gaussian mixture models ;\ndiff√©rentes m√©thodes de clustering hi√©rarchique ;\netc."
  },
  {
    "objectID": "content/course/annexes/corrections/index.html#partie-1-manipuler-des-donn√©es",
    "href": "content/course/annexes/corrections/index.html#partie-1-manipuler-des-donn√©es",
    "title": "17¬† Corrections",
    "section": "17.1 Partie 1: manipuler des donn√©es",
    "text": "17.1 Partie 1: manipuler des donn√©es\nRetour sur numpy\n\n\n\n\n\n\n\n\n\n\n\n\nExercices pandas:\n\n\n\n\n\n\n\n\n\n\n\n\nExercices geopandas:\n\n\n\n\n\n\n\n\n\n\n\n\nWebscraping\n\n\n\n\n\n\n\n\n\n\n\n\nExpressions r√©guli√®res:"
  },
  {
    "objectID": "content/course/annexes/corrections/index.html#partie-2-visualiser-les-donn√©es",
    "href": "content/course/annexes/corrections/index.html#partie-2-visualiser-les-donn√©es",
    "title": "17¬† Corrections",
    "section": "17.2 Partie 2: visualiser les donn√©es",
    "text": "17.2 Partie 2: visualiser les donn√©es"
  },
  {
    "objectID": "content/course/annexes/corrections/index.html#partie-3-mod√©liser",
    "href": "content/course/annexes/corrections/index.html#partie-3-mod√©liser",
    "title": "17¬† Corrections",
    "section": "17.3 Partie 3: mod√©liser",
    "text": "17.3 Partie 3: mod√©liser"
  },
  {
    "objectID": "content/course/annexes/corrections/index.html#partie-4-natural-language-processing-nlp",
    "href": "content/course/annexes/corrections/index.html#partie-4-natural-language-processing-nlp",
    "title": "17¬† Corrections",
    "section": "17.4 Partie 4: Natural Language Processing (NLP)",
    "text": "17.4 Partie 4: Natural Language Processing (NLP)"
  },
  {
    "objectID": "content/course/annexes/corrections/index.html#partie-5-introduction-aux-outils-et-m√©thodes-√†-l√©tat-de-lart",
    "href": "content/course/annexes/corrections/index.html#partie-5-introduction-aux-outils-et-m√©thodes-√†-l√©tat-de-lart",
    "title": "17¬† Corrections",
    "section": "17.5 Partie 5: Introduction aux outils et m√©thodes √† l‚Äô√©tat de l‚Äôart",
    "text": "17.5 Partie 5: Introduction aux outils et m√©thodes √† l‚Äô√©tat de l‚Äôart"
  },
  {
    "objectID": "content/course/git/index.html",
    "href": "content/course/git/index.html",
    "title": "18¬† Git: un outil n√©cessaire pour les data-scientists",
    "section": "",
    "text": "Cette partie du site pr√©sente un √©l√©ment qui n‚Äôest pas propre √†\nPython mais qui est n√©anmoins indispensable : la pratique de Git.\nUne grande partie du contenu de la partie provient du cours\nTravail collaboratif avec R\nou d‚Äôun cours d√©di√© fait avec Romain Avouac.\nLe chapitre de pr√©sentation de Git propose\nune introduction visant √† pr√©senter l‚Äôint√©r√™t d‚Äôutiliser\ncet outil. Une mise en pratique est propos√©e\navec un cadavre exquis.\n\n19 Utilisation de Git avec Python\nGit est √† la fois un outil et un langage. Il\nest donc n√©cessaire d‚Äôinstaller, dans un premier\ntemps Git Bash, puis de connecter\nson outil pr√©f√©r√© pour faire du Python (qu‚Äôil\ns‚Äôagisse de Jupyter, VSCode ou PyCharm).\nL‚Äôun des int√©r√™ts d‚Äôutiliser une approche cloud\nest que l‚Äôutilisateur final n‚Äôa pas √† se pr√©occuper\nde l‚Äôinstallation de ces diff√©rentes briques.\nLes interfaces Git sont parfois d√©j√†\nconfigur√©es pour faciliter l‚Äôusage. C‚Äôest le\ncas sur le SSPCloud."
  },
  {
    "objectID": "content/course/git/introgit/index.html",
    "href": "content/course/git/introgit/index.html",
    "title": "19¬† Git : un √©l√©ment essentiel au quotidien",
    "section": "",
    "text": "20 Pourquoi faire du Git  ?\nDans un projet, il est commun de se demander (ou de demander √† quelqu‚Äôun) :\nIl existe un outil informatique puissant qui r√©pond √† tous ces besoins :\nla gestion de version (version control system (VCS) en anglais). Ses avantages sont incontestables et permettent de facilement :\nEn outre, ces outils fonctionnent avec tous les langages\ninformatiques (texte, R, Python, SAS, `LaTeX, Java, etc.)\ncar reposent sur la comparaison des lignes et des caract√®res des programmes.\nOn peut ainsi r√©sumer les principaux avantages du contr√¥le de version\nde la mani√®re suivante :\nIl existe plusieurs mani√®res d‚Äôutiliser le contr√¥le de version :\nGit a √©t√© con√ßu, initialement pour la ligne de commande. Il existe\nn√©anmoins des interfaces graphiques performantes\net pratiques, notamment lorsqu‚Äôon d√©sire comparer deux versions d‚Äôun m√™me\nfichier c√¥te √† c√¥te. Ces interfaces graphiques couvrent la majorit√© des\nbesoins quotidiens. N√©anmoins, pour certaines t√¢ches, il faut n√©cessairement\npasser par la ligne de commande.\nEn r√©sum√©,\nUn fichier √† ne pas n√©gliger est le .gitignore. Il s‚Äôagit d‚Äôun garde-fou car tous fichiers (notamment des\ndonn√©es, potentiellement volumineuses ou confidentielles) n‚Äôont pas vocation\n√† √™tre partag√©s.\nLe site gitignore.io est tr√®s pratique. Le fichier\nsuivant est par exemple propos√© pour les utilisateurs de Python, auquel on peut ajouter\nquelques lignes adapt√©es aux utilisateurs de donn√©es:"
  },
  {
    "objectID": "content/course/git/introgit/index.html#conserver-et-archiver-du-code",
    "href": "content/course/git/introgit/index.html#conserver-et-archiver-du-code",
    "title": "19¬† Git : un √©l√©ment essentiel au quotidien",
    "section": "20.1 Conserver et archiver du code",
    "text": "20.1 Conserver et archiver du code\nUne des principales fonctionnalit√©s de la gestion de version est de conserver\nl‚Äôensemble des fichiers de fa√ßon s√©curis√©e et de proposer un archivage\nstructur√© des codes. Les fichiers sont stock√©s dans un d√©p√¥t, qui constitue le projet.\nTout repose dans la gestion et la pr√©sentation de l‚Äôhistorique des modifications.\nChaque modification (ajout, suppression ou changement) sur un ou plusieurs fichiers est identifi√©e par son auteur,\nsa date et un bref descriptif1.\nChaque changement est donc unique et ais√©ment identifiable quand les modifications sont class√©es par ordre chronologique. Les groupes de modifications transmis au d√©p√¥t sont appel√©es commit.\nAvec des outils graphiques, on peut v√©rifier l‚Äô\nensemble des √©volutions d‚Äôun fichier (history),\nou l‚Äôhistoire d‚Äôun d√©p√¥t.\nOn peut aussi\nse concentrer sur une modification particuli√®re d‚Äôun fichier ou v√©rifier, pour un fichier, la\nmodification qui a entra√Æn√© l‚Äôapparition de telle ou telle ligne (blame)\nSur son poste de travail, les dizaines (voire centaines) de programmes organis√©s √† la main n‚Äôexistent plus. Tout est regroup√© dans un seul dossier, rassemblant les √©l√©ments du d√©p√¥t. Au sein du d√©p√¥t, tout l‚Äôhistorique est stock√© et accessible rapidement. Si on souhaite travailler sur la derni√®re version des programmes (ou sur une ancienne version sp√©cifique), il n‚Äôy a plus besoin de conserver les autres fichiers car ils sont dans l‚Äôhistorique du projet. Il est alors possible de choisir sur quelle version on veut travailler (la derni√®re commune √† tout le monde, la sienne en train d‚Äô√™tre d√©velopp√©e, celle de l‚Äôann√©e derni√®re, etc.)."
  },
  {
    "objectID": "content/course/git/introgit/index.html#travailler-efficacement-en-√©quipe",
    "href": "content/course/git/introgit/index.html#travailler-efficacement-en-√©quipe",
    "title": "19¬† Git : un √©l√©ment essentiel au quotidien",
    "section": "20.2 Travailler efficacement en √©quipe",
    "text": "20.2 Travailler efficacement en √©quipe\nLe deuxi√®me avantage de la gestion de version repr√©sente une am√©lioration notable du travail en √©quipe sur des codes en commun.\nLa gestion de version permet de collaborer simplement et avec m√©thode. De fa√ßon organis√©e, elle permet de:\n\ntravailler en parall√®le et fusionner facilement du code\npartager une documentation des programmes gr√¢ce :\n\naux commentaires des modifications\n√† la possibilit√© d‚Äôune documentation commune et collaborative\n\ntrouver rapidement des erreurs et en diffuser rapidement la\ncorrection\n\nA ces avantages s‚Äôajoutent les fonctionalit√©s collaboratives des forges\nqui sont des plateformes o√π peuvent √™tre stock√©s des d√©p√¥ts.\nN√©anmoins, ces forges proposent aujourd‚Äôhui beaucoup de fonctionalit√©s\nqui vont au-del√† de l‚Äôarchivage de code:\nint√©ragir via\ndes issues,\nfaire des suggestions de modifications, ex√©cuter du code dans des\nenvironnements normalis√©s, etc.\nIl faut vraiment les voir comme des r√©seaux sociaux du code.\nLes principales plateformes dans ce domaine √©tant Github et Gitlab.\nL‚Äôusage individuel, c‚Äôest-√†-dire seul sur son projet,\npermet aussi de ‚Äútravailler en √©quipe avec soi-m√™me‚Äù car il permet de retrouver des mois plus tard le contenu et le contexte des modifications. Cela est notamment pr√©cieux lors des changements de poste ou des travaux r√©guliers mais espac√©s dans le temps (par exemple, un mois par an chaque ann√©e). M√™me lorsqu‚Äôon travaille tout seul, on collabore avec un moi futur qui peut ne plus se souvenir de la modification des fichiers."
  },
  {
    "objectID": "content/course/git/introgit/index.html#am√©liorer-la-qualit√©-des-codes",
    "href": "content/course/git/introgit/index.html#am√©liorer-la-qualit√©-des-codes",
    "title": "19¬† Git : un √©l√©ment essentiel au quotidien",
    "section": "20.3 Am√©liorer la qualit√© des codes",
    "text": "20.3 Am√©liorer la qualit√© des codes\nLe fonctionnement de la gestion de version, reposant sur l‚Äôarchivage structur√© des modifications et les commentaires les accompagnant, renforce la qualit√© des programmes informatiques. Ils sont plus document√©s, plus riches et mieux structur√©s. C‚Äôest pour cette raison que le contr√¥le de version ne doit pas √™tre consid√©r√© comme un outil r√©serv√© √† des d√©veloppeurs : toute personne travaillant sur des programmes informatiques, gagne √† utiliser du contr√¥le de version.\nLes services d‚Äôint√©gration continue permettent de faire des tests automatiques\nde programmes informatiques, notamment de packages, qui renforcent la\nreplicabilit√© des programmes. Mettre en place des m√©thodes de travail fond√©es\nsur l‚Äôint√©gration continue rend les programmes plus robustes en for√ßant\nceux-ci √† tourner sur des machines autres que celles du d√©veloppeur du code."
  },
  {
    "objectID": "content/course/git/introgit/index.html#simplifier-la-communication-autour-dun-projet",
    "href": "content/course/git/introgit/index.html#simplifier-la-communication-autour-dun-projet",
    "title": "19¬† Git : un √©l√©ment essentiel au quotidien",
    "section": "20.4 Simplifier la communication autour d‚Äôun projet",
    "text": "20.4 Simplifier la communication autour d‚Äôun projet\nLes sites de d√©p√¥ts Github et Gitlab permettent de faire beaucoup plus\nque seulement archiver des codes. Les fonctionalit√©s de d√©ploiement\nen continu permettent ainsi de :\n\ncr√©er des sites web pour valoriser des projets (par exemple les sites\nreadthedocs en python)\nd√©ployer de la documentation en continu\nrendre visible la qualit√© d‚Äôun projet avec des services de code coverage,\nde tests automatiques ou d‚Äôenvironnements int√©gr√©s de travail (binder, etc.)\nqu‚Äôon rend g√©n√©ralement visible au moyen de badges\n(exemple ici )"
  },
  {
    "objectID": "content/course/git/introgit/index.html#copies-de-travail-et-d√©p√¥t-collectif",
    "href": "content/course/git/introgit/index.html#copies-de-travail-et-d√©p√¥t-collectif",
    "title": "19¬† Git : un √©l√©ment essentiel au quotidien",
    "section": "22.1 Copies de travail et d√©p√¥t collectif",
    "text": "22.1 Copies de travail et d√©p√¥t collectif\nGit est un syst√®me d√©centralis√© et asynchrone de gestion de version.\nCela signifie que:\n\nChaque membre d‚Äôun projet travail sur une copie locale du d√©p√¥t\n(syst√®me decentralis√©). Cette copie de travail s‚Äôappelle un clone.\nCela signifie qu‚Äôon n‚Äôa pas une coh√©rence en continue de notre version\nde travail avec le d√©p√¥t ; on peut tr√®s bien ne jamais vouloir les\nmettre en coh√©rence (par exemple, si on teste une piste qui s‚Äôav√®re\ninfructueuse) ;\nC‚Äôest lorsqu‚Äôon propose la publication de modifications sur le d√©p√¥t\ncollectif qu‚Äôon doit s‚Äôassurer de la coh√©rence avec la version disponible\nen ligne (syst√®me asynchrone).\n\nLe d√©p√¥t distant est g√©n√©ralement stock√© sur\nune forge logicielle (Github ou Gitlab) et sert √† centraliser la version\ncollective d‚Äôun projet. Les copies locales sont des copies de travail\nqu‚Äôon utilise pour faire √©voluer un projet:\n\nIl est tout √† fait possible de faire du contr√¥le de version sans\nmettre en place de d√©p√¥t distant. Cependant,\n\nc‚Äôest dangereux puisque le d√©p√¥t distant fait office de sauvegarde\nd‚Äôun projet. Sans d√©p√¥t distant, on peut tout perdre en cas de probl√®me\nsur la copie locale de travail ;\nc‚Äôest d√©sirer √™tre moins efficace car, comme nous allons le montrer, les\nfonctionalit√©s des plateformes Github et Gitlab sont √©galement tr√®s\nb√©n√©fiques lorsqu‚Äôon travaille tout seul."
  },
  {
    "objectID": "content/course/git/introgit/index.html#principe",
    "href": "content/course/git/introgit/index.html#principe",
    "title": "19¬† Git : un √©l√©ment essentiel au quotidien",
    "section": "22.2 Principe",
    "text": "22.2 Principe\nLes trois manipulations les plus courantes sont les suivantes et repr√©sent√©es sur le diagramme ci-apr√®s :\n\ncommit : je valide les modifications que j‚Äôai faites en local avec un message qui les expliquent\npull : je r√©cup√®re la derni√®re version des codes du d√©p√¥t distant\npush : je transmets mes modifications valid√©es au d√©p√¥t distant\n\n\nLes deux derni√®res manipulations correspondent aux interactions (notamment\nla mise en coh√©rence) avec\nle d√©p√¥t commun alors que la premi√®re manipulation commit correspond √†\nla modification des fichiers faite pour faire √©voluer un projet.\nDe mani√®re plus pr√©cise, il y a trois √©tapes avant d‚Äôenvoyer les modifications valid√©es (commit) au d√©p√¥t. Elles se d√©finissent en fonction des commandes qui permet de les appliquer quand Git est utilis√© en ligne de commandes :\n\ndiff : inspection des modifications. Cela permet de comparer les fichiers modifi√©s et de distinguer les fichiers ajout√©s ou supprim√©s\nstaging area : s√©lection des modifications.\ncommit : validation des modifications s√©lectionn√©es (avec commentaire).\n\n\nLors des √©tapes de push et pull, des conflits peuvent appara√Ætre, par exemple lorsque deux personnes ont modifi√© le m√™me programme simultan√©ment. Le terme conflit peut faire peur mais en fait c‚Äôest\nl‚Äôun des apports principaux de Git que de faciliter √©norm√©ment la gestion\nde versions diff√©rentes. Les exercices du chapitre suivant l‚Äôillustreront."
  },
  {
    "objectID": "content/course/git/introgit/index.html#les-branches",
    "href": "content/course/git/introgit/index.html#les-branches",
    "title": "19¬† Git : un √©l√©ment essentiel au quotidien",
    "section": "22.3 Les branches",
    "text": "22.3 Les branches\nC‚Äôest une des fonctionnalit√©s les plus pratiques de la gestion de version.\nLa cr√©ation de branches dans un projet (qui devient ainsi un arbre)\npermet de d√©velopper en parall√®le des correctifs ou une nouvelle fonctionnalit√©\nsans modifier le d√©p√¥t commun.\nCela permet de s√©parer le nouveau d√©veloppement et de faire cohabiter plusieurs versions, pouvant √©voluer s√©par√©ment ou pouvant √™tre facilement rassembl√©es. Git est optimis√© pour le travail sur les branches.\nDans un projet collaboratif, une branche dite master joue le r√¥le du tronc.\nC‚Äôest autour d‚Äôelle que vont pousser ou se greffer les branches.\nL‚Äôun des avantages de Git est qu‚Äôon peut toujours revenir en arri√®re. Ce\nfilet de s√©curit√© permet d‚Äôoser des exp√©rimentations, y compris au sein\nd‚Äôune branche. Il faut √™tre pr√™t √† aller dans la ligne de commande pour cela\nmais c‚Äôest extr√™mement confortable.\n{{% box status=‚Äúhint‚Äù title=‚ÄúHint‚Äù icon=‚Äúfa fa-lightbulb‚Äù %}}\nComment nommer les branches ? L√† encore, il y a √©norm√©ment de conventions diff√©rentes. Une fr√©quemment observ√©e est :\n\npour les nouvelles fonctionnalit√©s : feature/nouvelle-fonctionnalite o√π nouvelle-fontionnalite est un nom court r√©sumant la fonctionnalit√©\npour les corrections de bug : issue-num o√π num est le num√©ro de l‚Äôissue\n\nN‚Äôh√©sitez pas √† aller encore plus loin dans la normalisation !\n{{% /box %}}"
  },
  {
    "objectID": "content/course/git/exogit/index.html",
    "href": "content/course/git/exogit/index.html",
    "title": "20¬† Un cadavre exquis pour d√©couvrir Git",
    "section": "",
    "text": "21 Configuration du compte Github\nA ce stade, nous avons configur√© Git pour √™tre en mesure\nde s‚Äôauthentifier automatiquement et nous avons clon√© le d√©p√¥t pour avoir une\ncopie locale de travail.\nOn n‚Äôa encore ajout√© aucun fichier √† Git. D‚Äôailleurs, la premi√®re\nchose √† faire est d‚Äôexclure un certain nombre de fichiers, afin de ne pas\nfaire une erreur p√©nible √† r√©parer.\n{{% box status=‚Äúexercise‚Äù title=‚ÄúExercice‚Äù icon=‚Äúfas fa-pencil-alt‚Äù %}}\nExercice 3 : Le fichier .gitignore\nLorsqu‚Äôon utilise Git, il y a des fichiers qu‚Äôon ne veut pas partager\nou dont on ne veut pas suivre les modifications (typiquement les grosses bases de donn√©es).\nC‚Äôest le fichier .gitignore\nqui g√®re les fichiers exclus du contr√¥le de version.\n:one: Cr√©er un fichier nomm√© .gitignore (:warning: ne pas changer\nce nom, et s‚Äôassurer que celui-ci n‚Äôa pas d‚Äôextension) via le bloc note ou votre IDE.\nSur la session Jupyter d‚ÄôOnyxia, apr√®s vous √™tre assur√©s que vous vous situez bien dans le dossier  de l‚Äôarborescence, vous pouvez faire : File &gt; New &gt; Text file. Un fichier untitled.txt se cr√©e, que vous pouvez renommer en faisant un mv untitled.txt .gitignore dans le terminal.\n:two: Aller sur le site https://www.toptal.com/developers/gitignore. Vous pouvez\ndans la barre de recherche taper Python, Pycharm, JupyterNotebooks.\nCopier-coller dans votre .gitignore le contenu de la page.\n:three: Quand on cr√©e de la documentation, on veut exclure les extensions .pdf\net .html qui sont des r√©sultats √† partager et non des fichiers source √†\nsuivre. Pour cela, ajouter au d√©but du fichier .gitignore, les extensions:\n:four: Quand on fait de l‚Äôanalyse de donn√©es, on peut se retrouver avec des\nfichiers sources de donn√©es (par exemple des csv). On d√©sire g√©n√©ralement\nles exclure pour deux raisons:\nPour cela, ajouter au d√©but du fichier .gitignore, les extensions suivantes\nCette suite d‚Äôextensions est √† enrichir selon vos projets et les formats\nde donn√©es que vous utilisez.\n:five: On a aussi les checkpoints sont cr√©√©s que l‚Äôon ne d√©sire pas utiliser. Pour les exclure,\najouter la ligne suivante dans le fichier .gitignore\n{{% /box %}}\nOn a cr√©√© un fichier .gitignore mais on n‚Äôa encore rien fait jusqu‚Äô√† pr√©sent.\nEn effet, si en ligne de commande, on tape :\nPuis :\non voit appara√Ætre le r√©sultat suivant\nLe fichier .gitignore est Untracked ce qui signifie qu‚Äôil n‚Äôest pas\nencore contr√¥l√©.\nIl faut dire √† Git de contr√¥ler les √©volutions de chaque fichier\n(passage dans l‚Äôindex). On appelle cette √©tape git add.\n{{% box status=‚Äúexercise‚Äù title=‚ÄúExercice‚Äù icon=‚Äúfas fa-pencil-alt‚Äù %}}\nExercice 4 : Indexer des modifications\n:one: Se rendre dans l‚Äôextension Git de Jupyter. Vous devriez\nretrouver un cadre ayant cet aspect\n:two: En passant votre souris au dessus du .gitignore, vous devriez voir\nun + appara√Ætre. Cliquez dessus.\nSi vous aviez privil√©gi√© la ligne de commande, ce que vous avez fait est √©quivalent √† :\nPour se rem√©morer ce que signifie git add, vous pouvez vous rendre\nsur mon cours d√©di√© √† Git.\n:three: Observer le changement de statut du fichier .gitignore. Il est\nd√©sormais dans la partie Staged\nEn gros, vous venez de dire √† Git que vous allez rendre publique une √©volution\ndu fichier.\nSi vous √©tiez en ligne de commande vous auriez ce r√©sultat apr√®s un git status\nLes nouvelles modifications (en\nl‚Äôoccurrence la cr√©ation du fichier et la validation de son contenu actuel)\nne sont pas encore archiv√©es. Pour cela, il va falloir faire un\ncommit (on rend publique une modification)\n:four: Avant cela, regardons les modifications qu‚Äôon va prochainement\nvalider. Pour cela, passez la souris au dessus du nom du fichier\n.gitignore et cliquer sur le bouton Diff this file (+ -).\nUne page s‚Äôouvre et met en regard la version ant√©rieure avec\nles ajouts en vert et les suppressions en rouge. Nous retrouverons\ncette visualisation avec l‚Äôinterface Github, plus tard.\nEn l‚Äôoccurrence, comme le fichier n‚Äôexistait pas, normalement nous n‚Äôavons que\ndes ajouts.\nIl est √©galement possible d‚Äôeffectuer cela avec la ligne de commande mais c‚Äôest\nbeaucoup moins pratique. Pour cela, la commande √† appeler est git diff et\nil est n√©cessaire d‚Äôutiliser l‚Äôoption cached pour lui dire d‚Äôinspecter les\nfichiers pour lesquels on n‚Äôa pas encore effectu√© de commit. En vert\nappara√Ætront les modifications et en rouge les suppressions mais, cette fois,\nles r√©sultats ne seront pas mis c√¥te-√†-c√¥te ce qui est beaucoup moins\npratique.\n{{% /box %}}\nIl est temps de valider notre modification. Cette op√©ration\ns‚Äôappelle commit en langage Git et, comme son nom l‚Äôindique, il\ns‚Äôagit d‚Äôune proposition de modification sur laquelle, en quelques\nsortes, on s‚Äôengage.\nUn commit comporte un titre et √©ventuellement une description. A ces\ninformations, Git ajoutera automatiquement quelques √©l√©ments\nsuppl√©mentaires, notamment l‚Äôauteur du commit (pour identifier la personne\nayant propos√© cette modification) et l‚Äôhorodatage (pour identifier le moment\no√π cette modification a √©t√© propos√©e). Ces informations permettront d‚Äôidentifier\nde mani√®re unique le commit auquel sera ajout√© un identifiant al√©atoire\nunique (un num√©ro SHA) qui permettra de faire r√©f√©rence √† celui-ci sans\nambigu√Øt√©.\nLe titre est important car il s‚Äôagit, pour un humain, du point d‚Äôentr√©e\ndans l‚Äôhistoire d‚Äôun d√©p√¥t (voir par exemple\nl‚Äôhistoire du d√©p√¥t du cours.\nLes titres vagues\n(Mise √† jour du fichier, Update‚Ä¶) sont √† bannir car ils\nn√©cessiteront un effort inutile pour comprendre les fichiers modifi√©s.\nN‚Äôoubliez pas que votre premier collaborateur est votre moi futur qui,\ndans quelques semaines, ne se souviendra pas en quoi consistait\nle commit Update du 12 janvier et en quoi il se distingue du\nUpdate du 13 mars.\n{{% box status=‚Äúexercise‚Äù title=‚ÄúExercice‚Äù icon=‚Äúfas fa-pencil-alt‚Äù %}}\nExercice 5 : Premier commit (enfin !)\nTout se passe dans la partie inf√©rieure de l‚Äôinterface graphique.\n:one: Entrer le titre Initial commit et ajouter une description\nCr√©ation du fichier .gitignore : tada : (sans les espaces autour des :).\n: tada : (sans les espaces) sera converti en emoji :tada: par Github quand on voudra\nafficher la description du commit2.\nLe fait de nommer le premier commit ‚ÄúInitial commit‚Äù est une\nhabitude, vous\nn‚Äô√™tes pas oblig√© de suivre cette convention si elle ne vous pla√Æt pas.\n:two: Cliquer sur Commit. Le fichier a disparu de la liste, c‚Äôest normal,\nil n‚Äôa plus de modification √† valider. Pour le retrouver dans la liste\ndes fichiers Changed, il faudra le modifier √† nouveau\n:three: Cliquer sur l‚Äôonglet History. Votre commit appara√Æt √† ce niveau.\nSi vous cliquez dessus, vous obtenez des informations sur le commit\n{{% /box %}}\n{{% box status=‚Äúnote‚Äù title=‚ÄúNote‚Äù icon=‚Äúfa fa-comment‚Äù %}}\nSi vous utilisiez la ligne de commande, la mani√®re √©quivalente de faire\nserait\nL‚Äôoption m permet de cr√©er un message, qui sera disponible √† l‚Äôensemble\ndes contributeurs du projet. Avec la ligne de commande, ce n‚Äôest pas toujours\ntr√®s pratique. Les interfaces graphiques permettent des messages plus\nd√©velopp√©s (la bonne pratique veut qu‚Äôon √©crive un message de commit comme un\nmail succinct : un titre et un peu d‚Äôexplications, si besoin).\n{{% /box %}}\nJusqu‚Äô√† pr√©sent, apr√®s avoir clon√© le d√©p√¥t, on a travaill√© uniquement\nsur notre copie locale. On n‚Äôa pas cherch√© √† interagir √† nouveau\navec Github.\nCependant, il existe bien une connexion entre notre dossier local et\nle d√©p√¥t Github. On peut s‚Äôen assurer en tapant dans un terminal\nLe d√©p√¥t distant s‚Äôappelle remote en langage Git. L‚Äôoption -v (verbose)\npermet de lister le(s) d√©p√¥t(s) distant(s). Le r√©sultat devrait avoir la\nstructure suivante:\nPlusieurs informations sont int√©ressantes dans ce r√©sultat. D‚Äôabord on\nretrouve bien l‚Äôurl qu‚Äôon avait renseign√© √† Git lors de l‚Äôop√©ration\nde clonage. Ensuite, on remarque un terme origin. C‚Äôest un alias\npour l‚Äôurl qui suit. Cela √©vite d‚Äôavoir, √† chaque fois, √† taper l‚Äôensemble\nde l‚Äôurl, ce qui peut √™tre p√©nible et source d‚Äôerreur.\nfetch et push\nsont l√† pour nous indiquer qu‚Äôon r√©cup√®re (fetch) des modifications\nd‚Äôorigin mais qu‚Äôon envoie √©galement (push) des modifications vers\ncelui-ci. G√©n√©ralement, les url de ces deux d√©p√¥ts sont les m√™mes mais cela peut\narriver, lorsqu‚Äôon contribue √† des projets opensource qu‚Äôon n‚Äôa pas cr√©√©,\nqu‚Äôils diff√®rent3.\nAu d√©but d‚Äôune t√¢che particuli√®re ou d‚Äôun projet, il est recommand√© d‚Äôouvrir des issues. Prenant la forme d‚Äôun espace de discussion, elles correpondront √† la fin √† des nouvelles fonctionnalit√©s (en anglais, features). Les issues permettent √©galement de signaler des bugs constat√©s, de se les r√©partir et d‚Äôindiquer s‚Äôils sont r√©gl√©s ou s‚Äôils ont avanc√©s. Une utilisation intensive des issues, avec des labels ad√©quats, peut\nm√™me amener √† se passer d‚Äôoutils de gestion de projets comme Trello.\nLa branche master est la branche principale. Elle se doit d‚Äô√™tre ‚Äúpropre‚Äù. Si on veut √™tre rigoureux, on ne pousse pas des travaux non aboutis sur master.\nIl est possible de pousser directement sur master dans le cas de petites corrections, de modifications mineures dont vous √™tes certains qu‚Äôelles vont fonctionner. Mais sachez que dans le cadre de projets sensibles, c‚Äôest strictement interdit. N‚Äôayez pas peur de fixer comme r√®gle l‚Äôinterdiction de pousser sur master, cela obligera l‚Äô√©quipe projet √† travailler professionnellement.\nAu moindre doute, cr√©ez une branche. Les branches sont utilis√©es pour des travaux significatifs :\n{{% box status=‚Äúwarning‚Äù title=‚ÄúWarning‚Äù icon=‚Äúfa fa-exclamation-triangle‚Äù %}}\nLes branches ne sont pas personnelles : Toutes les branches sont publi√©es, le rebase est interdit. Le push force est √©galement interdit.\nIl faut absolument bannir les usages de push force qui peuvent d√©stabiliser les copies locales des collaborateurs. S‚Äôil est n√©cessaire de faire un push force, c‚Äôest qu‚Äôil y a un probl√®me dans la branche, √† identifier et r√©gler sans faire push force.\nTous les merges dans master doivent se faire par l‚Äôinterm√©diaire d‚Äôune pull request dans Github. En effet, il est tr√®s d√©conseill√© de merger une branche dans master localement.\n{{% /box %}}\n{{% box status=‚Äúexercise‚Äù title=‚ÄúExercice‚Äù icon=‚Äúfas fa-pencil-alt‚Äù %}}\nExercice 8: Cr√©er une nouvelle branche et l‚Äôint√©grer dans master\n:one: Ouvrir une issue sur Github. Signaler qu‚Äôil serait bien d‚Äôajouter un emoji chat dans le README. Dans la partie de droite, cliquer sur la petite roue √† c√¥t√© de Label et cliquer sur Edit Labels. Cr√©er un label Markdown. Normalement, le label a √©t√© ajout√©.\n:two: Retournez sur votre d√©p√¥t local. Vous allez cr√©er une branche nomm√©e\nissue-1\nAvec l‚Äôinterface graphique de JupyterLab, cliquez sur Current Branch - Master\npuis sur le bouton New Branch. Rentrez issue-1 comme nom de branche\n(la branche doit √™tre cr√©√©e depuis master, ce qui est normalement le choix\npar d√©faut) et cliquez sur Create Branch\nSi vous n‚Äôutilisez pas l‚Äôinterface graphique mais la ligne de commande, la\nmani√®re √©quivalente de faire est4\n:three: Ouvrez README.md et ajoutez un emoji chat (:cat:) √† la suite du titre.\nFaites un commit en refaisant les √©tapes vues dans les exercices\npr√©c√©dents. N‚Äôoubliez pas, cela se fait en deux √©tapes:\nSi vous passez par la ligne de commande, cela donnera:\n:four: Faire un deuxi√®me commit pour ajouter un emoji koala (:koala:) puis\npousser les modifications locales.\nCela peut √™tre fait avec l‚Äôinterface\nde JupyterLab gr√¢ce au bouton avec une fl√™che montante (il doit appara√Ætre\nen orange maintenant).\nSinon, si vous utilisez la ligne de commande, vous devrez taper\n:five: Dans Github, devrait appara√Ætre\nCliquer sur Compare & Pull Request. Donner un titre informatif √† votre pull request.\nDans le message en dessous, taper\nLe tiret est une petite astuce pour que Github\nremplace le num√©ro de l‚Äôissue par le titre.\nCliquez sur Create Pull Request mais\nne validez pas la fusion, on le fera dans un second temps.\nLe fait d‚Äôavoir mis un message close suivi d‚Äôun num√©ro d‚Äôissue #1\npermettra de fermer automatiquement l‚Äôissue 1 lorsque vous ferez le merge.\nEn attendant, vous avez cr√©√© un lien entre l‚Äôissue et la pull request\nAu passage, vous pouvez ajouter le label Markdown sur la droite.\n:six: En local, retourner sur master. Dans l‚Äôinterface Jupyter, il suffit\nde cliquer sur master dans la liste des branches. Si vous √™tes\nen ligne de commande, il faut faire\ncheckout est une commande Git qui permet de naviguer d‚Äôune branche √† l‚Äôautre\n(voire d‚Äôun commit √† l‚Äôautre).\nAjouter une phrase √† la suite de votre texte dans le README.md\n(ne touchez pas au titre !). Vous pouvez remarquer que les emojis\nne sont pas dans le titre, c‚Äôest normal vous n‚Äôavez pas encore fusionn√© les versions\n:seven: Faire un commit et un push. En ligne de commande, cela donne\n:eight: Sur Github, cliquer sur Insights en haut du d√©p√¥t puis, √† gauche sur Network (cela n‚Äôest\npossible que si vous avez rendu public votre d√©p√¥t).\nVous devriez voir appara√Ætre l‚Äôarborescence de votre d√©p√¥t. On peut voir issue-1 comme une ramification et master comme le tronc.\nL‚Äôobjectif est maintenant de ramener les modifications faites dans issue-1 dans la branche principale. Retournez dans l‚Äôonglet Pull Requests. L√†, changer le type de merge pour Squash and Merge, comme ci-dessous (petit conseil : choisissez toujours cette m√©thode de merge).\nUne fois que cela est fait, vous pouvez retourner dans Insights puis Network pour v√©rifier que tout s‚Äôest bien pass√© comme pr√©vu.\n:nine: Supprimer la branche (branch &gt; delete this branch). Puisqu‚Äôelle est merg√©e, elle ne servira plus. La conserver risque d‚Äôamener √† des push involontaires dessus.\n{{% /box %}}\nL‚Äôoption de fusion Squash and Merge permet de regrouper tous les commits d‚Äôune branche (potentiellement tr√®s nombreux) en un seul dans la branche de destination. Cela √©vite, sur les gros projets, des branches avec des milliers de commits.\nJe recommande de toujours utiliser cette technique et non les autres.\nPour d√©sactiver les autres techniques, vous pouvez aller dans\nSettings et dans la partie Merge button ne conserver coch√©e que la\nm√©thode Allow squash merging\nJusqu‚Äô√† pr√©sent, nous avons d√©couvert les vertus de Git dans un projet\nindividuel. Nous allons maintenant aller plus loin dans un projet\ncollectif."
  },
  {
    "objectID": "content/course/git/exogit/index.html#rappels-sur-la-notion-de-d√©p√¥t-distant",
    "href": "content/course/git/exogit/index.html#rappels-sur-la-notion-de-d√©p√¥t-distant",
    "title": "20¬† Un cadavre exquis pour d√©couvrir Git",
    "section": "21.1 Rappels sur la notion de d√©p√¥t distant",
    "text": "21.1 Rappels sur la notion de d√©p√¥t distant\nPour rappel, comme expliqu√© pr√©c√©demment, il convient de distinguer\nle d√©p√¥t distant (remote) et la copie ou les copies locales (les clones)\nd‚Äôun d√©p√¥t. Le d√©p√¥t distant est g√©n√©ralement stock√© sur une forge\nlogicielle (Github ou Gitlab) et sert √† centraliser la version\ncollective d‚Äôun projet. Les copies locales sont des copies de travail\nqu‚Äôon utilise pour faire √©voluer un projet:\n\nGit est un syst√®me de contr√¥le de version asynchrone c‚Äôest-√†-dire\nqu‚Äôon n‚Äôinteragit pas en continu avec le d√©p√¥t distant (comme c‚Äôest le\ncas dans le syst√®me SVN) mais qu‚Äôil est possible d‚Äôavoir une version\nlocale qui se diff√©rencie du d√©p√¥t commun et qu‚Äôon rend coh√©rente\nde temps en temps.\nBien qu‚Äôil soit possible d‚Äôavoir une utilisation hors-ligne de Git,\nc‚Äôest-√†-dire un pur contr√¥le de version local sans d√©p√¥t\ndistant, cela est une utilisation\nrare et qui comporte un int√©r√™t limite. L‚Äôint√©r√™t de Git est\nd‚Äôoffrir une mani√®re robuste et efficace d‚Äôinteragir avec un\nd√©p√¥t distant facilitant ainsi la collaboration en √©quipe ou en\nsolitaire.\nPour ces exercices, je propose d‚Äôutiliser Github dont les fonctionalit√©s\nnous suffiront amplement1. Si,\ndans le futur, les fonctionnalit√©s ne vous conviennent pas (sans l‚Äôapport de fonctionnalit√©s\nexternes, Github propose moins de fonctionalit√©s que Gitlab) ou si vous √™tes\nmal √† l‚Äôaise concernant le possesseur de Github (Microsoft), vous pourrez utiliser\nGitlab , son concurrent.\nL‚Äôavantage de Github par rapport √† Gitlab est que le premier est plus visible, car\nmieux index√© par Google et concentre, en partie pour des raisons historiques, plus\nde d√©veloppeurs Python et R (ce qui est important dans des domaines comme\nle code o√π les externalit√©s de r√©seau jouent). Le d√©bat Github vs Gitlab n‚Äôa\nplus beaucoup de sens aujourd‚Äôhui car les fonctionnalit√©s ont converg√© (Github\na rattrap√© une partie de son retard sur l‚Äôint√©gration continue) et, de toute\nmani√®re, on peut tout √† fait connecter des d√©p√¥ts Gitlab et Github."
  },
  {
    "objectID": "content/course/git/exogit/index.html#premi√®re-√©tape-cr√©er-un-compte-github",
    "href": "content/course/git/exogit/index.html#premi√®re-√©tape-cr√©er-un-compte-github",
    "title": "20¬† Un cadavre exquis pour d√©couvrir Git",
    "section": "21.2 Premi√®re √©tape: cr√©er un compte Github",
    "text": "21.2 Premi√®re √©tape: cr√©er un compte Github\nLes deux premi√®res √©tapes se font sur Github.\n{{% box status=‚Äúexercise‚Äù title=‚ÄúExercice‚Äù icon=‚Äúfab fa-github‚Äù %}}\nExercice 1 : Cr√©er un compte Github\n\nSi vous n‚Äôen avez pas d√©j√† un, cr√©er un compte sur https://github.com\nCr√©er un d√©p√¥t vide. Cr√©ez ce d√©p√¥t priv√©, cela permettra\ndans l‚Äôexercice 2 d‚Äôactiver notre jeton. Vous pourrez le rendre public\napr√®s l‚Äôexercice 2, c‚Äôest comme vous le souhaitez.\n\n{{% /box %}}"
  },
  {
    "objectID": "content/course/git/exogit/index.html#deuxi√®me-√©tape-cr√©er-un-token-jeton-https",
    "href": "content/course/git/exogit/index.html#deuxi√®me-√©tape-cr√©er-un-token-jeton-https",
    "title": "20¬† Un cadavre exquis pour d√©couvrir Git",
    "section": "21.3 Deuxi√®me √©tape: cr√©er un token (jeton) HTTPS",
    "text": "21.3 Deuxi√®me √©tape: cr√©er un token (jeton) HTTPS"
  },
  {
    "objectID": "content/course/git/exogit/index.html#principe",
    "href": "content/course/git/exogit/index.html#principe",
    "title": "20¬† Un cadavre exquis pour d√©couvrir Git",
    "section": "21.4 Principe",
    "text": "21.4 Principe\nGit est un syst√®me d√©centralis√© de contr√¥le de version :\nles codes sont modifi√©s par chaque personne sur son poste de travail,\npuis sont mis en conformit√© avec la version collective disponible\nsur le d√©p√¥t distant au moment o√π le contributeur le d√©cide.\nIl est donc n√©cessaire que la forge connaisse l‚Äôidentit√© de chacun des\ncontributeurs, afin de d√©terminer qui est l‚Äôauteur d‚Äôune modification apport√©e\naux codes stock√©s dans le d√©p√¥t distant.\nPour que Github reconnaisse un utilisateur proposant des modifications,\nil est n√©cessaire de s‚Äôauthentifier (un d√©p√¥t distant, m√™me public, ne peut pas √™tre modifi√© par n‚Äôimporte qui). L‚Äôauthentification consiste ainsi √† fournir un √©l√©ment que seul vous et la forge sont cens√©s conna√Ætre : un mot de passe, une cl√© compliqu√©e, un jeton d‚Äôacc√®s‚Ä¶\nPlus pr√©cis√©ment, il existe deux modalit√©s pour faire conna√Ætre son identit√© √† Github :\n\nune authentification HTTPS (d√©crite ici) : l‚Äôauthentification se fait avec un login et un mot de passe (qu‚Äôil faut renseigner √† chaque interaction avec le d√©p√¥t), ou avec un token (m√©thode √† privil√©gier).\nune authentification SSH : l‚Äôauthentification se fait par une cl√© crypt√©e disponible sur le poste de travail et que GitHub ou GitLab conna√Æt. Une fois configur√©e, cette m√©thode ne n√©cessite plus de faire conna√Ætre son identit√© : l‚Äôempreinte digitale que constitue la cl√© suffit √† reconna√Ætre un utilisateur.\n\nLa documentation collaborative utilitR pr√©sente les raisons pour lesquelles il convient de favoriser\nla m√©thode HTTPS sur la m√©thode SSH.\nDepuis ao√ªt 2021, Github n‚Äôautorise plus l‚Äôauthentification par mot de passe\nlorsqu‚Äôon interagit (pull/push) avec un d√©p√¥t distant\n(raisons ici).\nIl est n√©cessaire d‚Äôutiliser un token (jeton d‚Äôacc√®s) qui pr√©sente l‚Äôavantage\nd‚Äô√™tre r√©voquable (on peut √† tout moment supprimer un jeton si, par exemple,\non suspecte qu‚Äôil a √©t√© diffus√© par erreur) et √† droits limit√©s\n(le jeton permet certaines op√©rations standards mais\nn‚Äôautorise pas certaines op√©rations d√©terminantes comme la suppression\nd‚Äôun d√©p√¥t).\n{{% box status=‚Äúnote‚Äù title=‚ÄúNote‚Äù icon=‚Äúfa fa-comment‚Äù %}}\nIl est important de ne jamais stocker un token, et encore moins son mot de passe, dans un projet.\nIl est possible de stocker un mot de passe ou token de mani√®re s√©curis√©e et durable\navec le credential helper de Git. Celui-ci est pr√©sent√© par la suite.\nS‚Äôil n‚Äôest pas possible d‚Äôutiliser le credential helper de Git, un mot de passe\nou token peut √™tre stock√© de mani√®re s√©curis√© dans\nun syst√®me de gestion de mot de passe comme Keepass.\nNe jamais stocker un jeton Github, ou pire un mot de passe, dans un fichier\ntexte non crypt√©. Les logiciels de gestion de mot de passe\n(comme Keepass, recommand√© par l‚ÄôAnssi)\nsont simples\nd‚Äôusage et permettent de ne conserver sur l‚Äôordinateur qu‚Äôune version\nhash√©e du mot de passe qui ne peut √™tre d√©crypt√©e qu‚Äôavec un mot de passe\nconnu de vous seuls.\n{{% /box %}}"
  },
  {
    "objectID": "content/course/git/exogit/index.html#cr√©er-un-jeton",
    "href": "content/course/git/exogit/index.html#cr√©er-un-jeton",
    "title": "20¬† Un cadavre exquis pour d√©couvrir Git",
    "section": "21.5 Cr√©er un jeton",
    "text": "21.5 Cr√©er un jeton\nLa documentation officielle comporte un certain nombre de captures d‚Äô√©cran expliquant\ncomme proc√©der.\nNous allons utiliser le credential helper associ√© √† Git pour stocker\nce jeton. Ce credential helper permet de conserver de mani√®re p√©renne\nun jeton (on peut aussi faire en sorte que le mot de passe soit automatiquement\nsupprim√© de la m√©moire de l‚Äôordinateur au bout, par\nexemple, d‚Äôune heure).\nL‚Äôinconv√©nient de cette m√©thode est que Git √©crit en clair le jeton dans\nun fichier de configuration. C‚Äôest pour cette raison qu‚Äôon utilise des jetons\npuisque, si ces derniers sont r√©v√©l√©s, on peut toujours les r√©voquer et √©viter\nles probl√®mes (pour ne pas stocker en clair un jeton il faudrait utiliser\nune librairie suppl√©mentaire comme libsecrets qui est au-del√† du programme\nde ce cours).\nMa recommandation,\nsi vous d√©sirez conserver de mani√®re plus durable ou plus s√©curis√©e votre jeton\n(en ne conservant pas le jeton en clair mais de mani√®re hash√©e),\nest d‚Äôutiliser un gestionnaire de mot de passe comme\nKeepass (recommand√© par l‚ÄôAnssi).\n{{% box status=‚Äúexercise‚Äù title=‚ÄúExercice‚Äù icon=‚Äúfas fa-pencil-alt‚Äù %}}\nExercice 2 : Cr√©er et stocker un token\n:one: Suivre la\ndocumentation officielle en ne donnant que les droits repo au jeton (ajouter les droits\nworkflow si vous d√©sirez que votre jeton soit utilisable pour des projets\no√π l‚Äôint√©gration continue est n√©cessaire)\nPour r√©sumer les √©tapes devraient √™tre les suivantes:\nSettings &gt; Developers Settings &gt; Personal Access Token &gt; Generate a new token &gt; ‚ÄúMy bash script‚Äù &gt; Expiration ‚Äú30 days‚Äù &gt; cocher juste ‚Äúrepo‚Äù &gt; Generate token &gt; Le copier\n:two: Ouvrir un terminal depuis Jupyter (par exemple File &gt; New &gt; Terminal).\n:three: [Optionnel] Taper dans le terminal la commande\nqui convient selon votre syst√®me d‚Äôexploitation pour activer le\ncredential helper:\n# Sous mac et linux et le datalab\ngit config --global credential.helper store\n\n# Sous windows\ngit config --global credential.helper manager-core\n:four: R√©cup√©rer, sur la page d‚Äôaccueil de votre d√©p√¥t, l‚Äôurl du d√©p√¥t distant.\nIl prend la forme suivante\nhttps://github.com/&lt;username&gt;/&lt;reponame&gt;.git\nVous pouvez utiliser l‚Äôicone √† droite pour copier l‚Äôurl.\n:five: Retournez dans le terminal Jupyter. Taper\ngit clone repo_url\no√π repo_url est l‚Äôurl du d√©p√¥t en question (vous pouvez utiliser\nMAJ+Inser pour coller l‚Äôurl pr√©c√©demment copi√©)\nTapez Entr√©e. Dans le cas d‚Äôun r√©pertoire priv√© et sans credential helper, renseignez ensuite votre identifiant, faites Entr√©e, puis votre personal access token, Entr√©e. Si vous n‚Äôavez pas d‚Äôerreur, cela signifie\nque l‚Äôauthentification a bien fonctionn√© et donc que tout va\nbien. Sinon, il vous suffit de r√©√©crire l‚Äôinstruction git clone et de retenter de taper votre personal access token. Normalement, si vous avez cr√©√© un d√©p√¥t vide dans l‚Äôexercice 1,\nvous avez un message de Git:\n\nwarning: You appear to have cloned an empty repository.\n\nCeci est normal, ce n‚Äôest pas une erreur. Le dossier de votre projet a bien\n√©t√© cr√©√©.\nSi vous avez une erreur, suivez la consigne pr√©sent√©e ci-apr√®s\npour r√©initialiser\nvotre credential helper\n:six: Si vous le d√©sirez, vous pouvez changer la visibilit√© de votre d√©p√¥t\nen le rendant public.\n{{% /box %}}\n{{% box status=‚Äúnote‚Äù title=‚ÄúNote‚Äù icon=‚Äúfa fa-comment‚Äù %}}\nSi vous avez fait une faute de frappe dans le mot de passe ou dans le jeton, il est possible de vider la m√©moire\nde la mani√®re suivante, sous Mac ou Linux :\ngit config --global --unset credential.helper\nSous Windows, si vous avez utilis√© l‚Äôoption manager-core √©voqu√©e ci-dessus, vous pouvez utiliser une interface graphique pour effacer le mot de passe ou jeton erron√©. Pour cela, dans le menu d√©marrer, taper Gestionnaire d'identification (ou Credential Manager si Windows ne trouve pas). Dans l‚Äôinterface graphique qui s‚Äôouvre, il est possible de supprimer le mot de passe ou jeton en question. Apr√®s cela, vous devriez √† nouveau avoir l‚Äôopportunit√© de taper un mot de passe ou jeton lors d‚Äôune authentification HTTPS.\n{{% /box %}}"
  },
  {
    "objectID": "content/course/git/exogit/index.html#envoyer-des-modifications-sur-le-d√©p√¥t-distant-push",
    "href": "content/course/git/exogit/index.html#envoyer-des-modifications-sur-le-d√©p√¥t-distant-push",
    "title": "20¬† Un cadavre exquis pour d√©couvrir Git",
    "section": "23.1 Envoyer des modifications sur le d√©p√¥t distant: push",
    "text": "23.1 Envoyer des modifications sur le d√©p√¥t distant: push\n{{% box status=‚Äúexercise‚Äù title=‚ÄúExercice‚Äù icon=‚Äúfas fa-pencil-alt‚Äù %}}\nExercice 6 : Interagir avec Github\nIl convient maintenant d‚Äôenvoyer les fichiers sur le d√©p√¥t distant.\n\n:one:\nL‚Äôobjectif est d‚Äôenvoyer vos modifications vers origin.\nOn va passer par la ligne de commande car les boutons push/pull\nde l‚Äôextension Jupyter ne fonctionnent pas de mani√®re syst√©matique.\nTaper\ngit push origin master\nCela signifie: ‚Äúgit envoie (push) mes modifications sur la\nbranche master (la branche sur laquelle on a travaill√©, on reviendra\ndessus) vers mon d√©p√¥t (alias\norigin)‚Äù\nNormalement, si vous avez utilis√© le credential helper, Git ne\nvous demande pas vos identifiants de connexion. Sinon,\nil faut taper\nvotre identifiant github et votre mot de passe correspond au personal access token nouvellement cr√©√© !\n:two: Retournez voir le d√©p√¥t sur Github, vous devriez maintenant voir le fichier\n.gitignore s‚Äôafficher en page d‚Äôaccueil.\n{{% /box %}}"
  },
  {
    "objectID": "content/course/git/exogit/index.html#la-fonctionnalit√©-pull",
    "href": "content/course/git/exogit/index.html#la-fonctionnalit√©-pull",
    "title": "20¬† Un cadavre exquis pour d√©couvrir Git",
    "section": "23.2 La fonctionnalit√© pull",
    "text": "23.2 La fonctionnalit√© pull\nLa deuxi√®me mani√®re d‚Äôinteragir avec le d√©p√¥t est de r√©cup√©rer des\nr√©sultats disponibles en ligne sur sa copie de travail. On appelle\ncela pull.\nPour le moment, vous √™tes tout seul sur le d√©p√¥t. Il n‚Äôy a donc pas de\npartenaire pour modifier un fichier dans le d√©p√¥t distant. On va simuler ce\ncas en utilisant l‚Äôinterface graphique de Github pour modifier\ndes fichiers. On rappatriera les r√©sultats en local dans un deuxi√®me temps.\n{{% box status=‚Äúexercise‚Äù title=‚ÄúExercice‚Äù icon=‚Äúfas fa-pencil-alt‚Äù %}}\nExercice 7 : Rapatrier des modifs en local\n:one: Se rendre sur votre d√©p√¥t depuis l‚Äôinterface https://github.com.\n2 mani√®res de faire √† ce niveau :\n\nCliquer sur Add file &gt; Create new file et appeler le fichier README.md\nCliquer sur le bouton ADD A README qui est affich√© sur la page d‚Äôaccueil.\nSupprimez tout autre texte si Github vous a sugg√©r√© un contenu pour le\nREADME\n\n:two: L‚Äôobjectif est de\ndonner au README.md un titre en ajoutant, au d√©but du document, la ligne suivante :\n# Mon oeuvre d'art surr√©aliste \nSautez une ligne et entrez le texte que vous d√©sirez, sans ponctuation. Par exemple,\nle ch√™ne un jour dit au roseau\n:three: Cliquez sur l‚Äôonglet Preview pour voir le texte mis en forme au format Markdown\n:four: R√©diger un titre et un message compl√©mentaire pour faire le commit. Conserver\nl‚Äôoption par d√©faut Commit directly to the master branch\n:five: Editer √† nouveau le README en cliquant sur le crayon juste au dessus\nde l‚Äôaffichage du contenu du README.\nAjouter une deuxi√®me phrase et corrigez la\nponctuation de la premi√®re. Ecrire un message de commit et valider.\nLe Ch√™ne un jour dit au roseau :\nVous avez bien sujet d'accuser la Nature\n:six: Au dessus de l‚Äôaborescence des fichiers, vous devriez voir s‚Äôafficher le\ntitre du dernier commit. Vous pouvez cliquer dessus pour voir la modification\nque vous avez faite.\n:seven: Les r√©sultats sont sur le d√©p√¥t distant mais ne sont pas sur votre\ndossier de travail dans Jupyter. Il faut re-synchroniser votre copie locale\navec le d√©p√¥t distant :\n\nAvec l‚Äôinterface Jupyter, si cela est possible, appuyez tout simplement sur la petite fl√®che vers le bas, qui est celle qui a d√©sormais la pastille orange.\nSi cette fl√®che n‚Äôest pas disponible ou si vous travaillez dans un autre\nenvironnement, vous pouvez utiliser la ligne de\ncommande et taper\n\ngit pull origin master\nCela signifie : ‚Äúgit r√©cup√®re (pull) les modifications sur la\nbranche master vers mon d√©p√¥t (alias\norigin)‚Äù\n:eight: Regarder, sur JupyterLab, l‚Äôonglet History. Cliquez sur le\ndernier commit et affichez les changements sur le fichier. Vous pouvez\nremarquer la finesse du contr√¥le de version : Git d√©tecte au sein de\nla premi√®re ligne de votre texte que vous avez mis des majuscules\nou de la ponctuation.\n{{% /box %}}\nL‚Äôop√©ration pull permet :\n\nA votre syst√®me local de v√©rifier les modifications sur le d√©p√¥t distant\nque vous n‚Äôauriez pas faites (cette op√©ration s‚Äôappelle fetch)\nDe les fusionner s‚Äôil n‚Äôy a pas de conflit de version ou si les conflits de\nversion sont automatiquement fusionnables (deux modifications d‚Äôun fichier mais\nqui ne portent pas sur le m√™me emplacement)."
  },
  {
    "objectID": "content/course/git/exogit/index.html#le-workflow-adopt√©",
    "href": "content/course/git/exogit/index.html#le-workflow-adopt√©",
    "title": "20¬† Un cadavre exquis pour d√©couvrir Git",
    "section": "25.1 Le workflow adopt√©",
    "text": "25.1 Le workflow adopt√©\nNous allons adopter le mode de travail le plus simple, le Github Flow.\nIl correspond √† cette forme caract√©ristique d‚Äôarbre:\n\nLa branche master constitue le tronc\nLes branches partent de master et divergent\nLorsque les modifications aboutissent, elles sont int√©gr√©es √† master ;\nla branche en question dispara√Æt:\n\n\nIl existe des workflows plus complexes, notamment le Git Flow que j‚Äôutilise\npour d√©velopper ce cours. Ce tutoriel, tr√®s bien fait,\nillustre avec un graphique la complexit√© accrue de ce flow:\n\nCette fois, une branche interm√©diaire, par exemple une branche development\nint√®gre des modifications √† tester avant de les int√©grer dans la version\nofficielle (master).\n{{% box status=‚Äúhint‚Äù title=‚ÄúHint‚Äù icon=‚Äúfa fa-lightbulb‚Äù %}}\nVous pourrez trouvez des dizaines d‚Äôarticles et d‚Äôouvrages sur ce sujet dont chacun pr√©tend avoir trouv√© la meilleure organisation du travail (Git flow, GitHub flow, GitLab flow‚Ä¶). Ne lisez pas trop ces livres et articles sinon vous serez perdus (un peu comme avec les magazines destin√©s aux jeunes parents‚Ä¶).\nLa m√©thode de travail la plus simple est le Github flow qu‚Äôon vous a propos√© d‚Äôadopter. L‚Äôarborescence est reconnaissable: des branches divergent et reviennent syst√©matiquement vers master.\nPour des projets plus complexes dans des √©quipes d√©veloppant des applications, on pourra utiliser d‚Äôautres m√©thodes de travail, notamment le Git flow. Il n‚Äôexiste pas de r√®gles universelles pour d√©terminer la m√©thode de travail ; l‚Äôimportant c‚Äôest, avant tout, de se mettre d‚Äôaccord sur des r√®gles communes de travail avec votre √©quipe.\n{{% /box %}}"
  },
  {
    "objectID": "content/course/git/exogit/index.html#m√©thode-pour-les-merges",
    "href": "content/course/git/exogit/index.html#m√©thode-pour-les-merges",
    "title": "20¬† Un cadavre exquis pour d√©couvrir Git",
    "section": "25.2 M√©thode pour les merges",
    "text": "25.2 M√©thode pour les merges\nLes merges vers master doivent imp√©rativement passer par Github (ou Gitlab). Cela permet de garder une trace explicite de ceux-ci (par exemple ici), sans avoir √† chercher dans l‚Äôarborescence, parfois complexe, d‚Äôun projet.\nLa bonne pratique veut qu‚Äôon fasse un squash commit pour √©viter une inflation du nombre de commits dans master: les branches ont vocation √† proposer une multitude de petits commits, les modifications dans master doivent √™tre simples √† tracer d‚Äôo√π le fait de modifier des petits bouts de code.\nComme on l‚Äôa fait dans un exercice pr√©c√©dent, il est tr√®s pratique d‚Äôajouter dans le corps du message close #xx o√π xx est le num√©ro d‚Äôune issue associ√©e √† la pull request. Lorsque la pull request sera fusionn√©e, l‚Äôissue sera automatiquement ferm√©e et un lien sera cr√©√© entre l‚Äôissue et la pull request. Cela vous permettra de comprendre, plusieurs mois ou ann√©es plus tard comment et pourquoi telle ou telle fonctionnalit√© a √©t√© impl√©ment√©e.\nEn revanche, l‚Äôint√©gration des derni√®res modifications de master vers une branche se fait en local. Si votre branche est en conflit, le conflit doit √™tre r√©solu dans la branche et pas dans master.\nmaster doit toujours rester propre."
  },
  {
    "objectID": "content/course/git/exogit/index.html#mise-en-pratique",
    "href": "content/course/git/exogit/index.html#mise-en-pratique",
    "title": "20¬† Un cadavre exquis pour d√©couvrir Git",
    "section": "25.3 Mise en pratique",
    "text": "25.3 Mise en pratique\n{{% box status=‚Äúexercise‚Äù title=‚ÄúExercice‚Äù icon=‚Äúfas fa-pencil-alt‚Äù %}}\nExercice 9 : Interactions avec le d√©p√¥t distant\nCet exercice se fait par groupe de trois ou quatre. Il y aura deux r√¥les dans ce sc√©nario :\n\nUne personne aura la responsabilit√© d‚Äô√™tre mainteneur\nDeux √† trois personnes seront d√©veloppeurs.\n\n:one: Le mainteneur cr√©e un d√©p√¥t sur Github. Il/Elle donne des droits au(x) d√©veloppeur(s) du projet (Settings &gt; Manage Access &gt; Invite a collaborator).\n:two: Chaque membre du projet, cr√©e une copie locale du projet gr√¢ce √† la commande git clone ou\navec le bouton Clone a repository de JupyterLab.\nPour cela, r√©cup√©rer l‚Äôurl HTTPS du d√©p√¥t en copiant l‚Äôurl du d√©p√¥t que vous pouvez trouver, par exemple, dans la page d‚Äôaccueil du d√©p√¥t, en dessous de Quick setup ‚Äî if you‚Äôve done this kind of thing before\nEn ligne de commande, cela donnera:\ngit clone https://github.com/&lt;username&gt;/&lt;reponame&gt;.git\n:three: Chaque membre du projet cr√©e un fichier avec son nom et son pr√©nom, selon cette structure nom-prenom.md en √©vitant les caract√®res sp√©ciaux. Il √©crit dedans trois phrases de son choix sans ponctuation ni majuscules (pour pouvoir effectuer une correction ult√©rieurement). Enfin, il commit sur le projet.\nPour rappel, en ligne de commande cela donnera les commandes suivantes √† modifier\ngit add nom-prenom.md\ngit commit -m \"C'est l'histoire de XXXXX\"\n:four: Chacun essaie d‚Äôenvoyer (push) ses modifications locales sur le d√©p√¥t:\ngit push origin master\n:five: A ce stade, une seule personne (la plus rapide) devrait ne pas avoir rencontr√© de rejet du push. C‚Äôest normal, avant d‚Äôaccepter une modification Git v√©rifie en premier lieu la coh√©rence de la branche avec le d√©p√¥t distant. Le premier ayant fait un push a modifi√© le d√©p√¥t commun ; les autres doivent int√©grer ces modifications dans leur version locale (pull) avant d‚Äôavoir le droit de proposer un changement.\nPour celui/celle/ceux dont le push a √©t√© refus√©, faire\ngit pull origin master\npour ramener les modifications distantes en local.\n:six: Taper git log et regarder la mani√®re dont a √©t√© int√©gr√© la modification de votre camarade ayant pu faire son push\nVous remarquerez que les commits de vos camarades sont int√©gr√©s tels quels √†\nl‚Äôhistoire du d√©p√¥t.\n:seven: Faire √† nouveau\ngit pull origin master\nLe dernier doit refaire, √† nouveau, les √©tapes 5 √† 7 (dans une √©quipe de quatre\nil faudra encore le refaire une fois).\n{{% /box %}}\n{{% box status=‚Äúwarning‚Äù title=‚ÄúWarning √† nouveau: ne JAMAIS FAIRE git push force‚Äù icon=‚Äúfa fa-exclamation-triangle‚Äù %}}\nQuand on fait face √† un rejet du push, on est tent√© de faire passer en force le push malgr√© la mise en garde pr√©c√©dente.\nIl faut imm√©diatement oublier cette solution, elle cr√©e de nombreux probl√®mes et, en fait, ne r√©sout rien. L‚Äôun des risques est de r√©√©crire enti√®rement l‚Äôhistorique rendant les copies locales, et donc les modifications de vos collaborateurs, caduques. Cela vous vaudra, √† raison, des remontrances de vos partenaires qui perdent le b√©n√©fice de leur historique Git qui, s‚Äôils ont des versions sans push depuis longtemps peuvent avoir diverger fortement du d√©p√¥t ma√Ætre.\n{{% /box %}}\n{{% box status=‚Äúexercise‚Äù title=‚ÄúExercice‚Äù icon=‚Äúfas fa-pencil-alt‚Äù %}}\nExercice 10 : G√©rer les conflits quand on travaille sur le m√™me fichier\nDans la continuit√© de l‚Äôexercice pr√©c√©dent, chaque personne va travailler sur les fichiers des autres membres de l‚Äô√©quipe.\n:one: Les deux ou trois d√©veloppeurs ajoutent la ponctuation et les majuscules du fichier du premier d√©veloppeur.\n:two: Ils sautent une ligne et ajoutent une phrase (pas tous la m√™me).\n:three: Valider les r√©sultats (git add . et commit) et faire un push\n:four: La personne la plus rapide n‚Äôa, normalement, rencontr√© aucune difficult√© (elle peut s‚Äôarr√™ter temporairement pour regarder ce qui va se passer chez les voisins). Les autres voient leur push refus√© et doivent faire un pull.\n:boom: Il y a conflit, ce qui doit √™tre signal√© par un message du type:\nAuto-merging XXXXXX\nCONFLICT (content): Merge conflict in XXXXXX.md\nAutomatic merge failed; fix conflicts and then commit the result.\n:five: Etudier le r√©sultat de git status\n:six: Si vous ouvrez les fichiers incrimin√©s, vous devriez voir des balises du type\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nthis is some content to mess with\ncontent to append\n=======\ntotally different content to merge later\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; new_branch_to_merge_later\n:seven: Corriger √† la main les fichiers en choisissant, pour chaque ligne, la version qui vous convient et en retirant les balises. Valider en faisant:\ngit add . && git commit -m \"R√©solution du conflit par XXXX\"\nRemplacer XXXX par votre nom. La balise && permet d‚Äôencha√Æner, en une seule ligne de code, les deux commandes.\n:eight: Faire un push. Pour la derni√®re personne, refaire les op√©rations 4 √† 8\n{{% /box %}}\nGit permet donc de travailler, en m√™me temps, sur le m√™me fichier et de limiter le nombre de gestes manuels n√©cessaires pour faire la fusion. Lorsqu‚Äôon travaille sur des bouts diff√©rents du m√™me fichier, on n‚Äôa m√™me pas besoin de faire de modification manuelle, la fusion peut √™tre automatique.\nGit est un outil tr√®s puissant. Mais, il ne remplace pas une bonne organisation du travail. Vous l‚Äôavez vu, ce mode de travail uniquement sur master peut √™tre p√©nible. Les branches prennent tout leur sens dans ce cas.\n{{% box status=‚Äúexercise‚Äù title=‚ÄúExercice‚Äù icon=‚Äúfas fa-pencil-alt‚Äù %}}\nExercice 11 : Gestion des branches\n:one: Le mainteneur va contribuer directement dans master et ne cr√©e pas de branche. Chaque d√©veloppeur cr√©e une branche, en local nomm√©e contrib-XXXXX o√π XXXXX est le pr√©nom:\ngit checkout -b contrib-XXXXX\n:two: Chaque membre du groupe cr√©e un fichier README.md o√π il √©crit une phrase sujet-verbe-compl√©ment. Le mainteneur est le seul √† ajouter un titre dans le README (qu‚Äôil commit dans master).\n:three: Chacun push le produit de son subconscient sur le d√©p√¥t.\n:four: Les d√©veloppeurs ouvrent, chacun, une pull request sur Github de leur branche vers master. Ils lui donnent un titre explicite.\n:five: Dans la discussion de chaque pull request, le mainteneur demande au d√©veloppeur d‚Äôint√©grer le titre qu‚Äôil a √©crit.\n:six: Chaque d√©veloppeur, en local, int√®gre cette modification en faisant\n# Pour √™tre s√ªr d'√™tre sur sa propre branche\ngit checkout branche-XXXX\ngit merge master\nR√©gler le conflit et valider (add et commit). Pousser le r√©sultat. Le mainteneur choisit une des pull request et la valide avec l‚Äôoption squash commits. V√©rifier sur la page d‚Äôaccueil le r√©sultat.\n:seven: L‚Äôauteur (si 2 d√©veloppeurs) ou les deux auteurs (si 3 d√©veloppeurs) de la pull request non valid√©e doivent √† nouveau r√©p√©ter l‚Äôop√©ration 6.\n:eight: Une fois le conflit de version r√©gl√© et pouss√©, le mainteneur valide la pull request selon la m√™me proc√©dure que pr√©cedemment.\n:nine: V√©rifier l‚Äôarborescence du d√©p√¥t dans Insights &gt; Network. Votre arbre doit avoir une forme caract√©ristique de ce qu‚Äôon appelle le Github flow:\n\n{{% /box %}}"
  },
  {
    "objectID": "content/course/getting-started/index.html#pourquoi-faire-du-python-pour-lanalyse-de-donn√©es",
    "href": "content/course/getting-started/index.html#pourquoi-faire-du-python-pour-lanalyse-de-donn√©es",
    "title": "21¬† Introduction",
    "section": "21.1 Pourquoi faire du Python pour l‚Äôanalyse de donn√©es ?",
    "text": "21.1 Pourquoi faire du Python pour l‚Äôanalyse de donn√©es ?\nLe succ√®s de scikit-learn et\nde Tensorflow dans la communaut√©\nde la Data-Science ont beaucoup contribu√© √† l‚Äôadoption de Python. Cependant,\nr√©sumer Python √† ces quelques librairies serait r√©ducteur tant il s‚Äôagit\nd‚Äôun v√©ritable couteau-suisse pour les data-scientists,\nles social scientists ou les √©conomistes.\nL‚Äôint√©r√™t de Python pour un data scientist ou data economist\nva au-del√† du champ du Machine Learning.\nComme pour R, l‚Äôint√©r√™t de Python est son r√¥le central dans un\n√©cosyst√®me plus large autour d‚Äôoutils puissants, flexibles et open-source.\nPython concurrence tr√®s bien R dans son domaine de pr√©dilection, √†\nsavoir l‚Äôanalyse statistique sur des bases de donn√©es structur√©es.\nComme dans R, les dataframes sont un concept central de Python.\nPython est n√©anmoins bien plus complet dans certains domaines.\nOutre le Machine Learning,\nPython est mieux adapt√© aux donn√©es volumineuses que\nR. Python est √©galement meilleur que R pour faire\ndu webscraping ou acc√©der √† des donn√©es par le biais d‚ÄôAPI.\nDans le domaine de l‚Äô√©conom√©trie, Python offre\nl‚Äôavantage de la simplicit√© avec un nombre restreint de packages (scikit et\nstatsmodels) permettant d‚Äôavoir des mod√®les tr√®s g√©n√©raux\n(les generalized estimating equations)\nalors qu‚Äôil faut\nchoisir parmi une grande vari√©t√© de packages en R pour obtenir les\nmod√®les √©quivalents. Dans le domaine du Deep Learning, Python √©crase\nla concurrence.\nAu contraire, dans certains domaines, R reste meilleur, m√™me si les\n√©volutions tr√®s r√©centes de certains outils peuvent amener √† r√©viser\nce constant. Historiquement,\nR √©tait tr√®s bien int√©gr√© au langage de publication Markdown ce qui,\npermet la construction de documents reproductibles tr√®s raffin√©s.\nL‚Äô√©mergence r√©cente de Quarto, h√©ritier de R Markdown d√©velopp√© par\nla soci√©t√© Posit permet aux utilisateur de Python de b√©n√©ficier\n√©galement de la richesse de cette approche pour leur langage de pr√©dilection.\nCe site web, √† l‚Äôarborescence relativement complexe, est ainsi\nconstruit gr√¢ce √† cet outil qui permet √† la fois de tester les blocs\nde code pr√©sent√©s mais aussi de produire de mani√®re automatis√©e les\ntableaux et graphiques pr√©sent√©s. S‚Äôil fallait trouver un point faible\n√† Python par rapport √† R dans le domaine de la data-science\nc‚Äôest sur la production de graphiques. matplotlib et seaborn, qui sont\npr√©sent√©s dans la partie visualisation, sont d‚Äôexcellents outils. N√©anmoins,\nggplot2, l‚Äô√©quivalent en R est plus facile de prise en main et\npropose une syntaxe extr√™mement flexible, qu‚Äôil est difficile de ne pas\nappr√©cier. Cependant, l‚Äô√©cosyst√®me de la\nvisualisation de donn√©es est en pleine r√©volution avec le succ√®s\nd‚ÄôObservable qui\nrapproche l‚Äô√©cosyst√®me JavaScript des d√©veloppeurs web\nde la communaut√© des analystes de donn√©es.\nUn des avantages comparatifs de Python par rapport √† d‚Äôautres\nlangages (notamment R et Julia) est sa dynamique,\nce que montre l‚Äôexplosion du nombre de questions\nsur Stack Overflow.\nCependant, il ne s‚Äôagit pas b√™tement d‚Äôenterrer R.\nAu contraire, outre leur logique tr√®s proche,\nles deux langages sont dans une phase de convergence avec des initiatives comme\nreticulate,\nquarto ou\nsnakemake qui\npermettent, de mani√®re diff√©rente, de cr√©er des cha√Ænes de traitement\nm√©langeant R et Python.\nUne autre raison pour laquelle cette gu√©guerre R/Python n‚Äôa pas\nde sens est que les bonnes\npratiques peuvent √™tre transpos√©es de mani√®re presque transparente d‚Äôun\nlangage √† l‚Äôautre. Il s‚Äôagit d‚Äôun point qui est d√©velopp√© plus amplement\ndans le cours plus avanc√© que je donne avec Romain Avouac en derni√®re ann√©e\nd‚ÄôENSAE: ensae-reproductibilite.github.io/website.\nA terme, les data-scientists et chercheurs en sciences sociales ou\n√©conomie utiliseront\nde mani√®re presque indiff√©rente, et en alternance, Python et R. Ce cours\npr√©sentera ainsi r√©guli√®rement des analogies avec R pour aider les\npersonnes d√©couvrant Python, mais connaissant d√©j√† bien R, √†\nmieux comprendre certains messages."
  },
  {
    "objectID": "content/course/getting-started/index.html#objectif-du-cours",
    "href": "content/course/getting-started/index.html#objectif-du-cours",
    "title": "21¬† Introduction",
    "section": "21.2 Objectif du cours",
    "text": "21.2 Objectif du cours\nLe but de ce cours est de rendre autonome sur\nl‚Äôutilisation de Python\ndans un contexte de travail de data scientist ou de\nsocial scientist (√©conomie, sociologie, g√©ographie‚Ä¶).\nAutrement dit,\nil pr√©suppose qu‚Äôon d√©sire faire un usage intense\nde donn√©es dans un cadre statistique rigoureux.\nLa data-science est un ensemble de techniques\nvisant √† donner du sens √† des sources de donn√©es\ndiverses. Selon les organisations,\nles data-scientists peuvent ainsi √™tre √†\nl‚Äôinterface de projets n√©cessitant un\nlarge spectre de comp√©tences\n(analyse\nde donn√©es textuelles, repr√©sentation\ngraphique interactive‚Ä¶),\navoir des interactions avec des profils\ntr√®s diff√©rents (experts m√©tiers,\nd√©veloppeurs, data architect,\ndata engineer‚Ä¶) voire adopter\nun peu tous ces r√¥les.\nLes innovations\nr√©centes de la data-science ne se r√©duisent\nn√©anmoins\npas qu‚Äô√† des d√©couvertes m√©thodologiques.\nLa data-science propose un ensemble de\ntechniques et de m√©thodes de travail\npour r√©duire les co√ªts de passage\nd‚Äôun protype √† une chaine\nde production p√©renne.\nCe cours introduit √† quelques notions\nsur le sujet, notamment les\npipelines scikit, pour adopter\nd√®s l‚Äôapprentissage du langage\nquelques bons r√©flexes.\nJe donne √©galement un cours,\nplus avanc√©,\nsur ce sujet √† l‚ÄôENSAE avec\nRomain Avouac:\nhttps://ensae-reproductibilite.netlify.app/."
  },
  {
    "objectID": "content/course/getting-started/index.html#public-cible",
    "href": "content/course/getting-started/index.html#public-cible",
    "title": "21¬† Introduction",
    "section": "21.3 Public cible",
    "text": "21.3 Public cible\nCe cours ne revient que de mani√®re secondaire\nsur les fondements statistiques ou algorithmiques\nderri√®re certaines des techniques √©voqu√©es.\nNe pas conna√Ætre ces notions n‚Äôemp√™che n√©anmoins pas de comprendre\nle contenu de ce site web. En effet, la facilit√© d‚Äôusage de Python\n√©vite de devoir programmer soi-m√™me un mod√®le, ce qui rend\npossible l‚Äôapplication\nde mod√®les dont on n‚Äôest pas expert. La connaissance des mod√®les sera\nplut√¥t n√©cessaire dans l‚Äôinterpr√©tation des r√©sultats.\nCependant, la facilit√© avec laquelle il est possible de construire des mod√®les complexes\navec Python peut laisser appara√Ætre que conna√Ætre les sp√©cifit√©s de chaque\nmod√®le est inutile. Il\ns‚Äôagirait d‚Äôune grave erreur: m√™me si l‚Äôimpl√©mentation de mod√®les est ais√©e, il\nest n√©cessaire de bien comprendre la structure des donn√©es et leur ad√©quation\navec les hypoth√®ses d‚Äôun mod√®le."
  },
  {
    "objectID": "content/course/getting-started/index.html#reproductibilit√©",
    "href": "content/course/getting-started/index.html#reproductibilit√©",
    "title": "21¬† Introduction",
    "section": "21.4 Reproductibilit√©",
    "text": "21.4 Reproductibilit√©\nCe cours donne une place centrale √†\nla notion de reproductibilit√©. Cette exigence se traduit de diverses\nmani√®res dans cet enseignement, en particulier en insistant sur un\noutil indispensable pour favoriser le partage de codes informatiques,\n√† savoir Git.\nL‚Äôensemble du contenu du site web est reproductible dans des environnements\ninformatiques divers. Il est bien-s√ªr possible de copier-coller les morceaux\nde code pr√©sents dans ce site. Cette m√©thode montrant rapidement ses limites,\nle site pr√©sente un certain nombre de boutons disponibles pour\nouvrir la page sous un format Jupyter Notebook sur divers\npages web:\n\nSur l‚Äôensemble du site web,\nil est possible de cliquer\nsur la petite icone \npour √™tre redirig√© vers le d√©p√¥t Github associ√© √† ce cours.\nUn certain nombre de boutons permettent de transformer chaque\npage web en Jupyter Notebooks s‚Äôil est n√©cessaire de\nvisualiser ou ex√©cuter du code Python.\n\nVoici, par exemple, ces boutons pour le tutoriel Numpy\n\n\n\n\n\n\n\n\n\n\nPour les agents de la fonction publique, ou\nles √©l√®ves des √©coles partenaires, il est recommand√©\nde privil√©gier le bouton SSPCloud qui est\nune infrastructure cloud moderne, puissante et flexible\nd√©velopp√©e par l‚ÄôInsee et accessible √† l‚Äôurl\nhttps://datalab.sspcloud.fr1.\nL‚Äôensemble du contenu de ce site s‚Äôappuie sur des donn√©es\nouvertes, qu‚Äôil s‚Äôagisse de donn√©es fran√ßaises (principalement\nissues de la plateforme\ncentralisatrice data.gouv ou du site\nweb de l‚ÄôInsee) ou de donn√©es\nam√©ricaines. Les r√©sultats sont donc reproductibles pour quelqu‚Äôun\ndisposant d‚Äôun environnement identique."
  },
  {
    "objectID": "content/course/getting-started/index.html#architecture-du-site-web",
    "href": "content/course/getting-started/index.html#architecture-du-site-web",
    "title": "21¬† Introduction",
    "section": "21.5 Architecture du site web",
    "text": "21.5 Architecture du site web\nCe cours pr√©sente\ndes tutoriels et des exercices complets.\nChaque page est structur√©e sous la forme\nd‚Äôun probl√®me concret et pr√©sente la\nd√©marche g√©n√©rique pour r√©soudre ce probl√®me g√©n√©ral.\nVous pouvez naviguer dans l‚Äôarchitecture du site via la table des mati√®res\nou par les liens vers le contenu ant√©rieur ou post√©rieur √† la fin de chaque\npage. Certaines parties, notamment celle consacr√©e √† la mod√©lisation,\nproposent des exemples fil-rouge pour illustrer la d√©marche de mani√®re\nplus extensive."
  },
  {
    "objectID": "content/course/getting-started/index.html#evaluation",
    "href": "content/course/getting-started/index.html#evaluation",
    "title": "21¬† Introduction",
    "section": "21.6 Evaluation",
    "text": "21.6 Evaluation\nLes √©l√®ves de l‚ÄôENSAE valident le cours gr√¢ce √†\nun projet approfondi.\nLes √©l√©ments relatifs √† l‚Äô√©valuation du cours, ainsi qu‚Äôune\nliste des projets d√©j√† effectu√©s, sont disponibles dans la\nSection Evaluation."
  },
  {
    "objectID": "content/course/getting-started/index.html#r√©f√©rences",
    "href": "content/course/getting-started/index.html#r√©f√©rences",
    "title": "21¬† Introduction",
    "section": "21.7 R√©f√©rences",
    "text": "21.7 R√©f√©rences"
  },
  {
    "objectID": "content/course/getting-started/index.html#contenu-g√©n√©ral",
    "href": "content/course/getting-started/index.html#contenu-g√©n√©ral",
    "title": "21¬† Introduction",
    "section": "21.8 Contenu g√©n√©ral",
    "text": "21.8 Contenu g√©n√©ral"
  },
  {
    "objectID": "content/course/getting-started/index.html#el√©ments-suppl√©mentaires",
    "href": "content/course/getting-started/index.html#el√©ments-suppl√©mentaires",
    "title": "21¬† Introduction",
    "section": "21.9 El√©ments suppl√©mentaires",
    "text": "21.9 El√©ments suppl√©mentaires"
  },
  {
    "objectID": "content/course/getting-started/index.html#structuration-de-cette-partie",
    "href": "content/course/getting-started/index.html#structuration-de-cette-partie",
    "title": "21¬† Introduction",
    "section": "21.10 Structuration de cette partie",
    "text": "21.10 Structuration de cette partie"
  },
  {
    "objectID": "content/course/getting-started/04_python_practice/index.html",
    "href": "content/course/getting-started/04_python_practice/index.html",
    "title": "22¬† Bonne pratique de Python",
    "section": "",
    "text": "23 Structure d‚Äôun projet en python\nLa structure basique d‚Äôun projet d√©velopp√© en Python est la suivante, qu‚Äôon peut retrouver dans\nce d√©p√¥t:\nQuelques explications et parall√®les avec les packages R1 :\n2 La structure n√©cessaire des projets n√©cessaire pour pouvoir construire un package R est plus contrainte.\nLes packages devtools, usethis et testthat ont grandement facilit√© l‚Äô√©laboration d‚Äôun package R. A cet √©gard,\nil est recommand√© de lire l‚Äôincontournable livre d‚ÄôHadley Wickham\nPython est un langage tr√®s lisible. Avec un peu d‚Äôeffort sur le nom des objets, sur la gestion\ndes d√©pendances et sur la structure du programme, on peut\ntr√®s bien comprendre un script sans avoir besoin de l‚Äôex√©cuter. La communaut√© Python a abouti √† un certain\nnombre de normes, dites PEP (Python Enhancement Proposal), qui constituent un standard\ndans l‚Äô√©cosyst√®me Python. Les deux normes les plus connues sont\nla norme PEP8 (code) et la norme PEP257 (documentation).\nLa plupart de ces recommandations ne sont pas propres √† Python, on les retrouve aussi dans R\n(cf.¬†ici).\nOn retrouve de nombreux conseils dans cet ouvrage qu‚Äôil est\nrecommand√© de suivre. La suite se concentrera sur des √©l√©ments compl√©mentaires.\nLa documentation d‚Äôune fonction s‚Äôappelle le docstring. Elle prend la forme suivante:\nAvec PyCharm, lorsqu‚Äôon utilise trois guillemets sous la d√©finition d‚Äôune fonction, un template minimal √†\ncompleter est automatiquement g√©n√©r√©. Les normes √† suivre pour que la docstrings soit reconnue par le package\nsphinx sont pr√©sent√©es dans la PEP257. N√©anmoins,\nelles ont √©t√© enrichies par le style de docstrings NumPy qui est plus riche et permet ainsi des documentations\nplus explicites\n(voir ici et\nici).\nSuivre ces canons formels permet une lecture simplifi√©e du code source de la documentation. Mais cela a surtout\nl‚Äôavantage, lors de la g√©n√©ration d‚Äôun package, de permettre une mise en forme automatique des fichiers\nhelp d‚Äôune fonction √† partir de la docstrings. L‚Äôoutil canonique pour ce type de construction automatique est\nsphinx (dont l‚Äô√©quivalent R est Roxygen)\nTester ses fonctions peut appara√Ætre formaliste mais c‚Äôest, en fait, souvent d‚Äôun grand secours car cela permet de\nd√©tecter et corriger des bugs pr√©coces (ou au moins d‚Äô√™tre conscient de leur existence).\nAu-del√† de la correction de bug, cela permet de v√©rifier que\nla fonction produit bien un r√©sultat esp√©r√© dans une exp√©rience contr√¥l√©e.\nEn fait, il existe deux types de tests:\nIci, on va plut√¥t se focaliser sur la notion de test unitaire ; la notion de\ntest d‚Äôint√©gration n√©cessitant d‚Äôavoir une cha√Æne plus compl√®te de fonctions (mais il ne faut\npas la n√©gliger)\nOn peut partir du principe suivant:\nLe fichier tests/context.py sert √† d√©finir le contexte dans lequel le test de la fonction s‚Äôex√©cute, de mani√®re\nisol√©e. On peut adopter le mod√®le suivant, en changeant import monmodule par le nom de module ad√©quat\nChaque fichier du dossier de test\n(par exemple test_basic.py et test_advanced.py) incorpore ensuite la ligne suivante,\nen d√©but de script\nPour automatiser les tests, on peut utiliser le package unittest\n(doc ici). L‚Äôid√©e est que dans un cadre contr√¥l√©\n(on conna√Æt l‚Äôinput et en tant que concepteur de la fonction on conna√Æt l‚Äôoutput ou, a minima\nles propri√©t√©s de l‚Äôoutput) on peut tester la sortie d‚Äôune fonction.\nLa structure canonique de test est la suivante3\n4 Le code √©quivalent avec R serait testthat::expect_equal(fun(3),4)\nParler de codecov\nCe point est ici √©voqu√© en dernier mais, en fait, il est essentiel et m√©rite d‚Äô√™tre une r√©flexion prioritaire.\nTout travail n‚Äôa pas vocation √† √™tre public\nou √† d√©passer le cadre d‚Äôune √©quipe. Cependant, les m√™mes exigences qui s‚Äôappliquent lorsqu‚Äôun code est public m√©ritent\nde s‚Äôappliquer avec un projet personnel. Avant de partager un code avec d‚Äôautres, on le partage avec le ‚Äúfutur moi‚Äù.\nReprendre un code √©crit il y a plusieurs semaines est co√ªteux et m√©rite d‚Äôanticiper en adoptant des bonnes pratiques qui\nrendront quasi-indolore la r√©-appropriation du code.\nL‚Äôint√©gration d‚Äôun projet avec git fiabilise grandement le processus d‚Äô√©criture du code mais aussi, gr√¢ce aux\noutils d‚Äôint√©gration continue, la production de contenu (par exemple des visualisations html ou des rapports\nfinaux √©crits avec markdown). Il est recommand√© d‚Äôimm√©diatement connecter un projet √† git, m√™me avec un\nd√©p√¥t qui aura vocation √† √™tre personnel. Les instructions d‚Äôutilisation de git sont d√©taill√©es ici."
  },
  {
    "objectID": "content/course/getting-started/04_python_practice/index.html#import-des-modules",
    "href": "content/course/getting-started/04_python_practice/index.html#import-des-modules",
    "title": "22¬† Bonne pratique de Python",
    "section": "24.1 Import des modules",
    "text": "24.1 Import des modules\nLes √©l√©ments suivants concernent plut√¥t les scripts finaux, qui appellent de multiples fonctions, que des\nscripts qui d√©finissent des fonctions.\nUn module est un ensemble de fonctions stock√©es dans un fichier .py. Lorsqu‚Äôon √©crit dans un script\nimport modu\nPython commence par chercher le fichier modu.py dans le dossier de travail. Il n‚Äôest donc pas une bonne\nid√©e d‚Äôappeler un fichier du nom d‚Äôun module standard de python, par exemple math.py ou os.py. Si le fichier\nmodu.py n‚Äôest pas trouv√© dans le dossier de travail, Python va chercher dans le chemin et s‚Äôil ne le trouve pas\nretournera une erreur.\nUne fois que modu.py est trouv√©, il sera ex√©cut√© dans un environnement isol√© (reli√© de mani√®re coh√©rente\naux d√©pendances renseign√©es) et le r√©sultat rendu disponible √† l‚Äôinterpr√©teur Python pour un usage\ndans la session via le namespace (espace o√π python associe les noms donn√©s aux objets).\nEn premier lieu, ne jamais utiliser la syntaxe suivante:\n# A NE PAS UTILISER\nfrom modu import *\nx = sqrt(4)  # Is sqrt part of modu? A builtin? Defined above?\nL‚Äôutilisation de la syntaxe import * cr√©√© une ambiguit√© sur les fonctions disponibles dans l‚Äôenvironnement. Le code\nest ainsi moins clair, moins compartiment√© et ainsi moins robuste. La syntaxe √† privil√©gier est la suivante:\nimport modu\nx = modu.sqrt(4)  # Is sqrt part of modu? A builtin? Defined above?"
  },
  {
    "objectID": "content/course/getting-started/04_python_practice/index.html#structuration-du-code",
    "href": "content/course/getting-started/04_python_practice/index.html#structuration-du-code",
    "title": "22¬† Bonne pratique de Python",
    "section": "24.2 Structuration du code",
    "text": "24.2 Structuration du code\nIl est commun de trouver sur internet des codes tr√®s longs, g√©n√©ralement dans un fichier __init__.py\n(m√©thode pour passer d‚Äôun module √† un package, qui est un ensemble plus structur√© de fonctions).\nContrairement √† la l√©gende, avoir des scripts longs est peu d√©sirable et est m√™me mauvais ;\ncela rend le code difficilement √† s‚Äôapproprier et √† faire √©voluer. Mieux vaut avoir des scripts relativement courts\n(sans l‚Äô√™tre √† l‚Äôexc√®s‚Ä¶) qui font √©ventuellement appels √† des fonctions d√©finies dans d‚Äôautres scripts.\nPour la m√™me raison, la multiplication de conditions logiques if‚Ä¶else if‚Ä¶else est g√©n√©ralement tr√®s mauvais\nsigne (on parle de code spaghetti) ; mieux vaut\nutiliser des m√©thodes g√©n√©riques dans ce type de circonstances."
  },
  {
    "objectID": "content/course/getting-started/04_python_practice/index.html#ecrire-des-fonctions",
    "href": "content/course/getting-started/04_python_practice/index.html#ecrire-des-fonctions",
    "title": "22¬† Bonne pratique de Python",
    "section": "24.3 Ecrire des fonctions",
    "text": "24.3 Ecrire des fonctions\nLes fonctions sont un objet central en Python.\nLa fonction id√©ale est une fonction qui agit de mani√®re compartiment√©e:\nelle prend un certain nombre d‚Äôinputs et est reli√©e au monde ext√©rieur uniquement par les d√©pendances,\nelle effectue des op√©rations sans interaction avec le monde ext√©rieur et retourne un r√©sultat.\nCette d√©finition assez consensuelle masque un certain nombre d‚Äôenjeux:\n\nUne bonne gestion des d√©pendances n√©cessite d‚Äôavoir appliqu√© les recommandations √©voqu√©es pr√©c√©demment\nIsoler du monde ext√©rieur n√©cessite de ne pas faire appel √† un objet ext√©rieur √† l‚Äôenvironnement de la fonction.\nAutrement dit, aucun objet hors de la port√©e (scope) de la fonction ne doit √™tre alt√©r√© ou utilis√©.\n\nPar exemple, le script suivant est mauvais au sens o√π il utilise un objet y hors du scope de la fonction add\ndef add(x):\n    return x + y\nIl faudrait revoir la fonction pour y ajouter un √©l√©ment y:\ndef add(x, y):\n    return x + y\nPycharm offre des outils de diagnostics tr√®s pratiques pour d√©tecter et corriger ce type d‚Äôerreur."
  },
  {
    "objectID": "content/course/getting-started/04_python_practice/index.html#warning-aux-arguments-optionnels",
    "href": "content/course/getting-started/04_python_practice/index.html#warning-aux-arguments-optionnels",
    "title": "22¬† Bonne pratique de Python",
    "section": "24.4 :warning: aux arguments optionnels",
    "text": "24.4 :warning: aux arguments optionnels\nLa fonction la plus lisible (mais la plus contraignante) est celle\nqui utilise exclusivement des arguments positionnels avec des noms explicites.\nDans le cadre d‚Äôune utilisation avanc√©e des fonctions (par exemple un gros mod√®le de microsimulation), il est\ndifficile d‚Äôanticiper tous les objets qui seront n√©cessaires √† l‚Äôutilisateur. Dans ce cas, on retrouve g√©n√©ralement\ndans la d√©finition d‚Äôune fonction le mot-cl√© **kwargs (√©quivalent du ... en R) qui capture les\narguments suppl√©mentaires et les stocke sous forme de dictionnaire. Il s‚Äôagit d‚Äôune technique avanc√©e de\nprogrammation qui est √† utiliser avec parcimonie."
  },
  {
    "objectID": "content/course/getting-started/02_DS_environment/index.html",
    "href": "content/course/getting-started/02_DS_environment/index.html",
    "title": "23¬† L‚Äôenvironnement Python pour la data-science",
    "section": "",
    "text": "24 Rester au courant des √©volutions\nL‚Äô√©cosyst√®me riche et foisonnant de Python a comme contrepartie\nqu‚Äôil faut rester attentif √† ses √©volutions pour ne pas\nvoir son capital humain vieillir et ainsi devenir has-been.\nAlors qu‚Äôavec des langages\nmonolithiques comme\nSAS ou Stata on pouvait se permettre de ne faire de vieille technique\nmais seulement consulter la documentation officielle, avec Python\nou R c‚Äôest impossible. Ce cours lui-m√™me est en √©volution continue, ce\nqui est assez exigeant :sweating:, pour √©pouser les √©volutions\nde l‚Äô√©cosyst√®me.\nTwitter est une excellente source d‚Äôinformation pour √™tre rapidement\nau courant des √©volutions du monde de la data-science. Les agr√©gateurs\nde contenu comme medium ou towarddatascience proposent des posts\nde qualit√© h√©t√©rog√®ne mais il peut √™tre utile de recevoir par mail\nle feed des nouveaux posts: au bout d‚Äôun certain temps, cela peut\npermettre de d√©gager les nouvelles tendances. Le site\nrealpython propose g√©n√©ralement de tr√®s bon posts, complets et\np√©dagogiques.\nEn ce qui concerne les ouvrages papiers, certains sont de tr√®s bonne qualit√©.\nCependant, il convient de faire attention √† la date de mise √† jour de ceux-ci:\nla vitesse d‚Äô√©volution de certains √©l√©ments de l‚Äô√©cosyst√®me peut les\np√©rimer tr√®s rapidement."
  },
  {
    "objectID": "content/course/getting-started/02_DS_environment/index.html#les-packages-python-essentiels-pour-le-cours-et-la-vie-des-data-scientists",
    "href": "content/course/getting-started/02_DS_environment/index.html#les-packages-python-essentiels-pour-le-cours-et-la-vie-des-data-scientists",
    "title": "23¬† L‚Äôenvironnement Python pour la data-science",
    "section": "23.1 Les packages python essentiels pour le cours et la vie des data-scientists",
    "text": "23.1 Les packages python essentiels pour le cours et la vie des data-scientists\n\nCe\npost,\ndont l‚Äôimage ci-dessus est tir√©e, r√©sume la plupart des packages utiles\npour un data-scientist ou un √©conomiste/sociologue. Nous nous bornerons\nici √† √©voquer ceux utilis√©s quotidiennement."
  },
  {
    "objectID": "content/course/getting-started/02_DS_environment/index.html#numpy",
    "href": "content/course/getting-started/02_DS_environment/index.html#numpy",
    "title": "23¬† L‚Äôenvironnement Python pour la data-science",
    "section": "23.2 numpy",
    "text": "23.2 numpy\nnumpy g√®re tout ce qui est calcul matriciel.\nLe langage Python est un des langages les plus lents qui soient1.\nTous les calculs rapides ne sont pas √©crits en Python mais en C++, voire Fortran.\nC‚Äôest le cas du package numpy. Celui-ci est incontournable\nd√®s qu‚Äôon veut √™tre rapide. Le package\nscipy est une extension o√π l‚Äôon peut trouver\ndes fonctions statistiques, d‚Äôoptimisation.\nLa Cheat Sheet de numpy est pratique:\nhttps://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf\nComme numpy est la brique de base de l‚Äôanalyse de donn√©es, un chapitre\nde ce cours lui est consacr√©."
  },
  {
    "objectID": "content/course/getting-started/02_DS_environment/index.html#pandas",
    "href": "content/course/getting-started/02_DS_environment/index.html#pandas",
    "title": "23¬† L‚Äôenvironnement Python pour la data-science",
    "section": "23.3 pandas",
    "text": "23.3 pandas\nAvant tout, un bon data-scientist doit √™tre capable de\ns‚Äôapproprier et manipuler des donn√©es rapidement. Pour cette raison,\npandas est incontournable.\nIl g√®re la plupart des formats de donn√©es. Pour √™tre efficace,\nil est lui aussi impl√©ment√© en C++.\nLe package est rapide si on utilise les m√©thodes pr√©-impl√©ment√©es sur\ndes donn√©es d‚Äôune taille raisonnable (par rapport √† la RAM disponible). Il faut\nn√©anmoins s‚Äôen m√©fier avec des donn√©es volumineuses.\nEn r√®gle g√©n√©rale, un jeu de donn√©es n√©cessite\ntrois fois plus d‚Äôespace en m√©moire que les\ndonn√©es n‚Äôen prennent sur le disque.\nLa Cheat Sheet de pandas :\nhttps://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_Pandas_Cheat_Sheet_2.pdf\npandas √©tant un √©l√©ment incontournable, deux chapitres y sont consacr√©s.\n\n23.3.1 matplotlib et seaborn\nmatplotlib existe depuis une vingtaine d‚Äôann√©es pour doter Python de\nfonctionalit√©s graphiques. Il s‚Äôagit d‚Äôun package tr√®s flexible, offrant\nde nombreuses fonctionalit√©s. N√©anmoins, ces derni√®res ann√©es,\nseaborn a √©merg√© pour simplifier la cr√©ation de certains graphiques\nstandards de l‚Äôanalyse de donn√©es (histogrammes, diagramme en barre, etc. ).\nLe succ√®s de seaborn n‚Äô√©clipse n√©anmoins pas matplotlib puisque ce\ndernier est souvent n√©cessaire pour finaliser la customisation d‚Äôun\ngraphique produit par seaborn2\n\n\n23.3.2 scikit-learn\nscikit-learn est le module de machine learning le plus populaire pour\ntrois raisons:\n\nil s‚Äôappuie sur une API extr√™mement consistante (m√©thodes fit, transform\net predict, respectivement pour apprendre des donn√©es, appliquer des transformations et pr√©dire sur de nouvelles donn√©es) ;\nil permet de construire\ndes analyses reproductibles en construisant des pipelines de donn√©es ;\nsa documentation est un mod√®le √† suivre.\n\nL‚ÄôINRIA, institution fran√ßaise, est l‚Äôun des √©l√©ments moteurs dans\nla cr√©ation et la maintenance de scikit-learn"
  },
  {
    "objectID": "content/course/getting-started/02_DS_environment/index.html#tensorflow-pytorch-et-keras",
    "href": "content/course/getting-started/02_DS_environment/index.html#tensorflow-pytorch-et-keras",
    "title": "23¬† L‚Äôenvironnement Python pour la data-science",
    "section": "23.4 TensorFlow, PyTorch et Keras",
    "text": "23.4 TensorFlow, PyTorch et Keras\nLes librairies essentielles pour impl√©menter et utiliser des mod√®les\nde deep learning en Python ont √©t√© d√©velopp√©es par des acteurs du\nnum√©rique.\nTensorFlow est la librairie la plus mature, mais pas n√©cessairement la plus facile √† prendre en main. D‚Äôailleurs, Google semble l‚Äôabandonner en usage interne pour lui\npr√©f√©rer JAX.\nKeras propose une interface high-level,\ndonc plus facile d‚Äôutilisation,\nmais qui n‚Äôen reste pas moins suffisante pour une grande vari√©t√© d‚Äôusages.\nLa documentation de Keras est tr√®s bien faite.\nPyTorch est un framework plus r√©cent mais tr√®s complet,\ndont la syntaxe plaira aux amateurs de programmation orient√©-objet.\nD√©velopp√© par Facebook,\nil est tr√®s utilis√© dans certains domaines de recherche, comme le NLP.\nIl s‚Äôagit du framework dont la dynamique r√©cente a √©t√© la plus\nascensionnelle."
  },
  {
    "objectID": "content/course/getting-started/02_DS_environment/index.html#statsmodels",
    "href": "content/course/getting-started/02_DS_environment/index.html#statsmodels",
    "title": "23¬† L‚Äôenvironnement Python pour la data-science",
    "section": "23.5 statsmodels",
    "text": "23.5 statsmodels\nstatsmodels plaira plus aux statisticiens, il impl√©mente des mod√®les\n√©conom√©triques similaires √† scikit-learn.\nPar rapport √† scikit-learn,\nstatsmodels est plus orient√© √©conom√©trie. La pr√©sentation des\nr√©sultats est tr√®s proche de ce qu‚Äôon trouve en R."
  },
  {
    "objectID": "content/course/getting-started/02_DS_environment/index.html#requests-et-beautifulsoup",
    "href": "content/course/getting-started/02_DS_environment/index.html#requests-et-beautifulsoup",
    "title": "23¬† L‚Äôenvironnement Python pour la data-science",
    "section": "23.6 requests et beautifulsoup",
    "text": "23.6 requests et beautifulsoup\nrequests est l‚Äôune des librairies de base de Python, d√©di√©e\n√† g√©rer la connexion avec internet. Les amateurs d‚ÄôAPI\nseront des utilisateurs fr√©quents de celle-ci. Les\npersonnes plus sp√©cialistes de webscraping lui pr√©f√©reront\nbeautifulsoup qui offre une syntaxe extr√™mement puissante\npour r√©cup√©rer automatiquement du contenu de pages web."
  },
  {
    "objectID": "content/course/getting-started/02_DS_environment/index.html#nltk-et-spacy",
    "href": "content/course/getting-started/02_DS_environment/index.html#nltk-et-spacy",
    "title": "23¬† L‚Äôenvironnement Python pour la data-science",
    "section": "23.7 nltk et spaCy",
    "text": "23.7 nltk et spaCy\nDans le domaine du traitement automis√© du langage, plus connu\nsous son acronyme anglais NLP, les deux packages phares sont\nnltk et spaCy.\nnltk est le package historique. Il existe depuis les ann√©es\n1990 et propose de nombreuses ressources utiles pour l‚Äôanalyse\ntextuelle. N√©anmoins, ces derni√®res ann√©es, spaCy est venu\nmoderniser l‚Äôapproche en proposant une approche permettant\nde mieux int√©grer les diff√©rentes phases du traitement de donn√©es\ntextuelles, une excellente documentation et un meilleur support\ndes langues non anglo-saxonnes, comme le Fran√ßais.\nMais Python est √©galement un outil privil√©gi√© pour communiquer:\n\nUne bonne int√©gration de python √† Markdown (gr√¢ce notamment √† ‚Ä¶ R Markdown) qui facilite la construction de documents HTML ou PDF (via Latex)\nSphynx et JupyterBook proposent des mod√®les de documentation\ntr√®s complets\nbokeh ou streamlit comme alternative √† shiny (R)\nDjango et Flask permettent de construire des applications web en Python\nLes librairies dynamiques, notamment\nfolium ou\nplotly, sont tr√®s appr√©ci√©es pour construire des\nvisualisations dynamiques qui sont pratiques dans une analyse exploratoire\nmais √©galement lorsqu‚Äôil faut valoriser ses travaux aupr√®s de\npublics non experts de la donn√©e.\n\nL‚Äôun des nouveaux arrivants dans cet √©cosyst√®me d√©j√† riche\nest FastAPI). Avec ce package,\nil est tr√®s facile de transformer un code Python en API ce qui facilite\nla mise √† disposition de donn√©es mais aussi de productions par Python (comme\nla mise √† disposition d‚Äôune API pour permettre √† des personnes de tester\nles r√©sultats d‚Äôun mod√®le de machine learning).\nCe n‚Äôest qu‚Äôune petite partie de l‚Äô√©cosyst√®me Python, d‚Äôune richesse rare."
  },
  {
    "objectID": "content/course/getting-started/02_DS_environment/index.html#environnement-autour-de-python",
    "href": "content/course/getting-started/02_DS_environment/index.html#environnement-autour-de-python",
    "title": "23¬† L‚Äôenvironnement Python pour la data-science",
    "section": "23.8 Environnement autour de Python",
    "text": "23.8 Environnement autour de Python\nPython est un langage tr√®s riche, gr√¢ce √† sa logique open-source. Mais l‚Äôun\ndes principaux int√©r√™ts r√©side dans le riche √©cosyst√®me avec lequel Python\ns‚Äôint√®gre. On peut donner quelques √©l√©ments, dans un inventaire √† la Pr√©vert non exaustif.\nEn premier lieu, des √©l√©ments reli√©s au traitement des donn√©es:\n\nSpark,\nle framework dominant dans le domaine du traitement des big-data, tr√®s bien\ninterfac√© avec Python (gr√¢ce √† l‚ÄôAPI pyspark), qui facilite le traitement des donn√©es volumineuses. Son utilisation n√©cessite cependant d‚Äôavoir acc√®s √† une\ninfrastructure de calculs distribu√©e.\nCython permet d‚Äôint√©grer facilement du code C, tr√®s\nefficace avec Python (√©quivalent de Rcpp pour R).\nJulia est un langage r√©cent, qui propose une syntaxe famili√®re aux utilisateurs de languages scientifiques (Python, R, MATLAB), tout en permettant des performances proches du C gr√¢ce √† une compilation √† la vol√©e.\n\nEnfin, des √©l√©ments permettant un d√©ploiement de r√©sultats ou d‚Äôapplications\nen continu :\n* Les images Docker de Jupyterhub facilitent l‚Äôusage de l‚Äôint√©gration continue\npour construire des modules, les tester et d√©ployer des site web.\n* Les services type Binder, Google Colab et Kaggle proposent des kernels\nPython"
  },
  {
    "objectID": "content/course/getting-started/05_rappels_types/index.html",
    "href": "content/course/getting-started/05_rappels_types/index.html",
    "title": "24¬† Quelques rappels sur les principes de base de Python",
    "section": "",
    "text": "25 Les quelques r√®gles de Python\nPython est un peu susceptible et protocolaire, plus formaliste que ne l‚Äôest R.\nIl y a ainsi quelques r√®gles √† respecter :\nR√®gle 1: L‚Äôindentation est primordiale : un code mal indent√© provoque une erreur.\nL‚Äôindentation indique √† l‚Äôinterpr√©teur o√π se trouvent les\ns√©parations entre des blocs d‚Äôinstructions. Un peu comme des points dans un\ntexte.\nSi les lignes ne sont pas bien align√©es, l‚Äôinterpr√©teur ne sait plus √† quel\nbloc associer la ligne. Par exemple, le corps d‚Äôune fonction doit √™tre indent√©\nd‚Äôun niveau ; les √©l√©ments dans une clause logique (if, else, etc.) √©galement\nR√®gle 2: On commence √† compter √† 0, comme dans beaucoup de langages\n(C++, java‚Ä¶). Python diff√®re dans ce domaine de R o√π on commence\n√† compter √† 1.\nLe premier √©l√©ment d‚Äôune liste est ainsi, en Python, le 0-√®me.\nR√®gle 3: Comme dans une langue naturelle, les marques de\nponctuation sont importantes :\nQuand Python r√©alise des op√©rations, il faut lui pr√©ciser ce qu‚Äôil doit en faire :\nContrairement √† R, par d√©faut, Python ne renvoie pas le r√©sultat de la\nderni√®re op√©ration effectu√©e.\n{{% box status=‚Äúnote‚Äù title=‚ÄúRemarque‚Äù icon=‚Äúfas fa-pencil-alt‚Äù %}}\nDans l‚Äôenvironnement Jupyter Notebook, le dernier √©lement d‚Äôune cellule\nest automatiquement affich√© (print), qu‚Äôon lui demande ou non de le faire.\nCe comportement est particuli√®rement pratique pour afficher des figures\ng√©n√©r√©es via matplotlib ou seaborn.\nCe comportement\nn‚Äôest pas le cas dans un √©diteur classique comme VisualStudio,\nSpyder ou PyCharm. Pour afficher un r√©sultat dans la console,\nil faut utiliser\nprint ou la commande consacr√©e (par exemple plt.show()\npour afficher la derni√®re figure g√©n√©r√©e par matplotlib)\n{{% /box %}}\nPython permet de manipuler diff√©rents types de base. Nous en\nverrons des extensions dans la suite du cours (np.array par exemple)\nqui, d‚Äôune mani√®re ou d‚Äôune autre, s‚Äôappuient sur ces types de base.\nOn distingue deux types de variables : les immuables (immutables)\nqui ne peuvent √™tre\nmodifi√©s et les modifiables (mutables)\n{{% box status=‚Äúexercise‚Äù title=‚ÄúExercice 1‚Äù icon=‚Äúfas fa-pencil-alt‚Äù %}}\nliste_nombres = [1,2,7,5,3]\ndictionnaire_evangile = {\"Marc\" : \"Lion\", \"Matthieu\" : [\"Ange\",\"Homme ail√©\"] , \n                          \"Jean\" : \"Aigle\" , \"Luc\" : \"Taureau\"}\n{{% /box %}}\nMaintenant qu‚Äôon a vu quels objets existent en Python,\nnous allons\nvoir comment nous en servir.\nPour comprendre comment modifier un objet, il convient\nde distinguer deux concepts, d√©velopp√©s plus amplement\ndans le chapitre d√©di√©: les attributs et les m√©thodes:"
  },
  {
    "objectID": "content/course/getting-started/05_rappels_types/index.html#le-print",
    "href": "content/course/getting-started/05_rappels_types/index.html#le-print",
    "title": "24¬† Quelques rappels sur les principes de base de Python",
    "section": "26.1 Le print",
    "text": "26.1 Le print\n\n# on calcule : dans le cas d'une op√©ration par exemple une somme\n2+3 # Python calcule le r√©sultat mais n'affiche rien dans la sortie\n\n# le print : on affiche\n\nprint(2+3) # Python calcule et on lui demande juste de l'afficher\n# le r√©sultat est en dessous du code\n\n5\n\n\n\n# le print dans une fonction \n\ndef addition_v1(a,b) : \n    print(a+b)\n\nresultat_print = addition_v1(2,0) \nprint(type(resultat_print))\n\n# dans la sortie on a l'affichage du r√©sultat, car la sortie de la fonction est un print \n# en plus on lui demande quel est le type du r√©sultat. Un print ne renvoie aucun type, ce n'est ni un num√©rique,\n# ni une chaine de charact√®res, le r√©sultat d'un print n'est pas un format utilisable\n\n2\n&lt;class 'NoneType'&gt;\n\n\nLe r√©sultat de l‚Äôaddition est affich√©\ncar la fonction addition_v1 effectue un print\nPar contre, l‚Äôobjet cr√©√© n‚Äôa pas de type, il n‚Äôest pas un chiffre,\nce n‚Äôest qu‚Äôun affichage."
  },
  {
    "objectID": "content/course/getting-started/05_rappels_types/index.html#le-return",
    "href": "content/course/getting-started/05_rappels_types/index.html#le-return",
    "title": "24¬† Quelques rappels sur les principes de base de Python",
    "section": "26.2 Le return",
    "text": "26.2 Le return\nPour cr√©er un objet avec le r√©sultat de la fonction, il faut utiliser return\n\n# le return dans une fonction\ndef addition_v2(a,b) : \n    return a+b\n\nresultat_return = addition_v2(2,5) # \nprint(type(resultat_return))\n## l√† on a bien un r√©sultat qui est du type \"entier\"\n\n&lt;class 'int'&gt;\n\n\nLe r√©sultat de addition_v2 n‚Äôest pas affich√© comme dans addition_v1\nPar contre, la fonction addition_v2 permet d‚Äôavoir un objet de type int,\nun entier donc."
  },
  {
    "objectID": "content/course/getting-started/05_rappels_types/index.html#les-variables-immuables",
    "href": "content/course/getting-started/05_rappels_types/index.html#les-variables-immuables",
    "title": "24¬† Quelques rappels sur les principes de base de Python",
    "section": "27.1 Les variables immuables",
    "text": "27.1 Les variables immuables\nLes variables immuables ne peuvent √™tre modifi√©es\n\nNone : ce type est une convention de programmation pour dire que la valeur n‚Äôest pas calcul√©e\nbool : un bool√©en\nint : un entier\nfloat : un r√©el\nstr : une chaine de caract√®res\ntuple : un vecteur\n\n\ni = 3         # entier = type num√©rique (type int)\nr = 3.3       # r√©el   = type num√©rique (type float)\ns = \"exemple\" # cha√Æne de caract√®res = type str \nn = None      # None signifie que la variable existe mais qu'elle ne contient rien\n              # elle est souvent utilis√©e pour signifier qu'il n'y a pas de r√©sultat\na = (1,2)     # tuple\n\nprint(i,r,s,n,a)         \n\n3 3.3 exemple None (1, 2)\n\n\nSi on essaie de changer le premier √©l√©ment de la chaine de caract√®res s on va avoir un peu de mal.\nPar exemple si on voulait mettre une majuscule √† ‚Äúexemple‚Äù,\non aurait envie d‚Äô√©crire que le premier √©l√©ment de la chaine s est ‚ÄúE‚Äù majuscule\nMais Python ne va pas nous laisser faire, il nous dit que les objets ‚Äúchaine de caract√®re‚Äù ne peuvent √™tre modifi√©s\n\ns[0] = \"E\"  # d√©clenche une exception\n\nTypeError: 'str' object does not support item assignment\n\n\nTout ce qu‚Äôon peut faire avec une variable immuable,\nc‚Äôest la r√©affecter √† une autre valeur : elle ne peut pas √™tre modifi√©e.\nPour s‚Äôen convaincre, utilisons la fonction id() qui donne un identifiant √† chaque objet.\n\nprint(s)\nid(s)\n\nexemple\n\n\n139685226410480\n\n\n\ns = \"autre_mot\"\nid(s)\n\n139684085803120\n\n\nOn voit bien que s a chang√© d‚Äôidentifiant : il peut avoir le m√™me nom, ce n‚Äôest plus le m√™me objet"
  },
  {
    "objectID": "content/course/getting-started/05_rappels_types/index.html#les-types-modifiable-listes-et-dictionnaires",
    "href": "content/course/getting-started/05_rappels_types/index.html#les-types-modifiable-listes-et-dictionnaires",
    "title": "24¬† Quelques rappels sur les principes de base de Python",
    "section": "27.2 Les types modifiable : listes et dictionnaires",
    "text": "27.2 Les types modifiable : listes et dictionnaires\nHeureusement, il existe des variables modifiables comme les listes et les dictionnaires.\n\n27.2.1 Les listes - elles s‚Äô√©crivent entre [ ]\nLes listes sont des √©lements tr√®s utiles, notamment quand vous souhaitez faire des boucles.\nPour faire appel aux √©lements d‚Äôune liste, on donne leur position dans la liste : le 1er est le 0, le 2√®me est le 1 ‚Ä¶\n\nma_liste = [1,2,3,4]\n\n\nprint(\"La longueur de ma liste est de\", len(ma_liste))\nprint(\"Le premier √©l√©ment de ma liste est :\", ma_liste[0])\nprint(\"Le dernier √©l√©ment de ma liste est :\", ma_liste[3])\nprint(\"Le dernier √©l√©ment de ma liste est :\", ma_liste[-1])\n\nLa longueur de ma liste est de 4\nLe premier √©l√©ment de ma liste est : 1\nLe dernier √©l√©ment de ma liste est : 4\nLe dernier √©l√©ment de ma liste est : 4\n\n\n{{% box status=‚Äúexercise‚Äù title=‚ÄúExercice 1‚Äù icon=‚Äúfas fa-pencil-alt‚Äù %}}\nPour effectuer des boucles sur les listes, la m√©thode la plus lisible\nest d‚Äôutiliser les list comprehension. Cette approche consiste\n√† it√©rer les √©l√©ments d‚Äôune liste √† la vol√©e.\nPar exemple, si on reprend cet exemple,\nun code qui repose sur les list comprehension sera le suivant:\n\nfruits = [\"apple\", \"banana\", \"cherry\", \"kiwi\", \"mango\"]\nnewlist = [x for x in fruits if \"a\" in x]\nprint(newlist)\n\n['apple', 'banana', 'mango']\n\n\nLe m√™me code, ne reposant pas sur les compr√©hensions de liste, sera beaucoup\nmoins concis et ainsi inutilement verbeux:\n\nfruits = [\"apple\", \"banana\", \"cherry\", \"kiwi\", \"mango\"]\nnewlist = []\n\nfor x in fruits:\n  if \"a\" in x:\n    newlist.append(x)\n\nprint(newlist) \n\n['apple', 'banana', 'mango']\n\n\n{{% /box %}}\n\n\n27.2.2 Les dictionnaires - ils s‚Äô√©crivent entre accolades {}\nUn dictionnaire associe √† une cl√© un autre √©l√©ment, appel√© une valeur : un chiffre, un nom, une liste, un autre dictionnaire etc.\nLe format d‚Äôun dictionnaire est le suivant: {Cl√© : valeur}. Il s‚Äôagit\nd‚Äôun objet tr√®s pratique pour la recherche, beaucoup plus que les listes\nqui ne permettent pas de stocker de l‚Äôinformation diverse de mani√®re\nhi√©rarchis√©e.\n\n27.2.2.1 Dictionnaire avec des valeurs int\nOn peut par exemple associer √† un nom, un nombre\n\nmon_dictionnaire_notes = { 'Nicolas' : 18 , 'Pimprenelle' : 15} \n# un dictionnaire qui √† chaque nom associe un nombre\n# √† Nicolas, on associe 18\n\nprint(mon_dictionnaire_notes) \n\n{'Nicolas': 18, 'Pimprenelle': 15}\n\n\n\n\n27.2.2.2 Dictionnaire avec des valeurs qui sont des listes\nPour chaque cl√© d‚Äôun dictionnaire, il ne faut pas forc√©ment garder la m√™me forme de valeur\nDans l‚Äôexemple, la valeur de la cl√© ‚ÄúNicolas‚Äù est une liste, alors que celle de ‚ÄúPhilou‚Äù est une liste de liste\n\nmon_dictionnaire_loisirs =  \\\n{ 'Nicolas' : ['Rugby','Pastis','Belote'] , \n  'Pimprenelle' : ['Gin Rami','Tisane','Tara Jarmon','Barcelone','Mickey Mouse'],\n  'Philou' : [['Maths','Jeux'],['Guillaume','Jeanne','Thimoth√©e','Adrien']]}\n\nPour acc√©der √† un √©l√©ment du dictionnaire, on fait appel √† la cl√© et non plus √† la position, comme c‚Äô√©tait le cas dans les listes.\nC‚Äôest beaucoup plus pratique pour rechercher de l‚Äôinformation:\n\nprint(mon_dictionnaire_loisirs['Nicolas']) # on affiche une liste\n\n['Rugby', 'Pastis', 'Belote']\n\n\n\nprint(mon_dictionnaire_loisirs['Philou']) # on affiche une liste de listes\n\n[['Maths', 'Jeux'], ['Guillaume', 'Jeanne', 'Thimoth√©e', 'Adrien']]\n\n\nSi on ne veut avoir que la premi√®re liste des loisirs de Philou, on demande le premier √©l√©ment de la liste\n\nprint(mon_dictionnaire_loisirs['Philou'][0]) # on affiche alors juste la premi√®re liste\n\n['Maths', 'Jeux']\n\n\nOn peut aussi avoir des valeurs qui sont des int et des list\n\nmon_dictionnaire_patchwork_good = \\\n{ 'Nicolas' : ['Rugby','Pastis','Belote'] ,\n  'Pimprenelle' : 18 }"
  },
  {
    "objectID": "content/course/getting-started/05_rappels_types/index.html#a-retenir",
    "href": "content/course/getting-started/05_rappels_types/index.html#a-retenir",
    "title": "24¬† Quelques rappels sur les principes de base de Python",
    "section": "27.3 A retenir",
    "text": "27.3 A retenir\n\nL‚Äôindentation du code est importante (4 espaces et pas une tabulation)\nUne liste est entre [] et on peut appeler les positions par leur place\nUn dictionnaire, cl√© x valeur, s‚Äô√©crit entre {} et on appelle un √©l√©ment en fonction de la cl√©"
  },
  {
    "objectID": "content/course/getting-started/05_rappels_types/index.html#premiers-exemples-de-m√©thodes",
    "href": "content/course/getting-started/05_rappels_types/index.html#premiers-exemples-de-m√©thodes",
    "title": "24¬† Quelques rappels sur les principes de base de Python",
    "section": "29.1 Premiers exemples de m√©thodes",
    "text": "29.1 Premiers exemples de m√©thodes\nAvec les √©l√©ments d√©finis dans la partie 1\n(les listes, les dictionnaires) on peut faire appel √† des m√©thodes qui sont directement li√©es √† ces objets.\n\n29.1.1 Une m√©thode pour les listes\nPour ajouter un √©l√©ment (item) dans une liste : on va utiliser la m√©thode .append()\n\nma_liste = [\"Nicolas\",\"Michel\",\"Bernard\"]\n\nma_liste.append(\"Philippe\")\n\nprint(ma_liste)\n\n['Nicolas', 'Michel', 'Bernard', 'Philippe']\n\n\n\n\n29.1.2 Une m√©thode pour les dictionnaires\nPour connaitre l‚Äôensemble des cl√©s d‚Äôun dictionnaire, on appelle la m√©thode .keys()\n\nmon_dictionnaire = {\"Marc\" : \"Lion\", \"Matthieu\" : [\"Ange\",\"Homme ail√©\"] , \n                          \"Jean\" : \"Aigle\" , \"Luc\" : \"Taureau\"}\n\nprint(mon_dictionnaire.keys())\n\ndict_keys(['Marc', 'Matthieu', 'Jean', 'Luc'])"
  },
  {
    "objectID": "content/course/getting-started/05_rappels_types/index.html#connaitre-les-m√©thodes-dun-objet",
    "href": "content/course/getting-started/05_rappels_types/index.html#connaitre-les-m√©thodes-dun-objet",
    "title": "24¬† Quelques rappels sur les principes de base de Python",
    "section": "29.2 Connaitre les m√©thodes d‚Äôun objet",
    "text": "29.2 Connaitre les m√©thodes d‚Äôun objet\nPour savoir quelles sont les m√©thodes d‚Äôun objet vous pouvez :\n\ntaper help(mon_objet) ou mon_objet? dans la console Python\ntaper mon_objet. + touche tabulation dans la console Python ou dans le Notebook.\nPython permet la compl√©tion, c‚Äôest-√†-dire que vous pouvez faire appa√Ætre la liste\ndes m√©thodes possibles."
  },
  {
    "objectID": "content/course/getting-started/05_rappels_types/index.html#cr√©er-une-liste",
    "href": "content/course/getting-started/05_rappels_types/index.html#cr√©er-une-liste",
    "title": "24¬† Quelques rappels sur les principes de base de Python",
    "section": "30.1 Cr√©er une liste",
    "text": "30.1 Cr√©er une liste\nPour cr√©er un objet de la classe list, il suffit de le d√©clarer. Ici on affecte √† x une liste\n\nx = [4, 5] # cr√©ation d‚Äôune liste compos√©e de deux entiers\nx = [\"un\", 1, \"deux\", 2] # cr√©ation d‚Äôune liste compos√©e de 2 cha√Ænes de caract√®res\n# et de deux entiers, l‚Äôordre d‚Äô√©criture est important\nx = [3] # cr√©ation d‚Äôune liste d‚Äôun √©l√©ment, sans la virgule,\nx = [ ] # cr√©e une liste vide\nx = list () # cr√©e une liste vide"
  },
  {
    "objectID": "content/course/getting-started/05_rappels_types/index.html#un-premier-test-sur-les-listes",
    "href": "content/course/getting-started/05_rappels_types/index.html#un-premier-test-sur-les-listes",
    "title": "24¬† Quelques rappels sur les principes de base de Python",
    "section": "30.2 Un premier test sur les listes",
    "text": "30.2 Un premier test sur les listes\nSi on veut tester la pr√©sence d‚Äôun √©l√©ment dans une liste, on l‚Äô√©crit de la mani√®re suivante :\n\n# Exemple \n\nx = \"Marcel\"\n\nl = [\"Marcel\",\"Edith\",\"Maurice\",\"Jean\"]\n\nprint(x in l)\n\n#vrai si x est un des √©l√©ments de l\n\nTrue"
  },
  {
    "objectID": "content/course/getting-started/05_rappels_types/index.html#une-m√©thode-pour-concat√©ner-deux-listes",
    "href": "content/course/getting-started/05_rappels_types/index.html#une-m√©thode-pour-concat√©ner-deux-listes",
    "title": "24¬† Quelques rappels sur les principes de base de Python",
    "section": "30.3 +: une m√©thode pour concat√©ner deux listes",
    "text": "30.3 +: une m√©thode pour concat√©ner deux listes\nOn utilise le symbole +\n\nt = [\"Antoine\",\"David\"]\nprint(l + t) #concat√©nation de l et t\n\n['Marcel', 'Edith', 'Maurice', 'Jean', 'Antoine', 'David']"
  },
  {
    "objectID": "content/course/getting-started/05_rappels_types/index.html#pour-trouver-certains-√©l√©ments-dune-liste",
    "href": "content/course/getting-started/05_rappels_types/index.html#pour-trouver-certains-√©l√©ments-dune-liste",
    "title": "24¬† Quelques rappels sur les principes de base de Python",
    "section": "30.4 Pour trouver certains √©l√©ments d‚Äôune liste",
    "text": "30.4 Pour trouver certains √©l√©ments d‚Äôune liste\nPour chercher des √©lements dans une liste, on utilise la position dans la liste.\n\nl[1] # donne l'√©l√©ment qui est en 2√®me position de la liste\n\n'Edith'\n\n\n\nl[1:3] # donne les √©l√©ments de la 2√®me position de la liste √† la 4√®me exclue\n\n['Edith', 'Maurice']"
  },
  {
    "objectID": "content/course/getting-started/05_rappels_types/index.html#quelques-fonctions-des-listes",
    "href": "content/course/getting-started/05_rappels_types/index.html#quelques-fonctions-des-listes",
    "title": "24¬† Quelques rappels sur les principes de base de Python",
    "section": "30.5 Quelques fonctions des listes",
    "text": "30.5 Quelques fonctions des listes\nLes listes embarquent ainsi nativement un certain nombre de m√©thodes\nqui sont pratiques. Cependant, pour avoir certaines informations\nsur une liste, il faut parfois plut√¥t passer par\ndes fonctions natives comme les suivantes:\n\nlongueur = len(l) # nombre d‚Äô√©l√©ments de l\nminimum = min(l) # plus petit √©l√©ment de l, ici par ordre alphab√©tique\nmaximum = max(l) # plus grand √©l√©ment de l, ici par ordre alphab√©tique\nprint(longueur,minimum,maximum)\n\n4 Edith Maurice\n\n\n\ndel l[0 : 2] # supprime les √©l√©ments entre la position 0 et 2 exclue\nprint(l)\n\n['Maurice', 'Jean']"
  },
  {
    "objectID": "content/course/getting-started/05_rappels_types/index.html#les-m√©thodes-des-listes",
    "href": "content/course/getting-started/05_rappels_types/index.html#les-m√©thodes-des-listes",
    "title": "24¬† Quelques rappels sur les principes de base de Python",
    "section": "30.6 Les m√©thodes des listes",
    "text": "30.6 Les m√©thodes des listes\nOn les trouve dans l‚Äôaide de la liste.\nOn distingue les m√©thodes et les m√©thodes sp√©ciales : visuellement,\nles m√©thodes sp√©ciales sont celles qui pr√©c√©d√©es et suivis de deux caract√®res de soulignement,\nles autres sont des m√©thodes classiques.\n\nhelp(l)\n\nHelp on list object:\n\nclass list(object)\n |  list(iterable=(), /)\n |  \n |  Built-in mutable sequence.\n |  \n |  If no argument is given, the constructor creates a new empty list.\n |  The argument must be an iterable if specified.\n |  \n |  Methods defined here:\n |  \n |  __add__(self, value, /)\n |      Return self+value.\n |  \n |  __contains__(self, key, /)\n |      Return key in self.\n |  \n |  __delitem__(self, key, /)\n |      Delete self[key].\n |  \n |  __eq__(self, value, /)\n |      Return self==value.\n |  \n |  __ge__(self, value, /)\n |      Return self&gt;=value.\n |  \n |  __getattribute__(self, name, /)\n |      Return getattr(self, name).\n |  \n |  __getitem__(...)\n |      x.__getitem__(y) &lt;==&gt; x[y]\n |  \n |  __gt__(self, value, /)\n |      Return self&gt;value.\n |  \n |  __iadd__(self, value, /)\n |      Implement self+=value.\n |  \n |  __imul__(self, value, /)\n |      Implement self*=value.\n |  \n |  __init__(self, /, *args, **kwargs)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __iter__(self, /)\n |      Implement iter(self).\n |  \n |  __le__(self, value, /)\n |      Return self&lt;=value.\n |  \n |  __len__(self, /)\n |      Return len(self).\n |  \n |  __lt__(self, value, /)\n |      Return self&lt;value.\n |  \n |  __mul__(self, value, /)\n |      Return self*value.\n |  \n |  __ne__(self, value, /)\n |      Return self!=value.\n |  \n |  __repr__(self, /)\n |      Return repr(self).\n |  \n |  __reversed__(self, /)\n |      Return a reverse iterator over the list.\n |  \n |  __rmul__(self, value, /)\n |      Return value*self.\n |  \n |  __setitem__(self, key, value, /)\n |      Set self[key] to value.\n |  \n |  __sizeof__(self, /)\n |      Return the size of the list in memory, in bytes.\n |  \n |  append(self, object, /)\n |      Append object to the end of the list.\n |  \n |  clear(self, /)\n |      Remove all items from list.\n |  \n |  copy(self, /)\n |      Return a shallow copy of the list.\n |  \n |  count(self, value, /)\n |      Return number of occurrences of value.\n |  \n |  extend(self, iterable, /)\n |      Extend list by appending elements from the iterable.\n |  \n |  index(self, value, start=0, stop=9223372036854775807, /)\n |      Return first index of value.\n |      \n |      Raises ValueError if the value is not present.\n |  \n |  insert(self, index, object, /)\n |      Insert object before index.\n |  \n |  pop(self, index=-1, /)\n |      Remove and return item at index (default last).\n |      \n |      Raises IndexError if list is empty or index is out of range.\n |  \n |  remove(self, value, /)\n |      Remove first occurrence of value.\n |      \n |      Raises ValueError if the value is not present.\n |  \n |  reverse(self, /)\n |      Reverse *IN PLACE*.\n |  \n |  sort(self, /, *, key=None, reverse=False)\n |      Sort the list in ascending order and return None.\n |      \n |      The sort is in-place (i.e. the list itself is modified) and stable (i.e. the\n |      order of two equal elements is maintained).\n |      \n |      If a key function is given, apply it once to each list item and sort them,\n |      ascending or descending, according to their function values.\n |      \n |      The reverse flag can be set to sort in descending order.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods defined here:\n |  \n |  __class_getitem__(...) from builtins.type\n |      See PEP 585\n |  \n |  ----------------------------------------------------------------------\n |  Static methods defined here:\n |  \n |  __new__(*args, **kwargs) from builtins.type\n |      Create and return a new object.  See help(type) for accurate signature.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __hash__ = None"
  },
  {
    "objectID": "content/course/getting-started/05_rappels_types/index.html#a-retenir-et-questions",
    "href": "content/course/getting-started/05_rappels_types/index.html#a-retenir-et-questions",
    "title": "24¬† Quelques rappels sur les principes de base de Python",
    "section": "30.7 A retenir et questions",
    "text": "30.7 A retenir et questions\nA retenir :\n\nChaque objet Python a des attributs et des m√©thodes\nVous pouvez cr√©er des classes avec des attributs et des m√©thodes\nLes m√©thodes des listes et des dictionnaires qui sont les plus utilis√©es :\n\nlist.count()\nlist.sort()\nlist.append()\ndict.keys()\ndict.items()\ndict.values()\n\n\n{{% box status=‚Äúexercise‚Äù title=‚ÄúExercice 2‚Äù icon=‚Äúfas fa-pencil-alt‚Äù %}}\n\nD√©finir la liste allant de 1 √† 10, puis effectuez les actions suivantes :\n\n\ntriez et affichez la liste\najoutez l‚Äô√©l√©ment 11 √† la liste et affichez la liste\nrenversez et affichez la liste\naffichez l‚Äô√©l√©ment d‚Äôindice 7\nenlevez l‚Äô√©l√©ment 9 et affichez la liste\naffichez la sous-liste du 2e au 3e √©l√©ments inclus ;\naffichez la sous-liste du d√©but au 2e √©l√©ment inclus ;\naffichez la sous-liste du 3e √©l√©ment √† la fin de la liste ;\n\n\nConstruire le dictionnaire des 6 premiers mois de l‚Äôann√©e avec comme valeurs le nombre de jours respectif.\n\n\nRenvoyer la liste des mois\nRenvoyer la liste des jours\nAjoutez la cl√© du mois de Juillet\n\n{{% /box %}}"
  },
  {
    "objectID": "content/course/getting-started/05_rappels_types/index.html#passer-des-listes-dictionnaires-√†-pandas",
    "href": "content/course/getting-started/05_rappels_types/index.html#passer-des-listes-dictionnaires-√†-pandas",
    "title": "24¬† Quelques rappels sur les principes de base de Python",
    "section": "30.8 Passer des listes, dictionnaires √† pandas",
    "text": "30.8 Passer des listes, dictionnaires √† pandas\nSupposons que la variable ‚Äòdata‚Äô est une liste qui contient nos donn√©es.\nUne observation correspond √† un dictionnaire qui contient le nom, le type, l‚Äôambiance et la note d‚Äôun restaurant.\nIl est ais√© de transformer cette liste en dataframe gr√¢ce √† la fonction ‚ÄòDataFrame‚Äô.\n\nimport pandas \n\ndata = [{\"nom\": \"Little Pub\", \"type\" : \"Bar\", \"ambiance\": 9, \"note\": 7},\n     {\"nom\": \"Le Corse\", \"type\" : \"Sandwicherie\", \"ambiance\": 2, \"note\": 8},\n     {\"nom\": \"Caf√© Caumartin\", \"type\" : \"Bar\", \"ambiance\": 1}]\n\ndf = pandas.DataFrame(data)\n\nprint(data)\ndf\n\n[{'nom': 'Little Pub', 'type': 'Bar', 'ambiance': 9, 'note': 7}, {'nom': 'Le Corse', 'type': 'Sandwicherie', 'ambiance': 2, 'note': 8}, {'nom': 'Caf√© Caumartin', 'type': 'Bar', 'ambiance': 1}]\n\n\n\n\n\n\n\n\n\nnom\ntype\nambiance\nnote\n\n\n\n\n0\nLittle Pub\nBar\n9\n7.0\n\n\n1\nLe Corse\nSandwicherie\n2\n8.0\n\n\n2\nCaf√© Caumartin\nBar\n1\nNaN"
  },
  {
    "objectID": "content/course/getting-started/06_rappels_fonctions/index.html",
    "href": "content/course/getting-started/06_rappels_fonctions/index.html",
    "title": "25¬† Modules, tests, boucles, fonctions",
    "section": "",
    "text": "26 Utilisation d‚Äôun module install√©\nPython, comme R, sont des langages\nconstruits sur le principe de briques.\nCes briques sont ce qu‚Äôon appelle des packages.\nAu contraire de Stata mais comme pour R,\nil\nfaut toujours pr√©ciser les packages que vous utilisez au d√©but du code,\nsinon Python ne reconnait pas les fonctions appel√©es.\nLes tests permettent d‚Äôex√©cuter telle ou telle instruction\nselon la valeur d‚Äôune condition.\nPour faire un test avec un bloc d‚Äôinstructions, il faut toujours :\nIl existe deux types de boucles : les boucles for et les boucles while\nLa boucle for parcourt un ensemble, tandis que la boucle while\ncontinue tant qu‚Äôune condition est vraie.\nLes fonctions permettent de faire la m√™me chose sans avoir √† recopier le code plusieurs fois dans le m√™me programme. D√®s que vous le pouvez, faites des fonctions : le copier-coller est trop dangereux.\n# 1er exemple de fonction\n\ndef ma_fonction_increment(parametre) : \n    \"\"\"Cette fonction ajoute 1 au param√®tre qu'on lui donne\"\"\"\n    x = parametre + 1 \n    return x\n\n# pour documenter la fonction, on √©crit son aide\nhelp(ma_fonction_increment)\n\nHelp on function ma_fonction_increment in module __main__:\n\nma_fonction_increment(parametre)\n    Cette fonction ajoute 1 au param√®tre qu'on lui donne\nOn peut √©galement :\ndef ma_fonction(p,q = 2) :\n    y1 = p + q\n    y2 = y1%3 #reste de la division euclidienne\n    return y1,y2\n\nx = ma_fonction(11) \n# ici, on n'a pas de 2nd param√®tre\n#, par d√©faut, x = ma_fonction(10,2)\nprint(\"x=\", x)\n\nz = ma_fonction(10,-1)\nprint(\"z =\",z)\n\nx= (13, 1)\nz = (9, 0)\nUne fonction peut √©galement s‚Äôappeler elle m√™me : c‚Äôest ce qu‚Äôon appelle une fonction r√©cursive.\nDans cet exemple, somme_recursion() est une fonction que nous avons d√©finie de sorte √† ce qu‚Äôelle s‚Äôappelle elle-m√™me (r√©cursif).\nOn utilise l‚Äôargument k, qui d√©croit (-1) chaque fois qu‚Äôon fait appel √† la fonction.\nLa r√©cursion s‚Äôarr√™te quand k est nul.\nDans cet exemple, on va donc appeler 6 fois la fonction r√©cursive.\ndef somme_recursion(k):\n    if(k &gt; 0):\n        result = k + somme_recursion(k - 1)\n        print(k,result)\n    else:\n        result = 0\n    return result\nsomme_recursion(6)\n\n1 1\n2 3\n3 6\n4 10\n5 15\n6 21\n\n\n21\nLes fonctions sont tr√®s utiles et nous vous invitons √† les utiliser d√®s que vous le pouvez car elles permettent d‚Äôavoir un code clair et structur√©, plut√¥t que des bouts de code √©parpill√©s.\nPython peut rencontrer des erreurs en ex√©cutant votre programme.\nCes erreurs peuvent √™tre intercept√©es tr√®s facilement et c‚Äôest m√™me, dans certains cas, indispensable. Par exemple, si vous voulez faire une boucle mais que vous savez que l‚Äôinstruction ne marchera pas toujours : au lieu de lister les cas o√π une op√©ration n‚Äôest pas possible, on peut indiquer directement quelle erreur doit √™tre ignor√©e.\nCependant, il ne faut pas tout intercepter non plus : si Python envoie une erreur, c‚Äôest qu‚Äôil y a une raison. Si vous ignorez une erreur, vous risquez d‚Äôavoir des r√©sultats tr√®s √©tranges dans votre programme.\n# √©viter une division par 0, c'est une bonne id√©e : \n\ndef inverse(x) :\n    '''Cette fonction renvoie l inverse de l argument'''\n    y = 1/x\n    return y\n\ndiv = inverse(0)\n\nZeroDivisionError: division by zero\nL‚Äôerreur est √©crite noir sur blanc : ZeroDivisionError\nDans l‚Äôid√©al on aimerait √©viter que notre code bloque sur ce probl√®me. On pourrait passer par un test if et v√©rifier que x est diff√©rent de 0. Mais on se rend vite compte que dans certains cas, on ne peut lister tous les tests en fonction de valeurs.\nAlors on va lui pr√©cisier ce qu‚Äôil doit faire en fonction de l‚Äôerreur retourn√©e.\nSyntaxe\nTry :\nexcept TypeErreur :\ndef inverse(x) : \n    try :\n        y = 1/x\n    except ZeroDivisionError :\n        y = None\n    return y\n    \nprint(inverse(10))\nprint(inverse(0))\n\n0.1\nNone\nIl est recommand√© de toujours pr√©ciser le type d‚Äôerreur qu‚Äôon rencontre. Si on met uniquement ‚Äúexcept‚Äù sans pr√©ciser le type, on peut passer √† c√¥t√© d‚Äôerreurs pour lesquelles la solution n‚Äôest pas universelle."
  },
  {
    "objectID": "content/course/getting-started/06_rappels_fonctions/index.html#import-module",
    "href": "content/course/getting-started/06_rappels_fonctions/index.html#import-module",
    "title": "25¬† Modules, tests, boucles, fonctions",
    "section": "26.1 Import module",
    "text": "26.1 Import module\nOn charge un module gr√¢ce √† la commande import.\nPour chaque code que vous ex√©cutez,\nil faut charger les modules en introduction.\nUne fois qu‚Äôon a charg√© le module,\non peut faire appel aux commandes qui en d√©pendent en les appelant\napr√®s avoir tap√© le nom du module.\nSi vous ne pr√©cisez pas le nom du module avant celui de la fonction,\nil ne la trouvera pas forc√©ment.\nVoici un exemple avec le module numpy\nqui est tr√®s courant et permet de faire des\ncalculs matriciels sous Python.\n\nimport numpy\nprint(numpy.arange(5))\n\n[0 1 2 3 4]\n\n\n\n26.1.1 Import module as md - donner un nom au module\nOn peut aussi donner un pseudonyme au module pour\n√©viter de taper un nom trop long √† chaque fois\nqu‚Äôon utilise une fonction.\nClassiquement le nom raccourci de numpy est np,\ncelui de pandas est pd.\n\nimport pandas as pd\nimport numpy as np\nsmall_array = np.array([[1, 2], [3, 4]])\ndata = pd.DataFrame(small_array)\ndata.head()\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n1\n2\n\n\n1\n3\n4\n\n\n\n\n\n\n\n\n\n26.1.2 from Module Import fonction - seulement une partie du module\nSi on ne veut pas √™tre oblig√© de donner\nle nom du module avant d‚Äôappeler\nla fonction,\nil y a toujours la possibilit√© de n‚Äôimporter qu‚Äôune fonction du module.\nDans le cas de l‚Äôexemple, Python sait que la fonction arrange est celle de numpy.\nMais attention : si deux fonctions de modules diff√©rents\nont le m√™me nom,\nc‚Äôest toujours la derni√®re import√©e qui gagne.\nOn voit souvent from _module_ import *.\nC‚Äôest-√†-dire qu‚Äôon importe toutes\nles fonctions du module\nmais on n‚Äôa pas besoin de sp√©cifier le nom du module avant les m√©thodes.\n{{% box status=‚Äúwarning‚Äù title=‚ÄúWarning‚Äù icon=‚Äúfa fa-exclamation-triangle‚Äù %}}\nLa m√©thode from _module_ import * n‚Äôest pas recommand√©e car elle rend le code moins intelligible.\nEn effet, d‚Äôo√π vient la fonction floor ? De maths ou de numpy ?\nElle risque\naussi de cr√©er des conflits de fonction, qui malgr√© un nom commun peuvent ne\npas attendre les m√™mes arguments ou objets.\n{{% /box %}}\n\nfrom numpy import array\nprint(array(5))\n\n5"
  },
  {
    "objectID": "content/course/getting-started/06_rappels_fonctions/index.html#test-avec-contrepartie-if-et-else",
    "href": "content/course/getting-started/06_rappels_fonctions/index.html#test-avec-contrepartie-if-et-else",
    "title": "25¬† Modules, tests, boucles, fonctions",
    "section": "27.1 Test avec contrepartie : if et else",
    "text": "27.1 Test avec contrepartie : if et else\nComme dans les autres langages,\non teste une condition. Si elle est v√©rifi√©e,\nalors une instruction suit et sinon, une autre instruction est ex√©cut√©e.\nIl est conseill√© de toujours indiquer une contrepartie afin d‚Äô√©viter les surprises.\n\n27.1.1 Test d‚Äôune √©qualit√© ou in√©qualit√©\n\nx = 6\n\nif x &gt; 5 :\n    print(\"x est plus grand que 5\")\nelse : # la contrepartie si la condition if n'est pas r√©alis√©e\n    print(\"x est plus petit que 5\")\n\nx est plus grand que 5\n\n\n\n\n27.1.2 Test dans un intervalle\n\n# on peut avoir des intervalles directement\nx = 6\nif 5 &lt; x &lt; 10 : \n    print(\"x est entre 5 et 10\")\nelse : \n    print(\"x est plus grand que 10\")\n\nx est entre 5 et 10\n\n\n\n# tester plusieurs valeurs avec l'op√©rateur logique \"or\"\nx = 5\nif x == 5 or x == 10 : \n    print(\"x vaut 5 ou 10\")    \nelse : \n    print(\"x est diff√©rent de 5 et 10\")\n\nx vaut 5 ou 10\n\n\n\n\n27.1.3 Tests successifs : if, elif et else\nAvec if et elif,\nd√®s qu‚Äôon rencontre une condition qui est r√©alis√©e,\non n‚Äôen cherche pas d‚Äôautres potentiellement v√©rifi√©es.\nPlusieurs if √† la suite peuvent quant √† eux √™tre v√©rifi√©s.\nSuivant ce que vous souhaitez faire, les op√©rateurs ne sont pas substituables.\nNotez la diff√©rence entre ces deux bouts de code :\n\n#code 1\nx = 5\n\nif x != 10 : \n    print(\"x ne vaut pas 10\")   \nelif x &gt;= 5 : \n    print(\"x est √©gal ou sup√©rieur √† 5\")\n\nx ne vaut pas 10\n\n\nDans le cas de elif, on s‚Äôarr√™te √† la premi√®re condition v√©rifi√©e et dans le cas suivant, on continue √† chaque condition v√©rifi√©e\n\n#code 2\nx = 5\n\nif x != 10 : \n    print(\"x ne vaut pas 10\")   \nif x &gt;= 5 : \n    print(\"x est √©gal ou sup√©rieur √† 5\")\n\nx ne vaut pas 10\nx est √©gal ou sup√©rieur √† 5"
  },
  {
    "objectID": "content/course/getting-started/06_rappels_fonctions/index.html#boucle-for",
    "href": "content/course/getting-started/06_rappels_fonctions/index.html#boucle-for",
    "title": "25¬† Modules, tests, boucles, fonctions",
    "section": "28.1 Boucle for",
    "text": "28.1 Boucle for\n\n28.1.1 Parcourir une liste croissantes d‚Äôentiers\n\n# parcourt les entiers de 0 √† n-1 inclus\nfor i in range(0,3) : \n    print(i)\n\n0\n1\n2\n\n\n\n\n28.1.2 Parcourir une liste d√©croissante d‚Äôentiers\n\n# parcourt les entiers de 3 √† n+1 inclus\nfor i in range(3,0,-1) : \n    print(i)\n\n3\n2\n1\n\n\n\n\n28.1.3 Parcourir une liste de chaines de caract√®res\nOn va faire une boucle sur les √©l√©ments d‚Äôune liste\n\n28.1.3.1 Boucle sur les √©l√©ments d‚Äôune liste\n\nliste_elements = ['Nicolas','Romain','Florimond']\n\n# pour avoir l'ensemble des √©l√©ments de la liste\nfor item in liste_elements : \n    print(item)\n\nNicolas\nRomain\nFlorimond\n\n\n\n\n28.1.3.2 Boucle sur les √©l√©ments d‚Äôune liste dans une autre liste\n\n# pour avoir la place des √©l√©ments de la premi√®re liste dans la seconde liste  \n\nliste_globale = ['Violette','Nicolas','Mathilde','Romain','Florimond','Helene'] \n\nfor item in liste_elements : \n    print(item,liste_globale.index(item))\n\nNicolas 1\nRomain 3\nFlorimond 4\n\n\n\n\n\n28.1.4 Bonus : les list comprehension\nAvec les listes, il existe aussi un moyen tr√®s √©l√©gant de condenser son code pour √©viter de faire apparaitre des boucles sans arr√™t. Comme les boucles doivent etre indent√©es, le code peut rapidement devenir illisible.\nGrace aux list comprehension, vous pouvez en une ligne faire ce qu‚Äôune boucle vous permettait de faire en 3 lignes.\nPar exemple, imaginez que vous vouliez faire la liste de toutes les lettres contenues dans un mot, avec un boucle vous devrez d‚Äôabord cr√©er une liste vide, puis ajouter √† cette liste toutes les lettres en question avec un .append()\n\nliste_lettres = []\n\nfor lettre in 'ENSAE':\n    liste_lettres.append(lettre)\n\nprint(liste_lettres)\n\n['E', 'N', 'S', 'A', 'E']\n\n\navec une list comprehension, on condense la syntaxe de la mani√®re suivante :\n\nh_letters = [ letter for letter in 'ENSAE' ]\nprint(h_letters)\n\n['E', 'N', 'S', 'A', 'E']\n\n\nAvec une list comprehension\n[ expression for item in list if conditional ]\nest √©quivalent √†\nfor item in list:\n    if conditional:\n        expression  \n\n28.1.4.1 Mise en application\nMettez sous forme de list comprehension le bout de code suivant\n\nsquares = []\n\nfor x in range(10):\n    squares.append(x**2)\nprint(squares)\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\n\n# Correction\nsquares = [x**2 for x in range(10)]\nsquares\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
  },
  {
    "objectID": "content/course/getting-started/06_rappels_fonctions/index.html#boucle-while",
    "href": "content/course/getting-started/06_rappels_fonctions/index.html#boucle-while",
    "title": "25¬† Modules, tests, boucles, fonctions",
    "section": "28.2 Boucle while",
    "text": "28.2 Boucle while\nLe bloc d‚Äôinstruction d‚Äôune boucle while est ex√©cut√© tant que la condition est v√©rifi√©e.\nLe pi√®ge de ces boucles : la boucle while infinie ! Il faut toujours v√©rifier que votre boucle s‚Äôarr√™tera un jour, il faut qu‚Äô√† un moment ou √† un autre, il y ait un √©l√©ment qui s‚Äôincr√©mente ou qui soit modifi√©.\n\nx = 10\ny = 8\n# tant que y est plus petit que 10, je continue de lui ajouter 1\nwhile y &lt;= x : \n    print(\"y n'est pas encore plus grand que x\")\n    y += 1 # l'incr√©ment\nelse : \n    print(\"y est plus grand que x et vaut\",y)\n\ny n'est pas encore plus grand que x\ny n'est pas encore plus grand que x\ny n'est pas encore plus grand que x\ny est plus grand que x et vaut 11"
  },
  {
    "objectID": "content/course/getting-started/06_rappels_fonctions/index.html#break-et-continue",
    "href": "content/course/getting-started/06_rappels_fonctions/index.html#break-et-continue",
    "title": "25¬† Modules, tests, boucles, fonctions",
    "section": "28.3 Break et continue",
    "text": "28.3 Break et continue\nDans les boucles for ou while on peut avoir besoin d‚Äôignorer ou de ne pas effectuer certaines it√©rations. 2 instructions utiles :\n\nl‚Äôinstruction break : permet de sortir de la boucle\nl‚Äôinstruction continue : permet de passer √† l‚Äôit√©ration suivante sans ex√©cuter les instructions qui suivent\n\n\n# utilisation de break\nfor x in range(5) : \n    if x == 2 : \n        break\n    else :\n        print(x)\n\n0\n1\n\n\n\n# utilisation de continue\nfor x in range(5) : \n    if x == 2 : \n        continue\n    else :\n        print(x)\n\n0\n1\n3\n4"
  },
  {
    "objectID": "content/course/getting-started/06_rappels_fonctions/index.html#a-retenir",
    "href": "content/course/getting-started/06_rappels_fonctions/index.html#a-retenir",
    "title": "25¬† Modules, tests, boucles, fonctions",
    "section": "31.1 A retenir",
    "text": "31.1 A retenir\n\nToujours mettre ‚Äú:‚Äù avant un bloc d‚Äôinstructions\nIndenter avant un bloc d‚Äôinstructions (avec 4 espaces et non une tabulation !)\nIndiquer les modules n√©cessaires √† l‚Äôex√©cution en d√©but de code\nDocumenter les fonctions cr√©√©es\nPr√©ciser le type d‚Äôerreur pour les exceptions et potentiellement diff√©rencier les blocs d‚Äôinstructions en fonction de l‚Äôerreur"
  },
  {
    "objectID": "content/course/getting-started/06_rappels_fonctions/index.html#questions",
    "href": "content/course/getting-started/06_rappels_fonctions/index.html#questions",
    "title": "25¬† Modules, tests, boucles, fonctions",
    "section": "31.2 Questions",
    "text": "31.2 Questions\n\nQue fait ce programme ?\n\n\ndef inverse(x) : \n    try :\n        y = 1/x\n    except ZeroDivisionError :\n        y = None\n        return y\n\n\nEcrivez un programme qui peut trouver tous les nombres divisibles par 7 et non multiples de 5 entre 6523 et 8463 (inclus)\nEcrivez un programme qui prend une phrase en entr√©e et qui calcule le nombre de voyelles en Majuscules et de consonnes en minuscules :\n\nphrase = ‚ÄúVous savez, moi je ne crois pas qu‚Äôil y ait de bonne ou de mauvaise situation. Moi, si je devais r√©sumer ma vie aujourd‚Äôhui avec vous, je dirais que c‚Äôest d‚Äôabord des rencontres. Des gens qui m‚Äôont tendu la main, peut-√™tre √† un moment o√π je ne pouvais pas, o√π j‚Äô√©tais seul chez moi. Et c‚Äôest assez curieux de se dire que les hasards, les rencontres forgent une destin√©e‚Ä¶ Parce que quand on a le go√ªt de la chose, quand on a le go√ªt de la chose bien faite, le beau geste, parfois on ne trouve pas l‚Äôinterlocuteur en face je dirais, le miroir qui vous aide √† avancer. Alors √ßa n‚Äôest pas mon cas, comme je disais l√†, puisque moi au contraire, j‚Äôai pu : et je dis merci √† la vie, je lui dis merci, je chante la vie, je danse la vie‚Ä¶ je ne suis qu‚Äôamour ! Et finalement, quand beaucoup de gens aujourd‚Äôhui me disent ‚ÄòMais comment fais-tu pour avoir cette humanit√© ?‚Äô, et bien je leur r√©ponds tr√®s simplement, je leur dis que c‚Äôest ce go√ªt de l‚Äôamour ce go√ªt donc qui m‚Äôa pouss√© aujourd‚Äôhui √† entreprendre une construction m√©canique, mais demain qui sait ? Peut-√™tre simplement √† me mettre au service de la communaut√©, √† faire le don, le don de soi‚Ä¶‚Äù"
  },
  {
    "objectID": "content/course/getting-started/07_rappels_classes/index.html",
    "href": "content/course/getting-started/07_rappels_classes/index.html",
    "title": "26¬† Les classes en Python",
    "section": "",
    "text": "27 Qu‚Äôest-ce que la programmation orient√©e objet ?\nLe langage Python se base sur des objets et d√©finit pour eux des actions.\nSelon le type d‚Äôobjet, les actions seront diff√©rentes.\nOn parle √† ce propos de langage orient√© objet ce qui signifie\nque la syntaxe du langage Python permet de d√©finir de mani√®re conceptuelle\ndes objets et appliquer des traitements coh√©rents avec leur structure interne.\nPar exemple,\npour manipuler des donn√©es textuelles ou num√©riques, on aura\nbesoin d‚Äôappliquer des m√©thodes diff√©rentes. Prenons l‚Äôexemple\nde l‚Äôop√©ration +. Pour des donn√©es num√©riques, il s‚Äôagit\nde l‚Äôaddition. Pour des donn√©es textuelles, l‚Äôaddition n‚Äôa pas de sens\nmais on peut envisager d‚Äôappliquer cette op√©ration pour faire de la\nconcat√©nation.\nChaque type d‚Äôobjet se verra donc appliquer des actions\nadapt√©es. Cela offre une grande flexibilit√© au langage Python car on\npeut d√©finir une m√©thode g√©n√©rique (par exemple l‚Äôaddition) et l‚Äôadapter\n√† diff√©rents types d‚Äôobjets.\nLe fait que Python soit un langage orient√© objet a une influence sur la\nsyntaxe. On retrouvera r√©guli√®re la syntaxe objet.method qui est au coeur\nde Python. Par exemple pd.DataFrame.mean se traduit par\nappliquer la m√©thode mean a un objet de type pd.DataFrame.\nPour d√©finir un objet, il faut lui donner des caract√©ristiques et des actions, ce qu‚Äôil est, ce qu‚Äôil peut faire.\nAvec une liste, on peut ajouter des √©l√©ments par exemple avec l‚Äôaction .append(). On peut cr√©er autant d‚Äôobjets ‚Äúliste‚Äù qu‚Äôon le souhaite.\nUne classe regroupe des fonctions et des attributs qui d√©finissent un objet.\nUn objet est une instance d‚Äôune classe, c‚Äôest-√†-dire un exemplaire issu de la classe. L‚Äôobjet avec un comportement et un √©tat, tous deux d√©finis par la classe. On peut cr√©er autant d‚Äôobjets que l‚Äôon d√©sire avec une classe donn√©e.\nIci nous allons essayer de cr√©er une classe chat, avec des attributs pour caract√©riser le chat et des actions, pour voir ce qu‚Äôil peut faire avec un objet de la classe chat."
  },
  {
    "objectID": "content/course/getting-started/07_rappels_classes/index.html#quand-utilise-t-on-cela-dans-le-domaine-de-la-data-science",
    "href": "content/course/getting-started/07_rappels_classes/index.html#quand-utilise-t-on-cela-dans-le-domaine-de-la-data-science",
    "title": "26¬† Les classes en Python",
    "section": "27.1 Quand utilise-t-on cela dans le domaine de la data-science ?",
    "text": "27.1 Quand utilise-t-on cela dans le domaine de la data-science ?\nLes r√©seaux de neurone programm√©s avec Keras ou PyTorch fonctionnent de\ncette mani√®re. On part d‚Äôune structure de base et modifie les attributs (par\nexemple le nombre de couches) ou les m√©thodes."
  },
  {
    "objectID": "content/course/getting-started/07_rappels_classes/index.html#les-attributs-de-la-classe-chat",
    "href": "content/course/getting-started/07_rappels_classes/index.html#les-attributs-de-la-classe-chat",
    "title": "26¬† Les classes en Python",
    "section": "29.1 Les attributs de la classe chat",
    "text": "29.1 Les attributs de la classe chat\n\n29.1.1 Classe chat version 1 - premiers attributs\nOn veut pouvoir cr√©er un objet chat() qui nous permettra √† terme de cr√©er une colonie de chats (on sait\njamais ca peut servir ‚Ä¶).\nPour commencer, on va d√©finir un chat avec des attributs de base : une couleur et un nom.\n\nclass chat: # D√©finition de notre classe chat\n    \"\"\"Classe d√©finissant un chat caract√©ris√© par :\n    - son nom\n    - sa couleur \"\"\"\n    \n    def __init__(self): # Notre m√©thode constructeur - \n        # self c'est notre objet qu'on est en train de cr√©er\n        \"\"\"Pour l'instant, on ne va d√©finir que deux attributs - nom et couleur \"\"\"\n        self.couleur = \"Noir\"   \n        self.nom = \"Aucun nom\"\n\n\nmon_chat = chat()\n\nprint(type(mon_chat), mon_chat.couleur ,\",\", mon_chat.nom) \n\n&lt;class '__main__.chat'&gt; Noir , Aucun nom\n\n\nOn nous dit bien que Mon chat est d√©fini √† partir de la classe chat,\nc‚Äôest ce que nous apprend la fonction type.\nPour l‚Äôinstant il n‚Äôa pas de nom\n\n\n29.1.2 Classe chat version 2 - autres attributs\nAvec un nom et une couleur, on ne va pas loin. On peut continuer √† d√©finir des attributs pour la classe chat\nde la m√™me fa√ßon que pr√©c√©demment.\n\nclass chat: # D√©finition de notre classe chat\n    \"\"\"Classe d√©finissant un chat caract√©ris√© par :\n    - sa couleur\n    - son √¢ge\n    - son caract√®re\n    - son poids\n    - son maitre\n    - son nom \"\"\"\n\n    \n    def __init__(self): # Notre m√©thode constructeur - \n        #self c'est notre objet qu'on est en train de cr√©er\n        self.couleur = \"Noir\"    \n        self.age = 10\n        self.caractere = \"Joueur\"\n        self.poids = 3\n        self.maitre = \"Jeanne\"\n        self.nom = \"Aucun nom\"\n\n\nhelp(chat) \n# si on veut savoir ce que fait la classe \"chat\" on appelle l'aide\n\nHelp on class chat in module __main__:\n\nclass chat(builtins.object)\n |  Classe d√©finissant un chat caract√©ris√© par :\n |  - sa couleur\n |  - son √¢ge\n |  - son caract√®re\n |  - son poids\n |  - son maitre\n |  - son nom\n |  \n |  Methods defined here:\n |  \n |  __init__(self)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n\n\n\nmon_chat = chat()\nprint(\"L'√¢ge du chat est\", mon_chat.age,\"ans\") \n# on avait d√©fini l'attribut age de la classe chat comme √©tant √©gal √† 10\n#, si on demande l'attribut age de notre Martin on obtient 10\n\nL'√¢ge du chat est 10 ans\n\n\nPar d√©faut, les attributs de la classe Chat seront toujours les m√™mes √† chaque cr√©ation de chat √† partir\nde la classe Chat.\nMais une fois qu‚Äôune instance de classe est cr√©√©e (ici mon chat est une instance de classe) on peut d√©cider\nde changer la valeur de ses attributs.\n\n29.1.2.1 Un nouveau poids\n\nprint(mon_chat.poids)\n# si on veut changer le poids de mon chat, parce qu'il a un peu grossi apr√®s les f√™tes\nmon_chat.poids = 3.5\nprint(mon_chat.poids) # maintenant le poids est 3.5\n\n3\n3.5\n\n\n\n\n29.1.2.2 Un nouveau nom\n\n# on veut aussi lui donner un nom \nmon_chat.nom = \"Martin\"\nmon_chat.nom\n\n'Martin'\n\n\n\n\n29.1.2.3 Une autre instance de la classe Chat\nOn peut aussi cr√©er d‚Äôautres objets chat √† partir de la classe chat :\n\n# on appelle la classe\nl_autre_chat = chat()\n# on change les attributs qui nous int√©ressent\nl_autre_chat.nom = \"Ginette\"\nl_autre_chat.maitre = \"Roger\"\n# les attributs inchang√©s donnent la m√™me chose \n# que ceux d√©finis par d√©faut pour la classe\nprint(l_autre_chat.couleur)\n\nNoir"
  },
  {
    "objectID": "content/course/getting-started/07_rappels_classes/index.html#les-m√©thodes-de-la-classe-chat",
    "href": "content/course/getting-started/07_rappels_classes/index.html#les-m√©thodes-de-la-classe-chat",
    "title": "26¬† Les classes en Python",
    "section": "29.2 Les m√©thodes de la classe chat",
    "text": "29.2 Les m√©thodes de la classe chat\nLes attributs sont des variables propres √† notre objet, qui servent √† le caract√©riser.\nLes m√©thodes sont plut√¥t des actions, comme nous l‚Äôavons vu dans la partie pr√©c√©dente, agissant sur l‚Äôobjet.\nPar exemple, la m√©thode append de la classe list permet d‚Äôajouter un √©l√©ment dans l‚Äôobjet list manipul√©.\n\n29.2.1 Classe chat version 3 - premi√®re m√©thode\nOn peut d√©finir une premi√®re m√©thode : nourrir\n\nclass chat: # D√©finition de notre classe chat\n    \"\"\"Classe d√©finissant un chat caract√©ris√© par :\n    - sa couleur\n    - son √¢ge\n    - son caract√®re\n    - son poids\n    - son maitre\n    - son nom \n    \n    L'objet chat a une m√©thode : nourrir \"\"\"\n\n    \n    def __init__(self): # Notre m√©thode constructeur - \n        #self c'est notre objet qu'on est en train de cr√©er\n        self.couleur = \"Noir\"    \n        self.age = 10\n        self.caractere = \"Joueur\"\n        self.poids = 3\n        self.maitre = \"Jeanne\"\n        self.nom = \"Aucun nom\"\n        \n        \"\"\"Par d√©faut, notre ventre est vide\"\"\"\n        self.ventre = \"\"\n        \n    def nourrir(self, nourriture):\n        \"\"\"M√©thode permettant de donner √† manger au chat.\n        Si le ventre n'est pas vide, on met une virgule avant de rajouter\n        la nourriture\"\"\"       \n        if self.ventre != \"\":\n            self.ventre += \",\"\n        self.ventre += nourriture\n\n\nmon_chat = chat()\nmon_chat.nom = \"Martin\"\nmon_chat.ventre # On n'a rien donn√© √† Martin, son ventre est vide\n\n''\n\n\n\n# on appelle la m√©thode \"nourrir\" de la classe chat, \n# on lui donne un √©l√©ment, ici des croquettes\nmon_chat.nourrir('Croquettes')\nprint(\"Le contenu du ventre de martin : \",mon_chat.ventre)\n\nLe contenu du ventre de martin :  Croquettes\n\n\n\nmon_chat.nourrir('Saumon')\nprint(\"Le contenu du ventre de martin : \",mon_chat.ventre)\n\nLe contenu du ventre de martin :  Croquettes,Saumon\n\n\n\n\n29.2.2 Classe chat version 4 - autre m√©thode\nAvec un chat, on peut imaginer plein de m√©thodes. Ici on va d√©finir une action ‚Äúnourrir‚Äù et une autre action\n‚Äúlitiere‚Äù, qui consiste √† vider l‚Äôestomac du chat.\n\nclass chat: # D√©finition de notre classe Personne\n    \"\"\"Classe d√©finissant un chat caract√©ris√© par :\n    - sa couleur\n    - son √¢ge\n    - son caract√®re\n    - son poids\n    - son maitre\n    - son nom \n    \n    L'objet chat a deux m√©thodes : nourrir et litiere \"\"\"\n\n    \n    def __init__(self): # Notre m√©thode constructeur - \n        #self c'est notre objet qu'on est en train de cr√©er\n        self.nom = \"\"\n        self.couleur = \"Roux\"    \n        self.age = 10\n        self.caractere = \"Joueur\"\n        self.poids = 3\n        self.maitre = \"Jeanne\"\n        \"\"\"Par d√©faut, notre ventre est vide\"\"\"\n        self.ventre = \"\"\n        \n    def nourrir(self, nourriture):\n        \"\"\"M√©thode permettant de donner √† manger au chat.\n        Si le ventre n'est pas vide, on met une virgule avant de rajouter\n        la nourriture\"\"\"       \n        if self.ventre != \"\":\n            self.ventre += \",\"\n        self.ventre += nourriture\n\n    def litiere(self) : \n        \"\"\" M√©thode permettant au chat d'aller √† sa liti√®re : \n        en cons√©quence son ventre est vide \"\"\"       \n        self.ventre = \"\"\n        print(self.nom,\"a le ventre vide\")\n\n\n# on d√©finit Martin le chat\nmon_chat = chat()\nmon_chat.nom = \"Martin\"\n# on le nourrit avec des croquettes\nmon_chat.nourrir('croquettes')\nprint(\"Le contenu du ventre de martin\", mon_chat.ventre)\n\n\n# Il va dans sa litiere\nmon_chat.litiere()\n\nLe contenu du ventre de martin croquettes\nMartin a le ventre vide\n\n\n\nhelp(mon_chat.nourrir)\nhelp(mon_chat.litiere)\n\nHelp on method nourrir in module __main__:\n\nnourrir(nourriture) method of __main__.chat instance\n    M√©thode permettant de donner √† manger au chat.\n    Si le ventre n'est pas vide, on met une virgule avant de rajouter\n    la nourriture\n\nHelp on method litiere in module __main__:\n\nlitiere() method of __main__.chat instance\n    M√©thode permettant au chat d'aller √† sa liti√®re : \n    en cons√©quence son ventre est vide\n\n\n\n\n\n29.2.3 facultatif Les m√©thodes sp√©ciales\nSi on reprend notre classe chat, il y a en r√©alit√© des m√©thodes sp√©ciales que nous n‚Äôavons pas d√©finies mais\nqui sont implicites.\nPython comprend seul ce que doivent faire ces m√©thodes. Il a une id√©e pr√©concue de ce qu‚Äôelles doivent\neffectuer comme op√©ration. Si vous ne red√©finissez par une m√©thode sp√©ciale pour qu‚Äôelle fasse ce que vous\nsouhaitez, ca peut donner des r\u0013esultats inattendus.\nElles servent √† plusieurs choses :\n\n√† initialiser l‚Äôobjet instanci√© : __init__\n√† modifier son affichage : __repr__\n\n\n\n# pour avoir la valeur de l'attribut \"nom\"\n\nprint(mon_chat.__getattribute__(\"nom\"))\n# on aurait aussi pu faire plus simple :\nprint(mon_chat.nom)\n\nMartin\nMartin\n\n\n# si l'attribut n'existe pas : on a une erreur\n# Python recherche l'attribut et, s'il ne le trouve pas dans l'objet et si une m√©thode __getattr__ est sp√©cifi√©e, \n# il va l'appeler en lui passant en param√®tre le nom de l'attribut recherch√©, sous la forme d'une cha√Æne de caract√®res.\n\nprint(mon_chat.origine)\n## Error in py_call_impl(callable, dots$args, dots$keywords): AttributeError: 'chat' object has no attribute 'origine'\n## \n## Detailed traceback: \n##   File \"&lt;string&gt;\", line 1, in &lt;module&gt;\nMais on peut modifier les m√©thodes sp√©ciales de notre classe chat pour √©viter d‚Äôavoir des erreurs d‚Äôattributs. On va aussi en profiter pour modifier la repr√©sentation de l‚Äôinstance chat qui pour l‚Äôinstant donne &lt;_main_.chat object at 0x0000000005AB4C50&gt;\n\n\n29.2.4 Classe chat version 5 - m√©thode sp√©ciale\n\nclass chat: # D√©finition de notre classe Personne\n    \"\"\"Classe d√©finissant un chat caract√©ris√© par :\n    - sa couleur\n    - son √¢ge\n    - son caract√®re\n    - son poids\n    - son maitre\n    - son nom \n    \n    L'objet chat a deux m√©thodes : nourrir et litiere \"\"\"\n\n    \n    def __init__(self): # Notre m√©thode constructeur - \n        #self c'est notre objet qu'on est en train de cr√©er\n        self.nom = \"\"\n        self.couleur = \"Roux\"    \n        self.age = 10\n        self.caractere = \"Joueur\"\n        self.poids = 3\n        self.maitre = \"Jeanne\"\n        \"\"\"Par d√©faut, notre ventre est vide\"\"\"\n        self.ventre = \"\"\n        \n    def nourrir(self, nourriture):\n        \"\"\"M√©thode permettant de donner √† manger au chat.\n        Si le ventre n'est pas vide, on met une virgule avant de rajouter\n        la nourriture\"\"\"       \n        if self.ventre != \"\":\n            self.ventre += \",\"\n        self.ventre += nourriture\n\n    def litiere(self) : \n        \"\"\" M√©thode permettant au chat d'aller √† sa liti√®re : \n        en cons√©quence son ventre est vide \"\"\"       \n        self.ventre = \"\"\n        print(self.nom,\"a le ventre vide\")\n    \n    def __getattribute__(self, key):\n            return print(key,\"n'est pas un attribut de la classe chat\")   \n            \n    def __repr__(self):\n            return \"Je suis une instance de la classe chat\"\n\n\n# j'ai gard√© l'exemple chat d√©fini selon la classe version 4\n# Martin, le chat\n# on a vu pr√©c√©demment qu'il n'avait pas d'attribut origine\n# et que cela levait une erreur AttributeError\nprint(mon_chat.nom)\n\n\n# on va d√©finir un nouveau chat avec la version 5\n# on appelle √† nouveau un attribut qui n'existe pas \"origine\"\n# on a bien le message d√©fini par la m√©thode sp√©ciale _gettattribute\n\nmon_chat_nouvelle_version = chat()\nmon_chat_nouvelle_version.origine\n\n# Maintenant on a aussi une d√©finition de l'objet plus clair\nprint(mon_chat)\nprint(mon_chat_nouvelle_version)\n\nMartin\norigine n'est pas un attribut de la classe chat\n&lt;__main__.chat object at 0x7f5b10a7dac0&gt;\nJe suis une instance de la classe chat\n\n\n\n\n\n29.2.5 Conclusion sur les classes : ce qu‚Äôon retient\n\nLes m√©thodes se d√©finissent comme des fonctions, sauf qu‚Äôelles se trouvent dans le corps de la classe.\nOn d√©finit les attributs d‚Äôune instance dans le constructeur de sa classe, en suivant cette syntaxe : self.nom_attribut = valeur.\nfacultatif : Les m√©thodes d‚Äôinstance prennent en premier param√®tre ‚Äúself‚Äù, l‚Äôinstance de l‚Äôobjet manipul√©.\nfacultatif : On construit une instance de classe en appelant son constructeur, une m√©thode d‚Äôinstance appel√©e init."
  },
  {
    "objectID": "content/course/getting-started/01_installation/index.html",
    "href": "content/course/getting-started/01_installation/index.html",
    "title": "27¬† Configuration de Python",
    "section": "",
    "text": "28 Installer un environnement adapt√© √† la data-science sur son ordinateur personnel\nCette partie pr√©sente plusieurs √©l√©ments de configuration d‚Äôun environnement\nen local. Cependant, cette approche est de moins en moins fr√©quente. En effet,\nplusieurs facteurs conjoints ont amen√© √† privil√©gier des\nserveurs plut√¥t que des installations locales (√©volutions dans les technologies cloud,\nbesoins accrus de ressources, besoins de plus de contr√¥le sur la confidentialit√©\ndes donn√©es en limitant leur prolif√©ration‚Ä¶). Au sein des administrations et\ndes entreprises, les approches cloud, o√π l‚Äôutilisateur se voit mis √† disposition\nune interface graphique alors que les calculs sont d√©port√©s sur un serveur\ndistant, est de plus en plus fr√©quent.\nComme √©voqu√© pr√©cedemment, les technologies dominantes dans\nle domaine du traitement des donn√©es ont amen√© √† une √©volution des pratiques\ndepuis quelques ann√©es.\nLa multiplication de donn√©es volumineuses qui d√©passent les capacit√©s en RAM\nvoire en stockage des machines personnelles,\nles progr√®s dans les technologies de stockage type cloud,\nl‚Äôadh√©sion de la communaut√© aux outils de versioning\n(le plus connu √©tant Git) sont autant de facteurs\nayant amen√© √† repenser la mani√®re de traiter des donn√©es.\nLes infrastructures √† l‚Äô√©tat de l‚Äôart permettent ainsi de d√©coupler stockage\ndes donn√©es, stockage du code et ex√©cution des traitements sur les donn√©es.\nL‚Äôex√©cution des traitements s‚Äôeffectue ainsi sur des machines √† la dur√©e de vie\ncourte qui stockent temporairement donn√©es et code ensembles pour tester\nles traitements.\nAvec les d√©p√¥ts sur Github ou Gitlab,\non dissocie environnement de stockage des codes et\nd‚Äôex√©cution de ceux-ci. Un syst√®me de stockage S3, pr√©sent√© dans un\nchapitre ult√©rieur, permet en suppl√©ment de dissocier l‚Äôenvironnement\nde stockage des donn√©es de ces deux premiers environnements.\nSur le\nd√©p√¥t github de ce cours , on peut\nnaviguer dans les fichiers\n(et voir tout l‚Äôhistorique de modification de ceux-ci). Mais,\ncomment ex√©cuter les scripts sans passer par un poste local ?\nDepuis quelques ann√©es, des services en ligne permettant de\nlancer une instance Jupyter √† distance (analogue √† celle que vous pouvez\nlancer en local en utilisant Anaconda) ont √©merg√©. Parmi celles-ci :\nIl est √©galement possible d‚Äôex√©cuter des codes sur les services d‚Äôint√©gration continue de\nGitlab (service Gitlab CI)\nou de Github (via Github Actions). Il s‚Äôagit d‚Äôune approche\nbash, c‚Äôest-√†-dire que les scripts sont ex√©cut√©s par une console √† chaque interaction avec le d√©p√¥t\ndistant Gitlab/Github, sans session ouverte pour les √©diter.\nCette approche est tr√®s appropri√©e\npour assurer la reproductibilit√© d‚Äôune cha√Æne de traitement (on peut aller\njusqu‚Äôau\nd√©ploiement de visualisations automatiques[^2]) mais n‚Äôest pas tr√®s pratique pour\nle griffonnage.\n[^2] A cet √©gard, il est recommand√© de consulter le cours de derni√®re ann√©e\nde l‚ÄôENSAE d√©j√† cit√©: https://ensae-reproductibilite.netlify.app/\nKaggle \npropose des comp√©titions de code mais\ndonne √©galement la possibilit√© d‚Äôex√©cuter des notebooks,\ncomme les solutions pr√©c√©dentes.\nIl existe une API Kaggle pour\nacc√©der √† des donn√©es Kaggle hors du syst√®me Kaggle\n{{% box status=‚Äúwarning‚Äù title=‚ÄúWarning‚Äù icon=‚Äúfa fa-exclamation-triangle‚Äù %}}\nLes performances de ces solutions peuvent √™tre variables.\nLes serveurs publics mis √† disposition\nne sont pas forc√©ment des foudres de guerre. Avec ceux-ci,\non v√©rifie plut√¥t la reproductibilit√© des scripts avec des jeux d‚Äôexemples.\nIl est bien-s√ªr interdit de mettre des donn√©es confidentielles dessus: ces\nderni√®res doivent rester dans des infrastructures o√π elles sont autoris√©es.\nQuand on est dans une entreprise ou administration,\nqui dispose de serveurs propres,\non peut aller plus loin en utilisant ces outils\npour automatiser l‚Äôensemble de la cha√Æne de traitement.\nAttention: il n‚Äôy a pas de garantie de perennit√© de service\n(notamment avec Binder o√π\n10 minutes d‚Äôinactivit√© m√®nent √† l‚Äôextinction du service). Il s‚Äôagit plus d‚Äôun service pour griffoner\ndans le m√™me environnement que celui du d√©p√¥t Git que de solutions durables.\nLes sessions sur l‚Äôenvironnement SSPCloud sont plus durables mais il convient\nde garder √† l‚Äôesprit qu‚Äôelles sont √©galement temporaires.\n{{% /box %}}\nUn module est un script qui a vocation √† d√©finir des objets utilis√©s\npost√©rieurement par un interpr√©teur. C‚Äôest un script .py autosuffisant,\nd√©finissant des objets et des relations entre eux et le monde ext√©rieur\n(d‚Äôautres modules). Un package est un ensemble coh√©rent de modules. Par exemple\nscikit-learn propose de nombreux modules utiles pour le machine learning.\nPython, sans ajout de briques suppl√©mentaires,\ntrouvera rapidement ses limites.\nM√™me dans les scripts les plus simples, on a g√©n√©ralement besoin de packages qui\n√©vitent de r√©inventer la roue.\nLes packages sont les √©l√©ments qui font la richesse des\nlangages open-source.\nIls sont l‚Äô√©quivalent des packages R ou Stata.\nLe monde de d√©veloppeurs Python est tr√®s prolifique :\ndes mises √† jour sont tr√®s souvent disponibles,\nles biblioth√®ques de packages sont tr√®s nombreuses. Un data-scientist\nprendra l‚Äôhabitude de jongler avec des dizaines de packages dont il conna√Ætra\nquelques fonctions et o√π, surtout, il saura aller chercher de l‚Äôinformation.\nLe rythme des mises √† jour et des ajouts de fonctionalit√©s\ns‚Äôest acc√©l√©r√© ces derni√®res ann√©es. Les grandes compagnies du\nnum√©rique ont elles-m√™mes opensourc√©es des librairies\ndevenues centrales dans l‚Äô√©cosyst√®me de la data-science\n(TensorFlow par Google, PyTorch par Facebook‚Ä¶)\nLes forums, notamment StackOverflow\nregorgent de bons conseils.\nLes deux meilleurs conseils qu‚Äôon puisse donner :"
  },
  {
    "objectID": "content/course/getting-started/01_installation/index.html#installer-python-en-local",
    "href": "content/course/getting-started/01_installation/index.html#installer-python-en-local",
    "title": "27¬† Configuration de Python",
    "section": "28.1 Installer Python en local",
    "text": "28.1 Installer Python en local\nPour installer Python, il est recommand√© d‚Äôutiliser\nla distribution Anaconda\nqui permet d‚Äôinstaller une distribution minimale de Python ainsi qu‚Äô√©ventuellement\nun environnement plus complet :\n\nSous Windows, il suffit de t√©l√©charger l‚Äôex√©cutable puis\nl‚Äôex√©cuter (cf.¬†la doc officielle\nou ce site).\nSous Mac, se reporter √† la doc officielle\nSous Linux, suivre les instructions de la doc officielle selon sa distribution\n\nPasser par Anaconda permet:\n\nd‚Äôinstaller Python ;\nd‚Äôinstaller par d√©faut une multitude de packages utiles\n(liste ici) ;\nde pouvoir utiliser un gestionnaire de package nomm√© conda.\n\nAnaconda permet de cr√©er des environnements isol√©s et facilite l‚Äôinstallation\nde certaines librairies qui n√©cessitent l‚Äôusage de langages externes (par exemple\ndu C++)."
  },
  {
    "objectID": "content/course/getting-started/01_installation/index.html#installer-un-environnement-de-d√©veloppement",
    "href": "content/course/getting-started/01_installation/index.html#installer-un-environnement-de-d√©veloppement",
    "title": "27¬† Configuration de Python",
    "section": "28.2 Installer un environnement de d√©veloppement",
    "text": "28.2 Installer un environnement de d√©veloppement\nLes notebooks Jupyter (extension .ipynb)\nsont tr√®s utilis√©s en data science. Ils sont en\nparticulier tr√®s adapt√©s √† la r√©alisation d‚Äôanalyses exploratoires.\nLes notebooks permettent de m√™ler du code, du texte et des sorties\ngraphiques ou des tableaux. L‚Äôint√©r√™t principal des notebooks est qu‚Äôils\npermettent d‚Äôex√©cuter du code tr√®s facilement dans un environnement\nPython donn√© (le kernel Jupyter). Ils sont particuli√®rement pratiques\npour ajouter du code ou du texte √† un document d√©j√† existant, d‚Äôo√π le\nterme de notebook.\nN√©anmoins, pass√© l‚Äô√©tape d‚Äôexploration, il est recommand√© de plut√¥t recourir √† des\nscripts au format .py. L‚Äôutilisation du format .py est l‚Äôun des premiers\ngestes pour favoriser la reproductibilit√© des analyses.\nCes scripts peuvent √™tre √©dit√©s √† l‚Äôaide d‚Äô√©diteurs de texte adapt√©s au code, comme\nVisual Studio\n(mon pr√©f√©r√©),\nSublime Text,\nou PyCharm (privil√©gier Pycharm Community Edition)\nentre autres.\nCes √©diteurs\noffrent des fonctionalit√©s suppl√©mentaires pratiques :\n\nnombreux plugins pour une pleine utilisation de l‚Äô√©cosyst√®me Python: √©diteur de Markdown,\ninterface Git, etc.\nfonctionalit√©s classiques d‚Äôun IDE dont manque Jupyter: autocompl√©tion, diagnostic du code, etc.\nint√©gration avec les environnements Conda"
  },
  {
    "objectID": "content/course/getting-started/01_installation/index.html#installation-de-git",
    "href": "content/course/getting-started/01_installation/index.html#installation-de-git",
    "title": "27¬† Configuration de Python",
    "section": "28.3 Installation de Git",
    "text": "28.3 Installation de Git\nLe principe de Git ainsi que son usage avec Python sont pr√©sent√©s dans\nune partie d√©di√©e. Cette partie se concentre ainsi sur la question\nde la configuration de Git.\nGit est un langage dont la fonction est de tracer l‚Äôhistorique de modification\nd‚Äôun fichier. Pour disposer de ce langage, il est n√©cessaire d‚Äôinstaller\nle logiciel Git Bash. Gr√¢ce √† lui, Git sera disponible et des outils\nexternes, notamment les interfaces de d√©veloppement comme\nVisual Studio, pourront l‚Äôutiliser."
  },
  {
    "objectID": "content/course/getting-started/01_installation/index.html#ssp-cloud-onyxia",
    "href": "content/course/getting-started/01_installation/index.html#ssp-cloud-onyxia",
    "title": "27¬† Configuration de Python",
    "section": "29.1 SSP-Cloud ",
    "text": "29.1 SSP-Cloud \nOnyxia, l‚Äôautre petit nom du SSP-Cloud,\nest une plateforme libre service mutualis√©e de traitement\nde donn√©es statistiques et de datascience.\nCe cloud met √† disposition aux statisticiens et aux data scientists\nde l‚Äô√âtat un catalogue de services et un environnement de travail simple, rapide et collaboratif, permettant de lancer facilement ces outils et d‚Äôy connecter ses donn√©es et son code.\nAu-del√† des ressources techniques, cette plateforme\nrepr√©sente une opportunit√© pour les statisticiens publics et les\n√©tudiants de d√©couvrir\net d‚Äôadopter de nouvelles m√©thodes de travail.\nElle est aussi utilis√© √† des fins de formations et d‚Äôauto-formations.\nDans cet environnement, Jupyter et Visual Studio sont tous deux\ndisponibles."
  },
  {
    "objectID": "content/course/getting-started/01_installation/index.html#google-collaboratory-open-in-colab",
    "href": "content/course/getting-started/01_installation/index.html#google-collaboratory-open-in-colab",
    "title": "27¬† Configuration de Python",
    "section": "29.2 Google collaboratory ",
    "text": "29.2 Google collaboratory \nGoogle met √† disposition une plateforme de calculs bas√©e sur le format Jupyter Notebook.\nUn grand avantage de cette solution est la mise √† disposition gratuite de\nGPUs de qualit√© raisonnable,\noutil quasi-indispensable dans les projets bas√©s sur des m√©thodes de deep learning.\nIl est possible de connecter les notebooks ouverts √† Google Drive ou √†\ngithub. L‚Äôicone\n\nfournit un raccourci pour lancer le notebook dans un environnement d√©di√©."
  },
  {
    "objectID": "content/course/getting-started/01_installation/index.html#github-visual-studio-editor-githubdev",
    "href": "content/course/getting-started/01_installation/index.html#github-visual-studio-editor-githubdev",
    "title": "27¬† Configuration de Python",
    "section": "29.3 Github Visual Studio Editor ",
    "text": "29.3 Github Visual Studio Editor \nMicrosoft qui poss√®de √† la fois Github et Visual Studio a r√©cemment\nlanc√© une offre Github dev qui permet d‚Äôouvrir et lancer un notebook\nJupyter depuis un navigateur web.\nEn plus des fonctionalit√©s attendues du logiciel Visual Studio\nCette interface permet √©galement de g√©rer les issues et pull request\nd‚Äôun d√©p√¥t Github."
  },
  {
    "objectID": "content/course/getting-started/01_installation/index.html#binder-binder",
    "href": "content/course/getting-started/01_installation/index.html#binder-binder",
    "title": "27¬† Configuration de Python",
    "section": "29.4 Binder ",
    "text": "29.4 Binder \nEn cliquant sur cette ic√¥ne\n,\nqu‚Äôon peut retrouver un peu partout dans ce site ou sur le d√©p√¥t\n, vous pouvez lancer un environnement propre,\net disposant d‚Äôune copie\n(un clone en langage Git) du d√©p√¥t Github. Celui-ci n‚Äôint√®gre\npas forc√©ment les d√©pendances n√©cessaires pour un chapitre, il est\nalors n√©cessaire de les installer. Malheureusement, les environnements binder\npeuvent mettre du temps √† se lancer et il est plut√¥t recommand√© de privil√©gier\nune autre approche."
  },
  {
    "objectID": "content/course/getting-started/01_installation/index.html#la-technologie-en-arri√®re-plan-docker",
    "href": "content/course/getting-started/01_installation/index.html#la-technologie-en-arri√®re-plan-docker",
    "title": "27¬† Configuration de Python",
    "section": "29.5 La technologie en arri√®re-plan: Docker ",
    "text": "29.5 La technologie en arri√®re-plan: Docker \nDocker est l‚Äôoutil open-source de r√©f√©rence\nen mati√®re de cr√©ation d‚Äôenvironnements isol√©s et auto-suffisants (\nles conteneurs.\nEn pratique, une application cod√©e en Python ne repose que rarement seulement sur\ndu code produit par son d√©veloppeur, elle fait g√©n√©ralement intervenir des d√©pendances :\nd‚Äôautres librairies Python, ainsi que des librairies li√©es au syst√®me d‚Äôexploitation\nsur laquelle elle est d√©velopp√©e. Docker va permettre d‚Äôempaqueter l‚Äôapplication ainsi\nque toutes ses d√©pendances et rendre son ex√©cution portable, c‚Äôest √† dire ind√©pendante\ndu syst√®me sur laquelle elle est √©x√©cut√©e.\nDocker  est utilis√© dans\nle cadre de cours afin d‚Äôassurer la reproductibilit√© des exemples.\nPlus de d√©tails sont disponibles dans le cours de derni√®re ann√©e d‚ÄôENSAE\nd√©di√© √† la mise en production de projets data-science\n(ensae-reproductibilite.netlify.app/).\nIl est possible d‚Äôutiliser les images Docker sur lesquelles reposent\nl‚Äôenvironnement de reproductibilit√© du cours. Celles-ci sont mises √†\ndisposition sur DockerHub, le principal r√©seau de mise √† disposition\nd‚Äôimages Docker. Il existe une image minimale\nqui int√®gre Python et Quarto.\nPour utiliser l‚Äôimage Visual Studio:\ndocker pull linogaliana/python-datascientist-vstudio\ndocker run --rm -p 8787:8787 -e PASSWORD=test linogaliana/python-datascientist-vstudio\nEn se rendant depuis un navigateur sur localhost:8887/, et en rentrant\nle mot de passe test (d√©fini plus haut), on peut ainsi acc√©der\n√† l‚Äôinterface d√©sir√©e (attention il s‚Äôagit d‚Äôun environnement temporaire, pas\np√©renne)."
  },
  {
    "objectID": "content/course/getting-started/01_installation/index.html#les-gestionnaires-de-packages",
    "href": "content/course/getting-started/01_installation/index.html#les-gestionnaires-de-packages",
    "title": "27¬† Configuration de Python",
    "section": "30.1 Les gestionnaires de packages",
    "text": "30.1 Les gestionnaires de packages\nLes packages d‚Äôun langage open-source sont mis √† disposition sur\ndes d√©p√¥ts. Le CTAN est ainsi le d√©p√¥t \\(\\LaTeX\\) le plus connu, le\nCRAN celui du langage R.\nEn Python, il existe deux gestionnaires de packages qu‚Äôon utilise\nassoci√©s √† deux d√©p√¥ts diff√©rents :\n\npip associ√© au d√©p√¥t PyPi\nconda associ√© au d√©p√¥t Anaconda\n\nAnaconda a permis, il y a quelques ann√©es, de faciliter grandement\nl‚Äôinstallation de librairies d√©pendants d‚Äôautres langages\nque Python (notamment des librairies C pour am√©liorer\nla performance des calculs). Ces derni√®res sont\ncompliqu√©es √† installer, notamment sur Windows.\nLe fait de proposer des librairies pr√©-compil√©es sur une grande\nvari√©t√© de syst√®mes d‚Äôexploitation a √©t√© une avanc√©e\nd‚Äôanaconda. PyPi a adopt√© ce m√™me principe avec les\nwheels ce qui finalement, rend les installations\navec pip √† nouveau int√©ressantes (sauf pour certaines\nlibrairies en Windows).\nAnaconda a deux d√©fauts par rapport √† pip :\n\nl‚Äôinstallation de packages via pip est plus rapide que via\nconda. conda est en effet plus pr√©cautionneux sur l‚Äôinteraction\nentre les diff√©rentes versions des packages install√©s.\nmamba a r√©cemment\n√©t√© d√©velopp√© pour acc√©l√©rer l‚Äôinstallation de packages dans un\nenvironnement conda1\nles versions disponibles sur PyPi sont plus r√©centes\nque celles sur le canal par d√©faut d‚ÄôAnaconda. En effet,\npour un d√©veloppeur de packages, il est possible de publier\nun package de mani√®re automatique sur PyPi\nL‚Äôutilisation\ndu canal alternatif qu‚Äôest la conda forge permet de disposer de versions plus r√©centes des packages et limite l‚Äô√©cart avec les versions\ndisponibles sur PyPi.\n\n{{% box status=‚Äúnote‚Äù title=‚ÄúNote‚Äù icon=‚Äúfa fa-comment‚Äù %}}\nLes conditions d‚Äôutilisation du canal par d√©faut d‚ÄôAnaconda sont\nassez restrictives. L‚Äôutilisation d‚ÄôAnaconda dans un cadre commercial est ainsi, depuis 2020,\nsoumis √† l‚Äôachat de licences commerciales d‚ÄôAnaconda pour r√©duire le probl√®me de\npassager clandestin.\nIl est ainsi recommand√©, notamment lorsqu‚Äôon travaille dans le\nsecteur priv√© o√π du code Python peut √™tre utilis√©,\nde ne pas ignorer ces conditions pour ne pas se mettre en faute juridiquement.\nLa conda forge n‚Äôest pas soumise √† ces conditions et est ainsi pr√©f√©rable\ndans les entreprises.\n{{% /box %}}"
  },
  {
    "objectID": "content/course/getting-started/01_installation/index.html#comment-installer-des-packages",
    "href": "content/course/getting-started/01_installation/index.html#comment-installer-des-packages",
    "title": "27¬† Configuration de Python",
    "section": "30.2 Comment installer des packages",
    "text": "30.2 Comment installer des packages\nAvec Anaconda, il faut passer par la ligne de commande et taper\nconda install &lt;nom_module&gt;\nPar exemple conda install geopandas. Depuis une cellule de notebook\nJupyter, on ajoute un point d‚Äôexclamation pour indiquer √† Jupyter\nque la commande doit √™tre interpr√©t√©e comme une commande shell\net non une commande Python\n!conda install &lt;nom_module&gt; -y\nL‚Äôoption -y permet d‚Äô√©viter que conda nous demande confirmation\nsur l‚Äôinstallation du package. Pour mettre √† jour un package, on fera\nconda upgrade plut√¥t que conda install\nAvec pip, on va cette fois taper\npip install &lt;nom_module&gt;\npip permet √©galement d‚Äôinstaller des librairies directement depuis\nGithub √† condition que Anaconda et Git sachent\ncommuniquer (ce qui implique en g√©n√©ral que Git soit dans le PATH\ndu syst√®me d‚Äôexploitation). Par exemple, pour installer le package\npynsee\npip install git+https://github.com/InseeFrLab/Py-Insee-Data.git#egg=pynsee\nLa partie d√©di√©e aux environnement virtuels du cours de derni√®re ann√©e de\nl‚ÄôENSAE pr√©sente plus d‚Äô√©l√©ments sur les diff√©rences moins √©videntes\nentre pip et conda."
  },
  {
    "objectID": "content/course/getting-started/03_data_analysis/index.html",
    "href": "content/course/getting-started/03_data_analysis/index.html",
    "title": "28¬† Comment aborder un jeu de donn√©es",
    "section": "",
    "text": "29 D√©marche √† adopter face √† un jeu de donn√©es\nPour bien d√©buter des travaux sur une base de donn√©es,\nil est n√©cessaire de se poser quelques questions de bon sens\net de suivre une d√©marche assez simple."
  },
  {
    "objectID": "content/course/getting-started/03_data_analysis/index.html#une-d√©marche-scientifique",
    "href": "content/course/getting-started/03_data_analysis/index.html#une-d√©marche-scientifique",
    "title": "28¬† Comment aborder un jeu de donn√©es",
    "section": "29.1 Une d√©marche scientifique",
    "text": "29.1 Une d√©marche scientifique\nDans un projet sur des jeux de donn√©es, on peut sch√©matiquement\ns√©parer les √©tapes en quatre grandes parties :\n\nla r√©cup√©ration et structuration des donn√©es;\nleur analyse (notamment descriptive) ;\nla mod√©lisation ;\nla valorisation finale des √©tapes pr√©c√©dentes."
  },
  {
    "objectID": "content/course/getting-started/03_data_analysis/index.html#lors-de-la-r√©cup√©ration-des-donn√©es",
    "href": "content/course/getting-started/03_data_analysis/index.html#lors-de-la-r√©cup√©ration-des-donn√©es",
    "title": "28¬† Comment aborder un jeu de donn√©es",
    "section": "29.2 Lors de la r√©cup√©ration des donn√©es",
    "text": "29.2 Lors de la r√©cup√©ration des donn√©es\n\n29.2.1 R√©flexions √† mener en amont\nLa phase de constitution de son jeu de donn√©es sous-tend tout le projet qui suit.\nLa premi√®re question √† se poser est\n‚Äúde quelles donn√©es ai-je besoin pour r√©pondre √† ma probl√©matique ?‚Äù.\nCette probl√©matique pourra √©ventuellement\n√™tre affin√©e en fonction des besoins mais les travaux sont g√©n√©ralement\nde meilleure qualit√© lorsque la probl√©matique am√®ne √† la r√©flexion sur les donn√©es\ndisponibles plut√¥t que l‚Äôinverse.\nEnsuite, ‚Äúqui produit et met √† disposition ces donn√©es‚Äù ?\nLes sources disponibles sur internet sont-elles fiables ?\nPar exemple, les sites d‚Äôopen data gouvernementaux sont par exemple assez fiables mais autorisent parfois l‚Äôarchivage de donn√©es restructur√©es par des tiers et non des producteurs officiels. A l‚Äôinverse, sur Kaggle ou sur Github la source de certains jeux de donn√©es n‚Äôest pas trac√©e ce qui rend compliqu√©e la confiance sur la qualit√© de la donn√©e\nUne fois identifi√© une ou plusieurs sources de donn√©es,\nest-ce que je peux les compl√©ter avec d‚Äôautres donn√©es ?\n(dans ce cas, faire attention √† avoir des niveaux de granularit√© ad√©quats).\n\n\n29.2.2 Structuration des donn√©es\nVient ensuite la phase de mise en forme et nettoyage des jeux de donn√©es r√©cup√©r√©s.\nCette √©tape est primordiale et est g√©n√©ralement celle qui mobilise le plus\nde temps. Pendant quelques ann√©es, on parlait de data-cleaning. Cependant,\ncela a pu, implicitement, laisser penser qu‚Äôil s‚Äôagissait d‚Äôune t√¢che\nsubalterne. On commence √† lui pr√©f√©rer le concept de feature engineering\nqui souligne bien qu‚Äôil s‚Äôagit d‚Äôune comp√©tence qui n√©cessite beaucoup\nde comp√©tences.\nUn jeu de donn√©es propre est un jeu de donn√©es dont la structure est\nad√©quate et n‚Äôentra√Ænera pas d‚Äôerreur, visible ou non,\nlors de la phase d‚Äôanalyse. Voici quelques √©l√©ments structurants\nd‚Äôun jeu de donn√©es propre:\n\nles informations manquantes sont bien comprises et trait√©es. numpy et\npandas proposent un certain formalisme sur le sujet qu‚Äôil est utile\nd‚Äôadopter en rempla√ßant par NaN les observations manquantes. Cela\nimplique de faire attention √† la mani√®re dont certains producteurs\ncodent les valeurs manquantes: certains ont la facheuse tendance √†\n√™tre imaginatifs sur les codes pour valeurs manquantes: ‚Äú-999‚Äù, ‚ÄúXXX‚Äù, ‚ÄúNA‚Äù\nles variables servant d‚Äôidentifiants sont bien les m√™mes d‚Äôune table √† l‚Äôautre (notamment dans le cas de jointure) : m√™me format, m√™me modalit√©s\npour des variables textuelles, qui peuvent etre mal saisies, avoir corrig√© les √©ventuelles fautes (ex ‚ÄúRolland Garros‚Äù &gt; ‚ÄúRoland Garros‚Äù)\ncr√©er des variables qui synth√©tisent l‚Äôinformation dont vous avez besoin\nsupprimer les √©l√©ments inutiles (colonne ou ligne vide)\nrenommer les colonnes avec des noms compr√©hensibles"
  },
  {
    "objectID": "content/course/getting-started/03_data_analysis/index.html#lors-de-lanalyse-descriptive",
    "href": "content/course/getting-started/03_data_analysis/index.html#lors-de-lanalyse-descriptive",
    "title": "28¬† Comment aborder un jeu de donn√©es",
    "section": "29.3 Lors de l‚Äôanalyse descriptive",
    "text": "29.3 Lors de l‚Äôanalyse descriptive\nUne fois les jeux de donn√©es nettoy√©s, vous pouvez plus sereinement\n√©tudier l‚Äôinformation pr√©sente dans les donn√©es.\nCette phase et celle du nettoyage ne sont pas s√©quentielles,\nen r√©alit√© vous devrez r√©guli√®rement passer de votre nettoyage √† quelques statistiques descriptives qui vous montreront un probl√®me, retourner au nettoyage etc.\nLes questions √† se poser pour ‚Äúchallenger‚Äù le jeu de donn√©es :\n\nest-ce que mon √©chantillon est bien repr√©sentatif de ce qui m‚Äôint√©resse ? N‚Äôavoir que 2000 communes sur les 35000 n‚Äôest pas n√©cessairement un probl√®me mais il est bon de s‚Äô√™tre pos√© la question.\nest-ce que les ordres de grandeur sont bons ? pour cela, confronter vos premieres stats desc √† vos recherches internet. Par exemple trouver que les maisons vendues en France en 2020 font en moyenne 400 m¬≤ n‚Äôest pas un ordre de grandeur r√©aliste.\nest-ce que je comprends toutes les variables de mon jeu de donn√©es ? est-ce qu‚Äôelles se ‚Äúcomportent‚Äù de la bonne fa√ßon ? √† ce stade, il est parfois utile de se faire un dictionnaire de variable (qui explique comment elles sont construites ou calcul√©es). On peut √©galement mener des √©tudes de corr√©lation entre nos variables.\nest-ce que j‚Äôai des outliers, i.e.¬†des valeurs aberrantes pour certains individus ? Dans ce cas, il faut d√©cider quel traitement on leur apporte (les supprimer, appliquer une transformation logarithmique, les laisser tel quel) et surtout bien le justifier.\nest-ce que j‚Äôai des premiers grands messages sortis de mon jeu de donn√©es ? est-ce que j‚Äôai des r√©sultats surprenants ? Si oui, les ai-je creus√© suffisamment pour voir si les r√©sultats tiennent toujours ou si c‚Äôest √† cause d‚Äôun souci dans la construction du jeu de donn√©es (mal nettoy√©es, mauvaise variable‚Ä¶)"
  },
  {
    "objectID": "content/course/getting-started/03_data_analysis/index.html#lors-de-la-mod√©lisation",
    "href": "content/course/getting-started/03_data_analysis/index.html#lors-de-la-mod√©lisation",
    "title": "28¬† Comment aborder un jeu de donn√©es",
    "section": "29.4 Lors de la mod√©lisation",
    "text": "29.4 Lors de la mod√©lisation\nA cette √©tape, l‚Äôanalyse descriptive doit voir avoir donn√© quelques premi√®res pistes pour savoir dans quelle direction vous voulez mener votre mod√®le.\nUne erreur de d√©butant est de se lancer directement dans la mod√©lisation parce\nqu‚Äôil s‚Äôagirait d‚Äôune comp√©tence plus pouss√©e. Cela am√®ne g√©n√©ralement\n√† des analyses de pauvre qualit√©: la mod√©lisation tend g√©n√©ralement √† confirmer\nles intuitions issues de l‚Äôanalyse descriptive. Sans cette derni√®re,\nl‚Äôinterpr√©tation des r√©sultats d‚Äôun mod√®le peu s‚Äôav√©rer inutilement complexe.\nVous devrez plonger dans vos autres cours (Econom√©trie 1, Series Temporelles, Sondages, Analyse des donn√©es etc.) pour trouver le mod√®le le plus adapt√© √† votre question.\nLa m√©thode sera guid√©e par l‚Äôobjectif.\n\nEst-ce que vous voulez expliquer ou pr√©dire ? https://hal-cnam.archives-ouvertes.fr/hal-02507348/document\nEst-ce que vous voulez classer un √©l√©ment dans une cat√©gorie (classification ou clustering) ou pr√©dire une valeur num√©rique (r√©gression) ?\n\nEn fonction des mod√®les que vous aurez d√©j√† vu en cours et des questions que vous souhaiterez r√©soudre sur votre jeu de donn√©es, le choix du mod√®le sera souvent assez direct.\nVous pouvez √©galement vous r√©f√©rez √† la d√©marche propos√©e par Xavier Dupr√©\nhttp://www.xavierdupre.fr/app/ensae_teaching_cs/helpsphinx3/debutermlprojet.html#l-debutermlprojet\nPour aller plus loin (mais de mani√®re simplifi√©e) sur les algorithmes de Machine Learning :\nhttps://datakeen.co/8-machine-learning-algorithms-explained-in-human-language/\n\n29.4.1 Valorisation des travaux\nLa mise √† disposition de code sur Github ou Gitlab est une incitation\ntr√®s forte pour produire du code de qualit√©. Il est ainsi recommand√© de\nsyst√©matiquement utiliser ces plateformes pour la mise √† disposition de\ncode. Cependant, il ne s‚Äôagit que d‚Äôune petite partie des gains √†\nl‚Äôutiliser.\nLe cours que je donne avec Romain Avouac en troisi√®me ann√©e d‚ÄôENSAE\n(https://ensae-reproductibilite.netlify.app/) √©voque\nl‚Äôun des principaux gains √† utiliser ces plateformes, √† savoir\nla possibilit√© de mettre √† disposition automatiquement diff√©rents livrables\npour valoriser son travail aupr√®s de diff√©rents publics.\nSelon le public vis√©, la communication ne sera pas identique. Le code peut\nint√©resser les personnes d√©sirant avoir des d√©tails sur la m√©thodologie mise\nen oeuvre en pratique mais il peut s‚Äôagir d‚Äôun format rebutant pour d‚Äôautres\npublics. Une visualisation de donn√©es dynamiques parlera √† des publics\nmoins experts de la donn√©e mais est plus dure √† mettre en oeuvre\nqu‚Äôun graphique standard.\n{{% box status=‚Äúhint‚Äù title=‚ÄúConseil‚Äù icon=‚Äúfa fa-lightbulb‚Äù %}}\nLes Notebooks Jupyter ont eu beaucoup de succ√®s dans le monde de\nla data-science pour valoriser des travaux. Pourtant il ne s‚Äôagit\npas forc√©ment toujours du meilleur format. En effet, beaucoup\nde notebooks tentent √† empiler des pav√©s de code et du texte, ce\nqui les rend difficilement lisibles.\nSur un projet cons√©quent, il vaut mieux reporter le plus de code\npossible dans des scripts bien structur√©s et avoir un notebook\nqui appelle ces scripts pour produire des outputs. Ou alors ne\npas utiliser un notebook et privil√©gier un autre format (un\ntableau de bord, un site web, une appli r√©active‚Ä¶)\n{{% /box %}}"
  },
  {
    "objectID": "content/course/getting-started/03_data_analysis/index.html#la-reproductibilit√©-est-importante",
    "href": "content/course/getting-started/03_data_analysis/index.html#la-reproductibilit√©-est-importante",
    "title": "28¬† Comment aborder un jeu de donn√©es",
    "section": "30.1 La reproductibilit√© est importante",
    "text": "30.1 La reproductibilit√© est importante\nLes donn√©es sont une repr√©sentation synth√©tique de la vie des gens et les\nconclusions de certaines analyses peuvent avoir un vrai impact sur\nleur vie. Les chiffres erron√©s de\nReinhart et Rogoff ont ainsi pu servir de justification th√©orique √† des\npolitiques d‚Äôaust√©rit√© qui ont pu avoir des cons√©quences violentes\npour certains citoyens de\npays en crise1. En Grande-Bretagne, le recensement des personnes\ncontamin√©es par le Covid en 2020, et donc de leurs proches pour le\nsuivi de l‚Äô√©pid√©mie,\na √©t√© incomplet √† cause de\ntroncatures dues √† l‚Äôutilisation d‚Äôun format non adapt√© de stockage\ndes donn√©es (tableur Excel)2.\nDernier exemple avec le credit scoring mis en oeuvre aux Etats-Unis.\nLa citation ci-dessous, issue de l‚Äôarticle de Hurley and Adebayo (2016),\nillustre tr√®s bien les cons√©quences et les aspects probl√©matiques\nd‚Äôun syst√®me de construction automatis√©e d‚Äôun score de cr√©dit :\n\nConsumers have limited ability to identify and contest unfair credit\ndecisions, and little chance to understand what steps they\nshould take to improve their credit. Recent studies have also\nquestioned the accuracy of the data used by these tools, in some\ncases identifying serious flaws that have a substantial bearing\non lending decisions. Big-data tools may also risk creating a\nsystem of ‚Äúcreditworthinessby association‚Äù in which consumers‚Äô\nfamilial, religious, social, and other affiliations determine their\neligibility for an affordable loan.\nHurley and Adebayo (2016)"
  },
  {
    "objectID": "content/course/getting-started/03_data_analysis/index.html#lutter-contre-les-biais-cognitifs",
    "href": "content/course/getting-started/03_data_analysis/index.html#lutter-contre-les-biais-cognitifs",
    "title": "28¬† Comment aborder un jeu de donn√©es",
    "section": "30.2 Lutter contre les biais cognitifs",
    "text": "30.2 Lutter contre les biais cognitifs\nLa transparence sur les int√©r√™ts et limites d‚Äôune m√©thode mise en oeuvre\nest donc importante.\nCette exigence de la recherche, parfois oubli√©e √† cause de la course\naux r√©sultats novateurs, m√©rite √©galement d‚Äô√™tre appliqu√©e\nen entreprise ou administration.\nM√™me sans intention manifeste de la part de la personne qui analyse des donn√©es,\nune mauvaise interpr√©tation est toujours possible. Tout en valorisant un\nr√©sultat, il est possible d‚Äôalerter sur certaines limites. Il est important,\ndans ses recherches comme dans les discussions avec d‚Äôautres interlocuteurs,\nde faire attention au biais de confirmation qui consiste\n√† ne retenir que l‚Äôinformation qui correspond √† nos conceptions a priori et\n√† ne pas consid√©rer celles qui pourraient aller √† l‚Äôencontre de celles-ci:\n\nCertaines repr√©sentations de donn√©es sont √† exclure car des biais cognitifs\npeuvent amener √† des interpr√©tations erron√©es3. Dans le domaine de la\nvisualisation de donn√©es, les camemberts (pie chart) ou les diagrammes\nradar sont par exemple\n√† exclure car l‚Äôoeil humain per√ßoit mal ces formes circulaires. Pour une raison\nsimilaire, les cartes avec aplat de couleur (cartes\nchoropl√®thes) sont trompeuses."
  },
  {
    "objectID": "content/course/getting-started/03_data_analysis/index.html#r√©glementation",
    "href": "content/course/getting-started/03_data_analysis/index.html#r√©glementation",
    "title": "28¬† Comment aborder un jeu de donn√©es",
    "section": "30.3 R√©glementation",
    "text": "30.3 R√©glementation\nLe cadre r√©glementaire de protection des donn√©es a √©volu√© ces derni√®res\nann√©es avec le RGPD. Cette r√©glementation a permis de mieux faire\nsaisir le fait que la collecte de donn√©es se justifie au nom\nde finalit√©s plus ou moins bien identifi√©es. Prendre conscience que\nla confidentialit√© des donn√©es se justifie pour √©viter la diss√©mination\nnon contr√¥l√©e d‚Äôinformations sur une personne est important.\nDes donn√©es particuli√®rement sensibles, notamment les donn√©es de sant√©,\npeuvent √™tre plus contraignantes √† traiter que des donn√©es peu sensibles.\nEn Europe, par exemple, les agents du service statistique public\n(Insee ou services statistiques minist√©riels) sont tenus au secret professionnel\n(article L121-6 du Code g√©n√©ral de la fonction publique),\nqui leur interdit la communication des informations confidentielles\ndont ils sont d√©positaires au titre de leurs missions ou fonctions,\nsous peine des sanctions pr√©vues par l‚Äôarticle 226-13 du Code p√©nal\n(jusqu‚Äô√† un an d‚Äôemprisonnement et 15 000 ‚Ç¨ d‚Äôamende).\nLe secret statistique, d√©fini dans une loi de 1951,\nrenforce cette obligation dans le cas de donn√©es d√©tenues pour des usages statistiques.\nIl interdit strictement la communication de donn√©es individuelles\nou susceptibles d‚Äôidentifier les personnes,\nissues de traitements √† finalit√©s statistiques,\nque ces traitements proviennent d‚Äôenqu√™tes ou de bases de donn√©es.\nLe secret statistique exclut par principe de diffuser des donn√©es\nqui permettraient l‚Äôidentification des personnes concern√©es,\npersonnes physiques comme personnes morales.\nCette obligation limite la finesse des informations disponibles en diffusion\nCe cadre contraignant s‚Äôexplique par l‚Äôh√©ritage de la Seconde Guerre Mondiale\net le d√©sir de ne plus revivre une situation o√π la collecte d‚Äôinformation\nsert une action publique bas√©e sur la discrimination entre cat√©gories\nde la population."
  },
  {
    "objectID": "content/course/getting-started/03_data_analysis/index.html#partager-les-moyens-de-reproduire-une-analyse",
    "href": "content/course/getting-started/03_data_analysis/index.html#partager-les-moyens-de-reproduire-une-analyse",
    "title": "28¬† Comment aborder un jeu de donn√©es",
    "section": "30.4 Partager les moyens de reproduire une analyse",
    "text": "30.4 Partager les moyens de reproduire une analyse\nUn article r√©cent de Nature,\nqui reprend les travaux d‚Äôune √©quipe d‚Äô√©pid√©miologistes (Gabelica, Bojƒçiƒá, and Puljak 2022)\n√©voque le probl√®me de l‚Äôacc√®s aux donn√©es pour des chercheurs d√©sirant reproduire\nune √©tude. M√™me dans les articles scientifiques o√π il est mentionn√© que les\ndonn√©es peuvent √™tre mises √† disposition d‚Äôautres chercheurs, le partage\nde celles-ci est rare :\n\nGraphique issu de l‚Äôarticle de Nature\nAfin de partager les moyens de reproduire des publications sans diffuser des\ndonn√©es potentiellement confidentielles, les jeux de donn√©es synth√©tiques\nsont de plus en plus utilis√©s. Par le biais de mod√®les de deep learning,\nil est ainsi possible de g√©n√©rer des jeux de donn√©es synth√©tiques complexes\nqui permettent de reproduire les principales caract√©ristiques d‚Äôun jeu de donn√©es\ntout en √©vitant, si le mod√®le a √©t√© bien calibr√©, de diffuser une information\nindividuelle."
  },
  {
    "objectID": "content/course/getting-started/03_data_analysis/index.html#adopter-une-approche-√©cologique",
    "href": "content/course/getting-started/03_data_analysis/index.html#adopter-une-approche-√©cologique",
    "title": "28¬† Comment aborder un jeu de donn√©es",
    "section": "30.5 Adopter une approche √©cologique",
    "text": "30.5 Adopter une approche √©cologique\nLe num√©rique constitue une part croissante des\n√©missions de gaz √† effet de serre.\nRepr√©sentant aujourd‚Äôhui 4 % des √©missions mondiales\nde CO2, cette part devrait encore cro√Ætre (Arcep 2019).\nLe monde de la data-science est √©galement\nconcern√©.\nL‚Äôutilisation de donn√©es de plus en\nplus massives, notamment la constitution\nde corpus monumentaux de textes,\nr√©cup√©r√©s par scraping, est une premi√®re source\nde d√©pense d‚Äô√©nergie. De m√™me, la r√©cup√©ration\nen continue de nouvelles traces num√©riques\nn√©cessite d‚Äôavoir des serveurs fonctionnels\nen continu. A cette premi√®re source de\nd√©pense d‚Äô√©nergie, s‚Äôajoute l‚Äôentra√Ænement\ndes mod√®les qui peut prendre des jours,\ny compris sur des architectures tr√®s\npuissantes. Strubell, Ganesh, and McCallum (2019)\nestime que l‚Äôentra√Ænement d‚Äôun mod√®le √†\nl‚Äô√©tat de l‚Äôart dans le domaine du\nNLP n√©cessite autant d‚Äô√©nergie que ce que\nconsommeraient cinq voitures, en moyenne,\nau cours de l‚Äôensemble de leur\ncycle de vie.\nL‚Äôutilisation accrue de l‚Äôint√©gration\ncontinue, qui permet de mettre en oeuvre de mani√®re\nautomatis√©e l‚Äôex√©cution de certains scripts ou\nla production de livrables en continu,\nam√®ne √©galement √† une d√©pense d‚Äô√©nergie importante.\nIl convient donc d‚Äôessayer de limiter l‚Äôint√©gration\ncontinue √† la production d‚Äôoutput vraiment nouveaux.\nPar exemple, cet ouvrage utilise de mani√®re intensive\ncette approche. N√©anmoins, pour essayer de limiter\nles effets pervers de la production en continu d‚Äôun\nouvrage extensif, seuls les chapitres modifi√©s\nsont produits lors des pr√©visualisations mises en\noeuvre √† chaque pull request sur le d√©p√¥t\n.\nLes data-scientists doivent √™tre conscients\ndes implications de leur usage intensif de\nressources et essayer de minimiser leur\nimpact. Par exemple, plut√¥t que r√©-estimer\nun mod√®le de NLP,\nla m√©thode de l‚Äôapprentissage par transfert,\nqui permet de transf√©rer les poids d‚Äôapprentissage\nd‚Äôun mod√®le √† une nouvelle source, permet\nde r√©duire les besoins computationnels.\nDe m√™me, il peut √™tre utile, pour prendre\nconscience de l‚Äôeffet d‚Äôun code trop long,\nde convertir le temps de calcul en\n√©missions de gaz √† effet de serre.\nLe package codecarbon\npropose cette solution en adaptant l‚Äôestimation\nen fonction du mix √©nerg√©tique du pays\nen question. Mesurer √©tant le\npr√©requis pour prendre conscience puis comprendre,\nce type d‚Äôinitiatives peut amener √† responsabiliser\nles data-scientists et ainsi permettre un\nmeilleur partage des ressources."
  },
  {
    "objectID": "content/course/NLP/index.html#contenu-de-la-partie",
    "href": "content/course/NLP/index.html#contenu-de-la-partie",
    "title": "29¬† Partie 4 : Natural Language Processing (NLP)",
    "section": "29.1 Contenu de la partie",
    "text": "29.1 Contenu de la partie"
  },
  {
    "objectID": "content/course/NLP/index.html#pour-aller-plus-loin",
    "href": "content/course/NLP/index.html#pour-aller-plus-loin",
    "title": "29¬† Partie 4 : Natural Language Processing (NLP)",
    "section": "29.2 Pour aller plus loin",
    "text": "29.2 Pour aller plus loin\n\nCours sur HuggingFace"
  },
  {
    "objectID": "content/course/NLP/02_exoclean/index.html#librairies-n√©cessaires",
    "href": "content/course/NLP/02_exoclean/index.html#librairies-n√©cessaires",
    "title": "30¬† Nettoyer un texte: des exercices pour d√©couvrir l‚Äôapproche bag-of-words",
    "section": "30.1 Librairies n√©cessaires",
    "text": "30.1 Librairies n√©cessaires\nCette page √©voquera les principales librairies pour faire du NLP, notamment :\n\nWordCloud\nnltk\nSpaCy\nKeras\nTensorFlow\n\nIl faudra √©galement installer les librairies gensim et pywaffle\n\n\n Hint\nComme dans la partie pr√©c√©dente, il faut t√©l√©charger quelques √©l√©ments pour que NTLK puisse fonctionner correctement. Pour cela, faire :\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('genesis')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\n\nLa liste des modules √† importer est assez longue, la voici :\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport base64\nimport string\nimport re\nimport nltk\n\nfrom collections import Counter\nfrom time import time\n# from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords\nfrom sklearn.metrics import log_loss\nimport matplotlib.pyplot as plt\n#!pip install pywaffle\nfrom pywaffle import Waffle\n\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\n\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('genesis')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\n[nltk_data] Downloading package stopwords to /github/home/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /github/home/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n[nltk_data] Downloading package genesis to /github/home/nltk_data...\n[nltk_data]   Unzipping corpora/genesis.zip.\n[nltk_data] Downloading package wordnet to /github/home/nltk_data...\n[nltk_data] Downloading package omw-1.4 to /github/home/nltk_data...\n\n\nTrue"
  },
  {
    "objectID": "content/course/NLP/02_exoclean/index.html#donn√©es-utilis√©es",
    "href": "content/course/NLP/02_exoclean/index.html#donn√©es-utilis√©es",
    "title": "30¬† Nettoyer un texte: des exercices pour d√©couvrir l‚Äôapproche bag-of-words",
    "section": "30.2 Donn√©es utilis√©es",
    "text": "30.2 Donn√©es utilis√©es\n\n\n Exercice 1 : Importer les donn√©es spooky\nPour ceux qui ont envie de tester leurs connaissances en pandas\n\nImporter le jeu de donn√©es spooky √† partir de l‚ÄôURL https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv sous le nom train. L‚Äôencoding est latin-1\nMettre des majuscules au nom des colonnes.\nRetirer le prefix id de la colonne Id et appeler la nouvelle colonne ID.\nMettre l‚Äôancienne colonne Id en index.\n\n\n\nSi vous ne faites pas l‚Äôexercice 1, pensez √† charger les donn√©es en executant la fonction get_data.py :\n\nimport requests\n\nurl = 'https://raw.githubusercontent.com/linogaliana/python-datascientist/master/content/course/NLP/get_data.py'\nr = requests.get(url, allow_redirects=True)\nopen('getdata.py', 'wb').write(r.content)\n\nimport getdata\ntrain = getdata.create_train_dataframes()\n\nCe code introduit une base nomm√©e train dans l‚Äôenvironnement.\nLe jeu de donn√©es met ainsi en regard un auteur avec une phrase qu‚Äôil a √©crite :\n\ntrain.head()\n\n\n\n\n\n\n\n\nText\nAuthor\nID\n\n\nId\n\n\n\n\n\n\n\nid26305\nThis process, however, afforded me no means of...\nEAP\n26305\n\n\nid17569\nIt never once occurred to me that the fumbling...\nHPL\n17569\n\n\nid11008\nIn his left hand was a gold snuff box, from wh...\nEAP\n11008\n\n\nid27763\nHow lovely is spring As we looked from Windsor...\nMWS\n27763\n\n\nid12958\nFinding nothing else, not even gold, the Super...\nHPL\n12958\n\n\n\n\n\n\n\nOn peut se rendre compte que les extraits des 3 auteurs ne sont\npas forc√©ment √©quilibr√©s dans le jeu de donn√©es.\nIl faudra en tenir compte dans la pr√©diction.\n\nfig = plt.figure()\ng = sns.barplot(x=['Edgar Allen Poe', 'Mary W. Shelley', 'H.P. Lovecraft'], y=train['Author'].value_counts())\n\n\n\n\n\n\n Note\nL‚Äôapproche bag of words est pr√©sent√©e de\nmani√®re plus extensive dans le chapitre pr√©c√©dent.\nL‚Äôid√©e est d‚Äô√©tudier la fr√©quence des mots d‚Äôun document et la\nsurrepr√©sentation des mots par rapport √† un document de\nr√©f√©rence (appel√© corpus).\nCette approche un peu simpliste mais tr√®s\nefficace : on peut calculer des scores permettant par exemple de faire\nde classification automatique de document par th√®me, de comparer la\nsimilarit√© de deux documents. Elle est souvent utilis√©e en premi√®re analyse,\net elle reste la r√©f√©rence pour l‚Äôanalyse de textes mal\nstructur√©s (tweets, dialogue tchat, etc.).\nLes analyses tf-idf (term frequency-inverse document frequency) ou les\nconstructions d‚Äôindices de similarit√© cosinus reposent sur ce type d‚Äôapproche.\n\n\n\n30.2.1 Fr√©quence d‚Äôun mot\nAvant de s‚Äôadonner √† une analyse syst√©matique du champ lexical de chaque\nauteur, on se focaliser dans un premier temps sur un unique mot, le mot fear.\n\n\n Note\nL‚Äôexercice ci-dessous pr√©sente une repr√©sentation graphique nomm√©e\nwaffle chart. Il s‚Äôagit d‚Äôune approche pr√©f√©rable aux\ncamemberts qui sont des graphiques manipulables car l‚Äôoeil humain se laisse\nfacilement berner par cette repr√©sentation graphique qui ne respecte pas\nles proportions.\n\n\n\n\n Exercice 2 : Fr√©quence d'un mot\n\nCompter le nombre de phrases, pour chaque auteur, o√π appara√Æt le mot fear.\nUtiliser pywaffle pour obtenir les graphiques ci-dessous qui r√©sument\nde mani√®re synth√©tique le nombre d‚Äôoccurrences du mot ‚Äúfear‚Äù par auteur.\nRefaire l‚Äôanalyse avec le mot ‚Äúhorror‚Äù.\n\n\n\nA l‚Äôissue de la question 1, vous devriez obtenir le tableau\nde fr√©quence suivant:\n\n\n\n\n\n\n\n\n\nText\nID\nwordtoplot\n\n\nAuthor\n\n\n\n\n\n\n\nEAP\nThis process, however, afforded me no means of...\n2630511008096741351519322166071718908441148621...\n70\n\n\nHPL\nIt never once occurred to me that the fumbling...\n1756912958197641888620836080752790708121117330...\n160\n\n\nMWS\nHow lovely is spring As we looked from Windsor...\n2776322965009121673712799131170076400683052582...\n211\n\n\n\n\n\n\n\nCeci permet d‚Äôobtenir le waffle chart suivant:\n\n\n\n\n\nOn remarque ainsi de mani√®re tr√®s intuitive\nle d√©s√©quilibre de notre jeu de donn√©es\nlorsqu‚Äôon se focalise sur le terme ‚Äúpeur‚Äù\no√π Mary Shelley repr√©sente pr√®s de 50%\ndes observations.\nSi on reproduit cette analyse avec le terme ‚Äúhorror‚Äù, on peut\nen conclure que la peur est plus √©voqu√©e par Mary Shelley\n(sentiment assez naturel face √† la cr√©ature du docteur Frankenstein) alors\nque Lovecraft n‚Äôa pas vol√© sa r√©putation d‚Äô√©crivain de l‚Äôhorreur !"
  },
  {
    "objectID": "content/course/NLP/02_exoclean/index.html#premier-wordcloud",
    "href": "content/course/NLP/02_exoclean/index.html#premier-wordcloud",
    "title": "30¬† Nettoyer un texte: des exercices pour d√©couvrir l‚Äôapproche bag-of-words",
    "section": "30.3 Premier wordcloud",
    "text": "30.3 Premier wordcloud\nPour aller plus loin dans l‚Äôanalyse du champ lexical de chaque auteur,\non peut repr√©senter un wordcloud qui permet d‚Äôafficher chaque mot avec une\ntaille proportionnelle au nombre d‚Äôoccurrence de celui-ci.\n\n\n Exercice 3 : Wordcloud\n\nEn utilisant la fonction wordCloud, faire trois nuages de mot pour repr√©senter les mots les plus utilis√©s par chaque auteur.\nCalculer les 25 mots plus communs pour chaque auteur et repr√©senter les trois histogrammes des d√©comptes.\n\n\n\nLe wordcloud pour nos diff√©rents auteurs est le suivant:\n\n\n\n\n\nEnfin, si on fait un histogramme des fr√©quences,\ncela donnera :\n\n\n\n\n\nOn voit ici que ce sont des mots communs, comme ‚Äúthe‚Äù, ‚Äúof‚Äù, etc. sont tr√®s\npr√©sents. Mais ils sont peu porteurs d‚Äôinformation, on peut donc les √©liminer\navant de faire une analyse syntaxique pouss√©e.\nCeci est une d√©monstration par l‚Äôexemple qu‚Äôil vaut mieux nettoyer le texte avant de\nl‚Äôanalyser (sauf si on est int√©ress√©\npar la loi de Zipf, cf.¬†exercice suivant).\n\n30.3.1 Apart√©: la loi de Zipf\n\n\n La loi de Zipf\nDans son sens strict, la loi de Zipf pr√©voit que\ndans un texte donn√©, la fr√©quence d‚Äôoccurrence \\(f(n_i)\\) d‚Äôun mot est\nli√©e √† son rang \\(n_i\\) dans l‚Äôordre des fr√©quences par une loi de la forme\n\\(f(n_i) = c/n_i\\) o√π \\(c\\) est une constante. Zipf, dans les ann√©es 1930, se basait sur l‚Äôoeuvre\nde Joyce, Ulysse pour cette affirmation.\nPlus g√©n√©ralement, on peut d√©river la loi de Zipf d‚Äôune distribution exponentielle des fr√©quences: \\(f(n_i) = cn_{i}^{-k}\\). Cela permet d‚Äôutiliser la famille des mod√®les lin√©aires g√©n√©ralis√©s, notamment les r√©gressions poissonniennes, pour mesurer les param√®tres de la loi. Les mod√®les lin√©aire traditionnels en log souffrent en effet, dans ce contexte, de biais (la loi de Zipf est un cas particulier d‚Äôun mod√®le gravitaire, o√π appliquer des OLS est une mauvaise id√©e, cf.¬†Galiana et al. (2020) pour les limites).\n\n\nUn mod√®le exponentiel peut se repr√©senter par un mod√®le de Poisson ou, si\nles donn√©es sont tr√®s dispers√©es, par un mod√®le binomial n√©gatif. Pour\nplus d‚Äôinformations, consulter l‚Äôannexe de Galiana et al. (2020).\nLa technique √©conom√©trique associ√©e pour l‚Äôestimation est\nles mod√®les lin√©aires g√©n√©ralis√©s (GLM) qu‚Äôon peut\nutiliser en Python via le\npackage statsmodels2:\n\\[\n\\mathbb{E}\\bigg( f(n_i)|n_i \\bigg) = \\exp(\\beta_0 + \\beta_1 \\log(n_i))\n\\]\nPrenons les r√©sultats de l‚Äôexercice pr√©c√©dent et enrichissons les du rang et de la fr√©quence d‚Äôoccurrence d‚Äôun mot :\n\ncount_words = pd.DataFrame({'counter' : train\n    .groupby('Author')\n    .apply(lambda s: ' '.join(s['Text']).split())\n    .apply(lambda s: Counter(s))\n    .apply(lambda s: s.most_common())\n    .explode()}\n)\ncount_words[['word','count']] = pd.DataFrame(count_words['counter'].tolist(), index=count_words.index)\ncount_words = count_words.reset_index()\n\ncount_words = count_words.assign(\n    tot_mots_auteur = lambda x: (x.groupby(\"Author\")['count'].transform('sum')),\n    freq = lambda x: x['count'] /  x['tot_mots_auteur'],\n    rank = lambda x: x.groupby(\"Author\")['count'].transform('rank', ascending = False)\n)\n\nCommen√ßons par repr√©senter la relation entre la fr√©quence et le rang:\nNous avons bien, graphiquement, une relation log-lin√©aire entre les deux:\n\ng.figure.get_figure()\n\n\n\n\nAvec statsmodels, v√©rifions plus formellement cette relation:\n\nimport statsmodels.api as sm\n\nexog = sm.add_constant(np.log(count_words['rank'].astype(float)))\n\nmodel = sm.GLM(count_words['freq'].astype(float), exog, family = sm.families.Poisson()).fit()\n\n# Afficher les r√©sultats du mod√®le\nprint(model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                   freq   No. Observations:                69301\nModel:                            GLM   Df Residuals:                    69299\nModel Family:                 Poisson   Df Model:                            1\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -23.011\nDate:                Wed, 19 Jul 2023   Deviance:                     0.065676\nTime:                        02:25:48   Pearson chi2:                   0.0656\nNo. Iterations:                     5   Pseudo R-squ. (CS):          0.0002431\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.4388      1.089     -2.239      0.025      -4.574      -0.303\nrank          -0.9831      0.189     -5.196      0.000      -1.354      -0.612\n==============================================================================\n\n\nLe coefficient de la r√©gression est presque 1 ce qui sugg√®re bien une relation\nquasiment log-lin√©aire entre le rang et la fr√©quence d‚Äôoccurrence d‚Äôun mot.\nDit autrement, le mot le plus utilis√© l‚Äôest deux fois plus que le deuxi√®me\nmois le plus fr√©quent qui l‚Äôest trois plus que le troisi√®me, etc."
  },
  {
    "objectID": "content/course/NLP/02_exoclean/index.html#nettoyage-dun-texte",
    "href": "content/course/NLP/02_exoclean/index.html#nettoyage-dun-texte",
    "title": "30¬† Nettoyer un texte: des exercices pour d√©couvrir l‚Äôapproche bag-of-words",
    "section": "30.4 Nettoyage d‚Äôun texte",
    "text": "30.4 Nettoyage d‚Äôun texte\nLes premi√®res √©tapes dans le nettoyage d‚Äôun texte, qu‚Äôon a\nd√©velopp√© au cours du chapitre pr√©c√©dent, sont :\n\nsuppression de la ponctuation\nsuppression des stopwords\n\nCela passe par la tokenisation d‚Äôun texte, c‚Äôest-√†-dire la d√©composition\nde celui-ci en unit√©s lexicales (les tokens).\nCes unit√©s lexicales peuvent √™tre de diff√©rentes natures,\nselon l‚Äôanalyse que l‚Äôon d√©sire mener.\nIci, on va d√©finir les tokens comme √©tant les mots utilis√©s.\nPlut√¥t que de faire soi-m√™me ce travail de nettoyage,\navec des fonctions mal optimis√©es,\non peut utiliser la librairie nltk comme d√©taill√© pr√©c√©demment.\n\n\n Exercice 4 : Nettoyage du texte\nRepartir de train, notre jeu de donn√©es d‚Äôentra√Ænement. Pour rappel, train a la structure suivante:\n\nTokeniser chaque phrase avec nltk.\nRetirer les stopwords avec nltk.\n\n\n\nPour rappel, au d√©but de l‚Äôexercice, le DataFrame pr√©sente l‚Äôaspect suivant:\n\n\n\n\n\n\n\n\n\nText\nAuthor\nID\nwordtoplot\n\n\nId\n\n\n\n\n\n\n\n\nid26305\nThis process, however, afforded me no means of...\nEAP\n26305\n0\n\n\nid17569\nIt never once occurred to me that the fumbling...\nHPL\n17569\n0\n\n\n\n\n\n\n\nApr√®s tokenisation, il devrait avoir cet aspect :\n\n\nID     Author\n00001  MWS       [Idris, was, well, content, with, this, resolv...\n00002  HPL       [I, was, faint, even, fainter, than, the, hate...\ndtype: object\n\n\nApr√®s le retrait des stopwords, cela donnera:\n\n\n Hint\nLa m√©thode apply est tr√®s pratique ici car nous avons une phrase par ligne. Plut√¥t que de faire un DataFrame par auteur, ce qui n‚Äôest pas une approche tr√®s flexible, on peut directement appliquer la tokenisation\nsur notre DataFrame gr√¢ce √† apply, sans le diviser.\n\n\nCe petit nettoyage permet d‚Äôarriver √† un texte plus int√©ressant en termes d‚Äôanalyse lexicale. Par exemple, si on reproduit l‚Äôanalyse pr√©c√©dente‚Ä¶ :\n\n\n\n\n\nPour aller plus loin dans l‚Äôharmonisation d‚Äôun texte, il est possible de\nmettre en place les classes d‚Äô√©quivalence d√©velopp√©es dans la\npartie pr√©c√©dente afin de remplacer diff√©rentes variations d‚Äôun m√™me\nmot par une forme canonique :\n\nla racinisation (stemming) assez fruste mais rapide, notamment\nen pr√©sence de fautes d‚Äôorthographe. Dans ce cas, chevaux peut devenir chev\nmais √™tre ainsi confondu avec chevet ou cheveux.\nCette m√©thode est g√©n√©ralement plus simple √† mettre en oeuvre, quoique\nplus fruste.\nla lemmatisation qui requiert la connaissance des statuts\ngrammaticaux (exemple : chevaux devient cheval).\nElle est mise en oeuvre, comme toujours avec nltk, √† travers un\nmod√®le. En l‚Äôoccurrence, un WordNetLemmatizer (WordNet est une base\nlexicographique ouverte). Par exemple, les mots ‚Äúwomen‚Äù, ‚Äúdaughters‚Äù\net ‚Äúleaves‚Äù seront ainsi lemmatis√©s de la mani√®re suivante :\n\n\nfrom nltk.stem import WordNetLemmatizer\nlemm = WordNetLemmatizer()\n\nfor word in [\"women\",\"daughters\", \"leaves\"]:\n    print(\"The lemmatized form of %s is: {}\".format(lemm.lemmatize(word)) % word)\n\nThe lemmatized form of women is: woman\nThe lemmatized form of daughters is: daughter\nThe lemmatized form of leaves is: leaf\n\n\n\n\n Note\nPour disposer du corpus n√©cessaire √† la lemmatisation, il faut, la premi√®re fois,\nt√©l√©charger celui-ci gr√¢ce aux commandes suivantes:\nimport nltk\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\n\nOn va se restreindre au corpus d‚ÄôEdgar Allan Poe et repartir de la base de donn√©es\nbrute:\n\neap_clean = train[train[\"Author\"] == \"EAP\"]\neap_clean = ' '.join(eap_clean['Text'])\n#Tokenisation na√Øve sur les espaces entre les mots =&gt; on obtient une liste de mots\n#tokens = eap_clean.split()\nword_list = nltk.word_tokenize(eap_clean)\n\n\n\n Exercice 5 : Lemmatisation avec nltk\nUtiliser un WordNetLemmatizer et observer le r√©sultat.\nOptionnel: Effectuer la m√™me t√¢che avec spaCy\n\n\nLe WordNetLemmatizer donnera le r√©sultat suivant:"
  },
  {
    "objectID": "content/course/NLP/02_exoclean/index.html#tf-idf-calcul-de-fr√©quence",
    "href": "content/course/NLP/02_exoclean/index.html#tf-idf-calcul-de-fr√©quence",
    "title": "30¬† Nettoyer un texte: des exercices pour d√©couvrir l‚Äôapproche bag-of-words",
    "section": "30.5 TF-IDF: calcul de fr√©quence",
    "text": "30.5 TF-IDF: calcul de fr√©quence\nLe calcul tf-idf (term frequency‚Äìinverse document frequency)\npermet de calculer un score de proximit√© entre un terme de recherche et un\ndocument (c‚Äôest ce que font les moteurs de recherche).\n\nLa partie tf calcule une fonction croissante de la fr√©quence du terme de recherche dans le document √† l‚Äô√©tude ;\nLa partie idf calcule une fonction inversement proportionnelle √† la fr√©quence du terme dans l‚Äôensemble des documents (ou corpus).\n\nLe score total, obtenu en multipliant les deux composantes,\npermet ainsi de donner un score d‚Äôautant plus √©lev√© que le terme est surr√©pr√©sent√© dans un document\n(par rapport √† l‚Äôensemble des documents).\nIl existe plusieurs fonctions, qui p√©nalisent plus ou moins les documents longs,\nou qui sont plus ou moins smooth.\n\n\n Exercice 6 : TF-IDF: calcul de fr√©quence\n\nUtiliser le vectoriseur TF-IdF de scikit-learn pour transformer notre corpus en une matrice document x terms. Au passage, utiliser l‚Äôoption stop_words pour ne pas provoquer une inflation de la taille de la matrice. Nommer le mod√®le tfidf et le jeu entra√Æn√© tfs.\nApr√®s avoir construit la matrice de documents x terms avec le code suivant, rechercher les lignes o√π les termes ayant la structure abandon sont non-nuls.\nTrouver les 50 extraits o√π le score TF-IDF est le plus √©lev√© et l‚Äôauteur associ√©. Vous devriez obtenir le classement suivant:\n\n\n\n\nfeature_names = tfidf.get_feature_names_out()\ncorpus_index = [n for n in list(tfidf.vocabulary_.keys())]\nimport pandas as pd\ndf = pd.DataFrame(tfs.todense(), columns=feature_names)\n\ndf.head()\n\n\n\n\n\n\n\n\naaem\nab\naback\nabaft\nabandon\nabandoned\nabandoning\nabandonment\nabaout\nabased\n...\nzodiacal\nzoilus\nzokkar\nzone\nzones\nzopyrus\nzorry\nzubmizzion\nzuro\n√°¬º\n\n\n\n\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n0.0\n0.0\n0.0\n0.0\n0.0\n0.253506\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows √ó 24937 columns\n\n\n\nLes lignes o√π les termes de abandon sont non nuls\nsont les suivantes :\n\n\nIndex([    4,   116,   215,   571,   839,  1042,  1052,  1069,  2247,  2317,\n        2505,  3023,  3058,  3245,  3380,  3764,  3886,  4425,  5289,  5576,\n        5694,  6812,  7500,  9013,  9021,  9077,  9560, 11229, 11395, 11451,\n       11588, 11827, 11989, 11998, 12122, 12158, 12189, 13666, 15259, 16516,\n       16524, 16759, 17547, 18019, 18072, 18126, 18204, 18251],\n      dtype='int64')\n\n\n\n\n\n\n\n\n\naaem\nab\naback\nabaft\nabandon\nabandoned\nabandoning\nabandonment\nabaout\nabased\n...\nzodiacal\nzoilus\nzokkar\nzone\nzones\nzopyrus\nzorry\nzubmizzion\nzuro\n√°¬º\n\n\n\n\n4\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.253506\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n116\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.339101\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n215\n0.0\n0.0\n0.0\n0.0\n0.235817\n0.000000\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n571\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.143788\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n839\n0.0\n0.0\n0.0\n0.0\n0.285886\n0.000000\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows √ó 24937 columns\n\n\n\n\n\nAuthor\nMWS    22\nHPL    15\nEAP    13\nName: Text, dtype: int64\n\n\nLes 10 scores les plus √©lev√©s sont les suivants :\n\nprint(train.iloc[list_fear[:9]]['Text'].values)\n\n['We could not fear we did not.' '\"And now I do not fear death.'\n 'Be of heart and fear nothing.' 'I smiled, for what had I to fear?'\n 'Indeed I had no fear on her account.'\n 'I have not the slightest fear for the result.'\n 'At length, in an abrupt manner she asked, \"Where is he?\" \"O, fear not,\" she continued, \"fear not that I should entertain hope Yet tell me, have you found him?'\n '\"I fear you are right there,\" said the Prefect.'\n 'I went down to open it with a light heart, for what had I now to fear?']\n\n\nOn remarque que les scores les plus √©l√©v√©s sont soient des extraits courts o√π le mot apparait une seule fois, soit des extraits plus longs o√π le mot fear appara√Æt plusieurs fois.\n\n\n Note\nLa matrice document x terms est un exemple typique de matrice sparse puisque, dans des corpus volumineux, une grande diversit√© de vocabulaire peut √™tre trouv√©e."
  },
  {
    "objectID": "content/course/NLP/02_exoclean/index.html#approche-contextuelle-les-n-gramms",
    "href": "content/course/NLP/02_exoclean/index.html#approche-contextuelle-les-n-gramms",
    "title": "30¬† Nettoyer un texte: des exercices pour d√©couvrir l‚Äôapproche bag-of-words",
    "section": "30.6 Approche contextuelle: les n-gramms",
    "text": "30.6 Approche contextuelle: les n-gramms\nPour √™tre en mesure de mener cette analyse, il est n√©cessaire de t√©l√©charger un corpus suppl√©mentaire :\n\nimport nltk\nnltk.download('genesis')\nnltk.corpus.genesis.words('english-web.txt')\n\n[nltk_data] Downloading package genesis to /github/home/nltk_data...\n[nltk_data]   Package genesis is already up-to-date!\n\n\n['In', 'the', 'beginning', 'God', 'created', 'the', ...]\n\n\nIl s‚Äôagit maintenant de raffiner l‚Äôanalyse.\nOn s‚Äôint√©resse non seulement aux mots et √† leur fr√©quence, mais aussi aux mots qui suivent. Cette approche est essentielle pour d√©sambiguiser les homonymes. Elle permet aussi d‚Äôaffiner les mod√®les ‚Äúbag-of-words‚Äù. Le calcul de n-grams (bigrams pour les co-occurences de mots deux-√†-deux, tri-grams pour les co-occurences trois-√†-trois, etc.) constitue la m√©thode la plus simple pour tenir compte du contexte.\nnltk offre des methodes pour tenir compte du contexte : pour ce faire, nous calculons les n-grams, c‚Äôest-√†-dire l‚Äôensemble des co-occurrences successives de mots n-√†-n.¬†En g√©n√©ral, on se contente de bi-grams, au mieux de tri-grams :\n\nles mod√®les de classification, analyse du sentiment, comparaison de documents, etc. qui comparent des n-grams avec n trop grands sont rapidement confront√©s au probl√®me de donn√©es sparse, cela r√©duit la capacit√© pr√©dictive des mod√®les ;\nles performances d√©croissent tr√®s rapidement en fonction de n, et les co√ªts de stockage des donn√©es augmentent rapidement (environ n fois plus √©lev√© que la base de donn√©es initiale).\n\nOn va, rapidement, regarder dans quel contexte appara√Æt le mot fear dans\nl‚Äôoeuvre d‚ÄôEdgar Allan Poe (EAP). Pour cela, on transforme d‚Äôabord\nle corpus EAP en tokens `nltk :\n\neap_clean = train[train[\"Author\"] == \"EAP\"]\neap_clean = ' '.join(eap_clean['Text'])\ntokens = eap_clean.split()\nprint(tokens[:10])\ntext = nltk.Text(tokens)\nprint(text)\n\n['This', 'process,', 'however,', 'afforded', 'me', 'no', 'means', 'of', 'ascertaining', 'the']\n&lt;Text: This process, however, afforded me no means of...&gt;\n\n\nVous aurez besoin des fonctions BigramCollocationFinder.from_words et BigramAssocMeasures.likelihood_ratio :\n\nfrom nltk.collocations import BigramCollocationFinder\nfrom nltk.metrics import BigramAssocMeasures\n\n\n\n Exercice 7  : n-grams et contexte du mot fear\n\nUtiliser la m√©thode concordance pour afficher le contexte dans lequel appara√Æt le terme fear.\nS√©lectionner et afficher les meilleures collocation, par exemple selon le crit√®re du ratio de vraisemblance.\n\nLorsque deux mots sont fortement associ√©s, cela est parfois d√ª au fait qu‚Äôils apparaissent rarement. Il est donc parfois n√©cessaire d‚Äôappliquer des filtres, par exemple ignorer les bigrammes qui apparaissent moins de 5 fois dans le corpus.\n\nRefaire la question pr√©c√©dente en utilisant toujours un mod√®le BigramCollocationFinder suivi de la m√©thode apply_freq_filter pour ne conserver que les bigrammes pr√©sents au moins 5 fois. Puis, au lieu d‚Äôutiliser la m√©thode de maximum de vraisemblance, testez la m√©thode nltk.collocations.BigramAssocMeasures().jaccard.\nNe s‚Äôint√©resser qu‚Äôaux collocations qui concernent le mot fear\n\n\n\nAvec la m√©thode concordance (question 1),\nla liste devrait ressembler √† celle-ci:\n\n\nExemples d'occurences du terme 'fear' :\nDisplaying 13 of 13 matches:\nd quick unequal spoken apparently in fear as well as in anger. What he said wa\nhutters were close fastened, through fear of robbers, and so I knew that he co\nto details. I even went so far as to fear that, as I occasioned much trouble, \nyears of age, was heard to express a fear \"that she should never see Marie aga\nich must be entirely remodelled, for fear of serious accident I mean the steel\n my arm, and I attended her home. 'I fear that I shall never see Marie again.'\nclusion here is absurd. \"I very much fear it is so,\" replied Monsieur Maillard\nbt of ultimately seeing the Pole. \"I fear you are right there,\" said the Prefe\ner occurred before.' Indeed I had no fear on her account. For a moment there w\nerhaps so,\" said I; \"but, Legrand, I fear you are no artist. It is my firm int\n raps with a hammer. Be of heart and fear nothing. My daughter, Mademoiselle M\ne splendor. I have not the slightest fear for the result. The face was so far \narriers of iron that hemmed me in. I fear you have mesmerized\" adding immediat\n\n\n\n\nM√™me si on peut facilement voir le mot avant et apr√®s, cette liste est assez difficile √† interpr√©ter car elle recoupe beaucoup d‚Äôinformations.\nLa collocation consiste √† trouver les bi-grammes qui\napparaissent le plus fr√©quemment ensemble. Parmi toutes les paires de deux mots observ√©es,\nil s‚Äôagit de s√©lectionner, √† partir d‚Äôun mod√®le statistique, les ‚Äúmeilleures‚Äù.\nOn obtient donc avec cette m√©thode (question 2):\nSi on mod√©lise les meilleures collocations:\nCette liste a un peu plus de sens,\non a des noms de personnages, de lieux mais aussi des termes fr√©quemment employ√©s ensemble\n(Chess Player par exemple).\nEn ce qui concerne les collocations du mot fear:\nSi on m√®ne la m√™me analyse pour le terme love, on remarque que de mani√®re logique, on retrouve bien des sujets g√©n√©ralement accol√©s au verbe :\n\ncollocations_word(\"love\")\n\n[('love', 'me'), ('love', 'he'), ('will', 'love'), ('I', 'love'), ('love', ','), ('you', 'love'), ('the', 'love')]"
  },
  {
    "objectID": "content/course/NLP/02_exoclean/index.html#r√©f√©rences",
    "href": "content/course/NLP/02_exoclean/index.html#r√©f√©rences",
    "title": "30¬† Nettoyer un texte: des exercices pour d√©couvrir l‚Äôapproche bag-of-words",
    "section": "30.7 R√©f√©rences",
    "text": "30.7 R√©f√©rences\n\n\n\n\nGaliana, Lino, and Milena Suarez Castillo. 2022. ‚ÄúFuzzy Matching on Big-Data an Illustration with Scanner Data and Crowd-Sourced Nutritional Data.‚Äù\n\n\nGaliana, Lino, Fran√ßois S√©m√©curbe, Benjamin Sakarovitch, and Zbigniew Smoreda. 2020. ‚ÄúResidential Segregation, Daytime Segregation and Spatial Frictions: An Analysis from Mobile Phone Data.‚Äù"
  },
  {
    "objectID": "content/course/NLP/03_lda/index.html",
    "href": "content/course/NLP/03_lda/index.html",
    "title": "31¬† Latent Dirichlet Allocation (LDA)",
    "section": "",
    "text": "32 R√©f√©rences"
  },
  {
    "objectID": "content/course/NLP/03_lda/index.html#librairies-n√©cessaires",
    "href": "content/course/NLP/03_lda/index.html#librairies-n√©cessaires",
    "title": "31¬† Latent Dirichlet Allocation (LDA)",
    "section": "31.1 Librairies n√©cessaires",
    "text": "31.1 Librairies n√©cessaires\nCette page √©voquera les principales librairies pour faire du NLP, notamment :\n\nWordCloud\nNLTK\nSpaCy\nKeras\nTensorFlow\n\n\n\n Hint\nComme dans la partie pr√©c√©dente, il faut t√©l√©charger quelques\n√©l√©ments pour que NTLK puisse fonctionner correctement. Pour cela, faire:\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('genesis')\nnltk.download('omw-1.4')\n\n\nLa liste des modules √† importer est assez longue, la voici:\n\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('genesis')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n#from IPython.display import display\nimport base64\nimport string\nimport re\nimport nltk\n\nfrom collections import Counter\nfrom time import time\n# from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords\nfrom sklearn.metrics import log_loss\nimport matplotlib.pyplot as plt\n\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\n\n[nltk_data] Downloading package stopwords to /github/home/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /github/home/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package genesis to /github/home/nltk_data...\n[nltk_data]   Package genesis is already up-to-date!\n[nltk_data] Downloading package wordnet to /github/home/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /github/home/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!"
  },
  {
    "objectID": "content/course/NLP/03_lda/index.html#donn√©es-utilis√©es",
    "href": "content/course/NLP/03_lda/index.html#donn√©es-utilis√©es",
    "title": "31¬† Latent Dirichlet Allocation (LDA)",
    "section": "31.2 Donn√©es utilis√©es",
    "text": "31.2 Donn√©es utilis√©es\nSi vous avez d√©j√† lu la section pr√©c√©dente et import√© les donn√©es, vous\npouvez passer √† la section suivante\nLe code suivant permet d‚Äôimporter le jeu de donn√©es spooky:\n\nimport pandas as pd\n\nurl='https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv'\nimport pandas as pd\ntrain = pd.read_csv(url,\n                    encoding='latin-1')\ntrain.columns = train.columns.str.capitalize()\n                    \ntrain['ID'] = train['Id'].str.replace(\"id\",\"\")\ntrain = train.set_index('Id')\n\nLe jeu de donn√©es met ainsi en regard un auteur avec une phrase qu‚Äôil a √©crite:\n\ntrain.head()\n\n\n\n\n\n\n\n\nText\nAuthor\nID\n\n\nId\n\n\n\n\n\n\n\nid26305\nThis process, however, afforded me no means of...\nEAP\n26305\n\n\nid17569\nIt never once occurred to me that the fumbling...\nHPL\n17569\n\n\nid11008\nIn his left hand was a gold snuff box, from wh...\nEAP\n11008\n\n\nid27763\nHow lovely is spring As we looked from Windsor...\nMWS\n27763\n\n\nid12958\nFinding nothing else, not even gold, the Super...\nHPL\n12958\n\n\n\n\n\n\n\nLes √©tapes de preprocessing sont expliqu√©es dans le chapitre pr√©c√©dent. On applique les √©tapes suivantes :\n\nTokeniser\nRetirer la ponctuation et les stopwords\nLemmatiser le texte\n\n\nlemma = WordNetLemmatizer()\n\ntrain_clean = (train\n    .groupby([\"ID\",\"Author\"])\n    .apply(lambda s: nltk.word_tokenize(' '.join(s['Text'])))\n    .apply(lambda words: [word for word in words if word.isalpha()])\n)\n\nfrom nltk.corpus import stopwords  \nstop_words = set(stopwords.words('english'))\n\ntrain_clean = (train_clean\n    .apply(lambda words: [lemma.lemmatize(w) for w in words if not w in stop_words])\n    .reset_index(name='tokenized')\n)\n\ntrain_clean.head(2)\n\n\n\n\n\n\n\n\nID\nAuthor\ntokenized\n\n\n\n\n0\n00001\nMWS\n[Idris, well, content, resolve, mine]\n\n\n1\n00002\nHPL\n[I, faint, even, fainter, hateful, modernity, ..."
  },
  {
    "objectID": "content/course/NLP/03_lda/index.html#principe-de-la-lda-latent-dirichlet-allocation",
    "href": "content/course/NLP/03_lda/index.html#principe-de-la-lda-latent-dirichlet-allocation",
    "title": "31¬† Latent Dirichlet Allocation (LDA)",
    "section": "31.3 Principe de la LDA (Latent Dirichlet Allocation)",
    "text": "31.3 Principe de la LDA (Latent Dirichlet Allocation)\nLe mod√®le Latent Dirichlet Allocation (LDA) est un mod√®le probabiliste g√©n√©ratif qui permet\nde d√©crire des collections de documents de texte ou d‚Äôautres types de donn√©es discr√®tes. LDA fait\npartie d‚Äôune cat√©gorie de mod√®les appel√©s ‚Äútopic models‚Äù, qui cherchent √† d√©couvrir des structures\nth√©matiques cach√©es dans des vastes archives de documents.\nCeci permet d‚Äôobtenir des m√©thodes\nefficaces pour le traitement et l‚Äôorganisation des documents de ces archives: organisation automatique\ndes documents par sujet, recherche, compr√©hension et analyse du texte, ou m√™me r√©sumer des\ntextes.\nAujourd‚Äôhui, ce genre de m√©thodes s‚Äôutilisent fr√©quemment dans le web, par exemple pour\nanalyser des ensemble d‚Äôarticles d‚Äôactualit√©, les regrouper par sujet, faire de la recommandation\nd‚Äôarticles, etc.\nLa LDA est une m√©thode qui consid√®re les corpus comme des m√©langes de sujets et\nde mots. Chaque document peut √™tre repr√©sent√© comme le r√©sultat d‚Äôun m√©lange :\n\nde sujets\net, au sein de ces sujets, d‚Äôun choix de mots.\n\nL‚Äôestimation des\nparam√®tres de la LDA passe par l‚Äôestimation des distributions des variables\nlatentes √† partir des donn√©es observ√©es (posterior inference).\nMath√©matiquement, on peut se repr√©senter la LDA comme une\ntechnique de maximisation de log vraisemblance avec un algorithme EM (expectation maximisation)\ndans un mod√®le de m√©lange.\nLa matrice termes-documents qui sert de point de d√©part est la suivante:\n\n\n\n\nword_1\nword_2\nword_3\n‚Ä¶\nword_J\n\n\n\n\ndoc_1\n3\n0\n1\n‚Ä¶\n0\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\ndoc_N\n1\n0\n0\n‚Ä¶\n5\n\n\n\nOn dit que cette matrice est sparse (creuse en Fran√ßais) car elle contient principalement des 0. En effet, un document n‚Äôutilise qu‚Äôune partie mineure du vocabulaire complet.\nLa LDA consiste √† transformer cette matrice sparsedocument-terme en deux matrices de moindre dimension:\n\nUne matrice document-sujet\nUne matrice sujet-mots\n\nEn notant \\(K_i\\) le sujet \\(i\\). On obtient donc\n\nUne matrice document-sujet ayant la structure suivante:\n\n\n\n\n\nK_1\nK_2\nK_3\n‚Ä¶\nK_M\n\n\n\n\ndoc_1\n1\n0\n1\n‚Ä¶\n0\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\ndoc_N\n1\n1\n1\n‚Ä¶\n0\n\n\n\n\nUne matrice sujets-mots ayant la structure suivante:\n\n\n\n\n\nword_1\nword_2\nword_3\n‚Ä¶\nword_J\n\n\n\n\nK_1\n1\n0\n0\n‚Ä¶\n0\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\nK_M\n1\n1\n1\n‚Ä¶\n0\n\n\n\nCes deux matrices ont l‚Äôinterpr√©tation suivante :\n\nLa premi√®re nous renseigne sur la pr√©sence d‚Äôun sujet dans un document\nLa seconde nous renseigne sur la pr√©sence d‚Äôun mot dans un sujet\n\nEn fait, le principe de la LDA est de construire ces deux matrices √† partir des fr√©quences d‚Äôapparition des mots dans le texte.\nOn va se concentrer sur Edgar Allan Poe.\n\ncorpus = train_clean[train_clean[\"Author\"] == \"EAP\"]"
  },
  {
    "objectID": "content/course/NLP/03_lda/index.html#entra√Æner-une-lda",
    "href": "content/course/NLP/03_lda/index.html#entra√Æner-une-lda",
    "title": "31¬† Latent Dirichlet Allocation (LDA)",
    "section": "31.4 Entra√Æner une LDA",
    "text": "31.4 Entra√Æner une LDA\nIl existe plusieurs mani√®res d‚Äôentra√Æner une LDA.\nNous allons utiliser Scikit ici avec la m√©thode LatentDirichletAllocation.\nComme expliqu√© dans la partie mod√©lisation :\n\nOn initialise le mod√®le ;\nOn le met √† jour avec la m√©thode fit.\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\n# Initialise the count vectorizer with the English stop words\ncount_vectorizer = CountVectorizer(stop_words='english')# Fit and transform the processed titles\ncount_data = count_vectorizer.fit_transform(corpus['tokenized'].apply(lambda s: ' '.join(s)))\n\n# Tweak the two parameters below\nnumber_topics = 5\nnumber_words = 10# Create and fit the LDA model\nlda = LatentDirichletAllocation(n_components=11, max_iter=5,\n                                learning_method = 'online',\n                                learning_offset = 50.,\n                                random_state = 0,\n                                n_jobs = 1)\nlda.fit(count_data)"
  },
  {
    "objectID": "content/course/NLP/03_lda/index.html#visualiser-les-r√©sultats",
    "href": "content/course/NLP/03_lda/index.html#visualiser-les-r√©sultats",
    "title": "31¬† Latent Dirichlet Allocation (LDA)",
    "section": "31.5 Visualiser les r√©sultats",
    "text": "31.5 Visualiser les r√©sultats\nOn peut d√©j√† commencer par utiliser une fonction pour afficher les\nr√©sultats :\n\n# Helper function\ndef print_topics(model, count_vectorizer, n_top_words):\n    words = count_vectorizer.get_feature_names_out()\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"\\nTopic #%d:\" % topic_idx)\n        print(\" \".join([words[i]\n                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n\nprint_topics(lda, count_vectorizer, number_words)\n\n\nTopic #0:\narm looking thousand respect hour table woman rest ah seen\n\nTopic #1:\nsaid dupin ha end write smith chair phenomenon quite john\n\nTopic #2:\ntime thing say body matter course day place object immediately\n\nTopic #3:\nmere memory felt sat movement case sole green principle bone\n\nTopic #4:\ndoor room open small friend lady replied night window hand\n\nTopic #5:\nword man day idea good point house shall mind say\n\nTopic #6:\neye figure form left sea hour ordinary life deep world\n\nTopic #7:\nfoot great little earth let le year nature come nearly\n\nTopic #8:\nhand strange head color hair spoken read ear ghastly neck\n\nTopic #9:\ncame looked shadow low dream like death light spirit tree\n\nTopic #10:\neye know heart saw character far tell oh voice wall\n\n\nLa repr√©sentation sous forme de liste de mots n‚Äôest pas la plus pratique‚Ä¶\nOn peut essayer de se repr√©senter un wordcloud de chaque sujet pour mieux voir si cette piste est pertinente :\n\ntf_feature_names = count_vectorizer.get_feature_names_out()\n\ndef wordcloud_lda(lda, tf_feature_names):\n\n  fig, axs = plt.subplots(len(lda.components_) // 3 + 1, 3)\n  \n  for i in range(len(lda.components_)):\n      corpus_lda = lda.components_[i]\n      first_topic_words = [tf_feature_names[l] for l in corpus_lda.argsort()[:-50-1:-1]]\n      k = i // 3\n      j = (i - k*3)\n      wordcloud = WordCloud(stopwords=stop_words, background_color=\"black\",width = 2500, height = 1800)\n      wordcloud = wordcloud.generate(\" \".join(first_topic_words))\n      axs[k][j].set_title(\"Wordcloud pour le \\nsujet {}\".format(i))\n      axs[k][j].axis('off')\n      axs[k][j].imshow(wordcloud)\n  \n  r = len(lda.components_) % 3\n  [fig.delaxes(axs[len(lda.components_) // 3,k-1]) for k in range(r+1, 3+1) if r != 0]\n\nwc = wordcloud_lda(lda, tf_feature_names)\nwc\n\n\n\n\n\nwc\n\nLe module pyLDAvis offre quelques visualisations bien pratiques lorsqu‚Äôon\nd√©sire repr√©senter de mani√®re synth√©tique les r√©sultats d‚Äôune LDA et observer la distribution sujet x mots.\n\n\n Hint\nDans un notebook faire :\nimport pyLDAvis.sklearn\n\npyLDAvis.enable_notebook()\nPour les utilisateurs de Windows, il est n√©cessaire d‚Äôajouter l‚Äôargument\nn_jobs = 1. Sinon, Python tente d‚Äôentra√Æner le mod√®le avec de la\nparall√©lisation. Le probl√®me est que les processus sont des FORKs, ce que\nWindows ne supporte pas. Sur un syst√®me Unix (Linux, Mac OS), on peut se passer de cet\nargument.\n\n\n\n#!pip install pyLDAvis #√† faire en haut du notebook sur colab\nimport pyLDAvis\nimport pyLDAvis.sklearn\n\n# pyLDAvis.enable_notebook()\nvis_data = pyLDAvis.sklearn.prepare(lda, count_data, count_vectorizer, n_jobs = 1)\npyLDAvis.display(vis_data)\n\n\nChaque bulle repr√©sente un sujet. Plus la bulle est grande, plus il y a de documents qui traitent de ce sujet.\n\nPlus les barres sont loin les unes des autres, plus elles sont diff√©rentes. Un bon mod√®le aura donc tendance √† avoir de grandes bulles qui ne se recoupent pas. Ce n‚Äôest pas vraiment le cas ici‚Ä¶\n\nLes barres bleues repr√©sentent la fr√©quence de chaque mot dans le corpus.\nLes barres rouges repr√©sentent une estimation du nombre de termes g√©n√©r√©s dans un sujet pr√©cis. La barre rouge la plus longue correspond au mot le plus utilis√© dans ce sujet."
  },
  {
    "objectID": "content/course/NLP/05_exo_supp/index.html",
    "href": "content/course/NLP/05_exo_supp/index.html",
    "title": "32¬† Exercices suppl√©mentaires",
    "section": "",
    "text": "Cette page approfondit certains aspects pr√©sent√©s dans les autres tutoriels. Il s‚Äôagit d‚Äôune suite d‚Äôexercice, avec corrections, pour pr√©senter d‚Äôautres aspects du NLP ou pratiquer sur des donn√©es diff√©rentes\n\n33 Exploration des libell√©s de l‚Äôopenfood database\n{{% box status=‚Äúexercise‚Äù title=‚ÄúExercise: les noms de produits dans l‚Äôopenfood database‚Äù icon=‚Äúfas fa-pencil-alt‚Äù %}}\nL‚Äôobjectif de cet exercice est d‚Äôanalyser les termes les plus fr√©quents\ndans les noms de produits de l‚Äôopenfood database. Au passage, cela permet de r√©viser les √©tapes de preprocessing (LIEN XXXXX) et d‚Äôexplorer les enjeux de reconnaissance d‚Äôentit√©s nomm√©es.\n{{% /box %}}\nDans cet exercice:\n\ntokenisation (nltk)\nretrait des stop words (nltk)\nnuage de mots (wordcloud)\nreconnaissance du langage (fasttext)\nreconnaissance d‚Äôentit√©s nomm√©es (spacy)\n\nle tout sur l‚ÄôOpenFood Database, une base de donn√©es alimentaire qui est enrichie de mani√®re collaborative.\n{{% box status=‚Äúhint‚Äù title=‚ÄúHint‚Äù icon=‚Äúfa fa-lightbulb‚Äù %}}\nPour pouvoir utiliser les mod√®les pr√©-entra√Æn√©s de spaCy, il faut les t√©l√©charger. La m√©thode pr√©conis√©e est d‚Äôutiliser, depuis un terminal, la commande suivante\npython -m spacy download fr_core_news_sm\nDans un notebook jupyter, il se peut qu‚Äôil soit n√©cessaire de relancer le kernel.\nSi l‚Äôacc√®s √† la ligne de commande n‚Äôest pas possible, ou si la commande √©choue, il est possible de t√©l√©charger le mod√®le pr√©-entra√Æn√© directement depuis une session Python\nimport spacy\nspacy.cli.download('fr_core_news_sm')\n{{% /box %}}\n\nImporter le mod√®le de reconnaissance de langage qui sera utilis√© par la suite\nainsi que le corpus Fran√ßais utilis√© par spacy\n\n\nimport tempfile\nimport os\nimport spacy\n\ntemp_dir = tempfile.NamedTemporaryFile()\ntemp_dir = temp_dir.name\n\nos.system(\"wget -O {} https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\".format( \"%s.model.bin\" % temp_dir))\nspacy.cli.download('fr_core_news_sm')\n\n\nImporter les donn√©es de l‚Äôopenfood database √† partir du code suivant\n\n\nimport pandas as pd\nimport urllib.request\n\n\nurllib.request.urlretrieve('https://static.openfoodfacts.org/data/en.openfoodfacts.org.products.csv', \"%s.openfood.csv\" % temp_dir)\ndf_openfood = pd.read_csv(\"%s.openfood.csv\" % temp_dir, delimiter=\"\\t\",\n                          usecols=['product_name'], encoding = 'utf-8', dtype = \"str\")\n\nCes donn√©es devraient avoir l‚Äôaspect suivant:\n\ndf_openfood.iloc[:2, :5]\n\n\nCr√©er une fonction de nettoyage des noms de produits effectuant les\n√©tapes suivantes:\n\n\ntokeniser le texte en question\nretirer la ponctuation et les stopwords\n\nAppliquer cette fonction √† l‚Äôensemble des noms de produits (variable\nproduct_name)\n\nEffectuer un nuage de mot sur les libell√©s avant et apr√®s nettoyage\npour comprendre la structure du corpus en question.\nLe r√©sultat devrait avoir l‚Äôapparence suivante\n\n\nimport wordcloud as wc\nimport matplotlib.pyplot as plt\n\n\ndef graph_wordcloud(data, by = None, valueby = None, yvar = \"Text\"):\n    if (by is not None) & (valueby is not None):        \n        txt = data[data[by]==valueby][yvar].astype(str)\n    else:\n        txt = data[yvar].astype(str)\n    all_text = ' '.join([text for text in txt])\n    wordcloud = wc.WordCloud(width=800, height=500,\n                          random_state=21,\n                      max_words=2000).generate(all_text)\n    return wordcloud\n\ndef graph_wordcloud_by(data, by, yvar = \"Text\"):\n    n_topics = data[by].unique().tolist()\n    width=20\n    height=80\n    rows = len(n_topics)//2\n    cols = 2\n    fig=plt.figure(figsize=(width, height))\n    axes = []\n    for i in range(cols*rows):\n        b = graph_wordcloud(data, by = by, valueby = n_topics[i], yvar = yvar)\n        axes.append( fig.add_subplot(rows, cols, i+1) )\n        axes[-1].set_title(\"{}\".format(n_topics[i]))  \n        plt.imshow(b)\n        plt.axis('off')\n        plt.savefig('{}.png'.format(yvar), bbox_inches='tight')\n\n\ndef wordcount_words(data, yvar, by = None):\n    plt.figure( figsize=(15,15) )\n    if by is None:\n        wordcloud = graph_wordcloud(data, yvar = yvar, by = by)\n        plt.imshow(wordcloud)\n        plt.axis('off')\n        plt.savefig('{}.png'.format(yvar), bbox_inches='tight')\n    else:\n        graph_wordcloud_by(data, by = by, yvar = yvar)\n\nwordcount_words(df_openfood, yvar = \"product_name\")\nwordcount_words(df_openfood, \"tokenized\")\n\n\nUtiliser la librairie Fasttext pour extraire les noms de produits\nfran√ßais\n\n\nAppliquer le mod√®le t√©l√©charg√© pr√©cedemment pour d√©terminer le langage\nNe r√©cup√©rer que les libell√©s fran√ßais\n\n\nimport fasttext\n\nPRETRAINED_MODEL_PATH = \"%s.model.bin\" % temp_dir\nmodel = fasttext.load_model(PRETRAINED_MODEL_PATH)\nnewcols = ['language','score_language']\ndf_openfood[newcols] = pd.DataFrame(df_openfood['product_name'].astype(str).apply(lambda s: list(model.predict(s))).apply(lambda l: [l[0][0],l[1][0]]).tolist(), columns = newcols)\ndf_openfood['language'] = df_openfood['language'].str.replace(\"__label__\",\"\")\ndf_openfood_french = df_openfood[df_openfood['language'] == \"fr\"]\ndf_openfood_french.head(2)\n\n\nVisualiser avec spacy.displacy le r√©sultat d‚Äôune reconnaissance\nd‚Äôentit√©s nomm√©es sur 50 donn√©es al√©atoires. Cela vous semble-t-il satisfaisant ?\n\n\nimport spacy\nimport fr_core_news_sm\n\nnlp = fr_core_news_sm.load()\n\nexample = \" \\n \".join(df_openfood_french['product_name'].astype(\"str\").sample(50))\n\nfrom spacy import displacy\nhtml = displacy.render(nlp(example), style='ent', page=True)\n\n\nprint(html)\n\n\nR√©cup√©rer dans un vecteur les entit√©s nomm√©es reconnues par spaCy.\nRegarder les entit√©s reconnues dans les 20 premiers libell√©s de produits\n\n\nx = []\nfor doc in nlp.pipe(df_openfood_french.head(20)['product_name'].astype(\"unicode\"), disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n    # Do something with the doc here\n    x.append([(ent.text, ent.label_) for ent in doc.ents])\n    \nx"
  },
  {
    "objectID": "content/course/NLP/04_word2vec/index.html",
    "href": "content/course/NLP/04_word2vec/index.html",
    "title": "33¬† M√©thodes de vectorisation : comptages et word embeddings",
    "section": "",
    "text": "34 Nettoyage des donn√©es\nNous allons ainsi √† nouveau utiliser le jeu de donn√©es spooky:\ndata_url = 'https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv'\nspooky_df = pd.read_csv(data_url)\nLe jeu de donn√©es met ainsi en regard un auteur avec une phrase qu‚Äôil a √©crite:\nspooky_df.head()\nNous allons √† pr√©sent v√©rifier cette conjecture en comparant\nplusieurs mod√®les de vectorisation,\ni.e. de transformation du texte en objets num√©riques\npour que l‚Äôinformation contenue soit exploitable dans un mod√®le de classification.\nOn commence par une approche ‚Äúbag-of-words‚Äù,\ni.e.¬†qui revient simplement √† repr√©senter chaque document par un vecteur\nqui compte le nombre d‚Äôapparitions de chaque mot du vocabulaire dans le document.\ncv_bow = fit_vectorizers(CountVectorizer)\nOn s‚Äôint√©resse ensuite √† l‚Äôapproche TF-IDF,\nqui permet de tenir compte des fr√©quences relatives des mots.\nAinsi, pour un mot donn√©, on va multiplier la fr√©quence d‚Äôapparition du mot dans le document (calcul√© comme dans la m√©thode pr√©c√©dente) par un terme qui p√©nalise une fr√©quence √©lev√©e du mot dans le corpus. L‚Äôimage ci-dessous, emprunt√©e √† Chris Albon, illustre cette mesure:\nSource: https://chrisalbon\nLa vectorisation TF-IDF permet donc de limiter l‚Äôinfluence des stop-words\net donc de donner plus de poids aux mots les plus salients d‚Äôun document.\nOn observe clairement que la performance de classification est bien sup√©rieure,\nce qui montre la pertinence de cette technique.\ncv_tfidf = fit_vectorizers(TfidfVectorizer)\nOn va maintenant explorer les techniques de vectorisation bas√©es sur les\nembeddings de mots, et notamment la plus populaire : Word2Vec.\nL‚Äôid√©e derri√®re est simple, mais a r√©volutionn√© le NLP :\nau lieu de repr√©senter les documents par des\nvecteurs sparse de tr√®s grande dimension (la taille du vocabulaire)\ncomme on l‚Äôa fait jusqu‚Äô√† pr√©sent,\non va les repr√©senter par des vecteurs dense (continus)\nde dimension r√©duite (en g√©n√©ral, autour de 100-300).\nChacune de ces dimensions va repr√©senter un facteur latent,\nc‚Äôest √† dire une variable inobserv√©e,\nde la m√™me mani√®re que les composantes principales produites par une ACP.\nSource: https://medium.com/@zafaralibagh6/simple-tutorial-on-word-embedding-and-word2vec-43d477624b6d\nPourquoi est-ce int√©ressant ?\nPour de nombreuses raisons, mais pour r√©sumer :\ncela permet de beaucoup mieux capturer la similarit√© s√©mantique entre les documents.\nPar exemple, un humain sait qu‚Äôun document contenant le mot ‚ÄúRoi‚Äù\net un autre document contenant le mot ‚ÄúReine‚Äù ont beaucoup de chance\nd‚Äôaborder des sujets semblables.\nPourtant, une vectorisation de type comptage ou TF-IDF\nne permet pas de saisir cette similarit√© :\nle calcul d‚Äôune mesure de similarit√© (norme euclidienne ou similarit√© cosinus)\nentre les deux vecteurs ne prendra en compte la similarit√© des deux concepts, puisque les mots utilis√©s sont diff√©rents.\nA l‚Äôinverse, un mod√®le word2vec bien entra√Æn√© va capter\nqu‚Äôil existe un facteur latent de type ‚Äúroyaut√©‚Äù,\net la similarit√© entre les vecteurs associ√©s aux deux mots sera forte.\nLa magie va m√™me plus loin : le mod√®le captera aussi qu‚Äôil existe un\nfacteur latent de type ‚Äúgenre‚Äù,\net va permettre de construire un espace s√©mantique dans lequel les\nrelations arithm√©tiques entre vecteurs ont du sens ;\npar exemple :\n\\[\\text{king} - \\text{man} + \\text{woman} ‚âà \\text{queen}\\]\nComment ces mod√®les sont-ils entra√Æn√©s ?\nVia une t√¢che de pr√©diction r√©solue par un r√©seau de neurones simple.\nL‚Äôid√©e fondamentale est que la signification d‚Äôun mot se comprend\nen regardant les mots qui apparaissent fr√©quemment dans son voisinage.\nPour un mot donn√©, on va donc essayer de pr√©dire les mots\nqui apparaissent dans une fen√™tre autour du mot cible.\nEn r√©p√©tant cette t√¢che de nombreuses fois et sur un corpus suffisamment vari√©,\non obtient finalement des embeddings pour chaque mot du vocabulaire,\nqui pr√©sentent les propri√©t√©s discut√©es pr√©c√©demment.\nX_train_tokens = [text.split() for text in X_train]\nw2v_model = Word2Vec(X_train_tokens, vector_size=200, window=5, \n                     min_count=1, workers=4)\nw2v_model.wv.most_similar(\"mother\")\nOn voit que les mots les plus similaires √† ‚Äúmother‚Äù\nsont souvent des mots li√©s √† la famille, mais pas toujours.\nC‚Äôest li√© √† la taille tr√®s restreinte du corpus sur lequel on entra√Æne le mod√®le,\nqui ne permet pas de r√©aliser des associations toujours pertinentes.\nL‚Äôembedding (la repr√©sentation vectorielle) de chaque document correspond √† la moyenne des word-embeddings des mots qui le composent :\ndef get_mean_vector(w2v_vectors, words):\n    words = [word for word in words if word in w2v_vectors]\n    if words:\n        avg_vector = np.mean(w2v_vectors[words], axis=0)\n    else:\n        avg_vector = np.zeros_like(w2v_vectors['hi'])\n    return avg_vector\n\ndef fit_w2v_avg(w2v_vectors):\n    X_train_vectors = np.array([get_mean_vector(w2v_vectors, words)\n                                for words in X_train_tokens])\n    \n    scores = cross_val_score(clf, X_train_vectors, y_train, \n                         cv=4, scoring='f1_micro', n_jobs=4)\n\n    print(f\"CV scores {scores}\")\n    print(f\"Mean F1 {np.mean(scores)}\")\n    return scores\ncv_w2vec = fit_w2v_avg(w2v_model.wv)\nLa performance chute fortement ;\nla faute √† la taille tr√®s restreinte du corpus, comme annonc√© pr√©c√©demment.\nQuand on travaille avec des corpus de taille restreinte,\nc‚Äôest g√©n√©ralement une mauvaise id√©e d‚Äôentra√Æner son propre mod√®le word2vec.\nHeureusement, des mod√®les pr√©-entra√Æn√©s sur de tr√®s gros corpus sont disponibles.\nIls permettent de r√©aliser du transfer learning,\nc‚Äôest-√†-dire de b√©n√©ficier de la performance d‚Äôun mod√®le qui a √©t√© entra√Æn√© sur une autre t√¢che ou bien sur un autre corpus.\nL‚Äôun des mod√®les les plus connus pour d√©marrer est le glove_model de\nGensim (Glove pour Global Vectors for Word Representation)1:\nOn peut le charger directement gr√¢ce √† l‚Äôinstruction suivante :\nglove_model = gensim.downloader.load('glove-wiki-gigaword-200')\nPar exemple, la repr√©sentation vectorielle de roi est l‚Äôobjet\nmultidimensionnel suivant :\nglove_model['king']\nComme elle est peu intelligible, on va plut√¥t rechercher les termes les\nplus similaires. Par exemple,\nglove_model.most_similar('mother')\nOn peut retrouver notre formule pr√©c√©dente\n\\[\\text{king} - \\text{man} + \\text{woman} ‚âà \\text{queen}\\]\ndans ce plongement de mots:\nglove_model.most_similar(positive = ['king', 'woman'], negative = ['man'])\nVous pouvez vous r√©f√©rer √† ce tutoriel\npour en d√©couvrir plus sur Word2Vec.\nFaisons notre apprentissage par transfert :\ncv_w2vec_transfert = fit_w2v_avg(glove_model)\nLa performance remonte substantiellement.\nCela √©tant, on ne parvient pas √† faire mieux que les approches basiques,\non arrive √† peine aux performances de la vectorisation par comptage.\nEn effet, pour rappel, les performances sont les suivantes:\nperfs = pd.DataFrame(\n    [np.mean(cv_bow.cv_results_['mean_test_score']),\n     np.mean(cv_tfidf.cv_results_['mean_test_score']),\n    np.mean(cv_w2vec),\n    np.mean(cv_w2vec_transfert)],\n    index = ['Bag-of-Words','TF-IDF', 'Word2Vec non pr√©-entra√Æn√©', 'Word2Vec pr√©-entra√Æn√©'],\n    columns = [\"Mean F1 score\"]\n).sort_values(\"Mean F1 score\",ascending = False)\nperfs\nLes performences limit√©es du mod√®le Word2Vec sont cette fois certainement dues √† la mani√®re dont\nles word-embeddings sont exploit√©s : ils sont moyenn√©s pour d√©crire chaque document.\nCela a plusieurs limites :\nLes embeddings contextuels visent √† pallier les limites des embeddings\ntraditionnels √©voqu√©es pr√©c√©demment.\nCette fois, les mots n‚Äôont plus de repr√©sentation vectorielle fixe,\ncelle-ci est calcul√©e dynamiquement en fonction des mots du voisinage, et ainsi de suite.\nCela permet de tenir compte de la structure des phrases\net de tenir compte du fait que le sens d‚Äôun mot est fortement d√©pendant des mots\nqui l‚Äôentourent.\nPar exemple, dans les expressions ‚Äúle pr√©sident Macron‚Äù et ‚Äúle camembert Pr√©sident‚Äù le mot pr√©sident n‚Äôa pas du tout le m√™me r√¥le.\nCes embeddings sont produits par des architectures tr√®s complexes,\nde type Transformer (BERT, etc.).\nmodel = SentenceTransformer('all-mpnet-base-v2')\nX_train_vectors = model.encode(X_train)\nscores = cross_val_score(clf, X_train_vectors, y_train, \n                         cv=4, scoring='f1_micro', n_jobs=4)\n\nprint(f\"CV scores {scores}\")\nprint(f\"Mean F1 {np.mean(scores)}\")\nperfs = pd.concat(\n  [perfs,\n  pd.DataFrame(\n    [np.mean(scores)],\n    index = ['Contextual Embedding'],\n    columns = [\"Mean F1 score\"])]\n).sort_values(\"Mean F1 score\",ascending = False)\nperfs\nVerdict : on fait tr√®s l√©g√®rement mieux que la vectorisation TF-IDF.\nOn voit donc l‚Äôimportance de tenir compte du contexte.\nMais pourquoi, avec une m√©thode tr√®s compliqu√©e, ne parvenons-nous pas √† battre une m√©thode toute simple ?\nOn peut avancer plusieurs raisons :\nDans le cas de notre t√¢che de classification, il est probable que\ncertains mots (noms de personnage, noms de lieux) soient suffisants pour classifier de mani√®re pertinente,\nce que ne permettent pas de capter les embeddings qui accordent √† tous les mots la m√™me importance."
  },
  {
    "objectID": "content/course/NLP/04_word2vec/index.html#preprocessing",
    "href": "content/course/NLP/04_word2vec/index.html#preprocessing",
    "title": "33¬† M√©thodes de vectorisation : comptages et word embeddings",
    "section": "34.1 Preprocessing",
    "text": "34.1 Preprocessing\nEn NLP, la premi√®re √©tape est souvent celle du preprocessing, qui inclut notamment les √©tapes de tokenization et de nettoyage du texte. Comme celles-ci ont √©t√© vues en d√©tail dans le pr√©c√©dent chapitre, on se contentera ici d‚Äôun preprocessing minimaliste : suppression de la ponctuation et des stop words (pour la visualisation et les m√©thodes de vectorisation bas√©es sur des comptages).\nJusqu‚Äô√† pr√©sent, nous avons utilis√© principalement nltk pour le\npreprocessing de donn√©es textuelles. Cette fois, nous proposons\nd‚Äôutiliser la librairie spaCy qui permet de mieux automatiser sous forme de\npipelines de preprocessing.\nPour initialiser le processus de nettoyage,\non va utiliser le corpus en_core_web_sm (voir plus\nhaut pour l‚Äôinstallation de ce corpus):\n\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n\nOn va utiliser un pipe spacy qui permet d‚Äôautomatiser, et de parall√©liser,\nun certain nombre d‚Äôop√©rations. Les pipes sont l‚Äô√©quivalent, en NLP, de\nnos pipelines scikit ou des pipes pandas. Il s‚Äôagit donc d‚Äôun outil\ntr√®s appropri√© pour industrialiser un certain nombre d‚Äôop√©rations de\npreprocessing :\n\ndef clean_docs(texts, remove_stopwords=False, n_process = 4):\n    \n    docs = nlp.pipe(texts, \n                    n_process=n_process,\n                    disable=['parser', 'ner',\n                             'lemmatizer', 'textcat'])\n    stopwords = nlp.Defaults.stop_words\n\n    docs_cleaned = []\n    for doc in docs:\n        tokens = [tok.text.lower().strip() for tok in doc if not tok.is_punct]\n        if remove_stopwords:\n            tokens = [tok for tok in tokens if tok not in stopwords]\n        doc_clean = ' '.join(tokens)\n        docs_cleaned.append(doc_clean)\n        \n    return docs_cleaned\n\nOn applique la fonction clean_docs √† notre colonne pandas.\nLes pandas.Series √©tant it√©rables, elles se comportent comme des listes et\nfonctionnent ainsi tr√®s bien avec notre pipe spacy\n\nspooky_df['text_clean'] = clean_docs(spooky_df['text'])\n\n\nspooky_df.head()"
  },
  {
    "objectID": "content/course/NLP/04_word2vec/index.html#encodage-de-la-variable-√†-pr√©dire",
    "href": "content/course/NLP/04_word2vec/index.html#encodage-de-la-variable-√†-pr√©dire",
    "title": "33¬† M√©thodes de vectorisation : comptages et word embeddings",
    "section": "34.2 Encodage de la variable √† pr√©dire",
    "text": "34.2 Encodage de la variable √† pr√©dire\nOn r√©alise un simple encodage de la variable √† pr√©dire :\nil y a trois cat√©gories (auteurs), repr√©sent√©es par des entiers 0, 1 et 2.\nPour cela, on utilise le LabelEncoder de scikit d√©j√† pr√©sent√©\ndans la partie mod√©lisation. On va utiliser la m√©thode\nfit_transform qui permet, en un tour de main, d‚Äôappliquer √† la fois\nl‚Äôentra√Ænement (fit), √† savoir la cr√©ation d‚Äôune correspondance entre valeurs\nnum√©riques et labels, et l‚Äôappliquer (transform) √† la m√™me colonne.\n\nle = LabelEncoder()\nspooky_df['author_encoded'] = le.fit_transform(spooky_df['author'])\n\nOn peut v√©rifier les classes de notre LabelEncoder :\n\nle.classes_"
  },
  {
    "objectID": "content/course/NLP/04_word2vec/index.html#construction-des-bases-dentra√Ænement-et-de-test",
    "href": "content/course/NLP/04_word2vec/index.html#construction-des-bases-dentra√Ænement-et-de-test",
    "title": "33¬† M√©thodes de vectorisation : comptages et word embeddings",
    "section": "34.3 Construction des bases d‚Äôentra√Ænement et de test",
    "text": "34.3 Construction des bases d‚Äôentra√Ænement et de test\nOn met de c√¥t√© un √©chantillon de test (20 %) avant toute analyse (m√™me descriptive).\nCela permettra d‚Äô√©valuer nos diff√©rents mod√®les toute √† la fin de mani√®re tr√®s rigoureuse,\npuisque ces donn√©es n‚Äôauront jamais utilis√©es pendant l‚Äôentra√Ænement.\nNotre √©chantillon initial n‚Äôest pas √©quilibr√© (balanced) : on retrouve plus d‚Äôoeuvres de\ncertains auteurs que d‚Äôautres. Afin d‚Äôobtenir un mod√®le qui soit √©valu√© au mieux, nous allons donc stratifier notre √©chantillon de mani√®re √† obtenir une r√©partition similaire d‚Äôauteurs dans nos\nensembles d‚Äôentra√Ænement et de test.\n\nX_train, X_test, y_train, y_test = train_test_split(spooky_df['text_clean'].values, \n                                                    spooky_df['author_encoded'].values, \n                                                    test_size=0.2, \n                                                    random_state=33,\n                                                    stratify = spooky_df['author_encoded'].values)\n\nPar exemple, les textes d‚ÄôEAP repr√©sentent 40 % des √©chantillons d‚Äôentra√Ænement et de test :\n\nprint(100*y_train.tolist().count(0)/(len(y_train)))\nprint(100*y_test.tolist().count(0)/(len(y_test)))\n\nAper√ßu du premier √©l√©ment de X_train :\n\nX_train[0]\n\nOn peut aussi v√©rifier qu‚Äôon est capable de retrouver\nla correspondance entre nos auteurs initiaux avec\nla m√©thode inverse_transform\n\nprint(y_train[0], le.inverse_transform([y_train[0]])[0])"
  },
  {
    "objectID": "content/course/NLP/04_word2vec/index.html#r√©partition-des-labels",
    "href": "content/course/NLP/04_word2vec/index.html#r√©partition-des-labels",
    "title": "33¬† M√©thodes de vectorisation : comptages et word embeddings",
    "section": "35.1 R√©partition des labels",
    "text": "35.1 R√©partition des labels\nRefaisons un graphique que nous avons d√©j√† produit pr√©c√©demment pour voir\nla r√©partition de notre corpus entre auteurs:\n\nfig = pd.Series(le.inverse_transform(y_train)).value_counts().plot(kind='bar')\nfig\n\nOn observe une petite asym√©trie : les passages des livres d‚ÄôEdgar Allen Poe sont plus nombreux que ceux des autres auteurs dans notre corpus d‚Äôentra√Ænement, ce qui peut √™tre probl√©matique dans le cadre d‚Äôune t√¢che de classification.\nL‚Äô√©cart n‚Äôest pas dramatique, mais on essaiera d‚Äôen tenir compte dans l‚Äôanalyse en choisissant une m√©trique d‚Äô√©valuation pertinente."
  },
  {
    "objectID": "content/course/NLP/04_word2vec/index.html#mots-les-plus-fr√©quemment-utilis√©s-par-chaque-auteur",
    "href": "content/course/NLP/04_word2vec/index.html#mots-les-plus-fr√©quemment-utilis√©s-par-chaque-auteur",
    "title": "33¬† M√©thodes de vectorisation : comptages et word embeddings",
    "section": "35.2 Mots les plus fr√©quemment utilis√©s par chaque auteur",
    "text": "35.2 Mots les plus fr√©quemment utilis√©s par chaque auteur\nOn va supprimer les stopwords pour r√©duire le bruit dans notre jeu\nde donn√©es.\n\n# Suppression des stop words\nX_train_no_sw = clean_docs(X_train, remove_stopwords=True)\nX_train_no_sw = np.array(X_train_no_sw)\n\nPour visualiser rapidement nos corpus, on peut utiliser la technique des\nnuages de mots d√©j√† vue √† plusieurs reprises.\nVous pouvez essayer de faire vous-m√™me les nuages ci-dessous\nou cliquer sur la ligne ci-dessous pour afficher le code ayant\ng√©n√©r√© les figures :\n\nCliquer pour afficher le code üëá\n\ndef plot_top_words(initials, ax, n_words=20):\n    # Calcul des mots les plus fr√©quemment utilis√©s par l'auteur\n    texts = X_train_no_sw[le.inverse_transform(y_train) == initials]\n    all_tokens = ' '.join(texts).split()\n    counts = Counter(all_tokens)\n    top_words = [word[0] for word in counts.most_common(n_words)]\n    top_words_counts = [word[1] for word in counts.most_common(n_words)]\n    \n    # Repr√©sentation sous forme de barplot\n    ax = sns.barplot(ax = ax, x=top_words, y=top_words_counts)\n    ax.set_title(f'Most Common Words used by {initials_to_author[initials]}')\n\n\ninitials_to_author = {\n    'EAP': 'Edgar Allen Poe',\n    'HPL': 'H.P. Lovecraft',\n    'MWS': 'Mary Wollstonecraft Shelley'\n}\n\nfig, axs = plt.subplots(3, 1, figsize = (12,12))\n\nplot_top_words('EAP', ax = axs[0])\nplot_top_words('HPL', ax = axs[1])\nplot_top_words('MWS', ax = axs[2])\n\n\n\nBeaucoup de mots se retrouvent tr√®s utilis√©s par les trois auteurs.\nIl y a cependant des diff√©rences notables : le mot ‚Äúlife‚Äù\nest le plus employ√© par MWS, alors qu‚Äôil n‚Äôappara√Æt pas dans les deux autres tops.\nDe m√™me, le mot ‚Äúold‚Äù est le plus utilis√© par HPL\nl√† o√π les deux autres ne l‚Äôutilisent pas de mani√®re surrepr√©sent√©e.\nIl semble donc qu‚Äôil y ait des particularit√©s propres √† chacun des auteurs\nen termes de vocabulaire,\nce qui laisse penser qu‚Äôil est envisageable de pr√©dire les auteurs √† partir\nde leurs textes dans une certaine mesure."
  },
  {
    "objectID": "content/course/NLP/04_word2vec/index.html#d√©marche",
    "href": "content/course/NLP/04_word2vec/index.html#d√©marche",
    "title": "33¬† M√©thodes de vectorisation : comptages et word embeddings",
    "section": "36.1 D√©marche",
    "text": "36.1 D√©marche\nComme nous nous int√©ressons plus √† l‚Äôeffet de la vectorisation qu‚Äô√† la t√¢che de classification en elle-m√™me,\nnous allons utiliser un algorithme de classification simple (un SVM lin√©aire), avec des param√®tres non fine-tun√©s (c‚Äôest-√†-dire des param√®tres pas n√©cessairement choisis pour √™tre les meilleurs de tous).\n\nclf = LinearSVC(max_iter=10000, C=0.1)\n\nCe mod√®le est connu pour √™tre tr√®s performant sur les t√¢ches de classification de texte, et nous fournira donc un bon mod√®le de r√©f√©rence (baseline). Cela nous permettra √©galement de comparer de mani√®re objective l‚Äôimpact des m√©thodes de vectorisation sur la performance finale.\n\n\n\n\nPour les deux premi√®res m√©thodes de vectorisation\n(bas√©es sur des fr√©quences et fr√©quences relatives des mots),\non va simplement normaliser les donn√©es d‚Äôentr√©e, ce qui va permettre au SVM de converger plus rapidement, ces mod√®les √©tant sensibles aux diff√©rences d‚Äô√©chelle dans les donn√©es.\nOn va √©galement fine-tuner via grid-search\ncertains hyperparam√®tres li√©s √† ces m√©thodes de vectorisation :\n\non teste diff√©rents ranges de n-grams (unigrammes et unigrammes + bigrammes)\non teste avec et sans stop-words\n\nAfin d‚Äô√©viter le surapprentissage,\non va √©valuer les diff√©rents mod√®les via validation crois√©e, calcul√©e sur 4 blocs.\nOn r√©cup√®re √† la fin le meilleur mod√®le selon une m√©trique sp√©cifi√©e.\nOn choisit le score F1,\nmoyenne harmonique de la pr√©cision et du rappel,\nqui donne un poids √©quilibr√© aux deux m√©triques, tout en p√©nalisant fortement le cas o√π l‚Äôune des deux est faible.\nPr√©cis√©ment, on retient le score F1 *micro-averaged* :\nles contributions des diff√©rentes classes √† pr√©dire sont agr√©g√©es,\npuis on calcule le score F1 sur ces donn√©es agr√©g√©es.\nL‚Äôavantage de ce choix est qu‚Äôil permet de tenir compte des diff√©rences\nde fr√©quences des diff√©rentes classes."
  },
  {
    "objectID": "content/course/NLP/04_word2vec/index.html#pipeline-de-pr√©diction",
    "href": "content/course/NLP/04_word2vec/index.html#pipeline-de-pr√©diction",
    "title": "33¬† M√©thodes de vectorisation : comptages et word embeddings",
    "section": "36.2 Pipeline de pr√©diction",
    "text": "36.2 Pipeline de pr√©diction\nOn va utiliser un pipeline scikit ce qui va nous permettre d‚Äôavoir\nun code tr√®s concis pour effectuer cet ensemble de t√¢ches coh√©rentes.\nDe plus, cela va nous assurer de g√©rer de mani√®re coh√©rentes nos diff√©rentes\ntransformations (cf.¬†partie sur les pipelines)\nPour se faciliter la vie, on d√©finit une fonction fit_vectorizers qui\nint√®gre dans un pipeline g√©n√©rique une m√©thode d‚Äôestimation scikit\net fait de la validation crois√©e en cherchant le meilleur mod√®le\n(en excluant/incluant les stopwords et avec unigrammes/bigrammes)\n\ndef fit_vectorizers(vectorizer):\n    pipeline = Pipeline(\n    [\n        (\"vect\", vectorizer()),\n        (\"scaling\", StandardScaler(with_mean=False)),\n        (\"clf\", clf),\n    ]\n    )\n\n    parameters = {\n        \"vect__ngram_range\": ((1, 1), (1, 2)),  # unigrams or bigrams\n        \"vect__stop_words\": (\"english\", None)\n    }\n\n    grid_search = GridSearchCV(pipeline, parameters, scoring='f1_micro',\n                               cv=4, n_jobs=4, verbose=1)\n    grid_search.fit(X_train, y_train)\n\n    best_parameters = grid_search.best_estimator_.get_params()\n    for param_name in sorted(parameters.keys()):\n        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n\n    print(f\"CV scores {grid_search.cv_results_['mean_test_score']}\")\n    print(f\"Mean F1 {np.mean(grid_search.cv_results_['mean_test_score'])}\")\n    \n    return grid_search"
  },
  {
    "objectID": "content/course/NLP/04_word2vec/index.html#aller-plus-loin",
    "href": "content/course/NLP/04_word2vec/index.html#aller-plus-loin",
    "title": "33¬† M√©thodes de vectorisation : comptages et word embeddings",
    "section": "41.1 Aller plus loin",
    "text": "41.1 Aller plus loin\n\nNous avons entra√Æn√© diff√©rents mod√®les sur l‚Äô√©chantillon d‚Äôentra√Ænement par validation crois√©e, mais nous n‚Äôavons toujours pas utilis√© l‚Äô√©chantillon test que nous avons mis de c√¥t√© au d√©but. R√©aliser la pr√©diction sur les donn√©es de test, et v√©rifier si l‚Äôon obtient le m√™me classement des m√©thodes de vectorisation.\nFaire un vrai split train/test : faire l‚Äôentra√Ænement avec des textes de certains auteurs, et faire la pr√©diction avec des textes d‚Äôauteurs diff√©rents. Cela permettrait de neutraliser la pr√©sence de noms de lieux, de personnages, etc.\nComparer avec d‚Äôautres algorithmes de classification qu‚Äôun SVM\n(Avanc√©) : fine-tuner le mod√®le d‚Äôembeddings contextuels sur la t√¢che de classification"
  },
  {
    "objectID": "content/course/NLP/01_intro/index.html",
    "href": "content/course/NLP/01_intro/index.html",
    "title": "34¬† Quelques √©l√©ments pour comprendre les enjeux du NLP",
    "section": "",
    "text": "35 Base d‚Äôexemple\nLa base d‚Äôexemple est le Comte de Monte Cristo d‚ÄôAlexandre Dumas.\nIl est disponible\ngratuitement sur le site\nProject Gutemberg comme des milliers\nd‚Äôautres livres du domaine public. La mani√®re la plus simple de le r√©cup√©rer\nest de t√©l√©charger avec le package request le fichier texte et le retravailler\nl√©g√®rement pour ne conserver que le corpus du livre :\nfrom urllib import request\n\nurl = \"https://www.gutenberg.org/files/17989/17989-0.txt\"\nresponse = request.urlopen(url)\nraw = response.read().decode('utf8')\n\ndumas = raw.split(\"*** START OF THE PROJECT GUTENBERG EBOOK LE COMTE DE MONTE-CRISTO, TOME I ***\")[1].split(\"*** END OF THE PROJECT GUTENBERG EBOOK LE COMTE DE MONTE-CRISTO, TOME I ***\")[0]\n\nimport re\n\ndef clean_text(text):\n    text = text.lower() # mettre les mots en minuscule\n    text = \" \".join(text.split())\n    return text\n\ndumas = clean_text(dumas)\n\ndumas[10000:10500]\n\n\" mes yeux. --vous avez donc vu l'empereur aussi? --il est entr√© chez le mar√©chal pendant que j'y √©tais. --et vous lui avez parl√©? --c'est-√†-dire que c'est lui qui m'a parl√©, monsieur, dit dant√®s en souriant. --et que vous a-t-il dit? --il m'a fait des questions sur le b√¢timent, sur l'√©poque de son d√©part pour marseille, sur la route qu'il avait suivie et sur la cargaison qu'il portait. je crois que s'il e√ªt √©t√© vide, et que j'en eusse √©t√© le ma√Ætre, son intention e√ªt √©t√© de l'acheter; mais je lu\""
  },
  {
    "objectID": "content/course/NLP/01_intro/index.html#la-particularit√©-des-donn√©es-textuelles",
    "href": "content/course/NLP/01_intro/index.html#la-particularit√©-des-donn√©es-textuelles",
    "title": "34¬† Quelques √©l√©ments pour comprendre les enjeux du NLP",
    "section": "35.1 La particularit√© des donn√©es textuelles",
    "text": "35.1 La particularit√© des donn√©es textuelles\n\n35.1.1 Objectif\nLe natural language processing (NLP) ou\ntraitement automatis√© de la langue (TAL) en Fran√ßais,\nvise √† extraire de l‚Äôinformation de textes √† partir d‚Äôune analyse statistique du contenu.\nCette d√©finition permet d‚Äôinclure de nombreux champs d‚Äôapplications au sein\ndu NLP (traduction, analyse de sentiment, recommandation, surveillance, etc. ) ainsi que de m√©thodes.\nCette approche implique de transformer un texte, qui est une information compr√©hensible par un humain, en un nombre, information appropri√©e pour un ordinateur et une approche statistique ou algorithmique.\nTransformer une information textuelle en valeurs num√©riques propres √† une analyse statistique n‚Äôest pas une t√¢che √©vidente. Les donn√©es textuelles sont non structur√©es puisque l‚Äôinformation cherch√©e, qui est propre √† chaque analyse, est perdue au milieu d‚Äôune grande masse d‚Äôinformations qui doit, de plus, √™tre interpr√©t√©e dans un certain contexte (un m√™me mot ou une phrase n‚Äôayant pas la m√™me signification selon le contexte).\nSi cette t√¢che n‚Äô√©tait pas assez difficile comme √ßa, on peut ajouter d‚Äôautres difficult√©s propres √† l‚Äôanalyse textuelle car ces donn√©es sont :\n\nbruit√©es : ortographe, fautes de frappe‚Ä¶\nchangeantes : la langue √©volue avec de nouveaux mots, sens‚Ä¶\ncomplexes : structures variables, accords‚Ä¶\nambigues : synonymie, polys√©mie, sens cach√©‚Ä¶\npropres √† chaque langue : il n‚Äôexiste pas de r√®gle de passage unique entre deux langues\ngrande dimension : des combinaisons infinies de s√©quences de mots\n\n\n\n35.1.2 M√©thode\nL‚Äôunit√© textuelle peut √™tre le mot ou encore une s√©quence de n\nmots (un n-gramme) ou encore une cha√Æne de caract√®res (e.g.¬†la\nponctuation peut √™tre signifiante). On parle de token. L‚Äôanalyse textuelle vise √† transformer le texte en donn√©es\nnum√©riques manipulables.\nOn peut ensuite utiliser diverses techniques (clustering,\nclassification supervis√©e) suivant l‚Äôobjectif poursuivi pour exploiter\nl‚Äôinformation transform√©e. Mais les √©tapes de nettoyage de texte sont indispensables car sinon un algorithme sera incapable de d√©tecter une information pertinente dans l‚Äôinfini des possibles."
  },
  {
    "objectID": "content/course/NLP/01_intro/index.html#nettoyer-un-texte",
    "href": "content/course/NLP/01_intro/index.html#nettoyer-un-texte",
    "title": "34¬† Quelques √©l√©ments pour comprendre les enjeux du NLP",
    "section": "35.2 Nettoyer un texte",
    "text": "35.2 Nettoyer un texte\nLes wordclouds sont des repr√©sentations graphiques assez pratiques pour visualiser\nles mots les plus fr√©quents. Elles sont tr√®s simples √† impl√©menter en Python\navec le module wordcloud qui permet m√™me d‚Äôajuster la forme du nuage √†\nune image :\n\nimport wordcloud\nimport numpy as np\nimport io\nimport requests\nimport PIL\nimport matplotlib.pyplot as plt\n\nimg = \"https://raw.githubusercontent.com/linogaliana/python-datascientist/master/content/course/NLP/book.png\"\nbook_mask = np.array(PIL.Image.open(io.BytesIO(requests.get(img).content)))\n\nfig = plt.figure()\n\ndef make_wordcloud(corpus):\n    wc = wordcloud.WordCloud(background_color=\"white\", max_words=2000, mask=book_mask, contour_width=3, contour_color='steelblue')\n    wc.generate(corpus)\n    return wc\n\nplt.imshow(make_wordcloud(dumas), interpolation='bilinear')\nplt.axis(\"off\")\n#plt.show()\n#plt.savefig('word.png', bbox_inches='tight')\n\n(-0.5, 1429.5, 783.5, -0.5)\n\n\n\n\n\nCela montre clairement qu‚Äôil est n√©cessaire de nettoyer notre texte. Le nom\ndu personnage principal, Dant√®s, est ainsi masqu√© par un certain nombre\nd‚Äôarticles ou mots de liaison qui perturbent l‚Äôanalyse. Ces mots sont des\nstop-words. La librairie NLTK (Natural Language ToolKit), librairie\nde r√©f√©rence dans le domaine du NLP, permet de facilement retirer ces\nstopwords (cela pourrait √©galement √™tre fait avec\nla librairie plus r√©cente, spaCy). Avant cela, il est n√©cessaire\nde transformer notre texte en le d√©coupant par unit√©s fondamentales (les tokens).\nLes exemples suivants, extraits de Galiana and Castillo (2022), montrent l‚Äôint√©r√™t du\nnettoyage de textes lorsqu‚Äôon d√©sire comparer des corpus\nentre eux. En l‚Äôoccurrence, il s‚Äôagit de comparer un corpus de\nnoms de produits dans des collectes automatis√©es de produits\nde supermarch√© (scanner-data) avec des noms de produits\ndans les donn√©es de l‚ÄôOpenFoodFacts, une base de donn√©es\ncontributive. Sans nettoyage, le bruit l‚Äôemporte sur le signal\net il est impossible de d√©celer des similarit√©s entre les jeux\nde donn√©es. Le nettoyage permet d‚Äôharmoniser\nun peu ces jeux de donn√©es pour avoir une chance d‚Äô√™tre en\nmesure de les comparer.\n\n\n\n\n\n\nOpenFoodFacts avant nettoyage\n\n\n\n\n\n\n\nScanner-data avant nettoyage\n\n\n\n\n\n\n\n\n\n\n\nOpenFoodFacts apr√®s nettoyage\n\n\n\n\n\n\n\nScanner-data apr√®s nettoyage\n\n\n\n\n\n\n35.2.1 Tokenisation\n\n\n Hint\nLors de la premi√®re utilisation de NLTK, il est n√©cessaire de t√©l√©charger\nquelques √©l√©ments n√©cessaires √† la tokenisation, notamment la ponctuation.\nPour cela, il est recommand√© d‚Äôutiliser la commande suivante:\nimport nltk\nnltk.download('punkt')\n\n\n\nimport nltk\nnltk.download('punkt')\n\n[nltk_data] Downloading package punkt to /github/home/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\nTrue\n\n\nLa tokenisation consiste √† d√©couper un texte en morceaux. Ces morceaux\npourraient √™tre des phrases, des chapitres, des n-grammes ou des mots. C‚Äôest\ncette derni√®re option que l‚Äôon va choisir, plus simple pour retirer les\nstopwords :\n\nimport nltk\n\nwords = nltk.word_tokenize(dumas, language='french')\nwords[1030:1050]\n\n['que',\n 'voulez-vous',\n ',',\n 'monsieur',\n 'edmond',\n ',',\n 'reprit',\n \"l'armateur\",\n 'qui',\n 'paraissait',\n 'se',\n 'consoler',\n 'de',\n 'plus',\n 'en',\n 'plus',\n ',',\n 'nous',\n 'sommes',\n 'tous']\n\n\nOn remarque que les mots avec apostrophes sont li√©s en un seul, ce qui est\npeut-√™tre faux sur le plan de la grammaire mais peu avoir un sens pour une\nanalyse statistique. Il reste des signes de ponctuation qu‚Äôon peut √©liminer\navec la m√©thode isalpha:\n\nwords = [word for word in words if word.isalpha()]\nwords[1030:1050]\n\n['assez',\n 'sombre',\n 'obs√©quieux',\n 'envers',\n 'ses',\n 'sup√©rieurs',\n 'insolent',\n 'envers',\n 'ses',\n 'subordonn√©s',\n 'aussi',\n 'outre',\n 'son',\n 'titre',\n 'comptable',\n 'qui',\n 'est',\n 'toujours',\n 'un',\n 'motif']\n\n\nComme indiqu√© ci-dessus, pour t√©l√©charger\nle corpus de ponctuation, il est\nn√©cessaire d‚Äôex√©cuter la ligne de\ncommande suivante :\n\n\n35.2.2 Retirer les stop-words\nLe jeu de donn√©es est maintenant propre. On peut d√©sormais retirer les\nmots qui n‚Äôapportent pas de sens et servent seulement √† faire le\nlien entre deux pr√©positions. On appelle ces mots des\nstop words dans le domaine du NLP.\n\n\n Hint\nLors de la premi√®re utilisation de NLTK, il est n√©cessaire de t√©l√©charger\nles stopwords.\nimport nltk\nnltk.download('stopwords')\n\n\nComme indiqu√© ci-dessus, pour t√©l√©charger\nle corpus de stopwords1, il est\nn√©cessaire d‚Äôex√©cuter la ligne de\ncommande suivante :\n\nimport nltk\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to /github/home/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\n\nfrom nltk.corpus import stopwords\nprint(stopwords.words(\"french\"))\n\nstop_words = set(stopwords.words('french'))\n\n\nwords = [w for w in words if not w in stop_words]\nprint(words[1030:1050])\n\n['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'm√™me', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', '√†', 'm', 'n', 's', 't', 'y', '√©t√©', '√©t√©e', '√©t√©es', '√©t√©s', '√©tant', '√©tante', '√©tants', '√©tantes', 'suis', 'es', 'est', 'sommes', '√™tes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', '√©tais', '√©tait', '√©tions', '√©tiez', '√©taient', 'fus', 'fut', 'f√ªmes', 'f√ªtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'f√ªt', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'e√ªmes', 'e√ªtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'e√ªt', 'eussions', 'eussiez', 'eussent']\n['celui', 'dant√®s', 'a', 'd√©pos√©', 'passant', 'comment', 'paquet', 'd√©poser', 'danglars', 'rougit', 'passais', 'devant', 'porte', 'capitaine', 'entrouverte', 'vu', 'remettre', 'paquet', 'cette', 'lettre']\n\n\nCes retraitements commencent √† porter leurs fruits puisque des mots ayant plus\nde sens commencent √† se d√©gager, notamment les noms des personnages\n(Fernand, Merc√©d√®s, Villefort, etc.)\n\nwc = make_wordcloud(' '.join(words))\n\nfig = plt.figure()\n\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\n\n(-0.5, 1429.5, 783.5, -0.5)\n\n\n\n\n\n\n\n35.2.3 Stemming\nPour r√©duire la complexit√© d‚Äôun texte, on peut tirer partie de\n‚Äúclasses d‚Äô√©quivalence‚Äù : on peut\nconsid√©rer que diff√©rentes formes d‚Äôun m√™me mot (pluriel,\nsingulier, conjugaison) sont √©quivalentes et les remplacer par une\nm√™me forme dite canonique. Il existe deux approches dans le domaine :\n\nla lemmatisation qui requiert la connaissance des statuts\ngrammaticaux (exemple : chevaux devient cheval)\nla racinisation (stemming) plus fruste mais plus rapide, notamment\nen pr√©sence de fautes d‚Äôorthographes. Dans ce cas, chevaux peut devenir chev\nmais √™tre ainsi confondu avec chevet ou cheveux\n\nLa racinisation est plus simple √† mettre en oeuvre car elle peut s‚Äôappuyer sur\ndes r√®gles simples pour extraire la racine d‚Äôun mot.\nPour r√©duire un mot dans sa forme ‚Äúracine‚Äù, c‚Äôest-√†-dire en s‚Äôabstrayant des\nconjugaisons ou variations comme les pluriels, on applique une m√©thode de\nstemming. Le but du stemming est de regrouper de\nnombreuses variantes d‚Äôun mot comme un seul et m√™me mot.\nPar exemple, une fois que l‚Äôon applique un stemming, ‚Äúchats‚Äù et ‚Äúchat‚Äù\ndeviennent un m√™me mot.\nCette approche a l‚Äôavantage de r√©duire la taille du vocabulaire √† ma√Ætriser\npour l‚Äôordinateur et le mod√©lisateur. Il existe plusieurs algorithmes de\nstemming, notamment le Porter Stemming Algorithm ou le\nSnowball Stemming Algorithm. Nous pouvons utiliser ce dernier en Fran√ßais :\n\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer(language='french')\n\nstemmed = [stemmer.stem(word) for word in words]\nprint(stemmed[1030:1050])\n\n['celui', 'dantes', 'a', 'd√©pos', 'pass', 'comment', 'paquet', 'd√©pos', 'danglar', 'roug', 'pass', 'dev', 'port', 'capitain', 'entrouvert', 'vu', 'remettr', 'paquet', 'cet', 'lettr']\n\n\nA ce niveau, les mots commencent √† √™tre moins intelligibles par un humain.\nLa machine prendra le relais, on lui a pr√©par√© le travail\n\n\n Note\nIl existe aussi le stemmer suivant :\nfrom nltk.stem.snowball import FrenchStemmer\nstemmer = FrenchStemmer()"
  },
  {
    "objectID": "content/course/NLP/01_intro/index.html#reconnaissance-des-entit√©s-nomm√©es",
    "href": "content/course/NLP/01_intro/index.html#reconnaissance-des-entit√©s-nomm√©es",
    "title": "34¬† Quelques √©l√©ments pour comprendre les enjeux du NLP",
    "section": "35.3 Reconnaissance des entit√©s nomm√©es",
    "text": "35.3 Reconnaissance des entit√©s nomm√©es\nCette √©tape n‚Äôest pas une √©tape de pr√©paration mais illustre la capacit√©\ndes librairies Python a extraire du sens d‚Äôun texte. La librairie\nspaCy permet de faire de la reconnaissance d‚Äôentit√©s nomm√©es, ce qui peut\n√™tre pratique pour extraire rapidement certains personnages de notre oeuvre.\n\n\nLa librairie spaCy\nNTLK est la librairie historique d‚Äôanalyse textuelle en Python. Elle existe\ndepuis les ann√©es 1990. L‚Äôutilisation industrielle du NLP dans le monde\nde la data-science est n√©anmoins plus r√©cente et doit beaucoup √† la collecte\naccrue de donn√©es non structur√©es par les r√©seaux sociaux. Cela a amen√© √†\nun renouvelement du champ du NLP, tant dans le monde de la recherche que dans\nsa mise en application dans l‚Äôindustrie de la donn√©e.\nLe package spaCy est l‚Äôun des packages qui a permis\ncette industrialisation des m√©thodes de NLP. Con√ßu autour du concept\nde pipelines de donn√©es, il est beaucoup plus pratique √† mettre en oeuvre\npour une cha√Æne de traitement de donn√©es textuelles mettant en oeuvre\nplusieurs √©tapes de transformation des donn√©es.\n\n\n#!pip install deplacy\n#!python -m spacy download fr_core_news_sm\nimport spacy\n\nnlp=spacy.load(\"fr_core_news_sm\")\ndoc = nlp(dumas)\nimport spacy\nfrom spacy import displacy\ndisplacy.render(doc, style=\"ent\", jupyter=True)"
  },
  {
    "objectID": "content/course/NLP/01_intro/index.html#repr√©sentation-dun-texte-sous-forme-vectorielle",
    "href": "content/course/NLP/01_intro/index.html#repr√©sentation-dun-texte-sous-forme-vectorielle",
    "title": "34¬† Quelques √©l√©ments pour comprendre les enjeux du NLP",
    "section": "35.4 Repr√©sentation d‚Äôun texte sous forme vectorielle",
    "text": "35.4 Repr√©sentation d‚Äôun texte sous forme vectorielle\nUne fois nettoy√©, le texte est plus propice √† une repr√©sentation vectorielle.\nEn fait, implicitement, on a depuis le d√©but adopt√© une d√©marche bag of words.\nIl s‚Äôagit d‚Äôune repr√©sentation, sans souci de contexte (ordre des mots, contexte d‚Äôutilisation),\no√π chaque token repr√©sente un √©l√©ment dans un vocabulaire de taille \\(|V|\\).\nOn peut ainsi avoir une repr√©sentation matricielle les occurrences de\nchaque token dans plusieurs documents (par exemple plusieurs livres,\nchapitres, etc.) pour, par exemple, en d√©duire une forme de similarit√©.\nAfin de r√©duire la dimension de la matrice bag of words,\non peut s‚Äôappuyer sur des pond√©rations.\nOn √©limine ainsi certains mots tr√®s fr√©quents ou au contraire tr√®s rares.\nLa pond√©ration la plus simple est bas√©e sur la fr√©quence des mots dans le document.\nC‚Äôest l‚Äôobjet de la m√©trique tf-idf (term frequency - inverse document frequency)\nabord√©e dans un prochain chapitre."
  },
  {
    "objectID": "content/course/NLP/01_intro/index.html#r√©f√©rences",
    "href": "content/course/NLP/01_intro/index.html#r√©f√©rences",
    "title": "34¬† Quelques √©l√©ments pour comprendre les enjeux du NLP",
    "section": "35.5 R√©f√©rences",
    "text": "35.5 R√©f√©rences\n\n\n\n\nGaliana, Lino, and Milena Suarez Castillo. 2022. ‚ÄúFuzzy Matching on Big-Data an Illustration with Scanner Data and Crowd-Sourced Nutritional Data.‚Äù"
  },
  {
    "objectID": "content/course/visualisation/index.html",
    "href": "content/course/visualisation/index.html",
    "title": "35¬† Partie 2: visualiser les donn√©es",
    "section": "",
    "text": "L‚Äô√©cosyst√®me Python pour la valorisation de donn√©es est tr√®s riche. Il est\npossible de consacrer des livres entiers √† celui-ci.\nDans le domaine de la visualisation, le parti pris est d‚Äôexplorer quelques\nlibrairies centrales √† partir d‚Äôun nombre restreint d‚Äôexemples en\nr√©pliquant des graphiques qu‚Äôon peut trouver sur le site d‚Äôopen data de la\nmairie de Paris.\nSeront principalement √©voqu√©s, dans la premi√®re partie :\n\nla repr√©sentation graphique fixe avec les librairies matplotlib et seaborn\nles graphiques r√©actifs avec plotly\nles cartes fixes avec geopandas ou geoplot\nles cartes r√©actives avec folium\n\nDes √©l√©ments suppl√©mentaires pour produire de belles\nvalorisations de donn√©es seront progressivement\najout√©s √† cette partie, notamment observableHQ."
  },
  {
    "objectID": "content/course/visualisation/maps/index.html",
    "href": "content/course/visualisation/maps/index.html",
    "title": "36¬† De belles cartes avec python: mise en pratique",
    "section": "",
    "text": "37 Exercices suppl√©mentaires"
  },
  {
    "objectID": "content/course/visualisation/maps/index.html#premi√®re-carte-avec-lapi-matplotlib-de-geopandas",
    "href": "content/course/visualisation/maps/index.html#premi√®re-carte-avec-lapi-matplotlib-de-geopandas",
    "title": "36¬† De belles cartes avec python: mise en pratique",
    "section": "36.1 Premi√®re carte avec l‚ÄôAPI matplotlib de geopandas",
    "text": "36.1 Premi√®re carte avec l‚ÄôAPI matplotlib de geopandas\n\n\n Exercice 1: Importer les donn√©es\nImporter les donn√©es de compteurs de v√©los en deux temps.\n\nD‚Äôabord, les comptages peuvent √™tre trouv√©s √† l‚Äôadresse https://github.com/linogaliana/python-datascientist/raw/master/data/bike.csv. :warning: Il s‚Äôagit de donn√©es\ncompress√©es au format gzip, il faut donc utiliser l‚Äôoption compression. Nommer cet objet comptages.\nImporter les donn√©es de localisation des compteurs √† partir de l‚Äôurl https://parisdata.opendatasoft.com/explore/dataset/comptage-velo-compteurs/download/?format=geojson&timezone=Europe/Berlin&lang=fr. Nommer cet objet compteurs.\nFaire attention √† deux valeurs aberrantes. Utiliser\nla fonctionalit√© str.contains pour exclure les\nobservations contenant ‚ÄúBike IN‚Äù ou ‚ÄúBike OUT‚Äù\ndans la variable\nnom_compteur\nOn va √©galement utiliser les donn√©es d‚Äôarrondissements de la ville de Paris. Importer ces donn√©es depuis https://opendata.paris.fr/explore/dataset/arrondissements/download/?format=geojson&timezone=Europe/Berlin&lang=fr. Nommer cet objet arrondissements.\nUtiliser la m√©thode plot pour repr√©senter les localisations des compteurs dans l‚Äôespace. C‚Äôest, on peut l‚Äôavouer, peu informatif sans apport ext√©rieur. Il va donc falloir travailler un peu l‚Äôesth√©tique\n\n\n\n\ncompteurs = compteurs.loc[~compteurs[\"nom_compteur\"].str.contains(r\"(Bike IN|Bike OUT)\")]\n\n/tmp/ipykernel_1368/3602001210.py:1: UserWarning:\n\nThis pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n\n\n\n\n\n Warning\nOn serait tent√© de faire un merge de la base compteurs et comptages.\nEn l‚Äôoccurrence, il s‚Äôagirait d‚Äôun produit cart√©sien puisqu‚Äôil s‚Äôagit de faire exploser la base spatiale.\nAvec des donn√©es spatiales, c‚Äôest souvent une tr√®s mauvaise id√©e. Cela duplique les points, cr√©ant des difficult√©s √† repr√©senter les donn√©es mais aussi ralentit les calculs.\nSauf √† utiliser la m√©thode dissolve (qui va agr√©ger k fois la m√™me g√©om√©trie‚Ä¶), les g√©om√©tries sont perdues lorsqu‚Äôon effectue des groupby.\n\n\nMaintenant, tout est pr√™t pour une premi√®re carte. matplotlib fonctionne selon\nle principe des couches. On va de la couche la plus lointaine √† celle le plus\nen surface. L‚Äôexception est lorsqu‚Äôon ajoute un fond de carte contextily via\nctx.add_basemap: on met cet appel en dernier.\n\n\n Exercice 2: Premi√®re carte\nRepr√©senter une carte des compteurs avec le fonds de carte des arrondissements\n\nFaire attention √† avoir des arrondissements dont l‚Äôint√©rieur est transparent (argument √† utiliser: facecolor).\nFaire des bordures d‚Äôarrondissements noires et affichez les compteurs en rouge.\nPour obtenir un graphique plus grand, vous pouvez utiliser l‚Äôargument figsize = (10,10).\nPour les localisations, les points doivent √™tre rouges en √©tant plus transparent au centre (argument √† utiliser: alpha)\n\n\n\nVous devriez obtenir cette carte:\n\n\n\n\n\n\n\n Exercice 3 : Ajouter un fonds de carte avec contextily\nRepartir de la carte pr√©c√©dente.\n\nUtiliser ctx.add_basemap pour ajouter un fonds de carte. Pour ne pas afficher les axes, vous pouvez utiliser ax.set_axis_off().\n\n:warning: Par d√©faut, contextily d√©sire un syst√®me de projection (crs) qui est le Web Mercator (epsg: 3857). Il faut changer la valeur de l‚Äôargument crs.\n:warning: Avec les versions anciennes des packages, il faut utiliser .to_string sur un objet CRS pour qu‚Äôil soit reconnu par contextily. Sur des versions r√©centes, la valeur num√©rique du code EPSG est suffisante.\n\nTrouver un fonds de carte plus esth√©tique, qui permette de visualiser les grands axes, parmi ceux possibles. Pour tester l‚Äôesth√©tique, vous pouvez utiliser cet url. La documentation de r√©f√©rence sur les tuiles disponibles est ici\n\n\n\n\nax.get_figure()\n\n\n\n\n\nax.get_figure()\n\n\n\n\n\nax.get_figure().savefig(\"featured.png\")\n\nLe principe de la heatmap est de construire, √† partir d‚Äôun nuage de point bidimensionnel, une distribution 2D liss√©e. La m√©thode repose sur les estimateurs √† noyaux qui sont des m√©thodes de lissage local.\n\n\n Exercice 3 : Ajouter un fonds de carte avec contextily\nPour le moment, la fonction geoplot.kdeplot n‚Äôincorpore pas toutes les fonctionalit√©s de seaborn.kdeplot. Pour √™tre en mesure de construire une heatmap avec des donn√©es pond√©r√©es (cf.¬†cette issue dans le d√©p√¥t seaborn), il y a une astuce. Il faut simuler k points de valeur 1 autour de la localisation observ√©e. La fonction ci-dessous, qui m‚Äôa √©t√© bien utile, est pratique\n\nimport numpy as np\ndef expand_points(shapefile,\n                  index_var = \"grid_id\",\n                  weight_var = 'prop',\n                  radius_sd = 100,\n                  crs = 2154):\n    \"\"\"\n    Multiply number of points to be able to have a weighted heatmap\n    :param shapefile: Shapefile to consider\n    :param index_var: Variable name to set index\n    :param weight_var: Variable that should be used\n    :param radius_sd: Standard deviation for the radius of the jitter\n    :param crs: Projection system that should be used. Recommended option\n      is Lambert 93 because points will be jitterized using meters\n    :return:\n      A geopandas point object with as many points by index as weight\n    \"\"\"\n\n    shpcopy = shapefile\n    shpcopy = shpcopy.set_index(index_var)\n    shpcopy['npoints'] = np.ceil(shpcopy[weight_var])\n    shpcopy['geometry'] = shpcopy['geometry'].centroid\n    shpcopy['x'] = shpcopy.geometry.x\n    shpcopy['y'] = shpcopy.geometry.y\n    shpcopy = shpcopy.to_crs(crs)\n    shpcopy = shpcopy.loc[np.repeat(shpcopy.index.values, shpcopy.npoints)]\n    shpcopy['x'] = shpcopy['x'] + np.random.normal(0, radius_sd, shpcopy.shape[0])\n    shpcopy['y'] = shpcopy['y'] + np.random.normal(0, radius_sd, shpcopy.shape[0])\n\n    gdf = gpd.GeoDataFrame(\n        shpcopy,\n        geometry = gpd.points_from_xy(shpcopy.x, shpcopy.y),\n        crs = crs)\n\n    return gdf\n\n\n\n\n\n Exercice 4 : Data cleaning avant de pouvoir faire une heatmap\n\nCalculer le trafic moyen, pour chaque station, entre 7 heures et 10 heures (bornes incluses) et nommer cet objet df1. Faire la m√™me chose, en nommant df2, pour le trafic entre 17 et 20 heures (bornes incluses)\nNous allons d√©sormais pr√©parer les donn√©es de mani√®re √† faire une heatmap. Apr√®s avoir compris ce que permet de faire la fonction expand_points ci-dessus, cr√©er une fonction explode_data qui suive les √©tapes suivantes.\n\n\nConvertir un DataFrame dans le syst√®me de projection Lambert 93 (epsg: 2154)\nAppliquer expand_points aux noms de variable ad√©quats. Vous pouvez fixer la valeur de radius_sd √† 100.\nReconvertir l‚Äôoutput au format WGS84 (epsg: 4326)\n\n\nAppliquer cette fonction √† df1 et df2\n\n\n\n\n\n Exercice 5 : Heatmap, enfin !\nRepr√©senter, pour ces deux moments de la journ√©e, la heatmap du trafic de v√©lo avec geoplot.kdeplot. Pour cela :\n\nAppliquer la fonction geoplot.kdeplot avec comme consignes :\n\nd‚Äôutiliser les arguments shade=True et shade_lowest=True pour colorer l‚Äôint√©rieur des courbes de niveaux obtenues ;\nd‚Äôutiliser une palette de couleur rouge avec une transparence mod√©r√©e (alpha = 0.6)\nd‚Äôutiliser l‚Äôargument clip pour ne pas d√©border hors de Paris (en cas de doute, se r√©f√©rer √† l‚Äôaide de geoplot.kdeplot)\nL‚Äôargument bw (pour bandwidth) d√©termine le plus ou moins fort lissage spatial. Vous pouvez partir d‚Äôun bandwidth √©gal √† 0.01 et le faire varier pour voir l‚Äôeffet sur le r√©sultat\n\nNe pas oublier d‚Äôajouter les arrondissements. Avec geoplot, il faut utiliser geoplot.polyplot.\n\n\n\n\nax.get_figure()"
  },
  {
    "objectID": "content/course/visualisation/maps/index.html#des-cartes-r√©actives-gr√¢ce-√†-folium",
    "href": "content/course/visualisation/maps/index.html#des-cartes-r√©actives-gr√¢ce-√†-folium",
    "title": "36¬† De belles cartes avec python: mise en pratique",
    "section": "36.2 Des cartes r√©actives gr√¢ce √† folium",
    "text": "36.2 Des cartes r√©actives gr√¢ce √† folium\nDe plus en plus de donn√©es de visualisation reposent sur la cartographie r√©active. Que ce soit dans l‚Äôexploration des donn√©es ou dans la repr√©sentation finale de r√©sultats, la cartographie r√©active est tr√®s appr√©ciable.\nfolium offre une interface tr√®s flexible et tr√®s facile √† prendre √† main. Les cartes sont construites gr√¢ce √† la librairie JavaScript Leaflet.js mais, sauf si on d√©sire aller loin dans la customisation du r√©sultat, il n‚Äôest pas n√©cessaire d‚Äôavoir des notions dans le domaine.\nUn objet folium se construit par couche. La premi√®re est l‚Äôinitialisation de la carte. Les couches suivantes sont les √©l√©ments √† mettre en valeur. L‚Äôinitialisation de la carte n√©cessite la d√©finition d‚Äôun point central (param√®tre location) et d‚Äôun zoom de d√©part (zoom_start). Plut√¥t que de fournir manuellement le point central et le zoom on peut :\n\nD√©terminer le point central en construisant des colonnes longitudes et latitudes et en prenant la moyenne de celles-ci ;\nUtiliser la m√©thode fit_bounds qui cale la carte sur les coins sud-ouest et nord-est. En supposant que la carte s‚Äôappelle m, on fera m.fit_bounds([sw, ne])\n\nLe bout de code suivant permet de calculer le centre de la carte\n\ncompteurs['lon'] = compteurs.geometry.x\ncompteurs['lat'] = compteurs.geometry.y\ncenter = compteurs[['lat', 'lon']].mean().values.tolist()\nprint(center)\n\n[48.8546401015625, 2.349260234375]\n\n\nAlors que le code suivant permet de calculer les coins:\n\nsw = compteurs[['lat', 'lon']].min().values.tolist()\nne = compteurs[['lat', 'lon']].max().values.tolist()\nprint(sw, ne)\n\n[48.81964, 2.26526] [48.898946, 2.41143]\n\n\n\n\n Hint\nSi un fond gris s‚Äôaffiche, c‚Äôest qu‚Äôil y a un probl√®me de localisation ou d‚Äôacc√®s √† internet. Pour le premier cas, cela provient g√©n√©ralement d‚Äôun probl√®me de projection ou d‚Äôune inversion des longitudes et latitudes.\nLes longitudes repr√©sentent les x (axe ouest-est) et les latitudes y (axe sud-nord). De mani√®re contrintuitive, folium attend qu‚Äôon lui fournisse les donn√©es sous la forme [latitude, longitude] donc [y,x]\n\n\n\n\n Exercice 6 : Visualiser la localisation des stations\n\nCalculer le centre centerde la carte des donn√©es compteurs. Il s‚Äôobtient en agr√®geant l‚Äôensemble des g√©om√©tries, calculant le centroid et r√©cup√®rant la valeur sous forme de liste. Avec une logique similaire, calculez les bornes du sud-ouest sw et du nord-est ne de la carte.\nRepr√©senter la localisation des stations en utilisant un zoom optimal.\n\n\n\n\n# Afficher la carte\nm\n\n\n\n Exercice 7: Repr√©senter les stations\nFaire la m√™me carte, avec des ronds proportionnels au nombre de comptages :\n\nPour le rayon de chaque cercle, vous pouvez appliquer la r√®gle 500*x/max(x) (r√®gle au doigt mouill√©)\nVous pouvez r√©duire la taille des bordures de cercle avec l‚Äôoption weight = 1 et fixer la couleur avec color = 'grey'\n(Optionnel) Colorer en rouge les 10 plus grosses stations. L‚Äôopacit√© √©tant, par d√©faut, un peu faible, le param√®tre fill_opacity = 0.4 am√©liore le rendu.\n(Optionnel) Afficher, en suppl√©ment du nom du compteur lorsqu‚Äôon clique, la valeur du comptage en revenant √† la ligne\n\n\n\nLa carte obtenue doit ressembler √† la suivante:\n\n# Afficher la carte\nm"
  },
  {
    "objectID": "content/course/visualisation/maps/index.html#densit√©-de-population-dans-la-petite-couronne-parisienne",
    "href": "content/course/visualisation/maps/index.html#densit√©-de-population-dans-la-petite-couronne-parisienne",
    "title": "36¬† De belles cartes avec python: mise en pratique",
    "section": "37.1 Densit√© de population dans la petite couronne parisienne",
    "text": "37.1 Densit√© de population dans la petite couronne parisienne\nPour cet exercice, le package cartiflette\nva √™tre pratique pour r√©cup√©rer un fonds de carte m√©langeant arrondissements\nparisiens et communes dans les autres villes.\nNous allons privil√©gier une carte √† ronds proportionnels (bubble map)\naux cartes chorol√®pthes qui trompent\nl‚Äôoeil. Les instructions d‚Äôinstallation du package topojson\nsont disponibles dans la partie manipulation\n\n\n Exercice: bubble map de densit√© des populations\n\nR√©cup√©rer le fond de carte des d√©partements 75, 92, 93 et 94\navec cartiflette. Pour cela, utiliser download_vectorfile_url_all\ndepuis cartiflette.s3 en fixant l‚Äôoption level √† COMMUNE_ARRONDISSEMENT.\nNommer cet objet df.\nAfin que les calculs ult√©rieurs de surface ne soient pas fauss√©s,\nassurez-vous que les donn√©es sont en Lambert 93 en reprojetant\nnos contours (code EPSG: 2154).\nCr√©er un objet departements avec dissolve pour √©galement disposer\nd‚Äôun fond de carte des d√©partements\nCr√©er une variable surface et utilisant la m√©thode area. L‚Äôunit√©\ndoit √™tre le km¬≤, il faut donc diviser par \\(10^6\\)\nCr√©er une variable densite\nUtiliser pd.cut avec les seuils 5000, 15000 et 30000 personnes\npar km¬≤. Vous pouvez utiliser l‚Äôoption label pour d√©nommer les tranches\nCr√©er un GeoDataFrame de points en utilisant la m√©thode centroid. Celui-ci\nnous servira √† localiser le centre de nos ronds.\nRepr√©senter la densit√© communale sous forme de carte avec ronds proportionnels.\nVous pouvez utiliser la variable cr√©√©e √† la question 5 pour les couleurs.\n\n\n\nLa carte obtenue devrait ressembler √† celle-ci:\n\n\nText(0.3, 0.15, 'Source: IGN - AdminExpress')"
  },
  {
    "objectID": "content/course/visualisation/matplotlib/index.html",
    "href": "content/course/visualisation/matplotlib/index.html",
    "title": "37¬† De beaux graphiques avec python: mise en pratique",
    "section": "",
    "text": "38 Exercices suppl√©mentaires\nPour ces exercices, il est recommand√© de s‚Äôinspirer\ndes mod√®les pr√©sents dans la librairie\nde graphiques Python pr√©sent√©e\ndans https://www.python-graph-gallery.com/\nhttps://plotly.com/python/v3/3d-network-graph/"
  },
  {
    "objectID": "content/course/visualisation/matplotlib/index.html#premier-graphique-avec-lapi-matplotlib-de-pandas",
    "href": "content/course/visualisation/matplotlib/index.html#premier-graphique-avec-lapi-matplotlib-de-pandas",
    "title": "37¬† De beaux graphiques avec python: mise en pratique",
    "section": "37.1 Premier graphique avec l‚ÄôAPI matplotlib de pandas",
    "text": "37.1 Premier graphique avec l‚ÄôAPI matplotlib de pandas\n{{% box status=‚Äúexercise‚Äù title=‚ÄúExercice‚Äù icon=‚Äúfas fa-pencil-alt‚Äù %}}\nExercice 1 : Importer les donn√©es et produire un premier graphique\n\nImporter les donn√©es de compteurs de v√©los. Vous pouvez utiliser l‚Äôurl https://github.com/linogaliana/python-datascientist/raw/master/data/bike.csv. :warning: Il s‚Äôagit de donn√©es\ncompress√©es au format gzip, il faut donc utiliser l‚Äôoption compression = 'gzip'\nEn premier lieu, sans se pr√©occuper des √©l√©ments de style ni des labels des\ngraphiques, reproduire les deux premiers graphiques de la\npage d‚Äôanalyse des donn√©es:\nLes 10 compteurs avec la moyenne horaire la plus √©lev√©e et Les 10 compteurs ayant comptabilis√©s le plus de v√©los. Les valeurs chiffr√©es des graphiques seront diff√©rentes de celles de la page en ligne, c‚Äôest normal, nous travaillons sur des donn√©es plus anciennes.\n\n\n\n\n\n\n\n\n\n\n\n{{% /box %}}\n{{% box status=‚Äúnote‚Äù title=‚ÄúConseil‚Äù icon=‚Äúfa fa-comment‚Äù %}}\nPour obtenir un graphique ordonn√© du plus grand au plus petit, il faut avoir les donn√©es ordonn√©es du plus petit au\nplus grand. C‚Äôest bizarre mais c‚Äôest comme √ßa‚Ä¶\n{{% /box %}}\nOn peut remarquer plusieurs √©l√©ments probl√©matiques (par exemple les labels) mais\naussi des √©l√©ments ne correspondant pas (les titres des axes, etc.) ou\nmanquants (le nom du graphique‚Ä¶)\nComme les graphiques produits par pandas suivent la logique tr√®s flexible\nde matplotlib, il est possible de les customiser. Cependant, c‚Äôest\nsouvent beaucoup de travail et il peut √™tre pr√©f√©rable de directement\nutiliser seaborn, qui offre quelques arguments pr√™ts √† l‚Äôemploi."
  },
  {
    "objectID": "content/course/visualisation/matplotlib/index.html#utiliser-directement-seaborn",
    "href": "content/course/visualisation/matplotlib/index.html#utiliser-directement-seaborn",
    "title": "37¬† De beaux graphiques avec python: mise en pratique",
    "section": "37.2 Utiliser directement seaborn",
    "text": "37.2 Utiliser directement seaborn\nVous pouvez repartir des deux dataframes pr√©c√©dents. On va suppose qu‚Äôils se\nnomment df1 et df2.\n{{% box status=‚Äúexercise‚Äù title=‚ÄúExercice‚Äù icon=‚Äúfas fa-pencil-alt‚Äù %}}\nExercice 2 : Un peu de style !\nIl y a plusieurs mani√®res de faire un bar plot en seaborn. La plus flexible,\nc‚Äôest-√†-dire celle qui permet le mieux d‚Äôinteragir avec matplotlib est\ncatplot\n\nR√©initialiser l‚Äôindex des df pour avoir une colonne ‚ÄòNom du compteur‚Äô\nRefaire le graphique pr√©c√©dent avec la fonction catplot de seaborn. Pour\ncontr√¥ler la taille du graphique vous pouvez utiliser les arguments height et\naspect.\nAjouter les titres des axes et le titre du graphique pour le premier graphique\n\n\n\n\n\n\n\nRefaites l‚Äôexercice avec la fonction sns.barplot.\n\n\n\n\n\n\n\ng.figure.get_figure().savefig('featured.png')\n\n\nEssayez de colorer en rouge l‚Äôaxe des x. Vous pouvez pr√©-d√©finir un\nstyle avec sns.set_style(\"ticks\", {\"xtick.color\": \"red\"})\n\n\n\n\n\n\n{{% /box %}}\n{{% box status=‚Äúexercise‚Äù title=‚ÄúExercice‚Äù icon=‚Äúfas fa-pencil-alt‚Äù %}}\nExercice 3 : Refaire les graphiques\n\nRefaire le graphique Les 10 compteurs ayant comptabilis√© le plus de v√©los\n\n\n\n\n\n\n\nLes graphiques qui suivent vont n√©cessiter un peu d‚Äôagilit√© dans la gestion des dates. Il faut en effet commencer par cr√©er une variable temporelle (vous pouvez la nommer\ntimestamp) et la transformer en variable mensuelle (gr√¢ce √†\ndt.to_period('M')) et l‚Äôappeler month. Vous pouvez essayer de le faire vous m√™me ou cliquer\nci-dessous pour la solution.\n\n\n\nSolution\n\n\ndf['timestamp'] = pd.to_datetime(df['Date et heure de comptage'], format='%Y-%m-%dT%H:%M:%SZ', errors='coerce')\ndf['month'] = df['timestamp'].dt.to_period('M')\n\n\n\nRefaire le graphique Moyenne mensuelle des comptages v√©los.\n\n\n\n\n\n\n\nRefaire le graphique Moyenne journali√®re des comptages v√©los (cr√©er d‚Äôabord une variable de jour avec .dt.day)\n\n\n\n\n\n\n\nRefaire le graphique Comptages v√©lo au cours des 7 derniers jours (de l‚Äô√©chantillon)\n\n\n\n\n\n\n{{% /box %}}"
  },
  {
    "objectID": "content/course/visualisation/matplotlib/index.html#des-graphiques-dynamiques-avec-plotly",
    "href": "content/course/visualisation/matplotlib/index.html#des-graphiques-dynamiques-avec-plotly",
    "title": "37¬† De beaux graphiques avec python: mise en pratique",
    "section": "37.3 Des graphiques dynamiques avec Plotly",
    "text": "37.3 Des graphiques dynamiques avec Plotly\nLe package Plotly est une surcouche √† la librairie Javascript\nPlotly.js qui permet de cr√©er et manipuler des objets graphiques de mani√®re\ntr√®s flexible afin de produire des objets r√©actifs sans avoir √† recourir\n√† Javascript.\nLe point d‚Äôentr√©e recommand√© est le module plotly.express\n(documentation ici) qui offre une arborescence\nriche mais n√©anmoins intuitive pour construire des graphiques\n(objets plotly.graph_objects.Figure) pouvant √™tre modifi√©s a posteriori\nsi besoin (par exemple pour customiser les axes).\n\n37.3.1 Comment visualiser un graphique plotly ?\nDans un notebook Jupyter classique, les lignes suivantes de code permettent\nd‚Äôafficher le r√©sultat d‚Äôune commande Plotly sous un bloc de code:\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected = True)\nPour JupyterLab, l‚Äôextension jupyterlab-plotly s‚Äôav√®re n√©cessaire:\njupyter labextension install jupyterlab-plotly\nPour les utilisateurs de python via l‚Äôexcellent package R reticulate, il\nest possible d‚Äô√©crire le r√©sultats dans un fichier .html et d‚Äôutiliser\nhtmltools::includeHTML pour l‚Äôafficher via R Markdown (les utilisateurs\nde R trouveront bien-s√ªr une technique bien plus simple: utiliser\ndirectement le package R plotly‚Ä¶)\n\n\n37.3.2 R√©plication de l‚Äôexemple pr√©c√©dent avec plotly\nLes modules suivants seront n√©cessaires pour construire des graphiques\navec plotly:\n\nimport plotly\nimport plotly.express as px\nfrom IPython.display import HTML #pour afficher les graphs\n# dans une cellule de notebook\n\n{{% box status=‚Äúexercise‚Äù title=‚ÄúExercice‚Äù icon=‚Äúfas fa-pencil-alt‚Äù %}}\nExercice 4 : Premier graphique avec plotly\nL‚Äôobjectif est de reconstuire le premier diagramme en barre rouge avec plotly.\n\nR√©alisez le graphique en utilisant la fonction ad√©quate avec plotly.express et‚Ä¶\n\n\nNe pas prendre le\nth√®me par d√©faut mais un √† fond blanc, pour avoir un r√©sultat ressemblant\n√† celui propos√© sur le site de l‚Äôopen-data.\nPour la couleur rouge,\nvous pouvez utiliser l‚Äôargument color_discrete_sequence.\nNe pas oublier de nommer les axes\nPensez √† la couleur du texte de l‚Äôaxe inf√©rieur\n\n\nTester un autre th√®me, √† fond sombre. Pour les couleurs, faire un\ngroupe stockant les trois plus fortes valeurs puis les autres.\n\n{{% /box %}}\nLa premi√®re question permet de construire le graphique suivant:\nAlors qu‚Äôavec le th√®me sombre (question 2), on obtient :"
  },
  {
    "objectID": "content/course/visualisation/matplotlib/index.html#les-lollipop-chart",
    "href": "content/course/visualisation/matplotlib/index.html#les-lollipop-chart",
    "title": "37¬† De beaux graphiques avec python: mise en pratique",
    "section": "38.1 Les lollipop chart",
    "text": "38.1 Les lollipop chart\nCet exercice permet de s‚Äôentra√Æner\nsur le fichier des naissances et des\nd√©c√®s de l‚ÄôInsee. Il s‚Äôinspire d‚Äôune\nexcellente visualisation faite\npar Jean Dupin\nsur Twitter mettant en avant l‚Äô√©volution,\nann√©e par ann√©e, des d√©comptes des\npersonnes nomm√©es ‚ÄúJean‚Äù parmi les\npersonnes n√©es ou d√©c√©d√©es:\nL‚Äôanimation de Jean Dupin\nest beaucoup plus raffin√©e que\ncelle que nous allons mettre en\noeuvre.\n\n38.1.1 R√©cup√©ration des donn√©es\nLa r√©cup√©ration des donn√©es √©tant un peu complexe,\nle code est donn√© pour vous permettre de vous\nconcentrer sur l‚Äôessentiel (si vous\nvoulez vous exercer avec le package requests,\nessayez de le faire vous-m√™me).\nLes donn√©es des d√©c√®s sont disponibles de mani√®re\nhistorique dans des zip pour chaque ann√©e.\n\nimport shutil\nimport requests\nimport zipfile\nimport os\nimport glob\nimport pandas as pd\n\ndef import_by_decade(decennie = 1970):\n\n    url = f\"https://www.insee.fr/fr/statistiques/fichier/4769950/deces-{decennie}-{decennie+9}-csv.zip\"\n\n    req = requests.get(url)\n\n    with open(f\"deces_{decennie}.zip\",'wb') as f:\n        f.write(req.content)\n\n    with zipfile.ZipFile(f\"deces_{decennie}.zip\", 'r') as zip_ref:\n        zip_ref.extractall(f\"deces_{decennie}\")\n\n    csv_files = glob.glob(os.path.join(f\"deces_{decennie}\", \"*.csv\"))\n\n    df = [pd.read_csv(f, sep = \";\", encoding=\"utf-8\").assign(annee = f) for f in csv_files]\n    df = pd.concat(df)\n    df[['nom','prenom']] = df['nomprenom'].str.split(\"*\", expand=True)\n    df['prenom'] = df['prenom'].str.replace(\"/\",\"\")\n    df['annee'] = df['annee'].str.rsplit(\"/\").str[-1].str.replace(\"(Deces_|.csv|deces-)\",\"\").astype(int)\n\n    shutil.rmtree(f\"deces_{decennie}\")    \n    os.remove(f\"deces_{decennie}.zip\")\n\n    return df\n\n\ndfs = [import_by_decade(d) for d in [1970, 1980, 1990, 2000, 2010]]\ndeces = pd.concat(dfs)\n\nLe fichier des naissances est plus simple √† r√©cup√©rer.\nVoici le code pour l‚Äôobtenir:\n\nyear = 2021\nurl_naissance = f\"https://www.insee.fr/fr/statistiques/fichier/2540004/nat{year}_csv.zip\"\n\nreq = requests.get(url_naissance)\n\nwith open(f\"naissance_{year}.zip\",'wb') as f:\n    f.write(req.content)\n\nwith zipfile.ZipFile(f\"naissance_{year}.zip\", 'r') as zip_ref:\n    zip_ref.extractall(f\"naissance_{year}\")\n\nnaissance = pd.read_csv(f\"naissance_{year}/nat{year}.csv\", sep = \";\")\nnaissance = naissance.dropna(subset = ['preusuel'] )\n\nOn peut enfin restructurer les DataFrames pour obtenir un\nseul jeu de donn√©es, en se restreignant aux ‚ÄúJEAN‚Äù:\n\njean_naiss = naissance.loc[naissance['preusuel'] == \"JEAN\"].loc[:, ['annais', 'nombre']]\njean_naiss = jean_naiss.rename({\"annais\": \"annee\"}, axis = \"columns\")\njean_naiss = jean_naiss.groupby('annee').sum().reset_index()\njean_deces = deces.loc[deces[\"prenom\"] == \"JEAN\"]\njean_deces = jean_deces.groupby('annee').size().reset_index()\njean_deces.columns = ['annee', \"nombre\"]\njean_naiss.columns = ['annee', \"nombre\"]\ndf = pd.concat(\n    [\n        jean_deces.assign(source = \"deces\"),\n        jean_naiss.assign(source = \"naissance\")\n    ])\ndf = df.loc[df['annee'] != \"XXXX\"]\ndf['annee']=df['annee'].astype(int)\ndf = df.loc[df['annee'] &gt; 1971]\n\ndf.head(3)\n\n\n\n\n\n\n\n\n\n\nannee\nnombre\nsource\n\n\n\n\n0\n1972\n3017\ndeces\n\n\n1\n1973\n3116\ndeces\n\n\n2\n1974\n3298\ndeces"
  },
  {
    "objectID": "content/course/visualisation/matplotlib/index.html#repr√©sentation-graphique",
    "href": "content/course/visualisation/matplotlib/index.html#repr√©sentation-graphique",
    "title": "37¬† De beaux graphiques avec python: mise en pratique",
    "section": "38.2 Repr√©sentation graphique",
    "text": "38.2 Repr√©sentation graphique\nVous pouvez vous aider du mod√®le pr√©sent\ndans https://www.python-graph-gallery.com\n{{% box status=‚Äúexercise‚Äù title=‚ÄúExercice‚Äù\nicon=‚Äúfas fa-pencil-alt‚Äù %}}\nPour commencer, on va se concentrer sur la\nproduction d‚Äôun seul graphique\n(d√©c√®s ou naissance, vous choisissez)\n\nCr√©er un objet df_plot qui se restreint √† une\nsource\nFixer une ann√©e sous le nom max_year (par exemple\nvotre ann√©e de naissance). Elle servira ensuite de param√®tre\n√† une fonction\nRestreindre df_plot aux ann√©es ant√©rieures √† max_year\nCr√©er une variable my_range fixant la s√©quence des ann√©es\nentre la plus petite ann√©e du dataset et max_year (inclus)\nCr√©er un array numpy qui vaut orange lorsque l‚Äôobservation\nen question est max_year et skyblue sinon\nUtiliser les fonctions ad√©quates de matplotlib pour cr√©er\nle lollipop chart\n{{% /box %}}\n\nA ce stade, vous devriez avoir une version fonctionnelle\nqui peut servir de\nbase √† la g√©n√©ralisation.\n{{% box status=‚Äúexercise‚Äù title=‚ÄúExercice‚Äù\nicon=‚Äúfas fa-pencil-alt‚Äù %}}\n\nA partir du code pr√©c√©dent, g√©n√©raliser en utilisant\nune boucle for √† partir du r√©sultat de\nenumerate(df.source.value_counts().index.values) pour\ncr√©er un graphique pour une ann√©e donn√©e de maxyear.\nAvant cette boucle, ne pas oublier de cr√©er un objet\nmatplotlib vide √† remplir dans la boucle\n\nfig, axes = plt.subplots(1, 2, sharey = True)\n\nEncapsuler ce code dans une fonction qui\nprend en argument un DataFrame et une\nann√©e max_year\n\nVoici un exemple d‚Äôoutput pour max_year = 2010:\n\n\n\n\n\n\nPour cr√©er une animation, on propose\nd‚Äôutiliser la solution pr√©sent√©e\ndans https://www.python-graph-gallery.com/animation/.\net qui n√©cessite le logiciel imagemagick.\nSauvegarder chaque it√©ration dans un fichier\ndont le nom a la structure figure_{year}.png.\n\nEnfin, pour animer les images, on peut utiliser\nla librairie imageio:\n\nimport glob\nimport imageio.v2 as imageio\n#os.system(\"convert -delay 15 figure_*.png animation.gif\")\n\nfilenames=glob.glob(\"figure_*.png\")\nfilenames.sort()\n\nwith imageio.get_writer('animation.gif', mode='I') as writer:\n    for filename in filenames:\n        image = imageio.imread(filename)\n        writer.append_data(image)\n\nL‚Äôanimation obtenue est la suivante:\n\n\n\nAnimation\n\n\n{{% /box %}}"
  },
  {
    "objectID": "content/course/manipulation/index.html#structure-de-la-partie",
    "href": "content/course/manipulation/index.html#structure-de-la-partie",
    "title": "38¬† Partie 1: manipuler des donn√©es",
    "section": "38.1 Structure de la partie",
    "text": "38.1 Structure de la partie\nCette partie du cours est une introduction\ng√©n√©rale √† l‚Äô√©cosyst√®me tr√®s riche de\nla manipulation de donn√©es avec Python.\nCes chapitres √©voquent aussi bien la r√©cup√©ration de donn√©es\nque la restructuration et la production d‚Äôanalyse\n√† partir de celles-ci. Les chapitres sont les suivants:"
  },
  {
    "objectID": "content/course/manipulation/index.html#exercices",
    "href": "content/course/manipulation/index.html#exercices",
    "title": "38¬† Partie 1: manipuler des donn√©es",
    "section": "38.2 Exercices",
    "text": "38.2 Exercices\nLes notebooks d‚Äôexercices sont list√©s ici. Il est\npossible de les consulter sur ce site ou d‚Äôutiliser l‚Äôun des\nbadges pr√©sents en d√©but de chapitre, par exemple\nceux-ci pour ouvrir le TP pandas:"
  },
  {
    "objectID": "content/course/manipulation/index.html#pour-aller-plus-loin",
    "href": "content/course/manipulation/index.html#pour-aller-plus-loin",
    "title": "38¬† Partie 1: manipuler des donn√©es",
    "section": "38.3 Pour aller plus loin",
    "text": "38.3 Pour aller plus loin\nCe cours n‚Äôaborde pas encore les questions de volum√©trie ou de vitesse de\ncalcul. pandas peut montrer ses limites dans ce domaine.\nIl est ainsi int√©ressant de porter attention √†:\n\nLe livre Modern Pandas\npour obtenir des √©l√©ments suppl√©mentaires sur la question de la performance\navec pandas\nLa question des\nobjets sparse\nLe package dask pour acc√©l√©rer les calculs\npySpark pour des donn√©es tr√®s volumineuses"
  },
  {
    "objectID": "content/course/manipulation/01_numpy/index.html#le-concept-darray",
    "href": "content/course/manipulation/01_numpy/index.html#le-concept-darray",
    "title": "39¬† Numpy, la brique de base de la data science",
    "section": "39.1 Le concept d‚Äôarray",
    "text": "39.1 Le concept d‚Äôarray\nLe concept central de NumPy (Numerical Python) est\nl‚Äôarray qui est un tableau de donn√©es multidimensionnel.\nL‚Äôarray numpy peut √™tre unidimensionnel et s‚Äôapparenter √† un\nvecteur (1d-array),\nbidimensionnel et ainsi s‚Äôapparenter √† une matrice (2d-array) ou,\nde mani√®re plus g√©n√©rale,\nprendre la forme d‚Äôun objet\nmultidimensionnel (Nd-array).\nLes tableaux simples (uni ou bi-dimensionnels) sont faciles √† se repr√©senter et seront particuli√®rement\nutilis√©s dans le paradigme des DataFrames mais\nla possibilit√© d‚Äôavoir des objets multidimensionnels permettra d‚Äôexploiter des\nstructures tr√®s complexes.\nUn DataFrame sera construit √† partir d‚Äôune collection\nd‚Äôarray uni-dimensionnels (les variables de la table), ce qui permettra d‚Äôeffectuer des op√©rations coh√©rentes\n(et optimis√©es) avec le type de la variable.\nPar rapport √† une liste,\n\nun array ne peut contenir qu‚Äôun type de donn√©es (integer, string, etc.),\ncontrairement √† une liste.\nles op√©rations impl√©ment√©es par numpy seront plus efficaces et demanderont moins\nde m√©moire\n\nLes donn√©es g√©ographiques constitueront une construction un peu plus complexe qu‚Äôun DataFrame traditionnel.\nLa dimension g√©ographique prend la forme d‚Äôun tableau plus profond, au moins bidimensionnel\n(coordonn√©es d‚Äôun point)."
  },
  {
    "objectID": "content/course/manipulation/01_numpy/index.html#cr√©er-un-array",
    "href": "content/course/manipulation/01_numpy/index.html#cr√©er-un-array",
    "title": "39¬† Numpy, la brique de base de la data science",
    "section": "39.2 Cr√©er un array",
    "text": "39.2 Cr√©er un array\nOn peut cr√©er un array de plusieurs mani√®res. Pour cr√©er un array √† partir d‚Äôune liste,\nil suffit d‚Äôutiliser la m√©thode array:\n\nnp.array([1,2,5])\n\narray([1, 2, 5])\n\n\nIl est possible d‚Äôajouter un argument dtype pour contraindre le type du array:\n\nnp.array([[\"a\",\"z\",\"e\"],[\"r\",\"t\"],[\"y\"]], dtype=\"object\")\n\narray([list(['a', 'z', 'e']), list(['r', 't']), list(['y'])], dtype=object)\n\n\nIl existe aussi des m√©thodes pratiques pour cr√©er des array:\n\ns√©quences logiques : np.arange (suite) ou np.linspace (interpolation lin√©aire entre deux bornes)\ns√©quences ordonn√©es: array rempli de z√©ros, de 1 ou d‚Äôun nombre d√©sir√© : np.zeros, np.ones ou np.full\ns√©quences al√©atoires: fonctions de g√©n√©ration de nombres al√©atoires: np.rand.uniform, np.rand.normal, etc.\ntableau sous forme de matrice identit√©: np.eye\n\n\nnp.arange(0,10)\nnp.arange(0,10,3)\nnp.linspace(0, 1, 5)\nnp.zeros(10, dtype=int)\nnp.ones((3, 5), dtype=float)\nnp.full((3, 5), 3.14)\nnp.eye(3)\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\n\n\n Exercice 1\nG√©n√©rer:\n\n\\(X\\) une variable al√©atoire, 1000 r√©p√©titions d‚Äôune loi \\(U(0,1)\\)\n\\(Y\\) une variable al√©atoire, 1000 r√©p√©titions d‚Äôune loi normale de moyenne nulle et de variance √©gale √† 2\nV√©rifier la variance de \\(Y\\) avec np.var"
  },
  {
    "objectID": "content/course/manipulation/01_numpy/index.html#indexation-et-slicing",
    "href": "content/course/manipulation/01_numpy/index.html#indexation-et-slicing",
    "title": "39¬† Numpy, la brique de base de la data science",
    "section": "39.3 Indexation et slicing",
    "text": "39.3 Indexation et slicing\n\n39.3.1 Logique dans le cas d‚Äôun array unidimensionnel\nLa structure la plus simple est l‚Äôarray unidimensionnel:\n\nx = np.arange(10)\nprint(x)\n\n[0 1 2 3 4 5 6 7 8 9]\n\n\nL‚Äôindexation est dans ce cas similaire √† celle d‚Äôune liste:\n\nle premier √©l√©ment est 0\nle √©ni√®me √©l√©ment est accessible √† la position \\(n-1\\)\n\nLa logique d‚Äôacc√®s aux √©l√©ments est ainsi la suivante:\nx[start:stop:step]\nAvec un array unidimensionnel, l‚Äôop√©ration de slicing (garder une coupe du array) est tr√®s simple.\nPar exemple, pour garder les K premiers √©l√©ments d‚Äôun array, on fera:\nx[:(K-1)]\nEn l‚Äôoccurrence, on s√©lectionne le K\\(^{eme}\\) √©l√©ment en utilisant\nx[K-1]\nPour s√©lectionner uniquement un √©l√©ment, on fera ainsi:\n\nx = np.arange(10)\nx[2]\n\n2\n\n\nLes syntaxes qui permettent de s√©lectionner des indices particuliers d‚Äôune liste fonctionnent √©galement\navec les arrays.\n\n\n Exercice 2\n\nS√©lectionner les √©l√©ments 0,3,5\nS√©lectionner les √©l√©ments pairs\nS√©lectionner tous les √©l√©ments sauf le premier\nS√©lectionner les 5 premiers √©l√©ments\n\n\n\n\n\n39.3.2 Sur la performance\nUn √©l√©ment d√©terminant dans la performance de numpy par rapport aux listes,\nlorsqu‚Äôil est question de\nslicing est qu‚Äôun array ne renvoie pas une\ncopie de l‚Äô√©l√©ment en question (copie qui co√ªte de la m√©moire et du temps)\nmais simplement une vue de celui-ci.\nLorsqu‚Äôil est n√©cessaire d‚Äôeffectuer une copie,\npar exemple pour ne pas alt√©rer l‚Äôarray sous-jacent, on peut\nutiliser la m√©thode copy:\nx_sub_copy = x[:2, :2].copy()\n\n\n\n39.3.3 Filtres logiques\nIl est √©galement possible, et plus pratique, de s√©lectionner des donn√©es √† partir de conditions logiques\n(op√©ration qu‚Äôon appelle un boolean mask).\nCette fonctionalit√© servira principalement √†\neffectuer des op√©rations de filtre sur les donn√©es.\nPour des op√©rations de comparaison simples, les comparateurs logiques peuvent √™tre suffisants.\nCes comparaisons fonctionnent aussi sur les tableaux multidimensionnels gr√¢ce au\nbroadcasting sur lequel nous reviendrons :\n\nx = np.arange(10)\nx2 = np.array([[-1,1,-2],[-3,2,0]])\nprint(x)\nprint(x2)\n\n[0 1 2 3 4 5 6 7 8 9]\n[[-1  1 -2]\n [-3  2  0]]\n\n\n\nx==2\nx2&lt;0\n\narray([[ True, False,  True],\n       [ True, False, False]])\n\n\nPour s√©lectionner les observations relatives √† la condition logique,\nil suffit d‚Äôutiliser la logique de slicing de numpy qui fonctionne avec les conditions logiques\n\n\n Exercice 3\nSoit\nx = np.random.normal(size=10000)\n\nNe conserver que les valeurs dont la valeur absolue est sup√©rieure √† 1.96\nCompter le nombre de valeurs sup√©rieures √† 1.96 en valeur absolue et leur proportion dans l‚Äôensemble\nSommer les valeurs absolues de toutes les observations sup√©rieures (en valeur absolue) √† 1.96\net rapportez les √† la somme des valeurs de x (en valeur absolue)\n\n\n\nLorsque c‚Äôest possible, il est recommand√© d‚Äôutiliser les fonctions logiques de numpy (optimis√©es et\nqui g√®rent bien la dimension).\nParmi elles, on peut retrouver:\n\ncount_nonzero\nisnan\nany ; all ; notamment avec l‚Äôargument axis\nnp.array_equal pour v√©rifier, √©l√©ment par √©l√©ment, l‚Äô√©galit√©\n\nSoit\n\nx = np.random.normal(0, size=(3, 4))\n\nun array multidimensionnel et\n\ny = np.array([np.nan, 0, 1])\n\nun array unidimensionnel pr√©sentant une valeur manquante.\n\n\n Exercice 4\n\nUtiliser count_nonzero sur y\nUtiliser isnan sur y et compter le nombre de valeurs non NaN\nV√©rifier que x comporte au moins une valeur positive dans son ensemble, en parcourant les lignes puis les colonnes.\n\nNote : Jetez un oeil √† ce que correspond le param√®tre axis dans numpy en vous documentant sur internet. Par exemple ici."
  },
  {
    "objectID": "content/course/manipulation/01_numpy/index.html#manipuler-un-array",
    "href": "content/course/manipulation/01_numpy/index.html#manipuler-un-array",
    "title": "39¬† Numpy, la brique de base de la data science",
    "section": "39.4 Manipuler un array",
    "text": "39.4 Manipuler un array\nDans cette section, on utilisera un array multidimensionnel:\n\nx = np.random.normal(0, size=(3, 4))\n\n\n39.4.1 Statistiques sur un array\nPour les statistiques descriptives classiques,\nnumpy propose un certain nombre de fonctions d√©j√† impl√©ment√©es,\nqui peuvent √™tre combin√©es avec l‚Äôargument axis\n\n\n Exercice 5\n\nFaire la somme de tous les √©l√©ments d‚Äôun array, des √©l√©ments en ligne et des √©l√©ments en colonne. V√©rifier\nla coh√©rence\nEcrire une fonction statdesc pour renvoyer les valeurs suivantes : moyenne, m√©diane, √©cart-type, minimum et maximum.\nL‚Äôappliquer sur x en jouant avec l‚Äôargument axis\n\n\n\n\n\n39.4.2 Fonctions de manipulation\nVoici quelques fonctions pour modifier un array,\n\n\n\n\n\n\n\nOp√©ration\nImpl√©mentation\n\n\n\n\nApplatir un array\nx.flatten() (m√©thode)\n\n\nTransposer un array\nx.T (m√©thode) ou np.transpose(x) (fonction)\n\n\nAjouter des √©l√©ments √† la fin\nnp.append(x, [1,2])\n\n\nAjouter des √©l√©ments √† un endroit donn√© (aux positions 1 et 2)\nnp.insert(x, [1,2], 3)\n\n\nSupprimer des √©l√©ments (aux positions 0 et 3)\nnp.delete(x, [0,3])\n\n\n\nPour combiner des array, on peut utiliser, selon les cas,\nles fonctions np.concatenate, np.vstack ou la m√©thode .r_ (concat√©nation rowwise).\nnp.hstack ou la m√©thode .column_stack ou .c_ (concat√©nation column-wise)\n\nx = np.random.normal(size = 10)\n\nPour ordonner un array, on utilise np.sort\n\nx = np.array([7, 2, 3, 1, 6, 5, 4])\n\nnp.sort(x)\n\narray([1, 2, 3, 4, 5, 6, 7])\n\n\nSi on d√©sire faire un r√©-ordonnement partiel pour trouver les k valeurs les plus petites d‚Äôun array sans les ordonner, on utilise partition:\n\nnp.partition(x, 3)\n\narray([2, 1, 3, 4, 6, 5, 7])"
  },
  {
    "objectID": "content/course/manipulation/01_numpy/index.html#broadcasting",
    "href": "content/course/manipulation/01_numpy/index.html#broadcasting",
    "title": "39¬† Numpy, la brique de base de la data science",
    "section": "39.5 Broadcasting",
    "text": "39.5 Broadcasting\nLe broadcasting d√©signe un ensemble de r√®gles permettant\nd‚Äôappliquer des op√©rations sur des tableaux de dimensions diff√©rentes. En pratique,\ncela consiste g√©n√©ralement √† appliquer une seule op√©ration √† l‚Äôensemble des membres d‚Äôun tableau numpy.\nLa diff√©rence peut √™tre comprise √† partir de l‚Äôexemple suivant. Le broadcasting permet\nde transformer le scalaire 5 en array de dimension 3:\n\na = np.array([0, 1, 2])\n\nb = np.array([5, 5, 5])\n\na + b\na + 5\n\narray([5, 6, 7])\n\n\nLe broadcasting peut √™tre tr√®s pratique pour effectuer de mani√®re efficace des op√©rations sur des donn√©es √†\nla structure complexe. Pour plus de d√©tails, se rendre\nici ou ici."
  },
  {
    "objectID": "content/course/manipulation/01_numpy/index.html#une-application-programmer-ses-propres-k-nearest-neighbors",
    "href": "content/course/manipulation/01_numpy/index.html#une-application-programmer-ses-propres-k-nearest-neighbors",
    "title": "39¬† Numpy, la brique de base de la data science",
    "section": "39.6 Une application: programmer ses propres k-nearest neighbors",
    "text": "39.6 Une application: programmer ses propres k-nearest neighbors\n\n\n\n Exercice (un peu plus cors√©)\n\nCr√©er X un tableau √† deux dimensions (i.e.¬†une matrice) comportant 10 lignes\net 2 colonnes. Les nombres dans le tableau sont al√©atoires.\nImporter le module matplotlib.pyplot sous le nom plt. Utiliser\nplt.scatter pour repr√©senter les donn√©es sous forme de nuage de points.\nConstuire une matrice 10x10 stockant, √† l‚Äô√©l√©ment \\((i,j)\\), la distance euclidienne entre les points \\(X[i,]\\) et \\(X[j,]\\). Pour cela, il va falloir jouer avec les dimensions en cr√©ant des tableaux embo√Æt√©s √† partir par des appels √† np.newaxis :\n\n\nEn premier lieu, utiliser X1 = X[:, np.newaxis, :] pour transformer la matrice en tableau embo√Æt√©. V√©rifier les dimensions\nCr√©er X2 de dimension (1, 10, 2) √† partir de la m√™me logique\nEn d√©duire, pour chaque point, la distance avec les autres points pour chaque coordonn√©es. Elever celle-ci au carr√©\nA ce stade, vous devriez avoir un tableau de dimension (10, 10, 2). La r√©duction √† une matrice s‚Äôobtient en sommant sur le dernier axe. Regarder dans l‚Äôaide de np.sum comme effectuer une somme sur le dernier axe.\nEnfin, appliquer la racine carr√©e pour obtenir une distance euclidienne en bonne et due forme.\n\n\nV√©rifier que les termes diagonaux sont bien nuls (distance d‚Äôun point √† lui-m√™me‚Ä¶)\nIl s‚Äôagit maintenant de classer, pour chaque point, les points dont les valeurs sont les plus similaires. Utiliser np.argsort pour obtenir, pour chaque ligne, le classement des points les plus proches\nOn va s‚Äôint√©resser aux k-plus proches voisins. Pour le moment, fixons k=2. Utiliser argpartition pour r√©ordonner chaque ligne de mani√®re √† avoir les 2 plus proches voisins de chaque point d‚Äôabord et le reste de la ligne ensuite\nUtiliser le morceau de code ci-dessous\n\n\n\n\nUn indice pour repr√©senter graphiquement les plus proches voisins\nplt.scatter(X[:, 0], X[:, 1], s=100)\n\n# draw lines from each point to its two nearest neighbors\nK = 2\n\nfor i in range(X.shape[0]):\n    for j in nearest_partition[i, :K+1]:\n        # plot a line from X[i] to X[j]\n        # use some zip magic to make it happen:\n        plt.plot(*zip(X[j], X[i]), color='black')\n\n\nPour la question 2, vous devriez obtenir un graphique ayant cet aspect :\n\nLe r√©sultat de la question 7 est le suivant:\n\nAi-je invent√© cet exercice cors√© ? Pas du tout, il vient de l‚Äôouvrage Python Data Science Handbook. Mais, si je vous l‚Äôavais indiqu√© imm√©diatement, auriez-vous cherch√© √† r√©pondre aux questions ?\nPar ailleurs, il ne serait pas une bonne id√©e de g√©n√©raliser cet algorithme √† de grosses donn√©es. La complexit√© de notre approche est \\(O(N^2)\\). L‚Äôalgorithme impl√©ment√© par Scikit-Learn est\nen \\(O[NlogN]\\).\nDe plus, le calcul de distances matricielles en utilisant la puissance des cartes graphiques serait plus rapide. A cet √©gard, la librairie faiss offre des performances beaucoup plus satisfaisantes que celles que permettraient numpy sur ce probl√®me pr√©cis."
  },
  {
    "objectID": "content/course/manipulation/01_numpy/index.html#exercices-suppl√©mentaires",
    "href": "content/course/manipulation/01_numpy/index.html#exercices-suppl√©mentaires",
    "title": "39¬† Numpy, la brique de base de la data science",
    "section": "39.7 Exercices suppl√©mentaires",
    "text": "39.7 Exercices suppl√©mentaires\n\nSimulations de variables al√©atoires ;\nTCL ;\n\n\n\n Comprendre le principe de l'algorithme PageRank\nGoogle est devenu c√©l√®bre gr√¢ce √† son algorithme PageRank. Celui-ci permet, √† partir\nde liens entre sites web, de donner un score d‚Äôimportance √† un site web qui va\n√™tre utilis√© pour √©valuer sa centralit√© dans un r√©seau.\nL‚Äôobjectif de cet exercice est d‚Äôutiliser Numpy pour mettre en oeuvre un tel\nalgorithme √† partir d‚Äôune matrice d‚Äôadjacence qui relie les sites entre eux.\n\nCr√©er la matrice suivante avec numpy. L‚Äôappeler M:\n\n\\[\n\\begin{bmatrix}\n0 & 0 & 0 & 0 & 1 \\\\\n0.5 & 0 & 0 & 0 & 0 \\\\\n0.5 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0.5 & 0 & 0 \\\\\n0 & 0 & 0.5 & 1 & 0\n\\end{bmatrix}\n\\]\n\nPour repr√©senter visuellement ce web minimaliste,\nconvertir en objet networkx (une librairie sp√©cialis√©e\ndans l‚Äôanalyse de r√©seau) et utiliser la fonction draw\nde ce package.\n\nIl s‚Äôagit de la transpos√©e de la matrice d‚Äôadjacence\nqui permet de relier les sites entre eux. Par exemple,\nle site 1 (premi√®re colonne) est r√©f√©renc√© par\nles sites 2 et 3. Celui-ci ne r√©f√©rence que le site 5.\n\nA partir de la page wikipedia anglaise de PageRank, tester\nsur votre matrice.\n\n\n\n\n\n\n\n\nLe site 1 est assez central car il est r√©f√©renc√© 2 fois. Le site\n5 est lui √©galement central puisqu‚Äôil est r√©f√©renc√© par le site 1.\n\n\narray([[0.25419178],\n       [0.13803151],\n       [0.13803151],\n       [0.20599017],\n       [0.26375504]])"
  },
  {
    "objectID": "content/course/manipulation/04a_webscraping_TP/index.html#enjeux",
    "href": "content/course/manipulation/04a_webscraping_TP/index.html#enjeux",
    "title": "40¬† Webscraping avec python",
    "section": "40.1 Enjeux",
    "text": "40.1 Enjeux\nUn certain nombre d‚Äôenjeux du webscraping ne seront √©voqu√©s\nque superficiellement dans le cadre de ce chapitre.\n\n40.1.1 La zone grise de la l√©galit√© du webscraping\nEn premier lieu, en ce qui concerne la question de la l√©galit√©\nde la r√©cup√©ration d‚Äôinformation par scraping, il existe\nune zone grise. Ce n‚Äôest pas parce qu‚Äôune information est\ndisponible sur internet, directement ou avec un peu de recherche,\nqu‚Äôelle peut √™tre r√©cup√©r√©e et r√©utilis√©e.\nL‚Äôexcellent cours d‚ÄôAntoine Palazzolo √©voque un certain nombre de cas\nm√©diatiques et judiciaires sur cette question.\nDans le champ fran√ßais, la CNIL a publi√© en 2020\nde nouvelles directives sur le webscraping repr√©cisant\nque toute donn√©e ne peut √™tre r√©utilis√©e √† l‚Äôinsu de la personne\n√† laquelle ces donn√©es appartiennent. Autrement dit, en principe,\nles donn√©es collect√©es par webscraping sont soumises au\nRGPD, c‚Äôest-√†-dire n√©cessitent le consentement des personnes\n√† partir desquelles la r√©utilisation des donn√©es est faite.\nIl est donc recommand√© d‚Äô√™tre vigilant avec les donn√©es r√©cup√©r√©es\npar webscraping pour ne pas se mettre en faute l√©galement.\n\n\n40.1.2 Stabilit√© et fiabilit√© des informations re√ßues\nLa r√©cup√©ration de donn√©es par webscraping\nest certes pratique mais elle ne correspond pas n√©cessairement\n√† un usage pens√©, ou d√©sir√©, par un fournisseur de donn√©es.\nLes donn√©es √©tant co√ªteuses √† collecter et √† mettre √† disposition,\ncertains sites ne d√©sirent pas n√©cessairement que celles-ci soient\nextraites gratuitement et facilement. A fortiori lorsque la donn√©e\npeut permettre √† un concurrent de disposer d‚Äôune information\nutile d‚Äôun point de vue commercial (prix d‚Äôun produit concurrent, etc.).\nLes acteurs mettent donc souvent en oeuvre des strat√©gies pour bloquer ou\nlimiter la quantit√© de donn√©es scrapp√©es. La m√©thode la plus\nclassique est la d√©tection et le blocage\ndes requ√™tes faites par des robots plut√¥t que par des humains.\nPour des acteurs sp√©cialis√©s, cette d√©tection est tr√®s facile car\nde nombreuses preuves permettent d‚Äôidentifier si une visite du site web\nprovient d‚Äôun utilisateur\nhumain derri√®re un navigateur ou d‚Äôun robot. Pour ne citer que quelques indices:\nvitesse de la navigation entre pages, rapidit√© √† extraire la donn√©e,\nempreinte digitale du navigateur utilis√©, capacit√© √† r√©pondre √† des\nquestions al√©atoires (captcha)‚Ä¶\nLes bonnes pratiques, √©voqu√©es par la suite, ont pour objectif de faire\nen sorte qu‚Äôun robot se comporte de mani√®re civile en adoptant un comportement\nproche de celui de l‚Äôhumain mais sans contrefaire le fait qu‚Äôil ne s‚Äôagit\npas d‚Äôun humain.\nIl convient d‚Äôailleurs\nd‚Äô√™tre prudent quant aux informations re√ßues par webscraping.\nLa donn√©e √©tant au coeur du mod√®le √©conomique de certains acteurs, certains\nn‚Äôh√©sitent pas √† renvoyer des donn√©es fausses aux robots\nplut√¥t que les bloquer. C‚Äôest de bonne guerre!\nUne autre technique pi√®ge s‚Äôappelle le honey pot. Il s‚Äôagit de pages qu‚Äôun humain\nn‚Äôirait jamais visiter - par exemple parce qu‚Äôelles n‚Äôapparaissent pas dans\nl‚Äôinterface graphique - mais sur lesquelles un robot, en recherche automatique\nde contenu, va rester bloquer.\nSans aller jusqu‚Äô√† la strat√©gie de blocage du webscraping, d‚Äôautres raisons\npeuvent expliquer qu‚Äôune r√©cup√©ration de donn√©es ait fonctionn√© par\nle pass√© mais ne fonctionne plus. La plus fr√©quente est un changement dans la structure\nd‚Äôun site web. Le webscraping pr√©sente en effet l‚Äôinconv√©nient d‚Äôaller chercher\nde l‚Äôinformation dans une structure tr√®s hi√©rarchis√©e. Un changement dans cette structure\npeut suffire √† rendre un robot incapable de r√©cup√©rer du contenu. Or, pour rester\nattractifs, les sites web changent fr√©quemment ce qui peut facilement\nrendre inop√©rant un robot.\nDe mani√®re g√©n√©rale, l‚Äôun des principaux messages de ce\nchapitre, √† retenir, est que le\nwebscraping est une solution de dernier ressort, pour des r√©cup√©rations ponctuelles de donn√©es sans garantie de fonctionnement ult√©rieur. Il est pr√©f√©rable de privil√©gier les API lorsque celles-ci sont disponibles.\nCes derni√®res ressemblent √† un contrat (formel ou non) entre un fournisseur de donn√©es\net un utilisateur o√π sont d√©finis des besoins (les donn√©es) mais aussi des\nconditions d‚Äôacc√®s (nombre de requ√™tes, volum√©trie, authentification‚Ä¶) l√†\no√π le webscraping est plus proche du comportement dans le Far West.\n\n\n40.1.3 Les bonnes pratiques\nLa possibilit√© de r√©cup√©rer des donn√©es par l‚Äôinterm√©diaire\nd‚Äôun robot ne signifie pas qu‚Äôon peut se permettre de n‚Äô√™tre\npas civilis√©. En effet, lorsqu‚Äôil est non-ma√Ætris√©, le\nwebscraping peut ressembler √† une attaque informatique\nclassique pour faire sauter un site web: le d√©ni de service.\nLe cours d‚ÄôAntoine Palazzolo revient\nsur certaines bonnes pratiques qui ont √©merg√© dans la communaut√©\ndes scrapeurs. Il est recommand√© de lire cette ressource\npour en apprendre plus sur ce sujet. Y sont √©voqu√©s\nplusieurs conventions, parmi lesquelles :\n\nSe rendre, depuis la racine du site,\nsur le fichier robots.txt pour v√©rifier les consignes\npropos√©es par les d√©veloppeurs du site web pour\ncadrer le comportement des robots ;\nEspacer chaque requ√™tes de plusieurs secondes, comme le ferait\nun humain, afin d‚Äô√©viter de surcharger le site web et de le\nfaire sauter par d√©ni de service ;\nFaire les requ√™tes dans les heures creuses de fr√©quentation du\nsite web s‚Äôil ne s‚Äôagit pas d‚Äôun site consult√© internationalement.\nPar exemple, pour un site en Fran√ßais, lancer le robot\npendant la nuit en France m√©tropolitaine, est une bonne pratique.\nPour lancer un robot depuis Python a une heure programm√©e\n√† l‚Äôavancer, il existe les cronjobs."
  },
  {
    "objectID": "content/course/manipulation/04a_webscraping_TP/index.html#un-d√©tour-par-le-web-comment-fonctionne-un-site",
    "href": "content/course/manipulation/04a_webscraping_TP/index.html#un-d√©tour-par-le-web-comment-fonctionne-un-site",
    "title": "40¬† Webscraping avec python",
    "section": "40.2 Un d√©tour par le Web : comment fonctionne un site ?",
    "text": "40.2 Un d√©tour par le Web : comment fonctionne un site ?\nM√™me si ce TP ne vise pas √† faire un cours de web, il vous faut n√©anmoins certaines bases sur la mani√®re dont un site internet fonctionne afin de comprendre comment sont structur√©es les informations sur une page.\nUn site Web est un ensemble de pages cod√©es en HTML qui permet de d√©crire √† la fois le contenu et la forme d‚Äôune page Web.\nPour voir cela, ouvrez n‚Äôimporte quelle page web et faites un clic-droit dessus.\n- Sous Chrome  : Cliquez ensuite sur ‚ÄúAffichez le code source de la page‚Äù (CTRL+U) ;\n- Sous Firefox  : ‚ÄúCode source de la page‚Äù (CTRL+MAJ+K) ;\n- Sous Edge  : ‚ÄúAffichez la page source‚Äù (CTRL+U) ;\n- Sous Safari  : voir comment faire ici\n\n40.2.1 Les balises\nSur une page web, vous trouverez toujours √† coup s√ªr des √©l√©ments comme &lt;head&gt;, &lt;title&gt;, etc. Il s‚Äôagit des codes qui vous permettent de structurer le contenu d‚Äôune page HTML et qui s‚Äôappellent des balises.\nCitons, par exemple, les balises &lt;p&gt;, &lt;h1&gt;, &lt;h2&gt;, &lt;h3&gt;, &lt;strong&gt; ou &lt;em&gt;.\nLe symbole &lt; &gt; est une balise : il sert √† indiquer le d√©but d‚Äôune partie. Le symbole &lt;/ &gt; indique la fin de cette partie. La plupart des balises vont par paires, avec une balise ouvrante et une balise fermante (par exemple &lt;p&gt; et &lt;/p&gt;).\n\n40.2.1.1 Exemple : les balise des tableaux\n\n\n\nBalise\nDescription\n\n\n\n\n&lt;table&gt;\nTableau\n\n\n&lt;caption&gt;\nTitre du tableau\n\n\n&lt;tr&gt;\nLigne de tableau\n\n\n&lt;th&gt;\nCellule d‚Äôen-t√™te\n\n\n&lt;td&gt;\nCellule\n\n\n&lt;thead&gt;\nSection de l‚Äôen-t√™te du tableau\n\n\n&lt;tbody&gt;\nSection du corps du tableau\n\n\n&lt;tfoot&gt;\nSection du pied du tableau\n\n\n\nApplication : un tableau en HTML\nLe code HTML du tableau suivant\n&lt;table&gt;\n&lt;caption&gt; Le Titre de mon tableau &lt;/caption&gt;\n\n   &lt;tr&gt;\n      &lt;th&gt;Pr√©nom&lt;/th&gt;\n      &lt;th&gt;Nom&lt;/th&gt;\n      &lt;th&gt;Profession&lt;/th&gt;\n   &lt;/tr&gt;\n   &lt;tr&gt;\n      &lt;td&gt;Mike &lt;/td&gt;\n      &lt;td&gt;Stuntman&lt;/td&gt;\n      &lt;td&gt;Cascadeur&lt;/td&gt;\n   &lt;/tr&gt;\n   &lt;tr&gt;\n      &lt;td&gt;Mister&lt;/td&gt;\n      &lt;td&gt;Pink&lt;/td&gt;\n      &lt;td&gt;Gangster&lt;/td&gt;\n   &lt;/tr&gt;\n&lt;/table&gt;\nDonnera dans le navigateur :\n\n\nLe Titre de mon tableau\n\n\nPr√©nom\nNom\nProfession\n\n\nMike\nStuntman\nCascadeur\n\n\nMister\nPink\nGangster\n\n\n\n\n\n\n40.2.1.2 Parent et enfant\nDans le cadre du langage HTML, les termes de parent (parent) et enfant (child) servent √† d√©signer des √©lements embo√Æt√©s les uns dans les autres. Dans la construction suivante, par exemple :\n&lt; div&gt; \n    &lt; p&gt;\n       bla,bla\n    &lt; /p&gt;\n&lt; /div&gt;\nSur la page web, cela apparaitra de la mani√®re suivante :\n\n&lt;p&gt;\n   bla,bla\n&lt;/p&gt;\n\nOn dira que l‚Äô√©l√©ment &lt;div&gt; est le parent de l‚Äô√©l√©ment &lt;p&gt; tandis que l‚Äô√©l√©ment &lt;p&gt; est l‚Äôenfant de l‚Äô√©l√©ment &lt;div&gt;.\n\nMais pourquoi apprendre √ßa pour ‚Äúscraper‚Äù ?\n\nParce que, pour bien r√©cup√©rer les informations d‚Äôun site internet, il faut pouvoir comprendre sa structure et donc son code HTML. Les fonctions Python qui servent au scraping sont principalement construites pour vous permettre de naviguer entre les balises.\nAvec Python, vous allez en fait reproduire votre comportement manuel de recherche de mani√®re\n√† l‚Äôautomatiser."
  },
  {
    "objectID": "content/course/manipulation/04a_webscraping_TP/index.html#scraper-avec-python-le-package-beautifulsoup",
    "href": "content/course/manipulation/04a_webscraping_TP/index.html#scraper-avec-python-le-package-beautifulsoup",
    "title": "40¬† Webscraping avec python",
    "section": "40.3 Scraper avec Python: le package BeautifulSoup",
    "text": "40.3 Scraper avec Python: le package BeautifulSoup\n\n40.3.1 Les packages disponibles\nDans la premi√®re partie de ce chapitre,\nnous allons essentiellement utiliser le package BeautifulSoup4,\nen conjonction avec urllib\nou requests. Ces deux derniers packages permettent de r√©cup√©rer le texte\nbrut d‚Äôune page qui sera ensuite\ninspect√© via BeautifulSoup4.\nBeautifulSoup sera suffisant quand vous voudrez travailler sur des pages HTML statiques. D√®s que les informations que vous recherchez sont g√©n√©r√©es via l‚Äôex√©cution de scripts Javascript, il vous faudra passer par des outils comme Selenium.\nDe m√™me, si vous ne connaissez pas l‚ÄôURL, il faudra passer par un framework comme Scrapy, qui passe facilement d‚Äôune page √† une autre. On appelle\ncette technique le ‚Äúwebcrawling‚Äù. Scrapy est plus complexe √† manipuler que BeautifulSoup : si vous voulez plus de d√©tails, rendez-vous sur la page du tutoriel Scrapy.\nLe webscraping est un domaine o√π la reproductibilit√© est compliqu√©e √† mettre en oeuvre.\nUne page web √©volue\npotentiellement r√©guli√®rement et d‚Äôune page web √† l‚Äôautre, la structure peut\n√™tre tr√®s diff√©rente ce qui rend certains codes difficilement exportables.\nPar cons√©quent, la meilleure mani√®re d‚Äôavoir un programme fonctionnel est\nde comprendre la structure d‚Äôune page web et dissocier les √©l√©ments exportables\n√† d‚Äôautres cas d‚Äôusages des requ√™tes ad hoc.\n\nimport urllib\nimport bs4\nimport pandas\nfrom urllib import request\n\n\n\n40.3.2 R√©cup√©rer le contenu d‚Äôune page HTML\nOn va commencer doucement. Prenons une page wikipedia,\npar exemple celle de la Ligue 1 de football, mill√©sime 2019-2020 : Championnat de France de football 2019-2020. On va souhaiter r√©cup√©rer la liste des √©quipes, ainsi que les url des pages Wikipedia de ces √©quipes.\nEtape :one: : se connecter √† la page wikipedia et obtenir le code source.\nPour cela, le plus simple est d‚Äôutiliser le package urllib ou, mieux, requests.\nNous allons ici utiliser la fonction request du package urllib:\n\nurl_ligue_1 = \"https://fr.wikipedia.org/wiki/Championnat_de_France_de_football_2019-2020\"\n    \nrequest_text = request.urlopen(url_ligue_1).read()\n# print(request_text[:1000])    \n\n\ntype(request_text)\n\nbytes\n\n\nEtape :two: : utiliser le package BeautifulSoup\nqui permet de rechercher efficacement\nles balises contenues dans la chaine de caract√®res\nrenvoy√©e par la fonction request:\n\npage = bs4.BeautifulSoup(request_text, \"lxml\")\n\nSi on print l‚Äôobjet page cr√©√©e avec BeautifulSoup,\non voit que ce n‚Äôest plus une chaine de caract√®res mais bien une page HTML avec des balises.\nOn peut √† pr√©sent chercher des √©lements √† l‚Äôint√©rieur de ces balises.\n\n\n40.3.3 La m√©thode find\nPar exemple, si on veut conna√Ætre le titre de la page, on utilise la m√©thode .find et on lui demande ‚Äútitle‚Äù\n\nprint(page.find(\"title\"))\n\n&lt;title&gt;Championnat de France de football 2019-2020 ‚Äî Wikip√©dia&lt;/title&gt;\n\n\nLa methode .find ne renvoie que la premi√®re occurence de l‚Äô√©l√©ment.\nPour vous en assurer vous pouvez :\n\ncopier le bout de code source obtenu,\nle coller dans une cellule de votre notebook\net passer la cellule en ‚ÄúMarkdown‚Äù\n\nLa cellule avec le copier-coller du code source donne :\n\nprint(page.find(\"table\"))\n\n&lt;table&gt;&lt;caption style=\"background-color:#99cc99;color:#000000;\"&gt;G√©n√©ralit√©s&lt;/caption&gt;&lt;tbody&gt;&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Sport&lt;/th&gt;\n&lt;td&gt;\n&lt;a href=\"/wiki/Football\" title=\"Football\"&gt;Football&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Organisateur(s)&lt;/th&gt;\n&lt;td&gt;\n&lt;a href=\"/wiki/Ligue_de_football_professionnel_(France)\" title=\"Ligue de football professionnel (France)\"&gt;LFP&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;√âdition&lt;/th&gt;\n&lt;td&gt;\n&lt;abbr class=\"abbr\" title=\"Quatre-vingt-deuxi√®me (huitante-deuxi√®me / octante-deuxi√®me)\"&gt;82&lt;sup&gt;e&lt;/sup&gt;&lt;/abbr&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Lieu(x)&lt;/th&gt;\n&lt;td&gt;\n&lt;span class=\"datasortkey\" data-sort-value=\"France\"&gt;&lt;span class=\"flagicon\"&gt;&lt;span class=\"mw-image-border noviewer\" typeof=\"mw:File\"&gt;&lt;a class=\"mw-file-description\" href=\"/wiki/Fichier:Flag_of_France.svg\" title=\"Drapeau de la France\"&gt;&lt;img alt=\"Drapeau de la France\" class=\"mw-file-element\" data-file-height=\"600\" data-file-width=\"900\" decoding=\"async\" height=\"13\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Flag_of_France.svg/20px-Flag_of_France.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Flag_of_France.svg/30px-Flag_of_France.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Flag_of_France.svg/40px-Flag_of_France.svg.png 2x\" width=\"20\"/&gt;&lt;/a&gt;&lt;/span&gt; &lt;/span&gt;&lt;a href=\"/wiki/France\" title=\"France\"&gt;France&lt;/a&gt;&lt;/span&gt; et &lt;br/&gt;&lt;span class=\"datasortkey\" data-sort-value=\"Monaco\"&gt;&lt;span class=\"flagicon\"&gt;&lt;span class=\"mw-image-border noviewer\" typeof=\"mw:File\"&gt;&lt;a class=\"mw-file-description\" href=\"/wiki/Fichier:Flag_of_Monaco.svg\" title=\"Drapeau de Monaco\"&gt;&lt;img alt=\"Drapeau de Monaco\" class=\"mw-file-element\" data-file-height=\"800\" data-file-width=\"1000\" decoding=\"async\" height=\"16\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Flag_of_Monaco.svg/20px-Flag_of_Monaco.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Flag_of_Monaco.svg/30px-Flag_of_Monaco.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Flag_of_Monaco.svg/40px-Flag_of_Monaco.svg.png 2x\" width=\"20\"/&gt;&lt;/a&gt;&lt;/span&gt; &lt;/span&gt;&lt;a href=\"/wiki/Monaco\" title=\"Monaco\"&gt;Monaco&lt;/a&gt;&lt;/span&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Date&lt;/th&gt;\n&lt;td&gt;\nDu &lt;time class=\"nowrap date-lien\" data-sort-value=\"2019-08-09\" datetime=\"2019-08-09\"&gt;&lt;a href=\"/wiki/9_ao%C3%BBt_en_sport\" title=\"9 ao√ªt en sport\"&gt;9&lt;/a&gt; &lt;a class=\"mw-redirect\" href=\"/wiki/Ao%C3%BBt_2019_en_sport\" title=\"Ao√ªt 2019 en sport\"&gt;ao√ªt&lt;/a&gt; &lt;a href=\"/wiki/2019_en_football\" title=\"2019 en football\"&gt;2019&lt;/a&gt;&lt;/time&gt;&lt;br/&gt;au &lt;time class=\"nowrap date-lien\" data-sort-value=\"2020-03-08\" datetime=\"2020-03-08\"&gt;&lt;a href=\"/wiki/8_mars_en_sport\" title=\"8 mars en sport\"&gt;8 mars&lt;/a&gt; &lt;a href=\"/wiki/2020_en_football\" title=\"2020 en football\"&gt;2020&lt;/a&gt;&lt;/time&gt; &lt;small&gt;(arr√™t d√©finitif)&lt;/small&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Participants&lt;/th&gt;\n&lt;td&gt;\n20 √©quipes&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Matchs jou√©s&lt;/th&gt;\n&lt;td&gt;\n279 (sur 380 pr√©vus)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;th scope=\"row\" style=\"width:10.5em;\"&gt;Site web officiel&lt;/th&gt;\n&lt;td&gt;\n&lt;a class=\"external text\" href=\"https://www.ligue1.fr/\" rel=\"nofollow\"&gt;Site officiel&lt;/a&gt;&lt;/td&gt;\n&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;\n\n\nce qui est le texte source permettant de g√©n√©rer le tableau suivant:\n\n\n\nG√©n√©ralit√©s\n\n\n\n\nSport\n\n\nFootball\n\n\n\n\nOrganisateur(s)\n\n\nLFP\n\n\n\n\n√âdition\n\n\n82e\n\n\n\n\nLieu(x)\n\n\n France et  Monaco\n\n\n\n\nDate\n\n\nDu 9 ao√ªt 2019au 8 mars 2020 (arr√™t d√©finitif)\n\n\n\n\nParticipants\n\n\n20 √©quipes\n\n\n\n\nMatchs jou√©s\n\n\n279 (sur 380 pr√©vus)\n\n\n\n\nSite web officiel\n\n\nSite officiel\n\n\n\n\n\n\n\n40.3.4 La m√©thode findAll\nPour trouver toutes les occurences, on utilise .findAll().\n\nprint(\"Il y a\", len(page.findAll(\"table\")), \"√©l√©ments dans la page qui sont des &lt;table&gt;\")\n\nIl y a 34 √©l√©ments dans la page qui sont des &lt;table&gt;"
  },
  {
    "objectID": "content/course/manipulation/04a_webscraping_TP/index.html#exercice-guid√©-obtenir-la-liste-des-√©quipes-de-ligue-1",
    "href": "content/course/manipulation/04a_webscraping_TP/index.html#exercice-guid√©-obtenir-la-liste-des-√©quipes-de-ligue-1",
    "title": "40¬† Webscraping avec python",
    "section": "40.4 Exercice guid√© : obtenir la liste des √©quipes de Ligue 1",
    "text": "40.4 Exercice guid√© : obtenir la liste des √©quipes de Ligue 1\nDans le premier paragraphe de la page ‚ÄúParticipants‚Äù,\non a le tableau avec les r√©sultats de l‚Äôann√©e.\n\n\n Exercice 1 : R√©cup√©rer les participants de la Ligue 1\nPour cela, nous allons proc√©der en 6 √©tapes:\n\nTrouver le tableau\nR√©cup√©rer chaque ligne du table\nNettoyer les sorties en ne gardant que le texte sur une ligne\nG√©n√©raliser sur toutes les lignes\nR√©cup√©rer les ent√™tes du tableau\nFinalisation du tableau\n\n\n\n1Ô∏è Trouver le tableau\n\n# on identifie le tableau en question : c'est le premier qui a cette classe \"wikitable sortable\"\ntableau_participants = page.find('table', {'class' : 'wikitable sortable'})\n\n\nprint(tableau_participants)\n\n\n\n\nClub\n\n\nDerni√®remont√©e\n\n\nBudget[3]en M‚Ç¨\n\n\nClassement2018-2019\n\n\nEntra√Æneur\n\n\nDepuis\n\n\nStade\n\n\nCapacit√©en L1[4]\n\n\nNombrede saisonsen L1\n\n\n\n\nParis Saint-Germain\n\n\n1974\n\n\n637\n\n\n1er\n\n\n Thomas Tuchel\n\n\n2018\n\n\nParc des Princes\n\n\n47¬†929\n\n\n46\n\n\n\n\nLOSC Lille\n\n\n2000\n\n\n120\n\n\n2e\n\n\n Christophe Galtier\n\n\n2017\n\n\nStade Pierre-Mauroy\n\n\n49¬†712\n\n\n59\n\n\n\n\nOlympique lyonnais\n\n\n1989\n\n\n310\n\n\n3e\n\n\n Rudi Garcia\n\n\n2019\n\n\nGroupama Stadium\n\n\n57¬†206\n\n\n60\n\n\n\n\nAS Saint-√âtienne\n\n\n2004\n\n\n100\n\n\n4e\n\n\n Claude Puel\n\n\n2019\n\n\nStade Geoffroy-Guichard\n\n\n41¬†965\n\n\n66\n\n\n\n\nOlympique de Marseille\n\n\n1996\n\n\n110\n\n\n5e\n\n\n Andr√© Villas-Boas\n\n\n2019\n\n\nOrange V√©lodrome\n\n\n66¬†226\n\n\n69\n\n\n\n\nMontpellier HSC\n\n\n2009\n\n\n40\n\n\n6e\n\n\n Michel Der Zakarian\n\n\n2017\n\n\nStade de la Mosson\n\n\n22¬†000\n\n\n27\n\n\n\n\nOGC Nice\n\n\n2002\n\n\n50\n\n\n7e\n\n\n Patrick Vieira\n\n\n2018\n\n\nAllianz Riviera\n\n\n35¬†596\n\n\n60\n\n\n\n\nStade de Reims\n\n\n2018\n\n\n45\n\n\n8e\n\n\n David Guion\n\n\n2017\n\n\nStade Auguste-Delaune\n\n\n20¬†546\n\n\n35\n\n\n\n\nN√Æmes Olympique\n\n\n2018\n\n\n27\n\n\n9e\n\n\n Bernard Blaquart\n\n\n2015\n\n\nStade des Costi√®res\n\n\n15¬†788\n\n\n35\n\n\n\n\nStade rennais FC\n\n\n1994\n\n\n65\n\n\n10e\n\n\n Julien St√©phan\n\n\n2018\n\n\nRoazhon Park\n\n\n29¬†194\n\n\n62\n\n\n\n\nRC Strasbourg Alsace\n\n\n2017\n\n\n43\n\n\n11e\n\n\n Thierry Laurey\n\n\n2016\n\n\nStade de la Meinau\n\n\n26¬†109\n\n\n58\n\n\n\n\nFC Nantes\n\n\n2013\n\n\n70\n\n\n12e\n\n\n Christian Gourcuff\n\n\n2019\n\n\nStade de la Beaujoire - Louis Fonteneau\n\n\n35¬†322\n\n\n51\n\n\n\n\nSCO d‚ÄôAngers\n\n\n2015\n\n\n32\n\n\n13e\n\n\n St√©phane Moulin\n\n\n2011\n\n\nStade Raymond-Kopa\n\n\n14¬†582\n\n\n27\n\n\n\n\nGirondins de Bordeaux\n\n\n1992\n\n\n70\n\n\n14e\n\n\n Paulo Sousa\n\n\n2019\n\n\nMatmut Atlantique\n\n\n42¬†115\n\n\n66\n\n\n\n\nAmiens SC\n\n\n2017\n\n\n30\n\n\n15e\n\n\n Luka Elsner\n\n\n2019\n\n\nStade Cr√©dit Agricole la Licorne\n\n\n12¬†999\n\n\n2\n\n\n\n\nToulouse FC\n\n\n2003\n\n\n35\n\n\n16e\n\n\n Denis Zanko\n\n\n2020\n\n\nStadium de Toulouse\n\n\n33¬†033\n\n\n32\n\n\n\n\nAS Monaco\n\n\n2013\n\n\n220\n\n\n17e\n\n\n Robert Moreno\n\n\n2019\n\n\nStade Louis-II\n\n\n16¬†500\n\n\n60\n\n\n\n\nDijon FCO\n\n\n2016\n\n\n38\n\n\n18e\n\n\n St√©phane Jobard\n\n\n2019\n\n\nParc des Sports Gaston-G√©rard\n\n\n15¬†459\n\n\n4\n\n\n\n\nFC Metz\n\n\n2019\n\n\n40\n\n\n1er (Ligue 2)\n\n\n Vincent Hognon\n\n\n2019\n\n\nStade Saint-Symphorien\n\n\n25¬†865\n\n\n61\n\n\n\n\nStade brestois 29\n\n\n2019\n\n\n30\n\n\n2e (Ligue 2)\n\n\n Olivier Dall‚ÄôOglio\n\n\n2019\n\n\nStade Francis-Le Bl√©\n\n\n14¬†920\n\n\n13\n\n\n\n\n\n2Ô∏è R√©cup√©rer chaque ligne du tableau.\nOn recherche d‚Äôabord toutes les lignes du tableau avec la balise tr\n\ntable_body = tableau_participants.find('tbody')\nrows = table_body.find_all('tr')\n\nOn obtient une liste o√π chaque √©l√©ment est une des lignes du tableau\nPour illustrer cela, on va d‚Äôabord afficher la premi√®re ligne.\nCelle-ci correspont aux ent√™tes de colonne:\n\nprint(rows[0])\n\n&lt;tr&gt;\n&lt;th scope=\"col\"&gt;Club\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Derni√®re&lt;br/&gt;mont√©e\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Budget&lt;sup class=\"reference\" id=\"cite_ref-3\"&gt;&lt;a href=\"#cite_note-3\"&gt;&lt;span class=\"cite_crochet\"&gt;[&lt;/span&gt;3&lt;span class=\"cite_crochet\"&gt;]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;&lt;br/&gt;en M&lt;a href=\"/wiki/Euro\" title=\"Euro\"&gt;‚Ç¨&lt;/a&gt;\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Classement&lt;br/&gt;&lt;a href=\"/wiki/Championnat_de_France_de_football_2018-2019\" title=\"Championnat de France de football 2018-2019\"&gt;2018-2019&lt;/a&gt;\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Entra√Æneur\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Depuis\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Stade\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Capacit√©&lt;br/&gt;en L1&lt;sup class=\"reference\" id=\"cite_ref-4\"&gt;&lt;a href=\"#cite_note-4\"&gt;&lt;span class=\"cite_crochet\"&gt;[&lt;/span&gt;4&lt;span class=\"cite_crochet\"&gt;]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;\n&lt;/th&gt;\n&lt;th scope=\"col\"&gt;Nombre&lt;br/&gt;de saisons&lt;br/&gt;en L1\n&lt;/th&gt;&lt;/tr&gt;\n\n\nLa seconde ligne va correspondre √† la ligne du premier club pr√©sent dans le tableau:\n\nprint(rows[1])\n\n&lt;tr bgcolor=\"#97DEFF\"&gt;\n&lt;td&gt;&lt;a href=\"/wiki/Paris_Saint-Germain_Football_Club\" title=\"Paris Saint-Germain Football Club\"&gt;Paris Saint-Germain&lt;/a&gt;\n&lt;/td&gt;\n&lt;td&gt;1974\n&lt;/td&gt;\n&lt;td&gt;637\n&lt;/td&gt;\n&lt;td&gt;&lt;span data-sort-value=\"101 !\"&gt;&lt;/span&gt;&lt;abbr class=\"abbr\" title=\"Premier\"&gt;1&lt;sup&gt;er&lt;/sup&gt;&lt;/abbr&gt;\n&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;span class=\"flagicon\"&gt;&lt;span class=\"mw-image-border noviewer\" typeof=\"mw:File\"&gt;&lt;a class=\"mw-file-description\" href=\"/wiki/Fichier:Flag_of_Germany.svg\" title=\"Drapeau : Allemagne\"&gt;&lt;img alt=\"\" class=\"mw-file-element\" data-file-height=\"600\" data-file-width=\"1000\" decoding=\"async\" height=\"12\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Flag_of_Germany.svg/20px-Flag_of_Germany.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Flag_of_Germany.svg/30px-Flag_of_Germany.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Flag_of_Germany.svg/40px-Flag_of_Germany.svg.png 2x\" width=\"20\"/&gt;&lt;/a&gt;&lt;/span&gt;&lt;/span&gt; &lt;a href=\"/wiki/Thomas_Tuchel\" title=\"Thomas Tuchel\"&gt;Thomas Tuchel&lt;/a&gt;\n&lt;/td&gt;\n&lt;td&gt;2018\n&lt;/td&gt;\n&lt;td&gt;&lt;a href=\"/wiki/Parc_des_Princes\" title=\"Parc des Princes\"&gt;Parc des Princes&lt;/a&gt;\n&lt;/td&gt;\n&lt;td&gt;47¬†929\n&lt;/td&gt;\n&lt;td&gt;46\n&lt;/td&gt;&lt;/tr&gt;\n\n\n3Ô∏è Nettoyer les sorties en ne gardant que le texte sur une ligne\nOn va utiliser l‚Äôattribut text afin de se d√©barasser de toute la couche de HTML qu‚Äôon obtient √† l‚Äô√©tape 2.\nUn exemple sur la ligne du premier club :\n- on commence par prendre toutes les cellules de cette ligne, avec la balise td.\n- on fait ensuite une boucle sur chacune des cellules et on ne garde que le texte de la cellule avec l‚Äôattribut text.\n- enfin, on applique la m√©thode strip() pour que le texte soit bien mis en forme (sans espace inutile etc).\n\ncols = rows[1].find_all('td')\nprint(cols[0])\nprint(cols[0].text.strip())\n\n&lt;td&gt;&lt;a href=\"/wiki/Paris_Saint-Germain_Football_Club\" title=\"Paris Saint-Germain Football Club\"&gt;Paris Saint-Germain&lt;/a&gt;\n&lt;/td&gt;\nParis Saint-Germain\n\n\n\nfor ele in cols : \n    print(ele.text.strip())\n\nParis Saint-Germain\n1974\n637\n1er\nThomas Tuchel\n2018\nParc des Princes\n47¬†929\n46\n\n\n4Ô∏è G√©n√©raliser sur toutes les lignes :\n\nfor row in rows:\n    cols = row.find_all('td')\n    cols = [ele.text.strip() for ele in cols]\n    print(cols)\n\n[]\n['Paris Saint-Germain', '1974', '637', '1er', 'Thomas Tuchel', '2018', 'Parc des Princes', '47\\xa0929', '46']\n['LOSC Lille', '2000', '120', '2e', 'Christophe Galtier', '2017', 'Stade Pierre-Mauroy', '49\\xa0712', '59']\n['Olympique lyonnais', '1989', '310', '3e', 'Rudi Garcia', '2019', 'Groupama Stadium', '57\\xa0206', '60']\n['AS Saint-√âtienne', '2004', '100', '4e', 'Claude Puel', '2019', 'Stade Geoffroy-Guichard', '41\\xa0965', '66']\n['Olympique de Marseille', '1996', '110', '5e', 'Andr√© Villas-Boas', '2019', 'Orange V√©lodrome', '66\\xa0226', '69']\n['Montpellier HSC', '2009', '40', '6e', 'Michel Der Zakarian', '2017', 'Stade de la Mosson', '22\\xa0000', '27']\n['OGC Nice', '2002', '50', '7e', 'Patrick Vieira', '2018', 'Allianz Riviera', '35\\xa0596', '60']\n['Stade de Reims', '2018', '45', '8e', 'David Guion', '2017', 'Stade Auguste-Delaune', '20\\xa0546', '35']\n['N√Æmes Olympique', '2018', '27', '9e', 'Bernard Blaquart', '2015', 'Stade des Costi√®res', '15\\xa0788', '35']\n['Stade rennais FC', '1994', '65', '10e', 'Julien St√©phan', '2018', 'Roazhon Park', '29\\xa0194', '62']\n['RC Strasbourg Alsace', '2017', '43', '11e', 'Thierry Laurey', '2016', 'Stade de la Meinau', '26\\xa0109', '58']\n['FC Nantes', '2013', '70', '12e', 'Christian Gourcuff', '2019', 'Stade de la Beaujoire - Louis Fonteneau', '35\\xa0322', '51']\n['SCO d‚ÄôAngers', '2015', '32', '13e', 'St√©phane Moulin', '2011', 'Stade Raymond-Kopa', '14\\xa0582', '27']\n['Girondins de Bordeaux', '1992', '70', '14e', 'Paulo Sousa', '2019', 'Matmut Atlantique', '42\\xa0115', '66']\n['Amiens SC', '2017', '30', '15e', 'Luka Elsner', '2019', 'Stade Cr√©dit Agricole la Licorne', '12\\xa0999', '2']\n['Toulouse FC', '2003', '35', '16e', 'Denis Zanko', '2020', 'Stadium de Toulouse', '33\\xa0033', '32']\n['AS Monaco', '2013', '220', '17e', 'Robert Moreno', '2019', 'Stade Louis-II', '16\\xa0500', '60']\n['Dijon FCO', '2016', '38', '18e', 'St√©phane Jobard', '2019', 'Parc des Sports Gaston-G√©rard', '15\\xa0459', '4']\n['FC Metz', '2019', '40', '1er (Ligue 2)', 'Vincent Hognon', '2019', 'Stade Saint-Symphorien', '25\\xa0865', '61']\n['Stade brestois 29', '2019', '30', '2e (Ligue 2)', \"Olivier Dall'Oglio\", '2019', 'Stade Francis-Le Bl√©', '14\\xa0920', '13']\n\n\nOn a bien r√©ussi √† avoir les informations contenues dans le tableau des participants du championnat.\nMais la premi√®re ligne est √©trange : c‚Äôest une liste vide ‚Ä¶\nIl s‚Äôagit des en-t√™tes : elles sont reconnues par la balise th et non td.\nOn va mettre tout le contenu dans un dictionnaire, pour le transformer ensuite en DataFrame pandas :\n\ndico_participants = dict()\nfor row in rows:\n    cols = row.find_all('td')\n    cols = [ele.text.strip() for ele in cols]\n    if len(cols) &gt; 0 : \n        dico_participants[cols[0]] = cols[1:]\ndico_participants\n\n{'Paris Saint-Germain': ['1974',\n  '637',\n  '1er',\n  'Thomas Tuchel',\n  '2018',\n  'Parc des Princes',\n  '47\\xa0929',\n  '46'],\n 'LOSC Lille': ['2000',\n  '120',\n  '2e',\n  'Christophe Galtier',\n  '2017',\n  'Stade Pierre-Mauroy',\n  '49\\xa0712',\n  '59'],\n 'Olympique lyonnais': ['1989',\n  '310',\n  '3e',\n  'Rudi Garcia',\n  '2019',\n  'Groupama Stadium',\n  '57\\xa0206',\n  '60'],\n 'AS Saint-√âtienne': ['2004',\n  '100',\n  '4e',\n  'Claude Puel',\n  '2019',\n  'Stade Geoffroy-Guichard',\n  '41\\xa0965',\n  '66'],\n 'Olympique de Marseille': ['1996',\n  '110',\n  '5e',\n  'Andr√© Villas-Boas',\n  '2019',\n  'Orange V√©lodrome',\n  '66\\xa0226',\n  '69'],\n 'Montpellier HSC': ['2009',\n  '40',\n  '6e',\n  'Michel Der Zakarian',\n  '2017',\n  'Stade de la Mosson',\n  '22\\xa0000',\n  '27'],\n 'OGC Nice': ['2002',\n  '50',\n  '7e',\n  'Patrick Vieira',\n  '2018',\n  'Allianz Riviera',\n  '35\\xa0596',\n  '60'],\n 'Stade de Reims': ['2018',\n  '45',\n  '8e',\n  'David Guion',\n  '2017',\n  'Stade Auguste-Delaune',\n  '20\\xa0546',\n  '35'],\n 'N√Æmes Olympique': ['2018',\n  '27',\n  '9e',\n  'Bernard Blaquart',\n  '2015',\n  'Stade des Costi√®res',\n  '15\\xa0788',\n  '35'],\n 'Stade rennais FC': ['1994',\n  '65',\n  '10e',\n  'Julien St√©phan',\n  '2018',\n  'Roazhon Park',\n  '29\\xa0194',\n  '62'],\n 'RC Strasbourg Alsace': ['2017',\n  '43',\n  '11e',\n  'Thierry Laurey',\n  '2016',\n  'Stade de la Meinau',\n  '26\\xa0109',\n  '58'],\n 'FC Nantes': ['2013',\n  '70',\n  '12e',\n  'Christian Gourcuff',\n  '2019',\n  'Stade de la Beaujoire - Louis Fonteneau',\n  '35\\xa0322',\n  '51'],\n 'SCO d‚ÄôAngers': ['2015',\n  '32',\n  '13e',\n  'St√©phane Moulin',\n  '2011',\n  'Stade Raymond-Kopa',\n  '14\\xa0582',\n  '27'],\n 'Girondins de Bordeaux': ['1992',\n  '70',\n  '14e',\n  'Paulo Sousa',\n  '2019',\n  'Matmut Atlantique',\n  '42\\xa0115',\n  '66'],\n 'Amiens SC': ['2017',\n  '30',\n  '15e',\n  'Luka Elsner',\n  '2019',\n  'Stade Cr√©dit Agricole la Licorne',\n  '12\\xa0999',\n  '2'],\n 'Toulouse FC': ['2003',\n  '35',\n  '16e',\n  'Denis Zanko',\n  '2020',\n  'Stadium de Toulouse',\n  '33\\xa0033',\n  '32'],\n 'AS Monaco': ['2013',\n  '220',\n  '17e',\n  'Robert Moreno',\n  '2019',\n  'Stade Louis-II',\n  '16\\xa0500',\n  '60'],\n 'Dijon FCO': ['2016',\n  '38',\n  '18e',\n  'St√©phane Jobard',\n  '2019',\n  'Parc des Sports Gaston-G√©rard',\n  '15\\xa0459',\n  '4'],\n 'FC Metz': ['2019',\n  '40',\n  '1er (Ligue 2)',\n  'Vincent Hognon',\n  '2019',\n  'Stade Saint-Symphorien',\n  '25\\xa0865',\n  '61'],\n 'Stade brestois 29': ['2019',\n  '30',\n  '2e (Ligue 2)',\n  \"Olivier Dall'Oglio\",\n  '2019',\n  'Stade Francis-Le Bl√©',\n  '14\\xa0920',\n  '13']}\n\n\n\ndata_participants = pandas.DataFrame.from_dict(dico_participants,orient='index')\ndata_participants.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\nParis Saint-Germain\n1974\n637\n1er\nThomas Tuchel\n2018\nParc des Princes\n47¬†929\n46\n\n\nLOSC Lille\n2000\n120\n2e\nChristophe Galtier\n2017\nStade Pierre-Mauroy\n49¬†712\n59\n\n\nOlympique lyonnais\n1989\n310\n3e\nRudi Garcia\n2019\nGroupama Stadium\n57¬†206\n60\n\n\nAS Saint-√âtienne\n2004\n100\n4e\nClaude Puel\n2019\nStade Geoffroy-Guichard\n41¬†965\n66\n\n\nOlympique de Marseille\n1996\n110\n5e\nAndr√© Villas-Boas\n2019\nOrange V√©lodrome\n66¬†226\n69\n\n\n\n\n\n\n\n5Ô∏è R√©cup√©rer les en-t√™tes du tableau:\n\nfor row in rows:\n    cols = row.find_all('th')\n    print(cols)\n    if len(cols) &gt; 0 : \n        cols = [ele.get_text(separator=' ').strip().title() for ele in cols]\n        columns_participants = cols\n\n[&lt;th scope=\"col\"&gt;Club\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Derni√®re&lt;br/&gt;mont√©e\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Budget&lt;sup class=\"reference\" id=\"cite_ref-3\"&gt;&lt;a href=\"#cite_note-3\"&gt;&lt;span class=\"cite_crochet\"&gt;[&lt;/span&gt;3&lt;span class=\"cite_crochet\"&gt;]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;&lt;br/&gt;en M&lt;a href=\"/wiki/Euro\" title=\"Euro\"&gt;‚Ç¨&lt;/a&gt;\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Classement&lt;br/&gt;&lt;a href=\"/wiki/Championnat_de_France_de_football_2018-2019\" title=\"Championnat de France de football 2018-2019\"&gt;2018-2019&lt;/a&gt;\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Entra√Æneur\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Depuis\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Stade\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Capacit√©&lt;br/&gt;en L1&lt;sup class=\"reference\" id=\"cite_ref-4\"&gt;&lt;a href=\"#cite_note-4\"&gt;&lt;span class=\"cite_crochet\"&gt;[&lt;/span&gt;4&lt;span class=\"cite_crochet\"&gt;]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;\n&lt;/th&gt;, &lt;th scope=\"col\"&gt;Nombre&lt;br/&gt;de saisons&lt;br/&gt;en L1\n&lt;/th&gt;]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n[]\n\n\n\ncolumns_participants\n\n['Club',\n 'Derni√®re Mont√©e',\n 'Budget [ 3 ] En M ‚Ç¨',\n 'Classement 2018-2019',\n 'Entra√Æneur',\n 'Depuis',\n 'Stade',\n 'Capacit√© En L1 [ 4 ]',\n 'Nombre De Saisons En L1']\n\n\n6Ô∏è Finalisation du tableau\n\ndata_participants.columns = columns_participants[1:]\n\n\ndata_participants.head()\n\n\n\n\n\n\n\n\nDerni√®re Mont√©e\nBudget [ 3 ] En M ‚Ç¨\nClassement 2018-2019\nEntra√Æneur\nDepuis\nStade\nCapacit√© En L1 [ 4 ]\nNombre De Saisons En L1\n\n\n\n\nParis Saint-Germain\n1974\n637\n1er\nThomas Tuchel\n2018\nParc des Princes\n47¬†929\n46\n\n\nLOSC Lille\n2000\n120\n2e\nChristophe Galtier\n2017\nStade Pierre-Mauroy\n49¬†712\n59\n\n\nOlympique lyonnais\n1989\n310\n3e\nRudi Garcia\n2019\nGroupama Stadium\n57¬†206\n60\n\n\nAS Saint-√âtienne\n2004\n100\n4e\nClaude Puel\n2019\nStade Geoffroy-Guichard\n41¬†965\n66\n\n\nOlympique de Marseille\n1996\n110\n5e\nAndr√© Villas-Boas\n2019\nOrange V√©lodrome\n66¬†226\n69"
  },
  {
    "objectID": "content/course/manipulation/04a_webscraping_TP/index.html#pour-aller-plus-loin",
    "href": "content/course/manipulation/04a_webscraping_TP/index.html#pour-aller-plus-loin",
    "title": "40¬† Webscraping avec python",
    "section": "40.5 Pour aller plus loin",
    "text": "40.5 Pour aller plus loin\n\n40.5.1 R√©cup√©ration des localisations des stades\nEssayez de comprendre pas √† pas ce qui est fait dans les √©tapes qui suivent (la r√©cup√©ration d‚Äôinformations suppl√©mentaires en naviguant dans les pages des diff√©rents clubs).\n\nimport urllib\nimport pandas as pd\nimport bs4 \n\ndivision=[]\nequipe=[]\nstade=[]\nlatitude_stade=[]        \nlongitude_stade=[]     \n\ndef dms2dd(degrees, minutes, seconds, direction):\n    dd = float(degrees) + float(minutes)/60 + float(seconds)/(60*60);\n    if direction == 'S' or direction == 'O':\n        dd *= -1\n    return dd;\n\nurl_list=[\"http://fr.wikipedia.org/wiki/Championnat_de_France_de_football_2019-2020\", \"http://fr.wikipedia.org/wiki/Championnat_de_France_de_football_de_Ligue_2_2019-2020\"]\n\nfor url_ligue in url_list :\n       \n    print(url_ligue)\n    sock = urllib.request.urlopen(url_ligue).read() \n    page=bs4.BeautifulSoup(sock)\n\n# Rechercher les liens des √©quipes dans la liste disponible sur wikipedia \n\n    for team in page.findAll('span' , {'class' : 'toponyme'}) :  \n        \n        # Indiquer si c'est de la ligue 1 ou de la ligue 2\n        \n        if url_ligue==url_list[0] :\n            division.append(\"L1\")\n        else :\n            division.append(\"L2\")\n\n       # Trouver le nom et le lien de l'√©quipe\n            \n        if team.find('a')!=None :\n            team_url=team.find('a').get('href')\n            name_team=team.find('a').get('title')\n            equipe.append(name_team)\n            url_get_info = \"http://fr.wikipedia.org\"+team_url\n            print(url_get_info)\n \n       # aller sur la page de l'√©quipe\n           \n            search = urllib.request.urlopen(url_get_info).read()\n            search_team=bs4.BeautifulSoup(search)\n\n       # trouver le stade             \n            compteur = 0\n            for stadium in search_team.findAll('tr'):\n                for x in stadium.findAll('th' , {'scope' : 'row'} ) :\n                    if x.contents[0].string==\"Stade\" and compteur == 0:\n                        compteur = 1\n                        # trouver le lien du stade et son nom\n                        url_stade=stadium.findAll('a')[1].get('href')\n                        name_stadium=stadium.findAll('a')[1].get('title')\n                        stade.append(name_stadium)\n                        url_get_stade = \"http://fr.wikipedia.org\"+url_stade\n                        print(url_get_stade)\n                        \n                        # Aller sur la page du stade et trouver ses coodronn√©es g√©ographiques\n                        \n                        search_stade = urllib.request.urlopen(url_get_stade).read()\n                        soup_stade=bs4.BeautifulSoup(search_stade) \n                        kartographer = soup_stade.find('a',{'class': \"mw-kartographer-maplink\"})\n                        if kartographer == None :\n                          latitude_stade.append(None)\n                          longitude_stade.append(None) \n                        else :\n                            for coordinates in kartographer :\n                                print(coordinates)\n                                liste =   coordinates.split(\",\")          \n                                latitude_stade.append(str(liste[0]).replace(\" \", \"\") + \"'\")\n                                longitude_stade.append(str(liste[1]).replace(\" \", \"\") + \"'\")\n                            \n\ndict = {'division' : division , 'equipe': equipe, 'stade': stade, 'latitude': latitude_stade, 'longitude' : longitude_stade}\ndata = pd.DataFrame(dict)\ndata = data.dropna()\n\nhttp://fr.wikipedia.org/wiki/Championnat_de_France_de_football_2019-2020\nhttp://fr.wikipedia.org/wiki/Paris_Saint-Germain_Football_Club\nhttp://fr.wikipedia.org/wiki/Parc_des_Princes\n48¬∞¬†50‚Ä≤¬†29‚Ä≥¬†N, 2¬∞¬†15‚Ä≤¬†11‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/LOSC_Lille\nhttp://fr.wikipedia.org/wiki/Stade_Pierre-Mauroy\n50¬∞¬†36‚Ä≤¬†43‚Ä≥¬†N, 3¬∞¬†07‚Ä≤¬†50‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/Olympique_lyonnais\nhttp://fr.wikipedia.org/wiki/Parc_Olympique_lyonnais\n45¬∞¬†45‚Ä≤¬†55‚Ä≥¬†N, 4¬∞¬†58‚Ä≤¬†55‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/Association_sportive_de_Saint-%C3%89tienne\nhttp://fr.wikipedia.org/wiki/Stade_Geoffroy-Guichard\n45¬∞¬†27‚Ä≤¬†39‚Ä≥¬†N, 4¬∞¬†23‚Ä≤¬†25‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/Olympique_de_Marseille\nhttp://fr.wikipedia.org/wiki/Orange_V%C3%A9lodrome\n43¬∞¬†16‚Ä≤¬†11‚Ä≥¬†N, 5¬∞¬†23‚Ä≤¬†45‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/Montpellier_H%C3%A9rault_Sport_Club\nhttp://fr.wikipedia.org/wiki/Stade_de_la_Mosson\n43¬∞¬†37‚Ä≤¬†19‚Ä≥¬†N, 3¬∞¬†48‚Ä≤¬†44‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/Stade_de_Reims\nhttp://fr.wikipedia.org/wiki/Stade_Auguste-Delaune\n49¬∞¬†14‚Ä≤¬†48‚Ä≥¬†N, 4¬∞¬†01‚Ä≤¬†30‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/Olympique_Gymnaste_Club_Nice\nhttp://fr.wikipedia.org/wiki/Allianz_Riviera\n43¬∞¬†42‚Ä≤¬†18‚Ä≥¬†N, 7¬∞¬†11‚Ä≤¬†33‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/N%C3%AEmes_Olympique\nhttp://fr.wikipedia.org/wiki/Stade_des_Antonins\n43¬∞¬†48‚Ä≤¬†39‚Ä≥¬†N, 4¬∞¬†21‚Ä≤¬†23‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/Racing_Club_de_Strasbourg_Alsace\nhttp://fr.wikipedia.org/wiki/Stade_de_la_Meinau\n48¬∞¬†33‚Ä≤¬†36‚Ä≥¬†N, 7¬∞¬†45‚Ä≤¬†18‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/Stade_rennais_Football_Club\nhttp://fr.wikipedia.org/wiki/Roazhon_Park\n48¬∞¬†06‚Ä≤¬†27‚Ä≥¬†N, 1¬∞¬†42‚Ä≤¬†46‚Ä≥¬†O\nhttp://fr.wikipedia.org/wiki/Angers_sporting_club_de_l%27Ouest\nhttp://fr.wikipedia.org/wiki/Stade_Raymond-Kopa\n47¬∞¬†27‚Ä≤¬†38‚Ä≥¬†N, 0¬∞¬†31‚Ä≤¬†51‚Ä≥¬†O\nhttp://fr.wikipedia.org/wiki/Football_Club_de_Metz\nhttp://fr.wikipedia.org/wiki/Stade_Saint-Symphorien\n49¬∞¬†06‚Ä≤¬†35‚Ä≥¬†N, 6¬∞¬†09‚Ä≤¬†33‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/Football_Club_de_Nantes\nhttp://fr.wikipedia.org/wiki/Stade_de_la_Beaujoire\n47¬∞¬†15‚Ä≤¬†20‚Ä≥¬†N, 1¬∞¬†31‚Ä≤¬†31‚Ä≥¬†O\nhttp://fr.wikipedia.org/wiki/Stade_brestois_29\nhttp://fr.wikipedia.org/wiki/Stade_Francis-Le_Bl%C3%A9\n48¬∞¬†24‚Ä≤¬†11‚Ä≥¬†N, 4¬∞¬†27‚Ä≤¬†42‚Ä≥¬†O\nhttp://fr.wikipedia.org/wiki/Football_Club_des_Girondins_de_Bordeaux\nhttp://fr.wikipedia.org/wiki/Matmut_Atlantique\n44¬∞¬†53‚Ä≤¬†50‚Ä≥¬†N, 0¬∞¬†33‚Ä≤¬†41‚Ä≥¬†O\nhttp://fr.wikipedia.org/wiki/Toulouse_Football_Club\nhttp://fr.wikipedia.org/wiki/Stadium_de_Toulouse\n43¬∞¬†35‚Ä≤¬†00‚Ä≥¬†N, 1¬∞¬†26‚Ä≤¬†03‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/Amiens_Sporting_Club\nhttp://fr.wikipedia.org/wiki/Stade_de_la_Licorne\n49¬∞¬†53‚Ä≤¬†38‚Ä≥¬†N, 2¬∞¬†15‚Ä≤¬†49‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/Association_sportive_de_Monaco_football_club\nhttp://fr.wikipedia.org/wiki/Stade_Louis-II\n43¬∞¬†43‚Ä≤¬†39‚Ä≥¬†N, 7¬∞¬†24‚Ä≤¬†56‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/Dijon_Football_C%C3%B4te-d%27Or\nhttp://fr.wikipedia.org/wiki/Stade_Gaston-G%C3%A9rard\n47¬∞¬†19‚Ä≤¬†28‚Ä≥¬†N, 5¬∞¬†04‚Ä≤¬†06‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/Championnat_de_France_de_football_de_Ligue_2_2019-2020\nhttp://fr.wikipedia.org/wiki/Athletic_Club_ajaccien\nhttp://fr.wikipedia.org/wiki/Stade_Fran%C3%A7ois_Coty\n41¬∞¬†55‚Ä≤¬†54‚Ä≥¬†N, 8¬∞¬†46‚Ä≤¬†44‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/Association_sportive_Nancy-Lorraine\nhttp://fr.wikipedia.org/wiki/Stade_Marcel-Picot\n48¬∞¬†41‚Ä≤¬†43‚Ä≥¬†N, 6¬∞¬†12‚Ä≤¬†38‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/Le_Havre_Athletic_Club_(football)\nhttp://fr.wikipedia.org/wiki/Stade_Oc%C3%A9ane\n49¬∞¬†29‚Ä≤¬†56‚Ä≥¬†N, 0¬∞¬†10‚Ä≤¬†11‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/Stade_Malherbe_Caen_Calvados_Basse-Normandie\nhttp://fr.wikipedia.org/wiki/Stade_Michel-d%27Ornano\n49¬∞¬†10‚Ä≤¬†46‚Ä≥¬†N, 0¬∞¬†23‚Ä≤¬†48‚Ä≥¬†O\nhttp://fr.wikipedia.org/wiki/Clermont_Foot_63\nhttp://fr.wikipedia.org/wiki/Stade_Gabriel-Montpied\n45¬∞¬†48‚Ä≤¬†57‚Ä≥¬†N, 3¬∞¬†07‚Ä≤¬†18‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/En_Avant_de_Guingamp\nhttp://fr.wikipedia.org/wiki/Stade_de_Roudourou\n48¬∞¬†33‚Ä≤¬†58‚Ä≥¬†N, 3¬∞¬†09‚Ä≤¬†52‚Ä≥¬†O\nhttp://fr.wikipedia.org/wiki/Football_Club_Lorient\nhttp://fr.wikipedia.org/wiki/Stade_du_Moustoir\n47¬∞¬†44‚Ä≤¬†56‚Ä≥¬†N, 3¬∞¬†22‚Ä≤¬†09‚Ä≥¬†O\nhttp://fr.wikipedia.org/wiki/Paris_Football_Club\nhttp://fr.wikipedia.org/wiki/Stade_Charl%C3%A9ty\n48¬∞¬†49‚Ä≤¬†07‚Ä≥¬†N, 2¬∞¬†20‚Ä≤¬†47‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/La_Berrichonne_de_Ch%C3%A2teauroux\nhttp://fr.wikipedia.org/wiki/Stade_Gaston-Petit\n46¬∞¬†48‚Ä≤¬†07‚Ä≥¬†N, 1¬∞¬†43‚Ä≤¬†18‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/Association_de_la_jeunesse_auxerroise\nhttp://fr.wikipedia.org/wiki/Stade_de_l%27Abb%C3%A9-Deschamps\n47¬∞¬†47‚Ä≤¬†12‚Ä≥¬†N, 3¬∞¬†35‚Ä≤¬†19‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/Union_sportive_Orl%C3%A9ans_Loiret_football\nhttp://fr.wikipedia.org/wiki/Stade_de_la_Source\n47¬∞¬†50‚Ä≤¬†25‚Ä≥¬†N, 1¬∞¬†56‚Ä≤¬†28‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/Valenciennes_Football_Club\nhttp://fr.wikipedia.org/wiki/Stade_du_Hainaut\n50¬∞¬†20‚Ä≤¬†55‚Ä≥¬†N, 3¬∞¬†31‚Ä≤¬†56‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/Chamois_niortais_Football_Club\nhttp://fr.wikipedia.org/wiki/Stade_Ren%C3%A9-Gaillard\n46¬∞¬†19‚Ä≤¬†01‚Ä≥¬†N, 0¬∞¬†29‚Ä≤¬†21‚Ä≥¬†O\nhttp://fr.wikipedia.org/wiki/Grenoble_Foot_38\nhttp://fr.wikipedia.org/wiki/Stade_des_Alpes\n45¬∞¬†11‚Ä≤¬†15‚Ä≥¬†N, 5¬∞¬†44‚Ä≤¬†24‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/Football_Club_Sochaux-Montb%C3%A9liard\nhttp://fr.wikipedia.org/wiki/Stade_Auguste-Bonal\n47¬∞¬†30‚Ä≤¬†44‚Ä≥¬†N, 6¬∞¬†48‚Ä≤¬†41‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/Rodez_Aveyron_Football\nhttp://fr.wikipedia.org/wiki/Stade_Paul-Lignon\n44¬∞¬†21‚Ä≤¬†06‚Ä≥¬†N, 2¬∞¬†33‚Ä≤¬†49‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/Football_Club_de_Chambly_Oise\nhttp://fr.wikipedia.org/wiki/Stade_Walter_Luzi\n49¬∞¬†10‚Ä≤¬†45‚Ä≥¬†N, 2¬∞¬†14‚Ä≤¬†01‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/Esp%C3%A9rance_sportive_Troyes_Aube_Champagne\nhttp://fr.wikipedia.org/wiki/Stade_de_l%27Aube\n48¬∞¬†18‚Ä≤¬†27‚Ä≥¬†N, 4¬∞¬†05‚Ä≤¬†55‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/Racing_Club_de_Lens\nhttp://fr.wikipedia.org/wiki/Stade_Bollaert-Delelis\n50¬∞¬†25‚Ä≤¬†58‚Ä≥¬†N, 2¬∞¬†48‚Ä≤¬†54‚Ä≥¬†E\nhttp://fr.wikipedia.org/wiki/Le_Mans_Football_Club\nhttp://fr.wikipedia.org/wiki/Stade_Marie-Marvingt\n47¬∞¬†57‚Ä≤¬†32‚Ä≥¬†N, 0¬∞¬†13‚Ä≤¬†29‚Ä≥¬†E\n\n\n\ndata.head(5)\n\n\n\n\n\n\n\n\ndivision\nequipe\nstade\nlatitude\nlongitude\n\n\n\n\n0\nL1\nParis Saint-Germain Football Club\nParc des Princes\n48¬∞¬†50‚Ä≤¬†29‚Ä≥¬†N'\n2¬∞¬†15‚Ä≤¬†11‚Ä≥¬†E'\n\n\n1\nL1\nLOSC Lille\nStade Pierre-Mauroy\n50¬∞¬†36‚Ä≤¬†43‚Ä≥¬†N'\n3¬∞¬†07‚Ä≤¬†50‚Ä≥¬†E'\n\n\n2\nL1\nOlympique lyonnais\nParc Olympique lyonnais\n45¬∞¬†45‚Ä≤¬†55‚Ä≥¬†N'\n4¬∞¬†58‚Ä≤¬†55‚Ä≥¬†E'\n\n\n3\nL1\nAssociation sportive de Saint-√âtienne\nStade Geoffroy-Guichard\n45¬∞¬†27‚Ä≤¬†39‚Ä≥¬†N'\n4¬∞¬†23‚Ä≤¬†25‚Ä≥¬†E'\n\n\n4\nL1\nOlympique de Marseille\nOrange V√©lodrome\n43¬∞¬†16‚Ä≤¬†11‚Ä≥¬†N'\n5¬∞¬†23‚Ä≤¬†45‚Ä≥¬†E'\n\n\n\n\n\n\n\nOn va transformer les coordonn√©es en degr√©s en coordonn√©es num√©riques\nafin d‚Äô√™tre en mesure de faire une carte\n\nimport re\n\ndef dms2dd(degrees, minutes, seconds, direction):\n    dd = float(degrees) + float(minutes)/60 + float(seconds)/(60*60);\n    if direction in ('S', 'O'):\n        dd *= -1\n    return dd\n\ndef parse_dms(dms):\n    parts = re.split('[^\\d\\w]+', dms)\n    lat = dms2dd(parts[0], parts[1], parts[2], parts[3])\n    #lng = dms2dd(parts[4], parts[5], parts[6], parts[7])\n    return lat\n\n\ndata['latitude'] = data['latitude'].apply(parse_dms)\ndata['longitude'] = data['longitude'].apply(parse_dms)\n\nTous les √©l√©ments sont en place pour faire une belle carte √† ce stade. On\nva utiliser folium pour celle-ci, qui est pr√©sent√© dans la partie\nvisualisation.\n\n\n40.5.2 Carte des stades avec folium\n\n#!pip install geopandas\nimport geopandas as gpd\nfrom pathlib import Path\nimport folium\n\ngdf = gpd.GeoDataFrame(\n    data, geometry=gpd.points_from_xy(data.longitude, data.latitude))\n\nPath(\"leaflet\").mkdir(parents=True, exist_ok=True)\n\ncenter = gdf[['latitude', 'longitude']].mean().values.tolist()\nsw = gdf[['latitude', 'longitude']].min().values.tolist()\nne = gdf[['latitude', 'longitude']].max().values.tolist()\n\nm = folium.Map(location = center, tiles='Stamen Toner')\n\n# I can add marker one by one on the map\nfor i in range(0,len(gdf)):\n    folium.Marker([gdf.iloc[i]['latitude'], gdf.iloc[i]['longitude']], popup=gdf.iloc[i]['stade']).add_to(m) \n\nm.fit_bounds([sw, ne])\n\nLa carte obtenue doit ressembler √† la suivante:\n\n# Afficher la carte\nm"
  },
  {
    "objectID": "content/course/manipulation/04a_webscraping_TP/index.html#r√©cup√©rer-des-informations-sur-les-pokemons",
    "href": "content/course/manipulation/04a_webscraping_TP/index.html#r√©cup√©rer-des-informations-sur-les-pokemons",
    "title": "40¬† Webscraping avec python",
    "section": "40.6 R√©cup√©rer des informations sur les pokemons",
    "text": "40.6 R√©cup√©rer des informations sur les pokemons\nLe prochain exercice pour mettre en pratique le webscraping\nconsiste √† r√©cup√©rer des informations sur les\npokemons √† partir du\nsite internet pokemondb.net.\n\n40.6.1 Version non guid√©e\n\n\n Exercice 2 : Les pokemon (version non guid√©e)\nPour cet exercice, nous vous demandons d‚Äôobtenir diff√©rentes informations sur les pok√©mons :\n\nles informations personnelles des 893 pokemons sur le site internet pokemondb.net.\nLes informations que nous aimerions obtenir au final dans un DataFrame sont celles contenues dans 4 tableaux :\n\n\nPok√©dex data\nTraining\nBreeding\nBase stats\n\n\nNous aimerions que vous r√©cup√©riez √©galement les images de chacun des pok√©mons et que vous les enregistriez dans un dossier\n\n\nPetit indice : utilisez les modules request et shutil\nPour cette question, il faut que vous cherchiez de vous m√™me certains √©l√©ments, tout n‚Äôest pas pr√©sent dans le TD.\n\n\n\nPour la question 1, l‚Äôobjectif est d‚Äôobtenir le code source d‚Äôun tableau comme\ncelui qui suit\n(Pokemon Nincada.)\n\n\n\nPok√©dex data\n\n\n\n\n\nNational ‚Ññ\n\n\n290\n\n\n\n\nType\n\n\nBug Ground\n\n\n\n\nSpecies\n\n\nTrainee Pok√©mon\n\n\n\n\nHeight\n\n\n0.5¬†m (1‚Ä≤08‚Ä≥)\n\n\n\n\nWeight\n\n\n5.5¬†kg (12.1¬†lbs)\n\n\n\n\nAbilities\n\n\n1. Compound EyesRun Away (hidden ability)\n\n\n\n\nLocal ‚Ññ\n\n\n042 (Ruby/Sapphire/Emerald)111 (X/Y ‚Äî Central Kalos)043 (Omega Ruby/Alpha Sapphire)104 (Sword/Shield)\n\n\n\n\n\n\n\n\n\nTraining\n\n\n\n\n\nEV yield\n\n\n1 Defense\n\n\n\n\nCatch rate\n\n\n255 (33.3% with Pok√©Ball, full HP)\n\n\n\n\nBase Friendship\n\n\n70 (normal)\n\n\n\n\nBase Exp.\n\n\n53\n\n\n\n\nGrowth Rate\n\n\nErratic\n\n\n\n\n\n\n\nBreeding\n\n\n\n\n\nEgg Groups\n\n\nBug\n\n\n\n\nGender\n\n\n50% male, 50% female\n\n\n\n\nEgg cycles\n\n\n15 (3,599‚Äì3,855 steps)\n\n\n\n\n\n\n\n\n\n\n\n\nBase stats\n\n\n\n\n\n\nHP\n\n\n31\n\n\n\n\n\n\n\n172\n\n\n266\n\n\n\n\nAttack\n\n\n45\n\n\n\n\n\n\n\n85\n\n\n207\n\n\n\n\nDefense\n\n\n90\n\n\n\n\n\n\n\n166\n\n\n306\n\n\n\n\nSp. Atk\n\n\n30\n\n\n\n\n\n\n\n58\n\n\n174\n\n\n\n\nSp. Def\n\n\n30\n\n\n\n\n\n\n\n58\n\n\n174\n\n\n\n\nSpeed\n\n\n40\n\n\n\n\n\n\n\n76\n\n\n196\n\n\n\n\n\n\nTotal\n\n\n266\n\n\n\n\nMin\n\n\nMax\n\n\n\n\n\n\n\nPour la question 2, l‚Äôobjectif est d‚Äôobtenir\nl‚Äôune des images suivantes:\n\n\n\n40.6.2 Version guid√©e\nLes prochaines parties permettront de faire l‚Äôexercice ci-dessus\n√©tape par √©tape,\nde mani√®re guid√©e.\nNous souhaitons tout d‚Äôabord obtenir les\ninformations personnelles de tous\nles pokemons sur pokemondb.net.\nLes informations que nous aimerions obtenir au final pour les pokemons sont celles contenues dans 4 tableaux :\n\nPok√©dex data\nTraining\nBreeding\nBase stats\n\nNous proposons ensuite de r√©cup√©rer et afficher les images.\n\n40.6.2.1 Etape 1: constituer un DataFrame de caract√©ristiques\n\n\n Exercice 2b : Les pok√©mons (version guid√©e)\nPour r√©cup√©rer les informations, le code devra √™tre divis√© en plusieurs √©tapes :\n\nTrouvez la page principale du site et la transformer en un objet intelligible pour votre code.\nLes fonctions suivantes vous seront utiles :\n\n\nurllib.request.Request\nurllib.request.urlopen\nbs4.BeautifulSoup\n\n\nCr√©ez une fonction qui permet de r√©cup√©rer la page d‚Äôun pok√©mon √† partir de son nom.\nA partir de la page de bulbasaur, obtenez les 4 tableaux qui nous int√©ressent :\n\n\non va chercher l‚Äô√©l√©ment suivant : ('table', { 'class' : \"vitals-table\"})\npuis stocker ses √©l√©ments dans un dictionnaire\n\n\nR√©cup√©rez par ailleurs la liste de noms des pok√©mons qui nous permettra de faire une boucle par la suite. Combien trouvez-vous de pok√©mons ?\nEcrire une fonction qui r√©cup√®re l‚Äôensemble des informations sur les dix premiers pok√©mons de la liste et les int√®gre dans un DataFrame\n\n\n\nA l‚Äôissue de la question 3,\nvous devriez obtenir une liste de caract√©ristiques proche de celle-ci:\n\n\ndefaultdict(None,\n            {'National ‚Ññ': '0001',\n             'name': 'bulbasaur',\n             'Type': ' Grass Poison ',\n             'Species': 'Seed Pok√©mon',\n             'Height': '0.7\\xa0m (2‚Ä≤04‚Ä≥)',\n             'Weight': '6.9\\xa0kg (15.2\\xa0lbs)',\n             'Abilities': '1. OvergrowChlorophyll (hidden ability)',\n             'Local ‚Ññ': \"0001 (Red/Blue/Yellow)0226 (Gold/Silver/Crystal)0001 (FireRed/LeafGreen)0231 (HeartGold/SoulSilver)0080 (X/Y ‚Äî Central Kalos)0001 (Let's Go Pikachu/Let's Go Eevee)0068 (The Isle of Armor)\",\n             'EV yield': ' 1 Sp. Atk ',\n             'Catch rate': ' 45 (5.9% with Pok√©Ball, full HP) ',\n             'Base Friendship': ' 50 (normal) ',\n             'Base Exp.': '64',\n             'Growth Rate': 'Medium Slow',\n             'Egg Groups': 'Grass, Monster',\n             'Gender': '87.5% male, 12.5% female',\n             'Egg cycles': '20 (4,884‚Äì5,140 steps) ',\n             'HP': '45',\n             'Attack': '49',\n             'Defense': '49',\n             'Sp. Atk': '65',\n             'Sp. Def': '65',\n             'Speed': '45'})\n\n\nLa structure est ici en dictionnaire, ce qui est pratique.\nEnfin, vous les\ninformations sur les dix premiers pok√©mons de la liste int√©gr√©es dans un\nDataFrame prendront l‚Äôaspect suivant:\n\n\n\n\n\n\n\n\n\nNational ‚Ññ\nname\nType\nSpecies\nHeight\nWeight\nAbilities\nLocal ‚Ññ\nEV yield\nCatch rate\n...\nGrowth Rate\nEgg Groups\nGender\nEgg cycles\nHP\nAttack\nDefense\nSp. Atk\nSp. Def\nSpeed\n\n\n\n\n0\n0001\nbulbasaur\nGrass Poison\nSeed Pok√©mon\n0.7¬†m (2‚Ä≤04‚Ä≥)\n6.9¬†kg (15.2¬†lbs)\n1. OvergrowChlorophyll (hidden ability)\n0001 (Red/Blue/Yellow)0226 (Gold/Silver/Crysta...\n1 Sp. Atk\n45 (5.9% with Pok√©Ball, full HP)\n...\nMedium Slow\nGrass, Monster\n87.5% male, 12.5% female\n20 (4,884‚Äì5,140 steps)\n45\n49\n49\n65\n65\n45\n\n\n1\n0002\nivysaur\nGrass Poison\nSeed Pok√©mon\n1.0¬†m (3‚Ä≤03‚Ä≥)\n13.0¬†kg (28.7¬†lbs)\n1. OvergrowChlorophyll (hidden ability)\n0002 (Red/Blue/Yellow)0227 (Gold/Silver/Crysta...\n1 Sp. Atk, 1 Sp. Def\n45 (5.9% with Pok√©Ball, full HP)\n...\nMedium Slow\nGrass, Monster\n87.5% male, 12.5% female\n20 (4,884‚Äì5,140 steps)\n60\n62\n63\n80\n80\n60\n\n\n2\n0003\nvenusaur\nGrass Poison\nSeed Pok√©mon\n2.0¬†m (6‚Ä≤07‚Ä≥)\n100.0¬†kg (220.5¬†lbs)\n1. OvergrowChlorophyll (hidden ability)\n0003 (Red/Blue/Yellow)0228 (Gold/Silver/Crysta...\n2 Sp. Atk, 1 Sp. Def\n45 (5.9% with Pok√©Ball, full HP)\n...\nMedium Slow\nGrass, Monster\n87.5% male, 12.5% female\n20 (4,884‚Äì5,140 steps)\n80\n82\n83\n100\n100\n80\n\n\n3\n0004\ncharmander\nFire\nLizard Pok√©mon\n0.6¬†m (2‚Ä≤00‚Ä≥)\n8.5¬†kg (18.7¬†lbs)\n1. BlazeSolar Power (hidden ability)\n0004 (Red/Blue/Yellow)0229 (Gold/Silver/Crysta...\n1 Speed\n45 (5.9% with Pok√©Ball, full HP)\n...\nMedium Slow\nDragon, Monster\n87.5% male, 12.5% female\n20 (4,884‚Äì5,140 steps)\n39\n52\n43\n60\n50\n65\n\n\n4\n0005\ncharmeleon\nFire\nFlame Pok√©mon\n1.1¬†m (3‚Ä≤07‚Ä≥)\n19.0¬†kg (41.9¬†lbs)\n1. BlazeSolar Power (hidden ability)\n0005 (Red/Blue/Yellow)0230 (Gold/Silver/Crysta...\n1 Sp. Atk, 1 Speed\n45 (5.9% with Pok√©Ball, full HP)\n...\nMedium Slow\nDragon, Monster\n87.5% male, 12.5% female\n20 (4,884‚Äì5,140 steps)\n58\n64\n58\n80\n65\n80\n\n\n\n\n5 rows √ó 22 columns\n\n\n\n\n\n40.6.2.2 Etape 2: r√©cup√©rer et afficher des photos de Pokemon\nNous aimerions que vous r√©cup√©riez √©galement les images des 5 premiers pok√©mons\net que vous les enregistriez dans un dossier.\n\n\n Exercice 2b : Les pok√©mons (version guid√©e)\n\nLes URL des images des pokemon prennent la forme ‚Äúhttps://img.pokemondb.net/artwork/{pokemon}.jpg‚Äù.\nUtiliser les modules requests et shutil pour t√©l√©charger\net enregistrer en local les images.\nImporter ces images stock√©es au format JPEG dans Python gr√¢ce √† la fonction imread du package skimage.io"
  },
  {
    "objectID": "content/course/manipulation/04a_webscraping_TP/index.html#selenium-mimer-le-comportement-dun-utilisateur-internet",
    "href": "content/course/manipulation/04a_webscraping_TP/index.html#selenium-mimer-le-comportement-dun-utilisateur-internet",
    "title": "40¬† Webscraping avec python",
    "section": "40.7 Selenium : mimer le comportement d‚Äôun utilisateur internet",
    "text": "40.7 Selenium : mimer le comportement d‚Äôun utilisateur internet\nJusqu‚Äô√† pr√©sent,\nnous avons raisonn√© comme si nous connaissions toujours l‚Äôurl qui nous int√©resse.\nDe plus, les pages que nous visitons sont ‚Äústatiques‚Äù,\nelles ne d√©pendent pas d‚Äôune action ou d‚Äôune recherche de l‚Äôinternaute.\nNous allons voir √† pr√©sent comment nous en sortir pour remplir\ndes champs sur un site web et r√©cup√©rer ce qui nous int√©resse.\nLa r√©action d‚Äôun site web √† l‚Äôaction d‚Äôun utilisateur passe r√©guli√®rement par\nl‚Äôusage de JavaScript dans le monde du d√©veloppement web.\nLe package Selenium permet\nde reproduire, depuis un code automatis√©, le comportement\nmanuel d‚Äôun utilisateur. Il permet ainsi\nd‚Äôobtenir des informations du site qui ne sont pas dans le\ncode HTML mais qui apparaissent uniquement √† la suite de\nl‚Äôex√©cution de script JavaScript en arri√®re plan.\nSelenium se comporte comme un utilisateur lambda sur internet :\nil clique sur des liens, il remplit des formulaires, etc.\n\n40.7.1 Premier exemple en scrapant un moteur de recherche\nDans cet exemple, nous allons essayer d‚Äôaller sur le\nsite de Bing Actualit√©s\net entrer dans la barre de recherche un sujet donn√©.\nPour tester, nous allons faire une recherche avec le mot-cl√© ‚ÄúTrump‚Äù.\nL‚Äôinstallation de Selenium n√©cessite d‚Äôavoir Chromium qui est un\nnavigateur Google Chrome minimaliste.\nLa version de chromedriver\ndoit √™tre &gt;= 2.36 et d√©pend de la version de Chrome que vous avez sur votre environnement\nde travail. Pour installer cette version minimaliste de Chrome sur un environnement\nLinux, vous pouvez\nvous r√©f√©rer √† l‚Äôencadr√© d√©di√©\n\n\n `Installation de Selenium`\nD‚Äôabord, il convient d‚Äôinstaller les d√©pendances.\nSur Colab, vous pouvez utiliser les commandes suivantes:\n\n!sudo apt-get update\n!sudo apt install -y unzip xvfb libxi6 libgconf-2-4 -y\n!sudo apt install chromium-chromedriver -y\n!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n\nSi vous √™tes sur le SSP-Cloud, vous pouvez\nex√©cuter les commandes suivantes:\n\n!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb -O /tmp/chrome.deb\n!sudo apt-get update\n!sudo -E apt-get install -y /tmp/chrome.deb\n!pip install chromedriver-autoinstaller selenium\n\nimport chromedriver_autoinstaller\nchromedriver_autoinstaller.install()\n\nVous pouvez ensuite installer Selenium. Par\nexemple, depuis une\ncellule de Notebook:\n\n!pip install selenium\n\n\n\nApr√®s avoir install√© Chromium,\nil est n√©cessaire d‚Äôindiquer √† Python o√π\nle trouver. Si vous √™tes sur Linux et que vous\navez suivi les consignes pr√©c√©dentes, vous\npouvez faire:\n\nimport sys\nsys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\nimport selenium\npath_to_web_driver = \"chromedriver\"\n\nEn premier lieu, il convient d‚Äôinitialiser le comportement\nde Selenium en r√©pliquant les param√®tres\ndu navigateur. Pour cela, on va d‚Äôabord initialiser\nnotre navigateur avec quelques options:\n\nimport time\n\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\n\nchrome_options = webdriver.ChromeOptions()\nchrome_options.add_argument('--headless')\nchrome_options.add_argument('--no-sandbox')\n#chrome_options.add_argument('--verbose') \n\nPuis on lance le navigateur:\n\nfrom selenium.webdriver.chrome.service import Service\nservice = Service(executable_path=path_to_web_driver)\n\nbrowser = webdriver.Chrome(service=service,\n                           options=chrome_options)\n\nOn va sur le site de Bing Actualit√©s, et on lui indique le mot cl√© que nous souhaitons chercher.\nEn l‚Äôoccurrence, on s‚Äôint√©resse aux actualit√©s de Donald Trump.\nApr√®s avoir inspect√© la page depuis les outils de d√©veloppement du navigateur,\non voit que la barre de recherche est un √©lement du code appel√© q (comme query).\nOn va ainsi demander √† selenium de chercher cet √©l√©ment:\n\nbrowser.get('https://www.bing.com/news')\n\nsearch = browser.find_element(\"name\", \"q\")\nprint(search)\nprint([search.text, search.tag_name, search.id])\n\n# on envoie √† cet endroit le mot qu'on aurait tap√© dans la barre de recherche\nsearch.send_keys(\"Trump\")\n\nsearch_button = browser.find_element(\"xpath\", \"//input[@id='sb_form_go']\") \nsearch_button.click()\n\nselenium permet de capturer l‚Äôimage qu‚Äôon verrait dans le navigateur\navec get_screenshot_as_png. Cela peut √™tre utile pour v√©rifier qu‚Äôon\na fait la bonne action:\n\npng = browser.get_screenshot_as_png()\n\n\nfrom IPython.display import Image\nImage(png, width='500')\n\n\n\n\nEnfin, on peut extraire les r√©sultats. Plusieurs\nm√©thodes sont disponibles. La m√©thode la plus\npratique, lorsqu‚Äôelle est disponible,\nest d‚Äôutiliser le XPath qui est un chemin\nnon ambigu pour acc√©der √† un √©lement. En effet,\nplusieurs √©l√©ments peuvent partager la m√™me classe ou\nle m√™me attribut ce qui peut faire qu‚Äôune recherche\nde ce type peut renvoyer plusieurs √©chos.\nPour d√©terminer le XPath d‚Äôun objet, les outils\nde d√©veloppeurs de votre site web sont pratiques.\nPar exemple, sous Firefox, une fois que vous\navez trouv√© un √©l√©ment dans l‚Äôinspecteur, vous\npouvez faire click droit &gt; Copier &gt; XPath.\n\nfrom selenium.common.exceptions import StaleElementReferenceException\nlinks = browser.find_elements(\"xpath\", \"//div/a[@class='title'][@href]\")\n\nresults = []\nfor link in links:\n    try:\n        url = link.get_attribute('href')\n    except StaleElementReferenceException as e:\n        print(\"Issue with '{0}' and '{1}'\".format(url, link))\n        print(\"It might be due to slow javascript which produces the HTML page.\")\n    results.append(url)\n\nEnfin, pour mettre fin √† notre session, on demande\n√† Python de quitter le navigateur\n\nbrowser.quit()\n\nOn a obtenu les r√©sultats suivants:\n\nprint(results)\n\n['https://www.usatoday.com/story/news/politics/2023/07/18/trump-town-hall-indictments-voters-hannity/70423744007/', 'https://www.newsweek.com/donald-trump-indictment-claim-sparks-speculation-rudy-giuliani-flipped-1813804', 'https://www.msn.com/en-us/news/politics/trump-thinks-hes-about-to-be-arrested-again/ar-AA1e1skv', 'https://www.msn.com/en-us/news/other/in-rare-criticism-desantis-says-trump-should-have-responded-more-forcefully-jan-6/ar-AA1e2F3X']\n\n\nLes autres m√©thodes utiles de Selenium:\nfind_element(****).click() | Une fois qu‚Äôon a trouv√© un √©l√©ment r√©actif, notamment un bouton, on peut cliquer dessus pour activer une nouvelle page |\nfind_element(****).send_keys(\"toto\") | Une fois qu‚Äôon a trouv√© un √©l√©ment, notamment un champ o√π s‚Äôauthentifier, on peut envoyer une valeur, ici ‚Äútoto‚Äù.\n\n\n40.7.2 Utiliser selenium pour jouer √† 2048\nDans cet exemple, on utilise le module pour que Python\nappuie lui m√™me sur les touches du clavier afin de jouer √† 2048.\nNote : ce bout de code ne donne pas une solution √† 2048,\nil permet juste de voir ce qu‚Äôon peut faire avec Selenium\n\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.common.keys import Keys\n\n# on ouvre la page internet du jeu 2048\nservice = Service(executable_path=path_to_web_driver)\n\nbrowser = webdriver.Chrome(service=service)\nbrowser.get('https://play2048.co//')\n\n# Ce qu'on va faire : une boucle qui r√©p√®te inlassablement la m√™me chose : haut / droite / bas / gauche\n\n# on commence par cliquer sur la page pour que les touches sachent \nbrowser.find_element(\"class name\", 'grid-container').click()\ngrid = browser.find_element(\"tag name\", 'body')\n\n# pour savoir quels coups faire √† quel moment, on cr√©e un dictionnaire\ndirection = {0: Keys.UP, 1: Keys.RIGHT, 2: Keys.DOWN, 3: Keys.LEFT}\ncount = 0\n\nwhile True:\n    try: # on v√©rifie que le bouton \"Try again\" n'est pas l√† - sinon √ßa veut dire que le jeu est fini\n        retryButton = browser.find_element(\"link text\",'Try again')\n        scoreElem = browser.find_element(\"class name\", 'score-container')\n        break\n    except:\n        #Do nothing.  Game is not over yet\n        pass\n    # on continue le jeu - on appuie sur la touche suivante pour le coup d'apr√®s\n    count += 1\n    grid.send_keys(direction[count % 4]) \n    time.sleep(0.1)\n\nprint('Score final : {} en {} coups'.format(scoreElem.text, count))    \nbrowser.quit()"
  },
  {
    "objectID": "content/course/manipulation/04a_webscraping_TP/index.html#exercices-suppl√©mentaires",
    "href": "content/course/manipulation/04a_webscraping_TP/index.html#exercices-suppl√©mentaires",
    "title": "40¬† Webscraping avec python",
    "section": "40.8 Exercices suppl√©mentaires",
    "text": "40.8 Exercices suppl√©mentaires\n\n40.8.1 R√©cup√©rer les noms et √¢ges des ministres fran√ßais\nPour cet exercice, on propose de scraper la liste des ministres fran√ßais depuis le site du gouvernement. L‚Äôobjectif sera, in fine de faire un graphique qui repr√©sente la distribution de leurs √¢ges.\nLa solution pour cet exercice a √©t√© propos√©e\npar Tien-Thinh\net Antoine Palazzolo.\nPour √™tre en mesure de faire cet exercice, il est\nrecommand√© d‚Äôinstaller le package dateparser\n\n!pip install dateparser\n#depuis un notebook. En ligne de commande, retirer le !\n\nPour cet exercice, nous proposons d‚Äôutiliser les packages\nsuivants:\n\nimport time\nfrom tqdm import tqdm\nimport urllib\nimport re, datetime\nfrom dateutil.parser import parse as parse_dt\nimport dateparser\n\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd\nimport bs4\n\nNous proposons √©galement d‚Äôutiliser la fonction suivante\npour calculer l‚Äô√¢ge √† partir de la date de naissance.\n\ndef from_birth_to_age(birth):\n    today = datetime.date.today()\n    return today.year - birth.year - ((today.month, today.day) &lt; (birth.month, birth.day))\n\n\n\n Exercice : Les ministres fran√ßais \n\nCr√©er des variables globales url_gouvernement et url_gouvernement qui repr√©senteront,\nrespectivement, la racine de l‚ÄôURL du site web et le chemin au sein de celui-ci ;\nUtiliser bs4 pour r√©cup√©rer la composition du gouvernement, qui est contenue dans un &lt;div&gt;\nayant une classe ad hoc. Nommer cet objet compo\nUtiliser find_all pour r√©cup√©rer la liste des ministres dans compo. Nommer\ncet objet ministres\nInspecter la structure des champs au sein de ministres. R√©p√©rer les id biography. Comme\nla structure est g√©n√©rique, on va √©crire une fonction from_bio_to_age sur laquelle on va it√©rer\npour chaque √©l√©ment de la liste ministres. Cette fonction effectuera les op√©rations suivantes:\n\nRemplacer les champs de dates de naissance non num√©riques (par exemple ‚Äú1er‚Äù), en valeur num√©rique (par exemple 1).\nUtiliser la regex [0-3]?\\d \\S* \\d{4} avec le package re pour extraire les dates\nde naissance. Nommer l‚Äôobjet str_date.\nAppliquer dateparser.parse pour convertir sous forme de date\nAppliquer from_birth_to_age pour transformer cette date de naissance en √¢ge\n\nPour chaque √©l√©ment de la liste ministres, faire une boucle (en introduisant un\ntime.sleep(0.25) entre chaque it√©ration pour ne pas surcharger le site):\n\nR√©cup√©rer les noms et pr√©noms, fonctions pour chaque ministre\nR√©cup√©rer l‚ÄôURL de la photo\nCr√©er un URL pour chaque ministre afin d‚Äôappliquer la fonction\nfrom_bio_to_age\n\nUtiliser matplotlib ou seaborn pour faire un histogramme d‚Äô√¢ge\n\n\n\nA l‚Äôissue de la question 4, on devrait\nretrouver les informations suivantes:\n\nprint(f\"Nous retrouvons ainsi {len(ministres)} ministres.\")\n\nNous retrouvons ainsi 44 ministres.\n\n\n\ndef from_bio_to_age(url):\n    html = urllib.request.urlopen(url).read()\n    page = bs4.BeautifulSoup(html)\n    s = page.find(\"div\", {\"id\":\"biography\"}).text.replace(\"1er\", \"1\") # un peu ad hoc\n    expression = re.compile(\"[0-3]?\\d \\S* \\d{4}\") # renvoie parfois des dates autres que dates de naissance\n    str_date = expression.findall(s)[0]\n    date_de_naissance = dateparser.parse(str_date).date()\n    return from_birth_to_age(date_de_naissance)\n\nIn fine, on obtient une liste dont le premier √©l√©ment\nprend la forme suivante:\n\nliste[0]\n\n{'Nom complet': '√âlisabeth Borne',\n 'Fonction': 'Premi√®re ministre',\n 'Photo': 'https://www.gouvernement.fr/sites/default/files/styles/composition_large/public/pm_elisabeth_borne_portrait_matignon_.jpg?itok=ay_WErm3',\n 'href': 'https://www.gouvernement.fr/ministre/elisabeth-borne',\n 'Age': nan}\n\n\nFinalement, le DataFrame pourra √™tre\nstructur√© sous la forme suivante. On va √©liminer\nles √¢ges √©gaux √† 0 sont qui sont des erreurs\nde scraping:\nlorsque la date de naissance compl√®te n‚Äôest pas disponible\nsur la biographie d‚Äôun ministre.\n\ndf = pd.DataFrame(liste)\ndf = df.loc[df['Age'] != 0]\ndf.head(3)\n\n\n\n\n\n\n\n\nNom complet\nFonction\nPhoto\nhref\nAge\n\n\n\n\n0\n√âlisabeth Borne\nPremi√®re ministre\nhttps://www.gouvernement.fr/sites/default/file...\nhttps://www.gouvernement.fr/ministre/elisabeth...\nNaN\n\n\n1\nOlivier V√©ran\nMinistre d√©l√©gu√© aupr√®s de la Premi√®re ministr...\nhttps://www.gouvernement.fr/sites/default/file...\nhttps://www.gouvernement.fr/ministre/olivier-v...\nNaN\n\n\n2\nFranck Riester\nMinistre d√©l√©gu√© aupr√®s de la Premi√®re ministr...\nhttps://www.gouvernement.fr/sites/default/file...\nhttps://www.gouvernement.fr/ministre/franck-ri...\nNaN\n\n\n\n\n\n\n\nFinalement, l‚Äôhistogramme aura l‚Äôaspect suivant:\n\nplt.hist(df.Age, bins=np.arange(25, 80, 4))\n\n/miniconda/envs/python-ENSAE/lib/python3.9/site-packages/matplotlib/axes/_axes.py:6762: RuntimeWarning:\n\nAll-NaN slice encountered\n\n/miniconda/envs/python-ENSAE/lib/python3.9/site-packages/matplotlib/axes/_axes.py:6763: RuntimeWarning:\n\nAll-NaN slice encountered\n\n\n\n(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n array([25., 29., 33., 37., 41., 45., 49., 53., 57., 61., 65., 69., 73.,\n        77.]),\n &lt;BarContainer object of 13 artists&gt;)"
  },
  {
    "objectID": "content/course/manipulation/02a_pandas_tutorial/index.html#logique-de-pandas",
    "href": "content/course/manipulation/02a_pandas_tutorial/index.html#logique-de-pandas",
    "title": "41¬† Introduction √† Pandas",
    "section": "41.1 Logique de pandas",
    "text": "41.1 Logique de pandas\nL‚Äôobjet central dans la logique pandas est le DataFrame.\nIl s‚Äôagit d‚Äôune structure particuli√®re de donn√©es\n√† deux dimensions, structur√©es en alignant des lignes et colonnes. Les colonnes\npeuvent √™tre de type diff√©rent.\nUn DataFrame est compos√© des √©l√©ments suivants:\n\nl‚Äôindice de la ligne ;\nle nom de la colonne ;\nla valeur de la donn√©e ;\n\nStructuration d‚Äôun DataFrame pandas,\nemprunt√©e √† https://medium.com/epfl-extension-school/selecting-data-from-a-pandas-dataframe-53917dc39953:\n\nLe concept de tidy data, popularis√© par Hadley Wickham via ses packages R,\nest parfaitement pertinent pour d√©crire la structure d‚Äôun DataFrame pandas.\nLes trois r√®gles sont les suivantes:\n\nChaque variable poss√®de sa propre colonne ;\nChaque observation poss√®de sa propre ligne ;\nUne valeur, mat√©rialisant une observation d‚Äôune variable,\nse trouve sur une unique cellule.\n\n\n\n\nConcept de tidy data (emprunt√© √† H. Wickham)\n\n\n\n\n Hint\nLes DataFrames sont assez rapides en Python[^2] et permettent de traiter en local de mani√®re efficace des tables de\ndonn√©es comportant plusieurs millions d‚Äôobservations (en fonction de la configuration de l‚Äôordinateur)\net dont la volum√©trie peut √™tre cons√©quente (plusieurs centaines\nde Mo). N√©anmoins, pass√© un certain seuil, qui d√©pend de la puissance de la machine mais aussi de la complexit√©\nde l‚Äôop√©ration effectu√©e, le DataFrame pandas peut montrer certaines limites. Dans ce cas, il existe diff√©rentes\nsolutions: Dask (dataframe aux op√©rations parall√©lis√©s), SQL (notamment Postgres),\nSpark (solution big data). Un chapitre sp√©cial de ce cours est consacr√© √† Dask.\n\n\n::: {.cell .markdown}\n[^2]: En R, les deux formes de dataframes qui se sont impos√©es r√©cemment sont les tibbles (package dplyr)\net les data.tables (package data.table). dplyr reprend la syntaxe SQL de mani√®re relativement\ntransparente ce qui rend la syntaxe tr√®s proche de celle de pandas. Cependant,\nalors que dplyr supporte tr√®s mal les donn√©es dont la volum√©trie d√©passe 1Go, pandas s‚Äôen\naccomode bien. Les performances de pandas sont plus proches de celles de data.table, qui est\nconnu pour √™tre une approche efficace avec des donn√©es de taille importante."
  },
  {
    "objectID": "content/course/manipulation/02a_pandas_tutorial/index.html#les-series",
    "href": "content/course/manipulation/02a_pandas_tutorial/index.html#les-series",
    "title": "41¬† Introduction √† Pandas",
    "section": "41.2 Les Series",
    "text": "41.2 Les Series\nEn fait, un DataFrame est une collection d‚Äôobjets appel√©s pandas.Series.\nCes Series sont des objets d‚Äôune dimension qui sont des extensions des\narray-unidimensionnels numpy. En particulier, pour faciliter le traitement\nde donn√©es cat√©gorielles ou temporelles, des types de variables\nsuppl√©mentaires sont disponibles dans pandas par rapport √†\nnumpy (categorical, datetime64 et timedelta64). Ces\ntypes sont associ√©s √† des m√©thodes optimis√©es pour faciliter le traitement\nde ces donn√©es.\nIl ne faut pas n√©gliger l‚Äôattribut dtype d‚Äôun objet\npandas.Series car cela a une influence d√©terminante sur les m√©thodes\net fonctions pouvant √™tre utilis√©es (on ne fait pas les m√™mes op√©rations\nsur une donn√©e temporelle et une donn√©e cat√©gorielle) et le volume en\nm√©moire d‚Äôune variable (le type de la variable d√©termine le volume\nd‚Äôinformation stock√© pour chaque √©l√©ment ; √™tre trop pr√©cis est parfois\nn√©faste).\nIl existe plusieurs types possibles pour un pandas.Series.\nLe type object correspond aux types Python str ou mixed.\nIl existe un type particulier pour les variables dont le nombre de valeurs\nest une liste finie et relativement courte, le type category.\nIl faut bien examiner les types de son DataFrame, et convertir √©ventuellement\nles types lors de l‚Äô√©tape de data cleaning.\n\n41.2.1 Indexation\nLa diff√©rence essentielle entre une Series et un objet numpy est l‚Äôindexation.\nDans numpy,\nl‚Äôindexation est implicite ; elle permet d‚Äôacc√©der √† une donn√©e (celle √†\nl‚Äôindex situ√© √† la position i).\nAvec une Series, on peut bien-s√ªr utiliser un indice de position mais on peut\nsurtout faire appel √† des indices plus explicites.\nPar exemple,\n\ntaille = pd.Series(\n    [1.,1.5,1],\n    index = ['chat', 'chien', 'koala']\n)\n\ntaille.head()\n\nchat     1.0\nchien    1.5\nkoala    1.0\ndtype: float64\n\n\nCette indexation permet d‚Äôacc√©der √† des valeurs de la Series\nvia une valeur de l‚Äôindice. Par\nexemple, taille['koala']:\n\ntaille['koala']\n\n1.0\n\n\nL‚Äôexistence d‚Äôindice rend le subsetting particuli√®rement ais√©, ce que vous\npouvez exp√©rimenter dans les TP\n\n\n\n\n\n\n\n\n\n\n\n\nPour transformer un objet pandas.Series en array numpy,\non utilise la m√©thode values. Par exemple, taille.values:\n\ntaille.values\n\narray([1. , 1.5, 1. ])\n\n\nUn avantage des Series par rapport √† un array numpy est que\nles op√©rations sur les Series alignent\nautomatiquement les donn√©es √† partir des labels.\nAvec des Series lab√©lis√©es, il n‚Äôest ainsi pas n√©cessaire\nde se poser la question de l‚Äôordre des lignes.\nL‚Äôexemple dans la partie suivante permettra de s‚Äôen assurer.\n\n\n41.2.2 Valeurs manquantes\nPar d√©faut, les valeurs manquantes sont affich√©es NaN et sont de type np.nan (pour\nles valeurs temporelles, i.e.¬†de type datatime64, les valeurs manquantes sont\nNaT).\nOn a un comportement coh√©rent d‚Äôagr√©gation lorsqu‚Äôon combine deux DataFrames (ou deux colonnes).\nPar exemple,\n\nx = pd.DataFrame(\n    {'prix': np.random.uniform(size = 5),\n     'quantite': [i+1 for i in range(5)]\n    },\n    index = ['yaourt','pates','riz','tomates','gateaux']\n)\nx\n\n\n\n\n\n\n\n\nprix\nquantite\n\n\n\n\nyaourt\n0.696469\n1\n\n\npates\n0.286139\n2\n\n\nriz\n0.226851\n3\n\n\ntomates\n0.551315\n4\n\n\ngateaux\n0.719469\n5\n\n\n\n\n\n\n\n\ny = pd.DataFrame(\n    {'prix': [np.nan, 0, 1, 2, 3],\n     'quantite': [i+1 for i in range(5)]\n    },\n    index = ['tomates','yaourt','gateaux','pates','riz']\n)\ny\n\n\n\n\n\n\n\n\nprix\nquantite\n\n\n\n\ntomates\nNaN\n1\n\n\nyaourt\n0.0\n2\n\n\ngateaux\n1.0\n3\n\n\npates\n2.0\n4\n\n\nriz\n3.0\n5\n\n\n\n\n\n\n\n\nx + y\n\n\n\n\n\n\n\n\nprix\nquantite\n\n\n\n\ngateaux\n1.719469\n8\n\n\npates\n2.286139\n6\n\n\nriz\n3.226851\n8\n\n\ntomates\nNaN\n5\n\n\nyaourt\n0.696469\n3\n\n\n\n\n\n\n\ndonne bien une valeur manquante pour la ligne tomates. Au passage, on peut remarquer que l‚Äôagr√©gation\na tenu compte des index.\nIl est possible de supprimer les valeurs manquantes gr√¢ce √† dropna().\nCette m√©thode va supprimer toutes les lignes o√π il y a au moins une valeur manquante.\nIl est aussi possible de supprimer seulement les colonnes o√π il y a des valeurs manquantes\ndans un DataFrame avec dropna() avec le param√®tre axis=1 (par d√©faut √©gal √† 0).\nIl est √©galement possible de remplir les valeurs manquantes gr√¢ce √† la m√©thode fillna()."
  },
  {
    "objectID": "content/course/manipulation/02a_pandas_tutorial/index.html#le-dataframe-pandas",
    "href": "content/course/manipulation/02a_pandas_tutorial/index.html#le-dataframe-pandas",
    "title": "41¬† Introduction √† Pandas",
    "section": "41.3 Le DataFrame Pandas",
    "text": "41.3 Le DataFrame Pandas\nLe DataFrame est l‚Äôobjet central de la librairie pandas.\nIl s‚Äôagit d‚Äôune collection de pandas.Series (colonnes) align√©es par les index.\nLes types des variables peuvent diff√©rer.\nUn DataFrame non-index√© a la structure suivante:\n\n\n\n\n\n\n\n\n\nindex\ntaille\npoids\n\n\n\n\n0\nchat\n1.0\n3.0\n\n\n1\nchien\n1.5\n5.0\n\n\n2\nkoala\n1.0\n2.5\n\n\n\n\n\n\n\nAlors que le m√™me DataFrame index√© aura la structure suivante:\n\n\n\n\n\n\n\n\n\ntaille\npoids\n\n\n\n\nchat\n1.0\n3.0\n\n\nchien\n1.5\n5.0\n\n\nkoala\n1.0\n2.5\n\n\n\n\n\n\n\n\n41.3.1 Les attributs et m√©thodes utiles\nPour pr√©senter les m√©thodes les plus pratiques pour l‚Äôanalyse de donn√©es,\non peut partir de l‚Äôexemple des consommations de CO2 communales issues\ndes donn√©es de l‚ÄôAdeme. Cette base de donn√©es est exploit√©e plus intens√©ment\ndans le TP.\n\n\n\n\n\n\n\n\n\n\n\n\nL‚Äôimport de donn√©es depuis un fichier plat se fait avec la fonction read_csv:\n\ndf = pd.read_csv(\"https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert\")\ndf\n\n\n\n\n\n\n\n\nINSEE commune\nCommune\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nD√©chets\nEnergie\nIndustrie hors-√©nergie\nR√©sidentiel\nRoutier\nTertiaire\n\n\n\n\n0\n01001\nL'ABERGEMENT-CLEMENCIAT\n3711.425991\nNaN\nNaN\n432.751835\n101.430476\n2.354558\n6.911213\n309.358195\n793.156501\n367.036172\n\n\n1\n01002\nL'ABERGEMENT-DE-VAREY\n475.330205\nNaN\nNaN\n140.741660\n140.675439\n2.354558\n6.911213\n104.866444\n348.997893\n112.934207\n\n\n2\n01004\nAMBERIEU-EN-BUGEY\n499.043526\n212.577908\nNaN\n10313.446515\n5314.314445\n998.332482\n2930.354461\n16616.822534\n15642.420313\n10732.376934\n\n\n3\n01005\nAMBERIEUX-EN-DOMBES\n1859.160954\nNaN\nNaN\n1144.429311\n216.217508\n94.182310\n276.448534\n663.683146\n1756.341319\n782.404357\n\n\n4\n01006\nAMBLEON\n448.966808\nNaN\nNaN\n77.033834\n48.401549\nNaN\nNaN\n43.714019\n398.786800\n51.681756\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n35793\n95676\nVILLERS-EN-ARTHIES\n1628.065094\nNaN\nNaN\n165.045396\n65.063617\n11.772789\n34.556067\n176.098160\n309.627908\n235.439109\n\n\n35794\n95678\nVILLIERS-ADAM\n698.630772\nNaN\nNaN\n1331.126598\n111.480954\n2.354558\n6.911213\n1395.529811\n18759.370071\n403.404815\n\n\n35795\n95680\nVILLIERS-LE-BEL\n107.564967\nNaN\nNaN\n8367.174532\n225.622903\n534.484607\n1568.845431\n22613.830247\n12217.122402\n13849.512001\n\n\n35796\n95682\nVILLIERS-LE-SEC\n1090.890170\nNaN\nNaN\n326.748418\n108.969749\n2.354558\n6.911213\n67.235487\n4663.232127\n85.657725\n\n\n35797\n95690\nWY-DIT-JOLI-VILLAGE\n1495.103542\nNaN\nNaN\n125.236417\n97.728612\n4.709115\n13.822427\n117.450851\n504.400972\n147.867245\n\n\n\n\n35798 rows √ó 12 columns\n\n\n\n\n\n Note\nDans un processus de production, o√π normalement on connait les types des variables du DataFrame qu‚Äôon va importer,\nil convient de pr√©ciser les types avec lesquels on souhaite importer les donn√©es\n(argument dtype, sous la forme d‚Äôun dictionnaire).\nCela est particuli√®rement important lorsqu‚Äôon d√©sire utiliser une colonne\ncomme une variable textuelle mais qu‚Äôelle comporte des attributs proches d‚Äôun nombre\nqui vont inciter pandas √† l‚Äôimporter sous forme de variable num√©rique.\nPar exemple, une colonne [00001,00002,...] risque d‚Äô√™tre import√©e comme une variable num√©rique, ignorant l‚Äôinformation des premiers 0 (qui peuvent pourtant la distinguer de la s√©quence 1, 2, etc.). Pour s‚Äôassurer que pandas importe sous forme textuelle la variable, on peut utiliser dtype = {\"code\": \"str\"}\nSinon, on peut importer le csv, et modifier les types avec astype().\nAvec astype, on peut g√©rer les erreurs de conversion avec le param√®tre errors.\n\n\nL‚Äôaffichage des DataFrames est tr√®s ergonomique. On obtiendrait le m√™me output\navec display(df)2. Les premi√®res et derni√®res lignes s‚Äôaffichent\nautomatiquement. Autrement, on peut aussi faire:\n\nhead qui permet, comme son\nnom l‚Äôindique, de n‚Äôafficher que les premi√®res lignes ;\ntail qui permet, comme son\nnom l‚Äôindique, de n‚Äôafficher que les derni√®res lignes\nsample qui permet d‚Äôafficher un √©chantillon al√©atoire de n lignes.\nCette m√©thode propose de nombreuses options.\n\n\n\n\n Warning\nIl faut faire attention au display et aux\ncommandes qui r√©v√®lent des donn√©es (head, tail, etc.)\ndans un Notebook ou un Markdown qui exploite\ndes donn√©es confidentielles lorsqu‚Äôon utilise Git.\nEn effet, on peut se\nretrouver √† partager des donn√©es, involontairement, dans l‚Äôhistorique\nGit. Avec un R Markdown, il suffit d‚Äôajouter les sorties au fichier\n.gitignore (par exemple avec une balise de type *.html). Avec un\nNotebook Jupyter, la d√©marche est plus compliqu√©e car les fichiers\n.ipynb int√®grent dans le m√™me document, texte, sorties et mise en forme.\nTechniquement, il est possible d‚Äôappliquer des filtres avec Git\n(voir\nici)\nmais c‚Äôest une d√©marche tr√®s complexe.\nCe post de l‚Äô√©quipe √† l‚Äôorigine de nbdev2\nr√©sume bien le probl√®me du contr√¥le de version avec Git et des solutions qui\npeuvent y √™tre apport√©es.\nUne solution est d‚Äôutiliser Quarto qui permet de g√©n√©rer les\n.ipynb en output d‚Äôun document texte, ce qui facilite le contr√¥le sur les\n√©l√©ments pr√©sents dans le document.\n\n\n\n41.3.2 Dimensions et structure du DataFrame\nLes premi√®res m√©thodes utiles permettent d‚Äôafficher quelques\nattributs d‚Äôun DataFrame.\n\ndf.axes\n\n[RangeIndex(start=0, stop=35798, step=1),\n Index(['INSEE commune', 'Commune', 'Agriculture', 'Autres transports',\n        'Autres transports international', 'CO2 biomasse hors-total', 'D√©chets',\n        'Energie', 'Industrie hors-√©nergie', 'R√©sidentiel', 'Routier',\n        'Tertiaire'],\n       dtype='object')]\n\n\n\ndf.columns\n\nIndex(['INSEE commune', 'Commune', 'Agriculture', 'Autres transports',\n       'Autres transports international', 'CO2 biomasse hors-total', 'D√©chets',\n       'Energie', 'Industrie hors-√©nergie', 'R√©sidentiel', 'Routier',\n       'Tertiaire'],\n      dtype='object')\n\n\n\ndf.index\n\nRangeIndex(start=0, stop=35798, step=1)\n\n\nPour conna√Ætre les dimensions d‚Äôun DataFrame, on peut utiliser quelques m√©thodes\npratiques:\n\ndf.ndim\n\n2\n\n\n\ndf.shape\n\n(35798, 12)\n\n\n\ndf.size\n\n429576\n\n\nPour d√©terminer le nombre de valeurs uniques d‚Äôune variable, plut√¥t que chercher √† √©crire soi-m√™me une fonction,\non utilise la\nm√©thode nunique. Par exemple,\n\ndf['Commune'].nunique()\n\n33338\n\n\npandas propose √©norm√©ment de m√©thodes utiles.\nVoici un premier r√©sum√©, accompagn√© d‚Äôun comparatif avec R\n\n\n\n\n\n\n\n\n\nOp√©ration\npandas\ndplyr (R)\ndata.table (R)\n\n\n\n\nR√©cup√©rer le nom des colonnes\ndf.columns\ncolnames(df)\ncolnames(df)\n\n\nR√©cup√©rer les indices[^4]\ndf.index\n\nunique(df[,get(key(df))])\n\n\nR√©cup√©rer les dimensions\ndf.shape\nc(nrow(df), ncol(df))\nc(nrow(df), ncol(df))\n\n\nR√©cup√©rer le nombre de valeurs uniques d‚Äôune variable\ndf['myvar'].nunique()\ndf %&gt;%  summarise(distinct(myvar))\ndf[,uniqueN(myvar)]\n\n\n\n::: {.cell .markdown}\n[^4]: Le principe d‚Äôindice n‚Äôexiste pas dans dplyr. Ce qui s‚Äôapproche le plus des indices, au sens de\npandas, sont les cl√©s en data.table.\n\n\n\n\n41.3.3 Statistiques agr√©g√©es\npandas propose une s√©rie de m√©thodes pour faire des statistiques\nagr√©g√©es de mani√®re efficace.\nOn peut, par exemple, appliquer des m√©thodes pour compter le nombre de lignes,\nfaire une moyenne ou une somme de l‚Äôensemble des lignes\n\ndf.count()\n\nINSEE commune                      35798\nCommune                            35798\nAgriculture                        35736\nAutres transports                   9979\nAutres transports international     2891\nCO2 biomasse hors-total            35798\nD√©chets                            35792\nEnergie                            34490\nIndustrie hors-√©nergie             34490\nR√©sidentiel                        35792\nRoutier                            35778\nTertiaire                          35798\ndtype: int64\n\n\n\ndf.mean(numeric_only = True)\n\nAgriculture                        2459.975760\nAutres transports                   654.919940\nAutres transports international    7692.344960\nCO2 biomasse hors-total            1774.381550\nD√©chets                             410.806329\nEnergie                             662.569846\nIndustrie hors-√©nergie             2423.127789\nR√©sidentiel                        1783.677872\nRoutier                            3535.501245\nTertiaire                          1105.165915\ndtype: float64\n\n\n\ndf.sum(numeric_only = True)\n\nAgriculture                        8.790969e+07\nAutres transports                  6.535446e+06\nAutres transports international    2.223857e+07\nCO2 biomasse hors-total            6.351931e+07\nD√©chets                            1.470358e+07\nEnergie                            2.285203e+07\nIndustrie hors-√©nergie             8.357368e+07\nR√©sidentiel                        6.384140e+07\nRoutier                            1.264932e+08\nTertiaire                          3.956273e+07\ndtype: float64\n\n\n\ndf.nunique()\n\nINSEE commune                      35798\nCommune                            33338\nAgriculture                        35576\nAutres transports                   9963\nAutres transports international     2883\nCO2 biomasse hors-total            35798\nD√©chets                            11016\nEnergie                             1453\nIndustrie hors-√©nergie              1889\nR√©sidentiel                        35791\nRoutier                            35749\nTertiaire                           8663\ndtype: int64\n\n\n\ndf.quantile(q = [0.1,0.25,0.5,0.75,0.9], numeric_only = True)\n\n\n\n\n\n\n\n\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nD√©chets\nEnergie\nIndustrie hors-√©nergie\nR√©sidentiel\nRoutier\nTertiaire\n\n\n\n\n0.10\n382.620882\n25.034578\n4.430792\n109.152816\n14.811230\n2.354558\n6.911213\n50.180933\n199.765410\n49.289082\n\n\n0.25\n797.682631\n52.560412\n10.050967\n197.951108\n25.655166\n2.354558\n6.911213\n96.052911\n419.700460\n94.749885\n\n\n0.50\n1559.381285\n106.795928\n19.924343\n424.849988\n54.748653\n4.709115\n13.822427\n227.091193\n1070.895593\n216.297718\n\n\n0.75\n3007.883903\n237.341501\n32.983111\n1094.749825\n110.820941\n51.800270\n152.046694\n749.469293\n3098.612157\n576.155869\n\n\n0.90\n5442.727470\n528.349529\n59.999169\n3143.759029\n190.695774\n367.311008\n1154.172630\n2937.699671\n8151.047248\n1897.732565\n\n\n\n\n\n\n\n\n\n Warning\nLa version 2.0 de Pandas a introduit un changement\nde comportement dans les m√©thodes d‚Äôagr√©gation.\nIl est dor√©navant n√©cessaire de pr√©ciser quand on d√©sire\neffectuer des op√©rations si on d√©sire ou non le faire\nexclusivement sur les colonnes num√©riques. C‚Äôest pour cette\nraison qu‚Äôon exlicite ici l‚Äôargument numeric_only = True.\nCe comportement\n√©tait par le pass√© implicite.\n\n\nIl faut toujours regarder les options de ces fonctions en termes de valeurs manquantes, car\nces options sont d√©terminantes dans le r√©sultat obtenu.\nLes exercices de TD visent √† d√©montrer l‚Äôint√©r√™t de ces m√©thodes dans quelques cas pr√©cis.\n\n\n\n\n\n\n\n\n\n\n\n\nLe tableau suivant r√©capitule le code √©quivalent pour avoir des\nstatistiques sur toutes les colonnes d‚Äôun dataframe en R.\n\n\n\n\n\n\n\n\n\nOp√©ration\npandas\ndplyr (R)\ndata.table (R)\n\n\n\n\nNombre de valeurs non manquantes\ndf.count()\ndf %&gt;% summarise_each(funs(sum(!is.na(.))))\ndf[, lapply(.SD, function(x) sum(!is.na(x)))]\n\n\nMoyenne de toutes les variables\ndf.mean()\ndf %&gt;% summarise_each(funs(mean((., na.rm = TRUE))))\ndf[,lapply(.SD, function(x) mean(x, na.rm = TRUE))]\n\n\n\nLa m√©thode describe permet de sortir un tableau de statistiques\nagr√©g√©es:\n\ndf.describe()\n\n\n\n\n\n\n\n\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nD√©chets\nEnergie\nIndustrie hors-√©nergie\nR√©sidentiel\nRoutier\nTertiaire\n\n\n\n\ncount\n35736.000000\n9979.000000\n2.891000e+03\n35798.000000\n35792.000000\n3.449000e+04\n3.449000e+04\n35792.000000\n35778.000000\n35798.000000\n\n\nmean\n2459.975760\n654.919940\n7.692345e+03\n1774.381550\n410.806329\n6.625698e+02\n2.423128e+03\n1783.677872\n3535.501245\n1105.165915\n\n\nstd\n2926.957701\n9232.816833\n1.137643e+05\n7871.341922\n4122.472608\n2.645571e+04\n5.670374e+04\n8915.902379\n9663.156628\n5164.182507\n\n\nmin\n0.003432\n0.000204\n3.972950e-04\n3.758088\n0.132243\n2.354558e+00\n1.052998e+00\n1.027266\n0.555092\n0.000000\n\n\n25%\n797.682631\n52.560412\n1.005097e+01\n197.951108\n25.655166\n2.354558e+00\n6.911213e+00\n96.052911\n419.700460\n94.749885\n\n\n50%\n1559.381285\n106.795928\n1.992434e+01\n424.849988\n54.748653\n4.709115e+00\n1.382243e+01\n227.091193\n1070.895593\n216.297718\n\n\n75%\n3007.883903\n237.341501\n3.298311e+01\n1094.749825\n110.820941\n5.180027e+01\n1.520467e+02\n749.469293\n3098.612157\n576.155869\n\n\nmax\n98949.317760\n513140.971691\n3.303394e+06\n576394.181208\n275500.374439\n2.535858e+06\n6.765119e+06\n410675.902028\n586054.672836\n288175.400126\n\n\n\n\n\n\n\n\n\n41.3.4 M√©thodes relatives aux valeurs manquantes\nLes m√©thodes relatives aux valeurs manquantes peuvent √™tre mobilis√©es\nen conjonction des m√©thodes de statistiques agr√©g√©es. C‚Äôest utiles lorsqu‚Äôon\nd√©sire obtenir une id√©e de la part de valeurs manquantes dans un jeu de\ndonn√©es\n\ndf.isnull().sum()\n\nINSEE commune                          0\nCommune                                0\nAgriculture                           62\nAutres transports                  25819\nAutres transports international    32907\nCO2 biomasse hors-total                0\nD√©chets                                6\nEnergie                             1308\nIndustrie hors-√©nergie              1308\nR√©sidentiel                            6\nRoutier                               20\nTertiaire                              0\ndtype: int64\n\n\nOn trouvera aussi la r√©f√©rence √† isna() qui est la m√™me m√©thode que isnull()."
  },
  {
    "objectID": "content/course/manipulation/02a_pandas_tutorial/index.html#graphiques-rapides",
    "href": "content/course/manipulation/02a_pandas_tutorial/index.html#graphiques-rapides",
    "title": "41¬† Introduction √† Pandas",
    "section": "41.4 Graphiques rapides",
    "text": "41.4 Graphiques rapides\nLes m√©thodes par d√©faut de graphique\n(approfondies dans la partie visualisation)\nsont pratiques pour\nproduire rapidement un graphique, notamment apr√®s des op√©rations\ncomplexes de maniement de donn√©es.\nEn effet, on peut appliquer la m√©thode plot() directement √† une pandas.Series:\n\ndf['D√©chets'].plot()\ndf['D√©chets'].hist()\ndf['D√©chets'].plot(kind = 'hist', logy = True)\n\n\nplt.figure()\nfig = df['D√©chets'].plot()\nfig\n#plt.savefig('plot_base.png', bbox_inches='tight')\n\nplt.figure()\nfig = df['D√©chets'].hist()\nfig\n#plt.savefig('plot_hist.png', bbox_inches='tight')\n\nplt.figure()\nfig = df['D√©chets'].plot(kind = 'hist', logy = True)\nfig\n#plt.show()\n#plt.savefig('plot_hist_log.png', bbox_inches='tight')\n\n&lt;Axes: ylabel='Frequency'&gt;\n\n\n\n\n\n\n\n\n\n\n\nLa sortie est un objet matplotlib. La customisation de ces\nfigures est ainsi\npossible (et m√™me d√©sirable car les graphiques matplotlib\nsont, par d√©faut, assez rudimentaires), nous en verrons quelques exemples."
  },
  {
    "objectID": "content/course/manipulation/02a_pandas_tutorial/index.html#acc√©der-√†-des-√©l√©ments-dun-dataframe",
    "href": "content/course/manipulation/02a_pandas_tutorial/index.html#acc√©der-√†-des-√©l√©ments-dun-dataframe",
    "title": "41¬† Introduction √† Pandas",
    "section": "41.5 Acc√©der √† des √©l√©ments d‚Äôun DataFrame",
    "text": "41.5 Acc√©der √† des √©l√©ments d‚Äôun DataFrame\n\n41.5.1 S√©lectionner des colonnes\nEn SQL, effectuer des op√©rations sur les colonnes se fait avec la commande\nSELECT. Avec pandas,\npour acc√©der √† une colonne dans son ensemble on peut\nutiliser plusieurs approches:\n\ndataframe.variable, par exemple df.Energie.\nCette m√©thode requiert n√©anmoins d‚Äôavoir des\nnoms de colonnes sans espace.\ndataframe[['variable']] pour renvoyer la variable sous\nforme de DataFrame ou dataframe['variable'] pour\nla renvoyer sous forme de Series. Par exemple, df[['Autres transports']]\nou df['Autres transports']. C‚Äôest une mani√®re pr√©f√©rable de proc√©der.\n\n\n\n41.5.2 Acc√©der √† des lignes\nPour acc√©der √† une ou plusieurs valeurs d‚Äôun DataFrame,\nil existe deux mani√®res conseill√©es de proc√©der, selon la\nforme des indices de lignes ou colonnes utilis√©s:\n\ndf.loc: utilise les labels\ndf.iloc: utilise les indices\n\n\n\n Warning\nLes bouts de code utilisant la structure df.ix\nsont √† bannir car la fonction est deprecated et peut\nainsi dispara√Ætre √† tout moment.\n\n\niloc va se r√©f√©rer √† l‚Äôindexation de 0 √† N o√π N est √©gal √† df.shape[0] d‚Äôun\npandas.DataFrame. loc va se r√©f√©rer aux valeurs de l‚Äôindex\nde df.\nPar exemple, avec le pandas.DataFrame df_example:\n\ndf_example = pd.DataFrame(\n    {'year': [2012, 2014, 2013, 2014], 'sale': [55, 40, 84, 31]})\ndf_example\n\n\n\n\n\n\n\n\nyear\nsale\n\n\n\n\n0\n2012\n55\n\n\n1\n2014\n40\n\n\n2\n2013\n84\n\n\n3\n2014\n31\n\n\n\n\n\n\n\n\ndf_example.loc[1, :] donnera la premi√®re ligne de df (ligne o√π l‚Äôindice month est √©gal √† 1) ;\ndf_example.iloc[1, :] donnera la deuxi√®me ligne (puisque l‚Äôindexation en Python commence √† 0) ;\ndf_example.iloc[:, 1] donnera la deuxi√®me colonne, suivant le m√™me principe."
  },
  {
    "objectID": "content/course/manipulation/02a_pandas_tutorial/index.html#principales-manipulation-de-donn√©es",
    "href": "content/course/manipulation/02a_pandas_tutorial/index.html#principales-manipulation-de-donn√©es",
    "title": "41¬† Introduction √† Pandas",
    "section": "41.6 Principales manipulation de donn√©es",
    "text": "41.6 Principales manipulation de donn√©es\nL‚Äôobjectif du TP pandas est de se familiariser plus avec ces\ncommandes √† travers l‚Äôexemple des donn√©es des √©missions de C02.\nLes op√©rations les plus fr√©quentes en SQL sont r√©sum√©es par le tableau suivant.\nIl est utile de les conna√Ætre (beaucoup de syntaxes de maniement de donn√©es\nreprennent ces termes) car, d‚Äôune\nmani√®re ou d‚Äôune autre, elles couvrent la plupart\ndes usages de manipulation des donn√©es\n\n\n\n\n\n\n\n\n\n\nOp√©ration\nSQL\npandas\ndplyr (R)\ndata.table (R)\n\n\n\n\nS√©lectionner des variables par leur nom\nSELECT\ndf[['Autres transports','Energie']]\ndf %&gt;% select(Autres transports, Energie)\ndf[, c('Autres transports','Energie')]\n\n\nS√©lectionner des observations selon une ou plusieurs conditions;\nFILTER\ndf[df['Agriculture']&gt;2000]\ndf %&gt;% filter(Agriculture&gt;2000)\ndf[Agriculture&gt;2000]\n\n\nTrier la table selon une ou plusieurs variables\nSORT BY\ndf.sort_values(['Commune','Agriculture'])\ndf %&gt;% arrange(Commune, Agriculture)\ndf[order(Commune, Agriculture)]\n\n\nAjouter des variables qui sont fonction d‚Äôautres variables;\nSELECT *, LOG(Agriculture) AS x FROM df\ndf['x'] = np.log(df['Agriculture'])\ndf %&gt;% mutate(x = log(Agriculture))\ndf[,x := log(Agriculture)]\n\n\nEffectuer une op√©ration par groupe\nGROUP BY\ndf.groupby('Commune').mean()\ndf %&gt;% group_by(Commune) %&gt;% summarise(m = mean)\ndf[,mean(Commune), by = Commune]\n\n\nJoindre deux bases de donn√©es (inner join)\nSELECT * FROM table1 INNER JOIN table2 ON table1.id = table2.x\ntable1.merge(table2, left_on = 'id', right_on = 'x')\ntable1 %&gt;% inner_join(table2, by = c('id'='x'))\nmerge(table1, table2, by.x = 'id', by.y = 'x')\n\n\n\n\n41.6.1 Op√©rations sur les colonnes: select, mutate, drop\nLes DataFrames pandas sont des objets mutables en langage Python,\nc‚Äôest-√†-dire qu‚Äôil est possible de faire √©voluer le DataFrame au gr√®s\ndes op√©rations. L‚Äôop√©ration la plus classique consiste √† ajouter ou retirer\ndes variables √† la table de donn√©es.\n\ndf_new = df.copy()\n\n\n\n Warning\nAttention au comportement de pandas lorsqu‚Äôon cr√©e une duplication\nd‚Äôun DataFrame.\nPar d√©faut, pandas effectue une copie par r√©f√©rence. Dans ce\ncas, les deux objets (la copie et l‚Äôobjet copi√©) restent reli√©s. Les colonnes\ncr√©es sur l‚Äôun vont √™tre r√©percut√©es sur l‚Äôautre. Ce comportement permet de\nlimiter l‚Äôinflation en m√©moire de Python. En faisant √ßa, le deuxi√®me\nobjet prend le m√™me espace m√©moire que le premier. Le package data.table\nen R adopte le m√™me comportement, contrairement √† dplyr.\nCela peut amener √† quelques surprises si ce comportement d‚Äôoptimisation\nn‚Äôest pas anticip√©. Si vous voulez, par s√©curit√©, conserver intact le\npremier DataFrame, faites appel √† une copie profonde (deep copy) en\nutilisant la m√©thode copy, comme ci-dessus.\nAttention toutefois, cela a un co√ªt m√©moire.\nAvec des donn√©es volumineuses, c‚Äôest une pratique √† utiliser avec pr√©caution.\n\n\nLa mani√®re la plus simple d‚Äôop√©rer pour ajouter des colonnes est\nd‚Äôutiliser la r√©assignation. Par exemple, pour cr√©er une variable\nx qui est le log de la\nvariable Agriculture:\n\ndf_new['x'] = np.log(df_new['Agriculture'])\n\nIl est possible d‚Äôappliquer cette approche sur plusieurs colonnes. Un des\nint√©r√™ts de cette approche est qu‚Äôelle permet de recycler le nom de colonnes.\n\nvars = ['Agriculture', 'D√©chets', 'Energie']\n\ndf_new[[v + \"_log\" for v in vars]] = np.log(df_new[vars])\ndf_new\n\n\n\n\n\n\n\n\nINSEE commune\nCommune\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nD√©chets\nEnergie\nIndustrie hors-√©nergie\nR√©sidentiel\nRoutier\nTertiaire\nx\nAgriculture_log\nD√©chets_log\nEnergie_log\n\n\n\n\n0\n01001\nL'ABERGEMENT-CLEMENCIAT\n3711.425991\nNaN\nNaN\n432.751835\n101.430476\n2.354558\n6.911213\n309.358195\n793.156501\n367.036172\n8.219171\n8.219171\n4.619374\n0.856353\n\n\n1\n01002\nL'ABERGEMENT-DE-VAREY\n475.330205\nNaN\nNaN\n140.741660\n140.675439\n2.354558\n6.911213\n104.866444\n348.997893\n112.934207\n6.164010\n6.164010\n4.946455\n0.856353\n\n\n2\n01004\nAMBERIEU-EN-BUGEY\n499.043526\n212.577908\nNaN\n10313.446515\n5314.314445\n998.332482\n2930.354461\n16616.822534\n15642.420313\n10732.376934\n6.212693\n6.212693\n8.578159\n6.906086\n\n\n3\n01005\nAMBERIEUX-EN-DOMBES\n1859.160954\nNaN\nNaN\n1144.429311\n216.217508\n94.182310\n276.448534\n663.683146\n1756.341319\n782.404357\n7.527881\n7.527881\n5.376285\n4.545232\n\n\n4\n01006\nAMBLEON\n448.966808\nNaN\nNaN\n77.033834\n48.401549\nNaN\nNaN\n43.714019\n398.786800\n51.681756\n6.106949\n6.106949\n3.879532\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n35793\n95676\nVILLERS-EN-ARTHIES\n1628.065094\nNaN\nNaN\n165.045396\n65.063617\n11.772789\n34.556067\n176.098160\n309.627908\n235.439109\n7.395148\n7.395148\n4.175366\n2.465791\n\n\n35794\n95678\nVILLIERS-ADAM\n698.630772\nNaN\nNaN\n1331.126598\n111.480954\n2.354558\n6.911213\n1395.529811\n18759.370071\n403.404815\n6.549122\n6.549122\n4.713854\n0.856353\n\n\n35795\n95680\nVILLIERS-LE-BEL\n107.564967\nNaN\nNaN\n8367.174532\n225.622903\n534.484607\n1568.845431\n22613.830247\n12217.122402\n13849.512001\n4.678095\n4.678095\n5.418865\n6.281303\n\n\n35796\n95682\nVILLIERS-LE-SEC\n1090.890170\nNaN\nNaN\n326.748418\n108.969749\n2.354558\n6.911213\n67.235487\n4663.232127\n85.657725\n6.994749\n6.994749\n4.691070\n0.856353\n\n\n35797\n95690\nWY-DIT-JOLI-VILLAGE\n1495.103542\nNaN\nNaN\n125.236417\n97.728612\n4.709115\n13.822427\n117.450851\n504.400972\n147.867245\n7.309951\n7.309951\n4.582194\n1.549500\n\n\n\n\n35798 rows √ó 16 columns\n\n\n\nIl est √©galement possible d‚Äôutiliser la m√©thode assign. Pour des op√©rations\nvectoris√©es, comme le sont les op√©rateurs de numpy, cela n‚Äôa pas d‚Äôint√©r√™t.\nCela permet notamment d‚Äôenchainer les op√©rations sur un m√™me DataFrame (notamment gr√¢ce au pipe que\nnous verrons plus loin).\nCette approche utilise g√©n√©ralement\ndes lambda functions. Par exemple le code pr√©c√©dent (celui concernant une\nseule variable) prendrait la forme:\n\ndf_new.assign(Energie_log = lambda x: np.log(x['Energie']))\n\n\n\n\n\n\n\n\nINSEE commune\nCommune\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nD√©chets\nEnergie\nIndustrie hors-√©nergie\nR√©sidentiel\nRoutier\nTertiaire\nx\nAgriculture_log\nD√©chets_log\nEnergie_log\n\n\n\n\n0\n01001\nL'ABERGEMENT-CLEMENCIAT\n3711.425991\nNaN\nNaN\n432.751835\n101.430476\n2.354558\n6.911213\n309.358195\n793.156501\n367.036172\n8.219171\n8.219171\n4.619374\n0.856353\n\n\n1\n01002\nL'ABERGEMENT-DE-VAREY\n475.330205\nNaN\nNaN\n140.741660\n140.675439\n2.354558\n6.911213\n104.866444\n348.997893\n112.934207\n6.164010\n6.164010\n4.946455\n0.856353\n\n\n2\n01004\nAMBERIEU-EN-BUGEY\n499.043526\n212.577908\nNaN\n10313.446515\n5314.314445\n998.332482\n2930.354461\n16616.822534\n15642.420313\n10732.376934\n6.212693\n6.212693\n8.578159\n6.906086\n\n\n3\n01005\nAMBERIEUX-EN-DOMBES\n1859.160954\nNaN\nNaN\n1144.429311\n216.217508\n94.182310\n276.448534\n663.683146\n1756.341319\n782.404357\n7.527881\n7.527881\n5.376285\n4.545232\n\n\n4\n01006\nAMBLEON\n448.966808\nNaN\nNaN\n77.033834\n48.401549\nNaN\nNaN\n43.714019\n398.786800\n51.681756\n6.106949\n6.106949\n3.879532\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n35793\n95676\nVILLERS-EN-ARTHIES\n1628.065094\nNaN\nNaN\n165.045396\n65.063617\n11.772789\n34.556067\n176.098160\n309.627908\n235.439109\n7.395148\n7.395148\n4.175366\n2.465791\n\n\n35794\n95678\nVILLIERS-ADAM\n698.630772\nNaN\nNaN\n1331.126598\n111.480954\n2.354558\n6.911213\n1395.529811\n18759.370071\n403.404815\n6.549122\n6.549122\n4.713854\n0.856353\n\n\n35795\n95680\nVILLIERS-LE-BEL\n107.564967\nNaN\nNaN\n8367.174532\n225.622903\n534.484607\n1568.845431\n22613.830247\n12217.122402\n13849.512001\n4.678095\n4.678095\n5.418865\n6.281303\n\n\n35796\n95682\nVILLIERS-LE-SEC\n1090.890170\nNaN\nNaN\n326.748418\n108.969749\n2.354558\n6.911213\n67.235487\n4663.232127\n85.657725\n6.994749\n6.994749\n4.691070\n0.856353\n\n\n35797\n95690\nWY-DIT-JOLI-VILLAGE\n1495.103542\nNaN\nNaN\n125.236417\n97.728612\n4.709115\n13.822427\n117.450851\n504.400972\n147.867245\n7.309951\n7.309951\n4.582194\n1.549500\n\n\n\n\n35798 rows √ó 16 columns\n\n\n\nDans les m√©thodes suivantes, il est possible de modifier le pandas.DataFrame\nen place, c‚Äôest √† dire en ne le r√©assignant pas, avec le param√®tre inplace = True.\nPar d√©faut, inplace est √©gal √† False et pour modifier le pandas.DataFrame,\nil convient de le r√©assigner.\nOn peut facilement renommer des variables avec la m√©thode rename qui\nfonctionne bien avec des dictionnaires (pour renommer des colonnes il faut\npr√©ciser le param√®tre axis = 1):\n\ndf_new = df_new.rename({\"Energie\": \"eneg\", \"Agriculture\": \"agr\"}, axis=1)\n\nEnfin, pour effacer des colonnes, on utilise la m√©thode drop avec l‚Äôargument\ncolumns:\n\ndf_new = df_new.drop(columns = [\"eneg\", \"agr\"])\n\n\n\n41.6.2 R√©ordonner\nLa m√©thode sort_values permet de r√©ordonner un DataFrame. Par exemple,\nsi on d√©sire classer par ordre d√©croissant de consommation de CO2 du secteur\nr√©sidentiel, on fera\n\ndf = df.sort_values(\"R√©sidentiel\", ascending = False)\n\nAinsi, en une ligne de code, on identifie les villes o√π le secteur\nr√©sidentiel consomme le plus.\n\n\n41.6.3 Filtrer\nL‚Äôop√©ration de s√©lection de lignes s‚Äôappelle FILTER en SQL. Elle s‚Äôutilise\nen fonction d‚Äôune condition logique (clause WHERE). On s√©lectionne les\ndonn√©es sur une condition logique. Il existe plusieurs m√©thodes en pandas.\nLa plus simple est d‚Äôutiliser les boolean mask, d√©j√† vus dans le chapitre\nnumpy.\nPar exemple, pour s√©lectionner les communes dans les Hauts-de-Seine, on\npeut utiliser le r√©sultat de la m√©thode str.startswith (qui renvoie\nTrue ou False) directement dans les crochets:\n\ndf[df['INSEE commune'].str.startswith(\"92\")].head(2)\n\n\n\n\n\n\n\n\nINSEE commune\nCommune\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nD√©chets\nEnergie\nIndustrie hors-√©nergie\nR√©sidentiel\nRoutier\nTertiaire\n\n\n\n\n35494\n92012\nBOULOGNE-BILLANCOURT\nNaN\n1250.483441\n34.234669\n51730.704250\n964.828694\n8817.818741\n25882.493998\n92216.971456\n64985.280901\n60349.109482\n\n\n35501\n92025\nCOLOMBES\nNaN\n411.371588\n14.220061\n53923.847088\n698.685861\n12855.885267\n50244.664227\n87469.549463\n52070.927943\n41526.600867\n\n\n\n\n\n\n\nPour remplacer des valeurs sp√©cifiques, on utilise la m√©thode where ou une\nr√©assignation coupl√©e √† la m√©thode pr√©c√©dente.\nPar exemple, pour assigner des valeurs manquantes aux d√©partements du 92,\non peut faire cela\n\ndf_copy = df.copy()\ndf_copy = df_copy.where(~df['INSEE commune'].str.startswith(\"92\"))\n\net v√©rifier les r√©sultats:\n\ndf_copy[df['INSEE commune'].str.startswith(\"92\")].head(2)\ndf_copy[~df['INSEE commune'].str.startswith(\"92\")].head(2)\n\n\n\n\n\n\n\n\nINSEE commune\nCommune\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nD√©chets\nEnergie\nIndustrie hors-√©nergie\nR√©sidentiel\nRoutier\nTertiaire\n\n\n\n\n12167\n31555\nTOULOUSE\n1434.045233\n4482.980062\n130.792683\n576394.181208\n88863.732538\n91549.914092\n277062.573234\n410675.902028\n586054.672836\n288175.400126\n\n\n16774\n44109\nNANTES\n248.019465\n138738.544337\n250814.701179\n193478.248177\n18162.261628\n17461.400209\n77897.138554\n354259.013785\n221068.632724\n173447.582779\n\n\n\n\n\n\n\nou alors utiliser une r√©assignation plus classique:\n\ndf_copy = df.copy()\ndf_copy[df_copy['INSEE commune'].str.startswith(\"92\")] = np.nan\n\nIl est conseill√© de filtrer avec loc en utilisant un masque.\nEn effet, contrairement √† df[mask], df.loc[mask, :] permet d‚Äôindiquer clairement\n√† Python que l‚Äôon souhaite appliquer le masque aux labels de l‚Äôindex.\nCe n‚Äôest pas le cas avec df[mask].\nD‚Äôailleurs, lorsqu‚Äôon utilise la syntaxe df[mask], pandas renvoie g√©n√©ralement un warning\n\n\n41.6.4 Op√©rations par groupe\nEn SQL, il est tr√®s simple de d√©couper des donn√©es pour\neffectuer des op√©rations sur des blocs coh√©rents et recollecter des r√©sultats\ndans la dimension appropri√©e.\nLa logique sous-jacente est celle du split-apply-combine qui est repris\npar les langages de manipulation de donn√©es, auxquels pandas\nne fait pas exception.\nL‚Äôimage suivante, issue de\nce site\nrepr√©sente bien la mani√®re dont fonctionne l‚Äôapproche\nsplit-apply-combine\n\n\n\nSplit-apply-combine\n\n\nCe tutoriel sur le sujet\nest particuli√®rement utile.\nPour donner quelques exemples, on peut cr√©er une variable d√©partementale qui\nservira de crit√®re de groupe.\n\ndf['dep'] = df['INSEE commune'].str[:2]\n\nEn pandas, on utilise groupby pour d√©couper les donn√©es selon un ou\nplusieurs axes. Techniquement, cette op√©ration consiste √† cr√©er une association\nentre des labels (valeurs des variables de groupe) et des\nobservations.\nPar exemple, pour compter le nombre de communes par d√©partement en SQL, on\nutiliserait la requ√™te suivante:\nSELECT dep, count(INSEE commune)\nFROM df\nGROUP BY dep;\nCe qui, en pandas, donne:\n\ndf.groupby('dep')[\"INSEE commune\"].count()\n\ndep\n01    410\n02    805\n03    318\n04    199\n05    168\n     ... \n91    196\n92     36\n93     40\n94     47\n95    185\nName: INSEE commune, Length: 96, dtype: int64\n\n\nLa syntaxe est quasiment transparente. On peut bien-s√ªr effectuer des op√©rations\npar groupe sur plusieurs colonnes. Par exemple,\n\ndf.groupby('dep').mean(numeric_only = True)\n\n\n\n\n\n\n\n\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nD√©chets\nEnergie\nIndustrie hors-√©nergie\nR√©sidentiel\nRoutier\nTertiaire\n\n\ndep\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n01\n1974.535382\n100.307344\n8.900375\n1736.353087\n671.743966\n280.485435\n1744.567552\n1346.982227\n3988.658995\n1021.089078\n\n\n02\n1585.417729\n202.878748\n17.390638\n767.072924\n223.907551\n76.316247\n932.135611\n793.615867\n1722.240298\n403.744266\n\n\n03\n6132.029417\n240.076499\n45.429978\n1779.630883\n349.746819\n326.904841\n1452.423506\n1401.650215\n3662.773062\n705.937016\n\n\n04\n1825.455590\n177.321816\nNaN\n583.198128\n253.975910\n62.808435\n313.913553\n587.116013\n1962.654370\n493.609329\n\n\n05\n1847.508592\n141.272766\nNaN\n502.012857\n132.548068\n34.971220\n102.649239\n728.734494\n2071.010178\n463.604908\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n91\n802.793163\n10114.998156\n73976.107892\n3716.906101\n1496.516194\n538.761253\n1880.810170\n6532.123033\n10578.452789\n3866.757200\n\n\n92\n8.309835\n362.964554\n13.132461\n29663.579634\n7347.163353\n6745.611611\n19627.706224\n40744.279029\n33289.456629\n23222.587595\n\n\n93\n50.461775\n1753.443710\n61188.896632\n18148.789684\n6304.173594\n2570.941598\n10830.409025\n32911.305703\n35818.236459\n21575.444794\n\n\n94\n48.072971\n5474.808839\n16559.384091\n14710.744314\n4545.099181\n1624.281505\n9940.192318\n28444.561597\n24881.531613\n16247.876321\n\n\n95\n609.172047\n682.143912\n37984.576873\n3408.871963\n1334.032970\n463.860672\n1729.692179\n6684.181989\n8325.948748\n4014.985843\n\n\n\n\n96 rows √ó 10 columns\n\n\n\nA noter que la variable de groupe, ici dep, devient, par d√©faut, l‚Äôindex\ndu DataFrame de sortie. Si on avait utilis√© plusieurs variables de groupe,\non obtiendrait un objet multi-index√©. Sur la gestion des multi-index, on\npourra se r√©f√©rer √† l‚Äôouvrage Modern Pandas dont la r√©f√©rence est\ndonn√©e en fin de cours.\nTant qu‚Äôon n‚Äôappelle pas une action sur un DataFrame par groupe, du type\nhead ou display, pandas n‚Äôeffectue aucune op√©ration. On parle de\nlazy evaluation. Par exemple, le r√©sultat de df.groupby('dep') est\nune transformation qui n‚Äôest pas encore √©valu√©e:\n\ndf.groupby('dep')\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f09880b4c10&gt;\n\n\nIl est possible d‚Äôappliquer plus d‚Äôune op√©ration √† la fois gr√¢ce √† la m√©thode\nagg. Par exemple, pour obtenir √† la fois le minimum, la m√©diane et le maximum\nde chaque d√©partement, on peut faire:\n\nnumeric_columns = df.select_dtypes(['number']).columns\ndf.loc[:, numeric_columns.tolist() + [\"dep\"] ].groupby('dep').agg(['min',\"median\",\"max\"], numeric_only = True)\n\n\n\n\n\n\n\n\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\n...\nIndustrie hors-√©nergie\nR√©sidentiel\nRoutier\nTertiaire\n\n\n\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\n...\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\n\n\ndep\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n01\n0.003432\n1304.519570\n14402.057335\n3.307596\n75.686090\n617.281080\n0.297256\n6.985161\n2.209492e+01\n30.571400\n...\n175185.892467\n9.607822\n351.182294\n57689.832901\n20.848982\n1598.934149\n45258.256406\n10.049230\n401.490676\n30847.366865\n\n\n02\n0.391926\n1205.725078\n13257.716591\n0.326963\n130.054615\n1126.961565\n0.517437\n15.492120\n5.799402e+01\n28.294993\n...\n220963.067245\n7.849347\n138.819865\n99038.124236\n22.936184\n700.826152\n49245.101730\n6.220952\n130.639994\n34159.345750\n\n\n03\n5.041238\n5382.194339\n24912.249269\n24.158870\n144.403590\n1433.217868\n29.958027\n42.762328\n8.269019e+01\n44.825515\n...\n154061.446374\n19.441088\n217.959697\n75793.882483\n120.667614\n1426.905646\n40957.845304\n17.705787\n191.892445\n31099.772884\n\n\n04\n30.985972\n1404.752852\n11423.535554\n33.513854\n158.780500\n362.637639\nNaN\nNaN\nNaN\n7.162928\n...\n16889.531061\n1.708652\n133.130946\n18088.189529\n30.206298\n687.390045\n31438.078325\n0.957070\n122.504902\n16478.024806\n\n\n05\n38.651727\n1520.896526\n13143.465812\n0.299734\n139.754980\n456.042002\nNaN\nNaN\nNaN\n20.931602\n...\n4271.129851\n6.871678\n211.945147\n46486.555748\n57.132270\n958.506314\n37846.651181\n4.785348\n151.695524\n23666.235898\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n91\n0.400740\n516.908303\n5965.349174\n25.785594\n177.177127\n513140.971691\n1.651873\n14.762210\n7.858782e+05\n41.661474\n...\n50288.560827\n15.886514\n2580.902085\n48464.979708\n20.260110\n3610.009634\n72288.020125\n36.368643\n1428.426303\n38296.204729\n\n\n92\n0.073468\n6.505185\n32.986132\n7.468879\n297.529178\n1250.483441\n1.104401\n11.482381\n3.423467e+01\n2173.614704\n...\n95840.512400\n4122.277198\n33667.904692\n92216.971456\n4968.382962\n23516.458236\n113716.853033\n800.588678\n18086.633085\n65043.364499\n\n\n93\n3.308495\n3.308495\n1362.351634\n24.188172\n320.755486\n45251.869710\n0.171075\n12.449476\n1.101146e+06\n899.762120\n...\n89135.302368\n4364.038661\n31428.227282\n87927.730552\n1632.496185\n22506.758771\n193039.792609\n2257.370945\n20864.923339\n71918.163984\n\n\n94\n1.781885\n1.781885\n556.939161\n6.249609\n294.204166\n103252.271268\n0.390223\n14.944807\n1.571965e+05\n928.232154\n...\n96716.055178\n2668.358896\n24372.900300\n100948.169898\n1266.101605\n19088.651049\n97625.957714\n1190.115985\n14054.223449\n58528.623477\n\n\n95\n8.779506\n445.279844\n2987.287417\n1.749091\n80.838639\n44883.982753\n0.201508\n13.149987\n1.101131e+06\n13.490977\n...\n66216.914749\n11.585833\n1434.343631\n104543.465908\n2.619451\n3417.197938\n147040.904455\n11.484835\n725.467969\n61497.821477\n\n\n\n\n96 rows √ó 30 columns\n\n\n\nLa premi√®re ligne est pr√©sente pour nous faciliter la r√©cup√©ration des noms de colonnes des variables\nnum√©riques\n\n\n41.6.5 Appliquer des fonctions\npandas est, comme on a pu le voir, un package tr√®s flexible, qui\npropose une grande vari√©t√© de m√©thodes optimis√©es. Cependant, il est fr√©quent\nd‚Äôavoir besoin de m√©thodes non impl√©ment√©es.\nDans ce cas, on recourt souvent aux lambda functions. Par exemple, si\non d√©sire conna√Ætre les communes dont le nom fait plus de 40 caract√®res,\non peut appliquer la fonction len de mani√®re it√©rative:\n\n# Noms de communes superieurs √† 40 caracteres\ndf[df['Commune'].apply(lambda s: len(s)&gt;40)]\n\n\n\n\n\n\n\n\nINSEE commune\nCommune\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\nD√©chets\nEnergie\nIndustrie hors-√©nergie\nR√©sidentiel\nRoutier\nTertiaire\ndep\n\n\n\n\n28082\n70058\nBEAUJEU-SAINT-VALLIER-PIERREJUX-ET-QUITTEUR\n4024.909815\n736.948351\n41.943384\n1253.135313\n125.101996\n2.354558\n6.911213\n549.734302\n1288.215480\n452.693897\n70\n\n\n4984\n14621\nSAINT-MARTIN-DE-BIENFAITE-LA-CRESSONNIERE\n1213.333523\nNaN\nNaN\n677.571743\n72.072503\n63.573059\n186.602760\n298.261044\n1396.353375\n260.801452\n14\n\n\n19276\n51513\nSAINT-REMY-EN-BOUZEMONT-SAINT-GENEST-ET-ISSON\n1927.401921\nNaN\nNaN\n595.583152\n71.675773\n4.709115\n13.822427\n273.826687\n521.864748\n259.365848\n51\n\n\n5402\n16053\nBORS (CANTON DE BAIGNES-SAINTE-RADEGONDE)\n1919.249545\nNaN\nNaN\n165.443226\n16.265904\n2.354558\n6.911213\n54.561623\n719.293151\n58.859777\n16\n\n\n\n\n\n\n\nCependant, toutes les lambda functions ne se justifient pas.\nPar exemple, prenons\nle r√©sultat d‚Äôagr√©gation pr√©c√©dent. Imaginons qu‚Äôon d√©sire avoir les r√©sultats\nen milliers de tonnes. Dans ce cas, le premier r√©flexe est d‚Äôutiliser\nla lambda function suivante:\n\nnumeric_columns = df.select_dtypes(['number']).columns\n(df\n    .loc[:, numeric_columns.tolist() + [\"dep\"] ]\n    .groupby('dep')\n    .agg(['min',\"median\",\"max\"])\n    .apply(lambda s: s/1000)\n)\n\n\n\n\n\n\n\n\nAgriculture\nAutres transports\nAutres transports international\nCO2 biomasse hors-total\n...\nIndustrie hors-√©nergie\nR√©sidentiel\nRoutier\nTertiaire\n\n\n\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\n...\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\nmin\nmedian\nmax\n\n\ndep\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n01\n0.000003\n1.304520\n14.402057\n0.003308\n0.075686\n0.617281\n0.000297\n0.006985\n0.022095\n0.030571\n...\n175.185892\n0.009608\n0.351182\n57.689833\n0.020849\n1.598934\n45.258256\n0.010049\n0.401491\n30.847367\n\n\n02\n0.000392\n1.205725\n13.257717\n0.000327\n0.130055\n1.126962\n0.000517\n0.015492\n0.057994\n0.028295\n...\n220.963067\n0.007849\n0.138820\n99.038124\n0.022936\n0.700826\n49.245102\n0.006221\n0.130640\n34.159346\n\n\n03\n0.005041\n5.382194\n24.912249\n0.024159\n0.144404\n1.433218\n0.029958\n0.042762\n0.082690\n0.044826\n...\n154.061446\n0.019441\n0.217960\n75.793882\n0.120668\n1.426906\n40.957845\n0.017706\n0.191892\n31.099773\n\n\n04\n0.030986\n1.404753\n11.423536\n0.033514\n0.158781\n0.362638\nNaN\nNaN\nNaN\n0.007163\n...\n16.889531\n0.001709\n0.133131\n18.088190\n0.030206\n0.687390\n31.438078\n0.000957\n0.122505\n16.478025\n\n\n05\n0.038652\n1.520897\n13.143466\n0.000300\n0.139755\n0.456042\nNaN\nNaN\nNaN\n0.020932\n...\n4.271130\n0.006872\n0.211945\n46.486556\n0.057132\n0.958506\n37.846651\n0.004785\n0.151696\n23.666236\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n91\n0.000401\n0.516908\n5.965349\n0.025786\n0.177177\n513.140972\n0.001652\n0.014762\n785.878155\n0.041661\n...\n50.288561\n0.015887\n2.580902\n48.464980\n0.020260\n3.610010\n72.288020\n0.036369\n1.428426\n38.296205\n\n\n92\n0.000073\n0.006505\n0.032986\n0.007469\n0.297529\n1.250483\n0.001104\n0.011482\n0.034235\n2.173615\n...\n95.840512\n4.122277\n33.667905\n92.216971\n4.968383\n23.516458\n113.716853\n0.800589\n18.086633\n65.043364\n\n\n93\n0.003308\n0.003308\n1.362352\n0.024188\n0.320755\n45.251870\n0.000171\n0.012449\n1101.145545\n0.899762\n...\n89.135302\n4.364039\n31.428227\n87.927731\n1.632496\n22.506759\n193.039793\n2.257371\n20.864923\n71.918164\n\n\n94\n0.001782\n0.001782\n0.556939\n0.006250\n0.294204\n103.252271\n0.000390\n0.014945\n157.196520\n0.928232\n...\n96.716055\n2.668359\n24.372900\n100.948170\n1.266102\n19.088651\n97.625958\n1.190116\n14.054223\n58.528623\n\n\n95\n0.008780\n0.445280\n2.987287\n0.001749\n0.080839\n44.883983\n0.000202\n0.013150\n1101.131222\n0.013491\n...\n66.216915\n0.011586\n1.434344\n104.543466\n0.002619\n3.417198\n147.040904\n0.011485\n0.725468\n61.497821\n\n\n\n\n96 rows √ó 30 columns\n\n\n\nEn effet, cela effectue le r√©sultat d√©sir√©. Cependant, il y a mieux: utiliser\nla m√©thode div:\n\nimport timeit\ndf_numeric = df.loc[:, numeric_columns.tolist() + [\"dep\"] ]\n%timeit df_numeric.groupby('dep').agg(['min',\"median\",\"max\"]).div(1000)\n%timeit df_numeric.groupby('dep').agg(['min',\"median\",\"max\"]).apply(lambda s: s/1000)\n\nLa m√©thode div est en moyenne plus rapide et a un temps d‚Äôex√©cution\nmoins variable. Dans ce cas, on pourrait m√™me utiliser le principe\ndu broadcasting de numpy (cf.¬†chapitre numpy) qui offre\ndes performances √©quivalentes:\n\n%timeit df_numeric.groupby('dep').agg(['min',\"median\",\"max\"])/1000\n\napply est plus rapide qu‚Äôune boucle (en interne, apply utilise Cython\npour it√©rer) mais reste moins rapide qu‚Äôune solution vectoris√©e quand\nelle existe. Ce site\npropose des solutions, par exemple les m√©thodes isin ou digitize, pour\n√©viter de manuellement cr√©er des boucles lentes.\nEn particulier, il faut noter que apply avec le param√®tre axis=1 est en g√©n√©rale lente.\n\n\n41.6.6 Joindre des donn√©es\nIl est commun de devoir combiner des donn√©es issues de sources diff√©rentes.\nNous allons ici nous focaliser sur le cas le plus favorable qui est la situation\no√π une information permet d‚Äôapparier de mani√®re exacte deux bases de donn√©es (autrement nous\nserions dans une situation, beaucoup plus complexe, d‚Äôappariement flou3).\nLa situation typique est l‚Äôappariement entre deux sources de donn√©es selon un identifiant\nindividuel. Ici, il s‚Äôagit d‚Äôun identifiant de code commune.\nIl est recommand√© de lire ce guide assez complet sur la question des jointures avec R\nqui donne des recommandations √©galement utiles pour un utilisateur de Python.\n\nOn utilise de mani√®re indiff√©rente les termes merge ou join.\nLe deuxi√®me terme provient de la syntaxe SQL.\nEn Pandas, dans la plupart des cas, on peut utiliser indiff√©remment df.join et df.merge\n\nIl est aussi possible de r√©aliser un merge en utilisant la fonction pandas.concat() avec axis=1.\nSe r√©f√©rer √† la documentation de concat pour voir les options possibles.\n\n\n41.6.7 Restructurer des donn√©es (reshape)\nOn pr√©sente g√©n√©ralement deux types de donn√©es:\n\nformat wide: les donn√©es comportent des observations r√©p√©t√©es, pour un m√™me individu (ou groupe), dans des colonnes diff√©rentes\nformat long: les donn√©es comportent des observations r√©p√©t√©es, pour un m√™me individu, dans des lignes diff√©rentes avec une colonne permettant de distinguer les niveaux d‚Äôobservations\n\nUn exemple de la distinction entre les deux peut √™tre emprunt√© √† l‚Äôouvrage de r√©f√©rence d‚ÄôHadley Wickham, R for Data Science:\n\nL‚Äôaide m√©moire suivante aidera √† se rappeler les fonctions √† appliquer si besoin:\n\nLe fait de passer d‚Äôun format wide au format long (ou vice-versa) peut √™tre extr√™mement pratique car\ncertaines fonctions sont plus ad√©quates sur une forme de donn√©es ou sur l‚Äôautre.\nEn r√®gle g√©n√©rale, avec Python comme avec R, les formats long sont souvent pr√©f√©rables.\nLe chapitre suivant, qui fait office de TP, proposera des applications de ces principes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n41.6.8 Les pipe\nEn g√©n√©ral, dans un projet, le nettoyage de donn√©es va consister en un ensemble de\nm√©thodes appliqu√©es √† un pandas.DataFrame.\nOn a vu que assign permettait de cr√©er une variable dans un DataFrame.\nIl est √©galement possible d‚Äôappliquer une fonction, appel√©e par exemple my_udf au\nDataFrame gr√¢ce √† pipe:\ndf = (pd.read_csv(path2data)\n            .pipe(my_udf))\nL‚Äôutilisation des pipe rend le code tr√®s lisible et peut √™tre tr√®s\npratique lorsqu‚Äôon enchaine des op√©rations sur le m√™me\ndataset."
  },
  {
    "objectID": "content/course/manipulation/02a_pandas_tutorial/index.html#quelques-enjeux-de-performance",
    "href": "content/course/manipulation/02a_pandas_tutorial/index.html#quelques-enjeux-de-performance",
    "title": "41¬† Introduction √† Pandas",
    "section": "41.7 Quelques enjeux de performance",
    "text": "41.7 Quelques enjeux de performance\nLa librairie Dask int√®gre la structure de numpy, pandas et sklearn.\nElle a vocation √† traiter de donn√©es en grande dimension, ainsi elle ne sera pas\noptimale pour des donn√©es qui tiennent tr√®s bien en RAM.\nIl s‚Äôagit d‚Äôune librairie construite sur la parall√©lisation.\nUn chapitre dans ce cours lui est consacr√©\nPour aller plus loin, se r√©f√©rer √† la documentation de Dask."
  },
  {
    "objectID": "content/course/manipulation/02a_pandas_tutorial/index.html#r√©f√©rences",
    "href": "content/course/manipulation/02a_pandas_tutorial/index.html#r√©f√©rences",
    "title": "41¬† Introduction √† Pandas",
    "section": "41.8 R√©f√©rences",
    "text": "41.8 R√©f√©rences\n\nLe site\npandas.pydata\nfait office de r√©f√©rence\nLe livre Modern Pandas de Tom Augspurger: https://tomaugspurger.github.io/modern-1-intro.html\n\n\n\nMcKinney, Wes. 2012. Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython. \" O‚ÄôReilly Media, Inc.\"."
  },
  {
    "objectID": "content/course/manipulation/03_geopandas_TP/index.html#lire-et-enrichir-des-donn√©es-spatiales",
    "href": "content/course/manipulation/03_geopandas_TP/index.html#lire-et-enrichir-des-donn√©es-spatiales",
    "title": "42¬† Pratique de geopandas avec les donn√©es v√©lib",
    "section": "42.1 Lire et enrichir des donn√©es spatiales",
    "text": "42.1 Lire et enrichir des donn√©es spatiales\nDans cette partie,\nnous utiliserons\nle package cartiflette\nqui facilite la r√©cup√©ration de contours de cartes.\nUne version ant√©rieure de cet exercice, pr√©sent√©e sous forme\nd‚Äôexercice suppl√©mentaire üëáÔ∏è, utilisait des fonds de carte issus\nde data.gouv.\n\n Exercice 1: lire et explorer la structure de fichiers g√©ographiques\n\nS‚Äôinspirer des exemples de code pr√©sents dans le chapitre pr√©c√©dent, mobilisant\nle package cartiflette\npour t√©l√©charger les donn√©es communales des d√©partements 75, 92, 93 et 94.\nVous pouvez nommer l‚Äôobjet communes_borders\nRegarder les premi√®res lignes des donn√©es. Identifier la diff√©rence avec\nun DataFrame standard.\nAfficher l‚Äôattribut crs de communes_borders. Ce dernier contr√¥le la\ntransformation de l‚Äôespace tridimensionnel terrestre en une surface plane.\nUtiliser to_crs pour transformer les donn√©es en Lambert 93 (code EPSG 2154).\nAfficher les communes des Hauts de Seine (d√©partement 92) et utiliser la m√©thode\nplot\nR√©pr√©senter la carte de Paris : quel est le probl√®me ?\n\n\n:::\n\n\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=75/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=92/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=93/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=94/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\n\n\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 12.0kiB [00:00, 81.8kiB/s]Downloading: : 14.5kiB [00:00, 98.5kiB/s]\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 8.00kiB [00:00, 72.5kiB/s]Downloading: : 32.0kiB [00:00, 137kiB/s] Downloading: : 62.3kiB [00:00, 225kiB/s]\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 12.0kiB [00:00, 81.7kiB/s]Downloading: : 36.0kiB [00:00, 129kiB/s] Downloading: : 84.0kiB [00:00, 217kiB/s]Downloading: : 119kiB [00:00, 244kiB/s] \nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 12.0kiB [00:00, 90.3kiB/s]Downloading: : 36.0kiB [00:00, 146kiB/s] Downloading: : 83.0kiB [00:00, 241kiB/s]Downloading: : 110kiB [00:00, 274kiB/s] \n\n\n\n\n\n\n\n\n\n\n\nid\nID\nNOM\nNOM_M\nINSEE_COM\nSTATUT\nPOPULATION\nINSEE_CAN\nINSEE_ARR\nINSEE_DEP\nINSEE_REG\nSIREN_EPCI\nsource\ngeometry\n\n\n\n\n0\nCOMMUNE_0000000009736048\nNaN\nParis\nPARIS\n75056\nCapitale d'√©tat\n2165423\nNR\n1\n75\n11\n200054781\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.36421 48.81640, 2.36333 48.81615, ...\n\n\n0\nCOMMUNE_0000000009736037\nNaN\nLevallois-Perret\nLEVALLOIS-PERRET\n92044\nCommune simple\n66082\n16\n2\n92\n11\n200054781/200057982\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.28739 48.90364, 2.28846 48.90302, ...\n\n\n1\nCOMMUNE_0000000009736055\nNaN\nBois-Colombes\nBOIS-COLOMBES\n92009\nCommune simple\n28841\n11\n2\n92\n11\n200054781/200057990\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.26639 48.90629, 2.26645 48.90615, ...\n\n\n2\nCOMMUNE_0000000009736538\nNaN\nMalakoff\nMALAKOFF\n92046\nCommune simple\n30950\n18\n1\n92\n11\n200054781/200057966\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.27818 48.81425, 2.28066 48.81469, ...\n\n\n3\nCOMMUNE_0000000009736038\nNaN\nClichy\nCLICHY\n92024\nCommune simple\n63089\n09\n2\n92\n11\n200054781/200057990\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.30377 48.89415, 2.30258 48.89487, ...\n\n\n\n\n\n\n\n\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nLa carte du 92 est la suivante:\n\n\n\n\n\nQuant √† Paris, √† l‚Äôissue de la question 5, la carte\naura l‚Äôaspect suivant:\n\n\n\n\n\nEn effet, on ne dispose ainsi pas des limites des arrondissements parisiens, ce\nqui appauvrit grandement la carte de Paris.\nOn pourrait les r√©cup√©rer directement\ndepuis le site d‚Äôopen-data du Grand Paris, ce qui est propos√©\nen exercice suppl√©mentaire üëáÔ∏è.\nOn propose ici d‚Äôutiliser √† nouveau\ncartiflette pour cela afin de disposer du fonds de carte officiel.\n\n\n Exercice 2: compl√©ter des donn√©es spatiales issues de sources diff√©rentes\n\nImporter les donn√©es de d√©coupage des arrondissements parisiens √† l‚Äôadresse √† l‚Äôaide de cartiflette.\nV√©rifier sur une carte que les d√©coupages des arrondissements sont bien pr√©sents.\nV√©rifier l‚Äôattribut crs. Est-il coh√©rent avec celui des donn√©es communales ?\nSi non, transformer en Lambert 93 (code EPSG 2154).\nRetirer Paris du jeu de donn√©es communales et utiliser les arrondissements\npour enrichir (nommer l‚Äôobjet obtenu data_borders).\nRepr√©senter √† nouveau les communes de la petite couronne parisienne (75, 92, 93, 94)\n\n\n\n\n\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=ARRONDISSEMENT_MUNICIPAL/crs=4326/DEPARTEMENT=75/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\n\n\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 8.00kiB [00:00, 66.8kiB/s]Downloading: : 37.0kiB [00:00, 149kiB/s] Downloading: : 38.0kiB [00:00, 141kiB/s]\n\n\nLa carte de Paris intra-muros est, apr√®s la\nr√©cup√©ration des arrondissements avec\ncartiflette de ce type l√†:\n\n\n\n\n\n\n\nEPSG:4326\nEPSG:4326\nFalse\n\n\nLa carte obtenue √† l‚Äôissue de la question 6, c‚Äôest-√†-dire apr√®s\navoir consolid√© les donn√©es, devrait avoir l‚Äôaspect suivant:"
  },
  {
    "objectID": "content/course/manipulation/03_geopandas_TP/index.html#utiliser-des-donn√©es-g√©ographiques-comme-des-couches-graphiques",
    "href": "content/course/manipulation/03_geopandas_TP/index.html#utiliser-des-donn√©es-g√©ographiques-comme-des-couches-graphiques",
    "title": "42¬† Pratique de geopandas avec les donn√©es v√©lib",
    "section": "42.2 Utiliser des donn√©es g√©ographiques comme des couches graphiques",
    "text": "42.2 Utiliser des donn√©es g√©ographiques comme des couches graphiques\nSouvent, le d√©coupage communal ne sert qu‚Äôen fond de cartes, pour donner des\nrep√®res. En compl√©ment de celui-ci, on peut d√©sirer exploiter\nun autre jeu de donn√©es.\nOn va partir des donn√©es de localisation des\nstations velib,\ndisponibles sur le site d‚Äôopen data de la ville de Paris et\nrequ√™tables directement par l‚Äôurl\nhttps://opendata.paris.fr/explore/dataset/velib-emplacement-des-stations/download/?format=geojson&timezone=Europe/Berlin&lang=fr\n\n\n Exercice 3: importer et explorer les donn√©es velib\n\nImporter les donn√©es velib sous le nom station\nV√©rifier la projection g√©ographique de station (attribut crs). Si celle-ci est diff√©rente des donn√©es communales, reprojeter ces\nderni√®res dans le m√™me syst√®me de projection que les stations de v√©lib\nRepr√©senter sur une carte les 50 stations les plus importantes (variable capacity). Vous pouvez √©galement afficher le fonds de carte des arrondissements de Paris.\nCette page peut vous aider pour comprendre comment afficher plusieurs couches √† la fois. Vous pouvez customiser la carte en retirant les axes gr√¢ce √† la m√©thode set_axis_off et mettre un titre tel que ‚ÄúLes 50 principales stations de V√©lib‚Äù avec la m√©thode set_title.\nAfficher √©galement (trait bleu et √©pais) les r√©seaux de transport en communs, disponibles ici. L‚Äôurl √† requ√™ter est\nhttps://data.iledefrance-mobilites.fr/explore/dataset/traces-du-reseau-ferre-idf/download/?format=geojson&timezone=Europe/Berlin&lang=fr\n\n\n\n\n\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 150kiB [00:00, 1.16MiB/s]Downloading: : 361kiB [00:00, 2.71MiB/s]\n\n\n\n\n\n\n\n\n\ncapacity\nname\nstationcode\ngeometry\n\n\n\n\n0\n30\nMairie de Rosny-sous-Bois\n31104\nPOINT (2.48658 48.87126)\n\n\n1\n25\nSilly - Galli√©ni\n21010\nPOINT (2.23255 48.83558)\n\n\n2\n36\nGuersant - Gouvion-Saint-Cyr\n17041\nPOINT (2.28767 48.88288)\n\n\n3\n27\nGare Saint-Lazare - Isly\n8009\nPOINT (2.32652 48.87478)\n\n\n4\n24\nThionville - Ourcq\n19015\nPOINT (2.38337 48.88918)\n\n\n\n\n\n\n\nLa carte attendu √† l‚Äôissue de la question 3 a l‚Äôaspect suivant:\n\n\nText(0.5, 1.0, 'Les 50 principales stations de V√©lib')\n\n\n\n\n\n\n\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 83.4kiB [00:00, 731kiB/s]Downloading: : 227kiB [00:00, 1.03MiB/s]Downloading: : 366kiB [00:00, 1.11MiB/s]Downloading: : 847kiB [00:00, 2.31MiB/s]Downloading: : 1.73MiB [00:00, 4.35MiB/s]Downloading: : 2.14MiB [00:00, 3.20MiB/s]Downloading: : 2.89MiB [00:00, 4.25MiB/s]Downloading: : 3.25MiB [00:00, 3.70MiB/s]\n\n\n['TRAMWAY' 'RER' 'TRAIN' 'TER' 'NAVETTE' 'METRO']\n\n\nL‚Äôajout du r√©seau de m√©tro permet d‚Äôobtenir une carte ressemblant √† celle-ci:\n\n\nText(0.5, 1.0, 'Les 50 principales stations de V√©lib')\n\n\n\n\n\nPour faire une belle carte, il faudrait couper les lignes de m√©tro via une jointure spatiale ou\nutiliser un fonds de carte conceptuel.\nL‚Äôexercice suivant propose de mettre en oeuvre la deuxi√®me m√©thode. La premi√®re\nest propos√©e en exercice suppl√©mentaire üëáÔ∏è.\n\n\n Exercice 4: ajouter un fond de carte\n\nRecr√©er par couche successive la carte pr√©c√©dente, que vous pouvez nommer base\nUtiliser add_basemap du package contextily\npour ajouter, en arri√®re plan, un fonds de carte\nJouer avec les fonds disponibles en utilisant l‚Äôargument source\n\n\n\nPar exemple, en utilisant le fond Stamen.Watercolor, on obtient la carte\nsuivante. Celle-ci permet d√©j√† de mieux localiser les stations."
  },
  {
    "objectID": "content/course/manipulation/03_geopandas_TP/index.html#jointures-spatiales",
    "href": "content/course/manipulation/03_geopandas_TP/index.html#jointures-spatiales",
    "title": "42¬† Pratique de geopandas avec les donn√©es v√©lib",
    "section": "42.3 Jointures spatiales",
    "text": "42.3 Jointures spatiales\nLes jointures attributaires fonctionnent comme avec un DataFrame pandas.\nPour conserver un objet spatial in fine, il faut faire attention √† utiliser en premier (base de gauche) l‚Äôobjet GeoPandas.\nEn revanche, l‚Äôun des int√©r√™ts des objets geopandas est qu‚Äôon peut √©galement faire une jointure sur la dimension spatiale gr√¢ce √† sjoin.\nLa documentation √† laquelle se r√©f√©rer est ici.\n\n\n Exercice 5 : Associer les stations aux communes et arrondissements auxquels elles appartiennent\n\nFaire une jointure spatiale pour enrichir les donn√©es de stations en y ajoutant des informations de data_paris. Appeler cet objet stations_info\nRepr√©senter la carte des stations du 19e arrondissement (s‚Äôaider de la variable c_ar). Vous pouvez mettre en fond de carte les arrondissements parisiens.\nCompter le nombre de stations velib et le nombre de places velib par arrondissement ou commune (pour vous aider, vous pouvez compl√©ter vos connaissances avec ce tutoriel). Repr√©senter sur une carte chacune des informations\nRepr√©senter les m√™mes informations mais en densit√© (diviser par la surface de l‚Äôarrondissement ou commune en km2)\n(optionnel) Choisir une des cartes de densit√© et la nettoyer (retirer les axes, mettre les titres‚Ä¶)\n\n\n\n\n\n\n\n\n\n\n\n\ncapacity\nname\nstationcode\ngeometry\nindex_right\nid\nID\nNOM\nNOM_M\nINSEE_COM\nSTATUT\nPOPULATION\nINSEE_CAN\nINSEE_ARR\nINSEE_DEP\nINSEE_REG\nSIREN_EPCI\nsource\nINSEE_ARM\n\n\n\n\n0\n30\nMairie de Rosny-sous-Bois\n31104\nPOINT (2.48658 48.87126)\n32\nCOMMUNE_0000000009736018\nNaN\nRosny-sous-Bois\nROSNY-SOUS-BOIS\n93064\nCommune simple\n46024\n12\n2\n93\n11\n200054781/200058790\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nNaN\n\n\n107\n19\nJules Guesde - Laennec\n31101\nPOINT (2.50069 48.87629)\n32\nCOMMUNE_0000000009736018\nNaN\nRosny-sous-Bois\nROSNY-SOUS-BOIS\n93064\nCommune simple\n46024\n12\n2\n93\n11\n200054781/200058790\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nNaN\n\n\n727\n36\nGare Rosny-Bois-Perrier\n31103\nPOINT (2.48044 48.88115)\n32\nCOMMUNE_0000000009736018\nNaN\nRosny-sous-Bois\nROSNY-SOUS-BOIS\n93064\nCommune simple\n46024\n12\n2\n93\n11\n200054781/200058790\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nNaN\n\n\n1301\n18\nLavoisier - Missak Manouchian\n31105\nPOINT (2.49568 48.86286)\n32\nCOMMUNE_0000000009736018\nNaN\nRosny-sous-Bois\nROSNY-SOUS-BOIS\n93064\nCommune simple\n46024\n12\n2\n93\n11\n200054781/200058790\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nNaN\n\n\n1\n25\nSilly - Galli√©ni\n21010\nPOINT (2.23255 48.83558)\n29\nCOMMUNE_0000000009736552\nNaN\nBoulogne-Billancourt\nBOULOGNE-BILLANCOURT\n92012\nSous-pr√©fecture\n121583\n96\n3\n92\n11\n200054781/200057974\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nNaN\n\n\n\n\n\n\n\nPour la question 2,\nla premi√®re m√©thode consiste √† afficher\ntoute la ville mais √† ne repr√©senter que\nles points des stations du 19e:\n\n\n&lt;Axes: &gt;\n\n\n\n\n\nN√©anmoins, il est pr√©f√©rable de se centrer sur\nle 19e en premier lieu, ce qui donne une\ncarte comme celle-ci:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nID\nNOM\nNOM_M\nINSEE_COM\nSTATUT\nPOPULATION\nINSEE_CAN\nINSEE_ARR\nINSEE_DEP\nINSEE_REG\nSIREN_EPCI\nsource\ngeometry\nINSEE_ARM\nstationcode\ncapacity\n\n\n\n\n0\nCOMMUNE_0000000009736037\nNaN\nLevallois-Perret\nLEVALLOIS-PERRET\n92044\nCommune simple\n66082\n16\n2\n92\n11\n200054781/200057982\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.28739 48.90364, 2.28846 48.90302, ...\nNaN\n11\n334\n\n\n1\nCOMMUNE_0000000009736055\nNaN\nBois-Colombes\nBOIS-COLOMBES\n92009\nCommune simple\n28841\n11\n2\n92\n11\n200054781/200057990\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.26639 48.90629, 2.26645 48.90615, ...\nNaN\n2\n60\n\n\n2\nCOMMUNE_0000000009736538\nNaN\nMalakoff\nMALAKOFF\n92046\nCommune simple\n30950\n18\n1\n92\n11\n200054781/200057966\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.27818 48.81425, 2.28066 48.81469, ...\nNaN\n8\n238\n\n\n3\nCOMMUNE_0000000009736038\nNaN\nClichy\nCLICHY\n92024\nCommune simple\n63089\n09\n2\n92\n11\n200054781/200057990\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.30377 48.89415, 2.30258 48.89487, ...\nNaN\n13\n422\n\n\n4\nCOMMUNE_0000000009736052\nNaN\nNanterre\nNANTERRE\n92050\nPr√©fecture\n96277\n99\n2\n92\n11\n200054781/200057982\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.22910 48.90603, 2.23037 48.90377, ...\nNaN\n8\n221\n\n\n\n\n\n\n\nLa carte des places disponibles est celle-ci:\n\n\nText(0.5, 1.0, 'Nombre de places disponibles')\n\n\n\n\n\nAlors que la carte des capacit√©s de stations est\nplut√¥t celle-l√†:\n\n\nText(0.5, 1.0, 'Nombre de stations')\n\n\n\n\n\nPas vraiment de diff√©rence marqu√©e entre les\ndeux, on peut se contenter de regarder la capacit√©.\nEnfin, dans la question 4,\nsi on repr√©sente plut√¥t la capacit√©\nsous forme de densit√©, pour tenir compte\nde la taille diff√©rente des arrondissements,\non obtient cette carte:\n\n\n&lt;Axes: &gt;\n\n\n\n\n\nAvec une palette plasma_r, cela donne plut√¥t cette carte:\n\n\n&lt;Axes: &gt;\n\n\n\n\n\nAvec un peu de travail sur l‚Äôesth√©tique, la carte\nque vous obtenez √† l‚Äôissue de l‚Äôexercice\nressemble √† celle-ci:"
  },
  {
    "objectID": "content/course/manipulation/03_geopandas_TP/index.html#trouver-les-toilettes-publiques-les-plus-proches",
    "href": "content/course/manipulation/03_geopandas_TP/index.html#trouver-les-toilettes-publiques-les-plus-proches",
    "title": "42¬† Pratique de geopandas avec les donn√©es v√©lib",
    "section": "42.4 Trouver les toilettes publiques les plus proches",
    "text": "42.4 Trouver les toilettes publiques les plus proches\n\n42.4.1 Objectif\nJusqu‚Äô√† pr√©sent, nous nous sommes familiaris√©s avec\nla manipulation de donn√©es spatiales et la repr√©sentation\nrapide de celle-ci gr√¢ce aux fonctionalit√©s de GeoPandas.\nA partir de maintenant, nous allons utiliser GeoPandas\npour des t√¢ches de manipulation g√©om√©trique.\nCes op√©rations reposeront sur des t√¢ches classiques\nde la g√©omatique qui sont facilit√©es par le fait que\nGeoPandas offre une surcouche au package Shapely\nde la m√™me mani√®re que Pandas √©tait une sur-couche\nde Numpy pour les op√©rations num√©riques.\nL‚Äôexemple suivant permet d‚Äôillustrer\nle principe d‚Äôune des\nop√©rations que nous allons utiliser,\n√† savoir la recherche de plus proche point:\n\nfrom shapely.ops import Polygon\nfrom shapely.ops import nearest_points\ntriangle = Polygon([(0, 0), (1, 0), (0.5, 1), (0, 0)])\nsquare = Polygon([(0, 2), (1, 2), (1, 3), (0, 3), (0, 2)])\n[o.wkt for o in nearest_points(triangle, square)]\n\n['POINT (0.5 1)', 'POINT (0.5 2)']\n\n\nGeoPandas va permettre de g√©n√©raliser ce processus\nen utilisant non plus deux listes modifi√©es (les\npolygones de Shapely) mais des DataFrames g√©ographiques.\nCela permettra, au passage, d‚Äôenrichir les\njointures spatiales avec les attributs des DataFrames\nconcern√©s.\nSur Shapely, vous pourrez trouver une aide ici.\nN√©anmoins, √† mesure que GeoPandas se d√©veloppe, il\ndevient de moins en moins n√©cessaire d‚Äôutiliser directement\nShapely.\n\n\n42.4.2 Mise en application\nNous allons rechercher les toilettes publiques les\nplus proches de chaque station.\nSans les fonctionalit√©s de GeoPandas,\ncette recherche serait assez p√©nible.\n\n\n Exercice 5 (optionnel) : Trouver les toilettes publiques les plus proches d'une station de v√©lib\n\nCharger la localisation des toilettes publiques pr√©sente ici : https://data.ratp.fr/explore/dataset/sanitaires-reseau-ratp/download/?format=geojson&timezone=Europe/Berlin&lang=fr. Appelez-la toilettes_publiques.\nConvertir les objets toilettes_publiques et stations en projection Lambert-93 (CRS 2154). Cette\nconversion permettra de mesurer en m√®tres les distances entre objets g√©ographiques. Sans\ncelle-ci, nous ferions des distances entre coordonn√©es GPS, ce qui n‚Äôaide pas l‚Äôanalyse\net l‚Äôinterpr√©tation.\nUtiliser la jointure spatiale par plus proche distance sjoin_nearest pour associer √† chaque station les toilettes publiques les plus proches\nTrouver les toilettes publiques les plus proches des stations de v√©lib autour d‚ÄôEdgard Quinet.\nRepr√©senter un histogramme des distances aux toilettes les plus proches\n\n\n\nLe jeu de donn√©es open-data des toilettes\npubliques pr√©sente l‚Äôaspect suivant:\n\n\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 27.1kiB [00:00, 17.2MiB/s]\n\n\n\n\n\n\n\n\n\naccessible_au_public\nacces_bouton_poussoir\ntarif_gratuit_payant\naccessibilite_pmr\ngestionnaire\nligne\nacces_passe_navigo_ou_ticket_t\nlocalisation\nen_zone_controlee\nstation\nhors_zone_controlee_station\nhors_zone_controlee_voie_publique\ngeometry\n\n\n\n\n0\nNaN\nNaN\nNaN\nNaN\nToilette publique RATP\n8\nNaN\nAcc√®s par la salle d'√©change, Espace Voyageur ...\nNaN\nGrands Boulevards\nNaN\nNaN\nPOINT (2.34351 48.87133)\n\n\n1\noui\noui\ngratuit\noui\nToilette publique RATP\n13\nNaN\nA proximit√© du point de vente - information.\nNaN\nLes Agnettes\noui\nNaN\nPOINT (2.28606 48.92318)\n\n\n\n\n\n\n\nLes toilettes les plus proches\nd‚ÄôEdgar Quinet sont les suivantes:\n\n\n\n\n\n\n\n\n\nlocalisation\nstation\nname\ndistance\n\n\n\n\n482\nSur le quai ligne B, en direction de Saint-R√©m...\nDenfert-Rochereau\nGare Montparnasse - Edgar Quinet\n1177.501002\n\n\n1403\nSur le quai ligne B, en direction de Saint-R√©m...\nDenfert-Rochereau\nEdgar Quinet - Raspail\n737.660246\n\n\n1413\nSur le quai ligne B, en direction de Saint-R√©m...\nDenfert-Rochereau\nEdgar Quinet - Gait√©\n1091.971078\n\n\n\n\n\n\n\nIl va donc falloir se\nretenir un peu car s‚Äôagit de toilettes situ√©es\n√† la station Denfert Rochereau !\nEnfin, de mani√®re plus globale, voici la distribution\ndes distances aux toilettes les plus proches:\n\n\n&lt;Axes: ylabel='Frequency'&gt;\n\n\n\n\n\nLe mode de la distribution est entre 1 et 2 km, ce\nqui est une petite distance tout de m√™me !\nC‚Äôest normal, il ne s‚Äôagit pas de l‚Äôensemble des\ntoilettes publiques de la ville de Paris mais\nde celles g√©r√©es par la RATP. Rassurez-vous, au\nmoins dans Paris intra-muros, vous n‚Äôavez pas\n√† syst√©matiquement marcher (ou rouler) autant."
  },
  {
    "objectID": "content/course/manipulation/03_geopandas_TP/index.html#exo-supp",
    "href": "content/course/manipulation/03_geopandas_TP/index.html#exo-supp",
    "title": "42¬† Pratique de geopandas avec les donn√©es v√©lib",
    "section": "42.5 Exercices suppl√©mentaires",
    "text": "42.5 Exercices suppl√©mentaires\nVoici une fonction pour t√©l√©charger et d√©zipper\nfacilement un fonds de carte issu de data.gouv\n\nimport requests\nimport tempfile\nimport zipfile\n\ntemporary_location = tempfile.gettempdir()\n\ndef download_unzip(url, dirname = tempfile.gettempdir(), destname = \"borders\"):\n  myfile = requests.get(url)\n  open(\"{}/{}.zip\".format(dirname, destname), 'wb').write(myfile.content)\n  with zipfile.ZipFile(\"{}/{}.zip\".format(dirname, destname), 'r') as zip_ref:\n      zip_ref.extractall(dirname + '/' + destname)\n\n\n\n Exercice optionnel 1: t√©l√©charger et d√©zipper vous-m√™me le fonds de carte\nImporter le fichier avec le package GeoPandas\n(si vous avez laiss√© les param√®tres par d√©faut,\nle fichier devrait\n√™tre √† l‚Äôemplacement temporary_location + \"/borders/communes-20210101.shp\").\n\n\n\n\n Exercice optionnel 2 : Utiliser les arrondissements fournis par l'open data parisien\n\nImporter les donn√©es de d√©coupage des arrondissements parisiens √† l‚Äôadresse\nhttps://opendata.paris.fr/explore/dataset/arrondissements/download/?format=geojson&timezone=Europe/Berlin&lang=fr\nV√©rifier sur une carte que les d√©coupages des arrondissements sont bien pr√©sents.\nV√©rifier l‚Äôattribut crs. Est-il coh√©rent avec celui des donn√©es communales ?\nRetirer Paris du jeu de donn√©es communales et utiliser les arrondissements\npour enrichir (nommer l‚Äôobjet obtenu data_borders). Ici, on peut ne pas se\nsoucier de la variable commune de superficie aux niveaux diff√©rents car on\nva la recr√©er. En revanche, renommer la variable c_arinsee en insee avec\nla m√©thode rename et faire attention aux types des variables\n\n\n\n\n\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 80.8kiB [00:00, 694kiB/s]Downloading: : 206kiB [00:00, 1.04MiB/s]\n\n\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\nEPSG:4326\nFalse\n\n\n\n\n\n\n\n\n\n\n\nid\nID\nNOM\nNOM_M\nINSEE_COM\nSTATUT\nPOPULATION\nINSEE_CAN\nINSEE_ARR\nINSEE_DEP\n...\nSIREN_EPCI\nsource\ngeometry\nc_ar\nl_aroff\nsurface\nl_ar\nn_sq_co\nn_sq_ar\nperimetre\n\n\n\n\n141\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n75\n...\nNaN\nNaN\nPOLYGON ((651908.510 6861756.243, 651885.552 6...\n6.0\nLuxembourg\n2.153096e+06\n6√®me Ardt\n750001537.0\n750000006.0\n6483.686786\n\n\n142\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n75\n...\nNaN\nNaN\nPOLYGON ((656975.542 6859439.335, 656984.927 6...\n12.0\nReuilly\n1.631478e+07\n12√®me Ardt\n750001537.0\n750000012.0\n24089.666298\n\n\n\n\n2 rows √ó 21 columns\n\n\n\n\n42.5.1 Jointures spatiales\nL‚Äôobjectif de cet exercice est de ne conserver que les\nlignes de transports √† l‚Äôint√©rieur de Paris intra-muros.\nIl s‚Äôagit d‚Äôappliquer les jointures spatiales de mani√®re\nun petit peu diff√©rente √† pr√©c√©demment.\n\n Exercice optionnel 3 : Les lignes de transport dans Paris\n\nUtiliser l‚ÄôURL https://data.iledefrance-mobilites.fr/explore/dataset/traces-du-reseau-ferre-idf/download/?format=geojson&timezone=Europe/Berlin&lang=fr pour r√©cup√©rer les lignes de transport\nde la RATP. L‚Äôappeler transports.\nA partir des arrondissements parisiens, utiliser unary_union pour cr√©er un unique polygone parisien. Utiliser within pour ne conserver que les points de transports qui se trouvent\ndans Paris intra-muros\nRepr√©senter graphiquement\n\n\n:::\n\n\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 83.4kiB [00:00, 737kiB/s]Downloading: : 227kiB [00:00, 1.04MiB/s]Downloading: : 366kiB [00:00, 1.11MiB/s]Downloading: : 749kiB [00:00, 1.98MiB/s]Downloading: : 1.54MiB [00:00, 3.84MiB/s]Downloading: : 3.08MiB [00:00, 7.18MiB/s]Downloading: : 3.25MiB [00:00, 4.80MiB/s]\n\n\nLa carte obtenue aura l‚Äôaspect suivant:\n\n\n&lt;Axes: &gt;\n\n\n\n\n\nCette fois, on a bien conserv√© que les lignes de transport dans\nParis. Un peu de travail sur le rendu serait n√©cessaire pour\nobtenir une belle carte. Vous pouvez le faire en exercice, apr√®s\navoir consult√© le chapitre relatif √† la cartographie dans\nla partie visualisation de donn√©es."
  },
  {
    "objectID": "content/course/manipulation/06a_exo_supp_webscraping/index.html#construction-automatis√©e-dune-liste-de-courses-via-webscraping-spaghetti-pizza-strawberry",
    "href": "content/course/manipulation/06a_exo_supp_webscraping/index.html#construction-automatis√©e-dune-liste-de-courses-via-webscraping-spaghetti-pizza-strawberry",
    "title": "43¬† Exercices suppl√©mentaires de webscraping",
    "section": "43.1 Construction automatis√©e d‚Äôune liste de courses via webscraping :spaghetti: :pizza: :strawberry:",
    "text": "43.1 Construction automatis√©e d‚Äôune liste de courses via webscraping :spaghetti: :pizza: :strawberry:\nLes comptes sont dans le rouge, le banquier appelle tous les jours.\nPlus le choix : fini les commandes de plats tout faits via des plateformes bien connues,\nil va falloir se faire des bons petits plats soi-m√™me.\nMais la cuisine √† l‚Äôancienne, c‚Äôest long : il faut trouver le bon livre de cuisine,\nla bonne recette, faire des r√®gles de trois pour calculer les bonnes proportions, etc.\nEt apr√®s √ßa, faire une liste de courses‚Ä¶\nHeureusement, Marmiton est l√† pour nous.\nDans ce TP, on va construire un outil Python qui permet d‚Äôexporter directement une liste de courses,\nen fonctions des plats que l‚Äôon a envie de manger cette semaine. Et tout √ßa en webscrapant les donn√©es de Marmiton. Plus d‚Äôexcuse !\nPour cet exercice, on va utiliser principalement trois librairies tr√®s utilis√©es en webscraping :\n\nrequests & BeautifulSoup pour scraper des pages statiques ;\nselenium lorsque l‚Äôon aura besoin d‚Äôinteragir avec les √©l√©ments script√©s des pages web.\n\nPour pouvoir utiliser selenium, il est n√©cessaire d‚Äôavoir install√© le chromedriver (instructions),\nou bien le driver adapt√© si vous utilisez un autre navigateur que Google Chrome.\n\nAnalyser comment fonctionne la recherche d‚Äôune recette sur Marmiton (structure de l‚ÄôURL) et coder un outil\npermettant de r√©cup√©rer (√† l‚Äôaide de requests) le code html des r√©sultats de la recherche pour une recette donn√©e.\nFormatter ce code en un arbre lxml √† l‚Äôaide de BeautifulSoup.\n\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n\nPLAT = \"pates carbonara\"\nBASE_URL = \"https://www.marmiton.org\"\nURL_SEARCH = BASE_URL + \"/recettes/recherche.aspx?aqt=\" + PLAT\n\nVous pouvez v√©rifier que python r√©cup√®re bien un r√©sultat √† l‚ÄôURL voulu en tapant\n\nrequests.get(URL_SEARCH).status_code\n\nSi le code retour est 200, il y a bien du contenu accessible sur la page.\n{{% box status=‚Äúnote‚Äù title=‚ÄúNote‚Äù icon=‚Äúfa fa-comment‚Äù %}}\nL‚Äôutilisation de l‚Äôoption ‚Äúlxml‚Äù avec BeautifulSoup n√©cessite d‚Äôavoir\ninstall√© avant cela la librairie lxml.\n{{% /box %}}\n\nresponse = requests.get(URL_SEARCH).text\nsoup = BeautifulSoup(response, \"lxml\")\n\n\nAfficher le code source html de la page des r√©sultats de la recherche √† l‚Äôaide de votre navigateur (click droit sur la page =&gt; Inspecter),\nanalyser la structure de l‚Äôarbre,\net r√©cup√©rer le code html de chacune des recettes.\nA l‚Äôaide d‚Äôune boucle, r√©cup√©rer pour chaque recette sa note moyenne et le nombre de fois o√π elle a √©t√© not√©e.\n\n\nimport re\nimport numpy as np\n\n\nfound_recipes = soup.find_all(name=\"a\", class_=re.compile(\"^SearchResultsstyle__SearchCardResult\"))\n\nall_ratings = []\nall_nb_ratings = []\nfor recipe in found_recipes:\n    try:\n        ratings_info = recipe.find(name=\"div\", class_=re.compile(\"^RecipeCardResultstyle__RatingLayout\")).text\n    except AttributeError:\n        continue\n    matches = re.search(r\"([\\d\\.]+)/5\\(([\\d]+) avis\\)\", ratings_info, re.IGNORECASE)\n    rating = matches.group(1)\n    nb_ratings = matches.group(2)\n    all_ratings.append(float(rating))\n    all_nb_ratings.append(int(nb_ratings))\n\n\nSur Marmiton, on peut tomber sur de mauvaises surprises.\nPour √©viter √ßa, restreindre les recettes √† celles qui ont une note moyenne &gt;= 4 et un nombre de notes &gt;= 50.\nChoisir la recette la mieux not√©e au sein de cette liste de candidates, et r√©cup√©rer son URL.\n\n\nMIN_RATING = 4\nMIN_NB_RATINGS = 50\n\nidxs_eligible = [i for i, x in enumerate(found_recipes)\n                 if all_ratings[i] &gt; MIN_RATING and all_nb_ratings[i] &gt;= MIN_NB_RATINGS]\nidx_chosen = np.argmax(np.array(all_ratings)[idxs_eligible])\nhref_chosen = found_recipes[idx_chosen].get(\"href\")\n\nif href_chosen is not None:\n    url_chosen_recipe = BASE_URL + href_chosen\nelse:\n    raise ValueError(\"Aucune recette n'a √©t√© trouv√©e pour les crit√®res demand√©s.\")\n\n\nR√©cup√©rer une photo de la recette et l‚Äôafficher dans le Notebook.\n\n\nlist_imgs = found_recipes[idx_chosen].find(name=\"source\", type=\"image/jpeg\").get(\"srcset\")\nurl_img_big = re.split(\"\\s\\d+w,?\\s?\", list_imgs)[-2]\n\n\n# Dans un notebook\nfrom IPython.display import Image\nImage(url_img_big, width=400, height=400)\n\nNous avons choisi cette recette, un classique ! :spaghetti:\nConvaincu ?\nSinon, ne pas h√©siter √† changer de recette au d√©but,\non ne va quand m√™me pas faire tout √ßa pour rien.\nLa recette est choisie, pour nous c‚Äôest pates carbo.\nNouvel objectif : faire la liste de courses ! :purse:\nMais les choses se compliquent : pour quantifier les ingr√©dients selon le nombre de convives\net afficher la liste au format courses sur Marmiton, on va devoir cliquer sur des boutons qui ex√©cutent du JavaScript.\nLes librairies requests et BeautifulSoup atteignent l√† leurs limites, mais pas de panique : Selenium est fait pour √ßa.\nIl va nous permettre d‚Äôouvrir un navigateur ‚Äúfant√¥me‚Äù, contr√¥l√© via Python, avec lequel on va pouvoir effectuer des actions sur la page\n(comme le ferait une personne naviguant sur la page web).\nAutre subtilit√© : jusqu‚Äô√† maintenant, on a rep√©r√© les √©l√©ments html par type et nom de classe.\nCette m√©thode fonctionne, mais elle pose √©galement des probl√®mes :\nparfois les noms de classes changent sans raison (c‚Äôest d‚Äôailleurs pour √ßa qu‚Äôon a utilis√© des regex pr√©c√©demment, pour faire du matching partiel),\net il est moins pratique d‚Äôinteragir avec les √©l√©ments d‚Äôune page de cette mani√®re. Parfois, il est\npertinent d‚Äôutiliser les s√©l√©cteurs XPath,\nqui permettent de s√©lectionner les √©l√©ments selon leur position dans l‚Äôarborescence html de la page.\nOn utilisera une combinaison des deux m√©thodes dans cette partie selon les cas.\nOn le voit, le webscraping reste une pratique assez instable,\ndans la mesure o√π les sites web √©voluent en permanence.\nIl y a ainsi toutes les chances qu‚Äôau moment o√π vous effectuerez ce TP, le code propos√© en solution ne fonctionne plus, car les balises auront chang√©.\nIl vous faudra alors revenir √† l‚Äôexploration du code source html de la page, rep√©rer les balises permanentes, et les substituer dans le code de solution.\n\nOuvrir la page de la recette choisie √† l‚Äôaide d‚Äôun navigateur fant√¥me.\nProbl√®me : la classique fen√™tre de politique des cookies :cookie: s‚Äôouvre, nous emp√™chant de naviguer sur la page.\nUtiliser Selenium pour cliquer sur le bouton permettant d‚Äôaccepter tous les cookies.\n\n\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.common.exceptions import NoSuchElementException\n\n\ndriver = webdriver.Chrome()\ndriver.get(url_chosen_recipe)\n\n\nWebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, \"didomi-notice-agree-button\"))).click()\n\n\nChoisir pour combien de personnes on va cuisiner.\nComparer ce nombre au nombre utilis√© par d√©faut sur Marmiton,\net construire une boucle qui va clicker automatiquement le bon nombre de fois,\nsur + ou - selon que le nombre de convives choisi est sup√©rieur ou inf√©rieur au nombre par d√©faut.\n\n\ntry:\n    counter = driver.find_element_by_class_name(\"quantity-counter\")\nexcept NoSuchElementException:\n    raise Exception(\"La structure de cette page est particuli√®re, il va falloir trouver les bonnes balises √† la main.\")\nelse:\n    xpath_current_count = './/input'\n    current_count = int(counter.find_element_by_xpath('.//div[2]/input').get_attribute(\"value\"))\n    xpath_minus = './/div[@class=\"quantity-counter__action minus\"]'\n    xpath_plus = './/div[@class=\"quantity-counter__action plus\"]'\n\n\nNB_PERSONNES = 8\n\nnb_clicks = NB_PERSONNES - current_count\nif nb_clicks &gt; 0:\n    xpath_button_nb_persons = xpath_plus\nelif nb_clicks &lt; 0:\n    xpath_button_nb_persons = xpath_minus\nfor i in range(abs(nb_clicks)):\n    driver.find_element_by_xpath(xpath_button_nb_persons).click()\n\n\nMarmiton a un mode liste de courses qui va nous √™tre bien pratique pour r√©cup√©rer les ingr√©dients au bon format.\nA l‚Äôaide de Selenium, cliquer sur le bouton ‚Äúliste‚Äù (√† droite de l‚Äôoutil pour ajuster le nombre de personnes).\n\n\ndisplay_options = driver.find_element_by_class_name(\"ingredient-list__display-options\")\ndisplay_options.find_element_by_xpath(\".//i[2]\").click()\n\n\nSelon les cas, il peut √™tre n√©cessaire de cliquer ensuite sur un autre bouton permettant de d√©velopper la liste.\nEffectuer cette action (si n√©cessaire !).\nCela permettra d‚Äô√™tre s√ªr que l‚Äôon r√©cup√®re bien tous les ingr√©dients pour construire notre liste de courses.\n\n\n# xpath_expand_list = \"/html/body/div[2]/div[3]/main/div/div/div[1]/div[1]/div[7]/div[2]/div[3]\"\n# try:\n#     driver.find_element_by_xpath(xpath_expand_list).click()\n# except ElementClickInterceptedException:\n#     pass\n\n# driver.implicitly_wait(2)  # Make sure that the elements are displayed after pressing button\n\n\nR√©cup√©rer la liste des ingr√©dients ainsi que des quantit√©s n√©cessaires. Stocker les √©l√©ments dans une liste.\n\n\nlist_ings_div = driver.find_element_by_class_name(\"ingredient-list__ingredient-group\")\nlist_ings = [x.text for x in list_ings_div.find_elements_by_tag_name(\"li\")]\nprint(list_ings)\n\n['500 g de lardons', 'poivre', '2 pinc√©es de sel', '1 kg de p√¢tes', '1 l de cr√®me fra√Æche', \"6 jaunes d'oeuf\", '2 oignons']\n\nExporter la liste dans un fichier texte sur votre ordinateur.\n\n\nwith open(\"shopping_list.txt\", \"w\") as f:\n    for ing in list_ings:\n        f.write(ing + \"\\n\")\n\n\nLa liste est pr√™te, mais il va aussi nous falloir la recette. R√©cup√©rer la recette, et l‚Äôexporter dans un fichier texte s√©par√©, qui porte le nom du plat choisi.\n\n\nrecipe = driver.find_element_by_class_name(\"recipe-step-list\").text\n\nwith open(f\"recipe_{PLAT}.txt\", \"w\") as f:\n    f.write(recipe)\n\n\nL‚Äôoutil fonctionne‚Ä¶ pour un plat donn√©. Adapter le code pr√©c√©dent pour prendre en entr√©e une liste de plats, et retourner en sortie la liste de courses compl√®te (en un seul fichier) pour pouvoir r√©aliser ces diff√©rents plats. Hint: il sera s√ªrement utile de faire une fonction qui prend en input un plat et exporte la liste de courses pour ce plat, et ensuite d‚Äôappeler cette fonction pour chaque plat dans le cadre d‚Äôune boucle. Attention de ne pas √©craser la liste de courses pr√©c√©dentes √† chaque fois !"
  },
  {
    "objectID": "content/course/manipulation/07_dask/index.html#pourquoi-utiliser-dask",
    "href": "content/course/manipulation/07_dask/index.html#pourquoi-utiliser-dask",
    "title": "44¬† Introduction √† dask gr√¢ce aux donn√©es DVF",
    "section": "44.1 Pourquoi utiliser Dask ?",
    "text": "44.1 Pourquoi utiliser Dask ?\nOn peut se r√©f√©rer √† la page https://docs.dask.org/en/stable/why.html\nPlusieurs points sont mis en avant dans la documentation officielle et sont r√©sum√©s ci-dessous:\n- Dask ressemble fortement en termes de syntaxe √† pandas et numpy ;\n- Dask peut √™tre utilis√© sur un ordinateur seul ou sur un cloud cluster. Avec Dask, on peut traiter des bases de 100GB sur un ordinateur portable, voire m√™me 1TB sans m√™me avoir besoin d‚Äôun cluster big data ;\n- Dask requiert peu de temps d‚Äôinstallation puisqu‚Äôil peut √™tre install√© avec le gestionnaire de packages conda (il est m√™me livr√© dans la distribution par d√©faut d‚ÄôAnaconda)\n\n44.1.1 Comment Dask se compare √† Spark ?\nDans le monde du big-data, un √©cosyst√®me concurrent existe: Spark. Globalement, lorsqu‚Äôon a compris la logique\nde l‚Äôun, il est tr√®s facile de faire la transition vers l‚Äôautre si besoin1. Pour ma part, j‚Äôai principalement fait du Spark sur\ndes donn√©es de t√©l√©phonie de plusieurs TB. En fait, la logique sera la m√™me que celle de Dask sur donn√©es moins volumineuses.\n\nSpark est √©crit en Scala √† l‚Äôorigine. Le package pyspark permet d‚Äô√©crire en Python et s‚Äôassure de la traduction en Python afin d‚Äôinteragir avec les machines virtuelles Java (JVM) n√©cessaires pour la parall√©lisation des op√©rations Spark. Dask est quant √† lui √©crit en Python, ce qui est un √©cosyst√®me plus l√©ger. Pour gagner en performance, il permet d‚Äôinteragir avec du code C/C++ entre autres ;\nL‚Äôinstallation de Spark est plus lourde que celle de Dask\nSpark est un projet Apache en lui-m√™me alors que Dask intervient comme une composante de l‚Äôunivers Python;\nSpark est un peu plus vieux (2010 versus 2014 pour Dask) ;\nSpark permet de tr√®s bien faire des op√©rations classiques SQL et des ETLs, et proposer ses propres librairies de parall√©lisation de mod√®les de machine learning. Pour faire du machine learning avec Spark il faut aller piocher dans Spark MLLib. Dask permet quant √† lui de bien interagir avec scikit-learn et de faire de la mod√©lisation.\n\nGlobalement, il faut retenir que Dask comme Spark ne sont int√©ressants que pour des donn√©es dont le traitement engendre des probl√®mes de RAM. Autrement, il\nvaut mieux se contenter de pandas."
  },
  {
    "objectID": "content/course/manipulation/07_dask/index.html#d√©monstration-de-quelques-features-de-dask",
    "href": "content/course/manipulation/07_dask/index.html#d√©monstration-de-quelques-features-de-dask",
    "title": "44¬† Introduction √† dask gr√¢ce aux donn√©es DVF",
    "section": "44.2 D√©monstration de quelques features de Dask",
    "text": "44.2 D√©monstration de quelques features de Dask\n\n44.2.1 Pr√©sentation du Dask.DataFrame\nNous allons utiliser les donn√©es immobili√®res DVF pour montrer quelques √©l√©ments clefs de Dask.\n\n# Import dvf files \nimport pandas as pd\nimport dask.dataframe as dd\n\nd_urls = {\n    \"2019\" : 'https://www.data.gouv.fr/fr/datasets/r/3004168d-bec4-44d9-a781-ef16f41856a2',\n    \"2020\" : \"https://www.data.gouv.fr/fr/datasets/r/90a98de0-f562-4328-aa16-fe0dd1dca60f\",\n    \"2021\": \"https://www.data.gouv.fr/fr/datasets/r/817204ac-2202-4b4a-98e7-4184d154d98c\"\n}\n\n\ndef import_dvf_one_year(year, dict_url = d_urls):\n    df = pd.read_csv(dict_url[year], sep = \"|\", decimal=\",\")\n    df[\"year\"] = year\n    return df\n\ndef import_dvf_all_years(dict_url = d_urls):\n    dfs = [import_dvf_one_year(y, dict_url) for y in dict_url.keys()]\n    df = pd.concat(dfs).reset_index()\n    df = df.drop([\"level_0\", \"level_1\"], axis=1)\n    return df\n\nDans un premier temps, on va utiliser pandas pour\nimporter une ann√©e de donn√©es (mill√©sime 2019), ces derni√®res tenant en m√©moire\nsur un ordinateur normalement dot√© en RAM2:\n\ndvf = import_dvf_one_year(\"2019\")\ndvf.shape\ndvf.head()\n\n/tmp/ipykernel_1829/455432909.py:13: DtypeWarning:\n\nColumns (18,23,24,26,28,41) have mixed types. Specify dtype option on import or set low_memory=False.\n\n\n\n\n\n\n\n\n\n\nIdentifiant de document\nReference document\n1 Articles CGI\n2 Articles CGI\n3 Articles CGI\n4 Articles CGI\n5 Articles CGI\nNo disposition\nDate mutation\nNature mutation\n...\nNombre de lots\nCode type local\nType local\nIdentifiant local\nSurface reelle bati\nNombre pieces principales\nNature culture\nNature culture speciale\nSurface terrain\nyear\n\n\n\n\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n04/01/2019\nVente\n...\n1\n2.0\nAppartement\nNaN\n20.0\n1.0\nNaN\nNaN\nNaN\n2019\n\n\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n04/01/2019\nVente\n...\n2\n2.0\nAppartement\nNaN\n62.0\n3.0\nNaN\nNaN\nNaN\n2019\n\n\n2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n04/01/2019\nVente\n...\n1\n3.0\nD√©pendance\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\n2019\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n08/01/2019\nVente\n...\n0\n1.0\nMaison\nNaN\n90.0\n4.0\nS\nNaN\n940.0\n2019\n\n\n4\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n07/01/2019\nVente\n...\n0\n1.0\nMaison\nNaN\n101.0\n5.0\nS\nNaN\n490.0\n2019\n\n\n\n\n5 rows √ó 44 columns\n\n\n\nIci on travaille sur un DataFrame d‚Äôenviron 3.5 millions de lignes et 44 variables.\nL‚Äôobjet dvf est un pandas.DataFrame\nqui tient en m√©moire sur le SSP-Cloud ou sur les serveurs utilis√©s\npour construire ce site web.\n\n\n Exercice 1\nOn aurait pu lire directement les csv dans un dask.DataFrame avec le read_csv de dask. Comme exercice, vous pouvez essayer de le faire\npour une ann√©e (analogue de la fonction import_dvf_one_year) puis sur toutes les donn√©es (analogue de la fonction import_dvf_all_years).\n\n\nOn peut cr√©er une structure Dask directement √† partir\nd‚Äôun DataFrame pandas avec la m√©thode from_pandas.\n\ndvf_dd = dd.from_pandas(dvf, npartitions=10) \ndvf_dd\n\nDask DataFrame Structure:\n\n\n\n\n\n\n\nIdentifiant de document\nReference document\n1 Articles CGI\n2 Articles CGI\n3 Articles CGI\n4 Articles CGI\n5 Articles CGI\nNo disposition\nDate mutation\nNature mutation\nValeur fonciere\nNo voie\nB/T/Q\nType de voie\nCode voie\nVoie\nCode postal\nCommune\nCode departement\nCode commune\nPrefixe de section\nSection\nNo plan\nNo Volume\n1er lot\nSurface Carrez du 1er lot\n2eme lot\nSurface Carrez du 2eme lot\n3eme lot\nSurface Carrez du 3eme lot\n4eme lot\nSurface Carrez du 4eme lot\n5eme lot\nSurface Carrez du 5eme lot\nNombre de lots\nCode type local\nType local\nIdentifiant local\nSurface reelle bati\nNombre pieces principales\nNature culture\nNature culture speciale\nSurface terrain\nyear\n\n\nnpartitions=10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nint64\nobject\nobject\nfloat64\nfloat64\nobject\nobject\nobject\nobject\nfloat64\nobject\nobject\nint64\nfloat64\nobject\nint64\nobject\nobject\nfloat64\nobject\nfloat64\nobject\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nint64\nfloat64\nobject\nfloat64\nfloat64\nfloat64\nobject\nobject\nfloat64\nobject\n\n\n362591\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3263313\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3625902\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n\n\n\nDask Name: from_pandas, 1 graph layer\n\n\nPour souligner la diff√©rence avec un pandas.DataFrame,\nl‚Äôaffichage diff√®re. Seule la structure du dask.DataFrame\nest affich√©e et non son contenu car les donn√©es\ndask ne sont pas charg√©es en m√©moire.\n\n\n Warning\nAttention, Dask ne peut cr√©er un Dask.DataFrame √† partir d‚Äôun pandas.DataFrame multi-index√©.\nDans ce cas il a fallu faire un reset_index() pour avoir un unique index.\n\n\nOn a ainsi la structure de notre dask.DataFrame, soit environ 3.5 millions de lignes, avec 44 colonnes en 10 partitions, soit environ 350 000 observations par partition.\nIl faut savoir que Dask produit des Array, Bag et DataFrames, qui fonctionnent comme dans Numpy et Pandas (il est possible de cr√©er d‚Äôautres structures ad hoc, cf plus loin).\nDask, comme Spark et en fait comme la plupart des frameworks permettant de\ntraiter des donn√©es plus volumineuses que la RAM disponible,\nrepose sur le principe du partitionnement et de la parall√©lisation\ndes op√©rations. Les donn√©es ne sont jamais import√©es dans leur\nensemble mais par bloc. Un plan des op√©rations √† effectuer est\nensuite appliquer sur chaque bloc (nous reviendrons\nsur ce principe), ind√©pendamment. La particularit√© de Dask,\npar rapport √† Spark,\nest que chaque bloc est un pandas.DataFrame, ce qui\nrend tr√®s facile l‚Äôapplication de manipulations de donn√©es\ntraditionnelles sur des sources volumineuses:\n\nLe site de Dask cite une r√®gle qui est la suivante :\n\n‚ÄúHave 5 to 10 times as much RAM as the size of your dataset‚Äù,\n@mckinney2017apache, 10 things I hate about pandas\n\nSur disque, en sauvegardant en CSV, on\nobtient une base de 1.4GB. Si l‚Äôon suit la r√®gle du pouce donn√©e plus haut, on va avoir besoin d‚Äôune RAM entre 7-14GB pour traiter la donn√©e, en fonction de nos traitements qui seront plus ou moins intensifs. Autrement dit, si on a moins de 8GB de RAM, il devient int√©ressant de faire appel √† dask, sinon il vaut mieux privil√©gier pandas (sauf si on fait des\ntraitements tr√®s intensifs en calculs).\nIl existe un autre objet dask, les Array pour reprendre la logique de numpy. De la m√™me mani√®re qu‚Äôun dask.DataFrame est en quelque sorte un ensemble de pandas.DataFrame, un dask.Array est un ensemble de numpy.Array qui sont plus importants en taille que la RAM. On pourra utiliser les op√©rations courantes numpy avec dask de la m√™me mani√®re que le dask DataFrame r√©plique la logique du pandas DataFrame.\n\n\n Hint\nLe choix du nombre de partition (10) est arbitraire ici. Bien qu‚Äôon puisse\ntrouver des r√®gles du pouce pour fixer un nombre optimal de\npartitions, cela d√©pend de beaucoup de facteurs et, en pratique,\nrien ne remplace l‚Äôessai-erreur. Par exemple, la documentation Dask recommande des blocs d‚Äôenviron\n100MB\nce qui peut convenir pour des ordinateurs √† la RAM limit√©e mais n‚Äôa pas\nforc√©ment de sens pour des machines ayant 16GB de RAM.\nUn nombre important de partition va permettre de faire des op√©rations\nsur des petits blocs de donn√©es, ce qui permettra de gagner en vitesse\nd‚Äôex√©cution. Le prix √† payer est beaucoup d‚Äôinput/output car\nDask va passer du temps √† lire beaucoup de blocs de donn√©es et √©crire\ndes bases interm√©diaires.\n\n\nOn peut acc√©der aux index que couvrent les partitions de la mani√®re suivante:\n\ndvf_dd.divisions\n\n(0,\n 362591,\n 725182,\n 1087773,\n 1450363,\n 1812953,\n 2175543,\n 2538133,\n 2900723,\n 3263313,\n 3625902)\n\n\nAutrement dit, la premi√®re partition couvrira les lignes 0 √† 362591. La deuxi√®me les lignes 362592 √† 725182, etc.\nEt on peut directement acc√©der √† une partition gr√¢ce aux crochets []:\n\ndvf_dd.partitions[0]\n\nDask DataFrame Structure:\n\n\n\n\n\n\n\nIdentifiant de document\nReference document\n1 Articles CGI\n2 Articles CGI\n3 Articles CGI\n4 Articles CGI\n5 Articles CGI\nNo disposition\nDate mutation\nNature mutation\nValeur fonciere\nNo voie\nB/T/Q\nType de voie\nCode voie\nVoie\nCode postal\nCommune\nCode departement\nCode commune\nPrefixe de section\nSection\nNo plan\nNo Volume\n1er lot\nSurface Carrez du 1er lot\n2eme lot\nSurface Carrez du 2eme lot\n3eme lot\nSurface Carrez du 3eme lot\n4eme lot\nSurface Carrez du 4eme lot\n5eme lot\nSurface Carrez du 5eme lot\nNombre de lots\nCode type local\nType local\nIdentifiant local\nSurface reelle bati\nNombre pieces principales\nNature culture\nNature culture speciale\nSurface terrain\nyear\n\n\nnpartitions=1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nint64\nobject\nobject\nfloat64\nfloat64\nobject\nobject\nobject\nobject\nfloat64\nobject\nobject\nint64\nfloat64\nobject\nint64\nobject\nobject\nfloat64\nobject\nfloat64\nobject\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nint64\nfloat64\nobject\nfloat64\nfloat64\nfloat64\nobject\nobject\nfloat64\nobject\n\n\n362591\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n\n\n\nDask Name: blocks, 2 graph layers\n\n\n\n\n44.2.2 La ‚Äúlazy evaluation‚Äù\nDask fait de la ‚Äúlazy evaluation‚Äù. Cela signifie que le r√©sultat n‚Äôest calcul√© que si on le demande explicitement. Dans le cas, contraire, ce que l‚Äôon appelle un dask task graph est produit (on verra plus bas comment voir ce graph).\nPour demander explicitement le r√©sultat d‚Äôun calcul, il faut utiliser la\nm√©thode compute.\nA noter que certaines m√©thodes vont d√©clencher un compute directement, comme par exemple len ou head.\nPar exemple, pour afficher le contenu des 100 premi√®res lignes :\n\ndvf_dd.loc[0:100,:].compute()\n\n\n\n\n\n\n\n\nIdentifiant de document\nReference document\n1 Articles CGI\n2 Articles CGI\n3 Articles CGI\n4 Articles CGI\n5 Articles CGI\nNo disposition\nDate mutation\nNature mutation\n...\nNombre de lots\nCode type local\nType local\nIdentifiant local\nSurface reelle bati\nNombre pieces principales\nNature culture\nNature culture speciale\nSurface terrain\nyear\n\n\n\n\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n04/01/2019\nVente\n...\n1\n2.0\nAppartement\nNaN\n20.0\n1.0\nNaN\nNaN\nNaN\n2019\n\n\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n04/01/2019\nVente\n...\n2\n2.0\nAppartement\nNaN\n62.0\n3.0\nNaN\nNaN\nNaN\n2019\n\n\n2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n04/01/2019\nVente\n...\n1\n3.0\nD√©pendance\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\n2019\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n08/01/2019\nVente\n...\n0\n1.0\nMaison\nNaN\n90.0\n4.0\nS\nNaN\n940.0\n2019\n\n\n4\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n07/01/2019\nVente\n...\n0\n1.0\nMaison\nNaN\n101.0\n5.0\nS\nNaN\n490.0\n2019\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n96\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n04/01/2019\nVente\n...\n0\n1.0\nMaison\nNaN\n48.0\n2.0\nS\nNaN\n935.0\n2019\n\n\n97\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n07/01/2019\nVente\n...\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nP\nNaN\n3264.0\n2019\n\n\n98\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n18/01/2019\nVente\n...\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nP\nNaN\n2870.0\n2019\n\n\n99\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n07/01/2019\nVente\n...\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nT\nNaN\n1423.0\n2019\n\n\n100\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1\n03/01/2019\nVente\n...\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nP\nNaN\n93.0\n2019\n\n\n\n\n101 rows √ó 44 columns\n\n\n\nCe qui est pratique avec dask.dataframe c‚Äôest que de nombreuses m√©thodes sont semblables √† celles de pandas. Par exemple, si l‚Äôon souhaite connaitre les types de locaux pr√©sents dans la base en 2019:\n\ndvf_dd.loc[:,\"Type local\"].value_counts().compute()\n\nType local\nMaison                                      712874\nAppartement                                 656261\nD√©pendance                                  495774\nLocal industriel. commercial ou assimil√©    143194\nName: count, dtype: int64\n\n\nA titre de comparaison, comparons les temps de calculs entre pandas et dask ici:\n\nimport time\nstart_time = time.time()\ndvf_dd.loc[:,\"Type local\"].value_counts().compute()\nprint(f\"{time.time() - start_time} seconds\")\n\n0.16141033172607422 seconds\n\n\n\nstart_time = time.time()\ndvf.loc[:,\"Type local\"].value_counts()\nprint(f\"{time.time() - start_time} seconds\")\n\n0.1491861343383789 seconds\n\n\nOn se rend compte que le pandas.DataFrame a un temps de calcul plus court, mais c‚Äôest parce que dask va nous servir avant tout √† lire des bases dont le traitement exc√®de notre RAM. Donc, cette comparaison n‚Äôexistera tout simplement pas car le pandas.DataFrame n‚Äôaura pas √©t√© charg√© en RAM. On voit dans cet exemple que lorsque le traitement du DataFrame tient en RAM, l‚Äôutilisation de Dask est inutile.\nLes m√©thodes dans Dask peuvent √™tre chain√©es, comme dans pandas, par exemple, on pourra √©crire:\n\nmean_by_year = dvf_dd.loc[~dvf_dd[\"Surface terrain\"].isna(),[\"Surface terrain\", \"year\"]].groupby(\"year\").mean()\n\n\nmean_by_year.compute()\n\n\n\n\n\n\n\n\nSurface terrain\n\n\nyear\n\n\n\n\n\n2019\n3064.674673\n\n\n\n\n\n\n\nLe principe de la lazy evaluation est donc d‚Äôannoncer √† Dask\nqu‚Äôon va effectuer une s√©rie d‚Äôop√©ration qui ne vont se r√©aliser\nque lorsqu‚Äôon fera un appel √† compute. Dask, quant √† lui,\nse chargera d‚Äôoptimiser les traitements.\nComme le plan d‚Äôaction peut devenir difficile √† suivre si on\nd√©sire effectuer beaucoup d‚Äôop√©rations encha√Æn√©es, on peut\nvouloir visualiser le graph de computation de dask.\nAvec celui-ci, on voit toutes les √©tapes que jusqu‚Äôici dask n‚Äôa pas execut√©\net qu‚Äôil va devoir ex√©cuter pour calculer le r√©sultat (compute()).\n\nmean_by_year.dask\n\nEn l‚Äôoccurence on voit l‚Äôencha√Ænement des √©tapes\nfrom_pandas(), getitem, isna, inv et loc-series qui r√©sultent de nos filtres sur le DataFrame. Ensuite,\non voit les √©tapes de groupby et, enfin, pour calculer la moyenne il convient de faire la somme et la division. Toutes ces √©tapes vont √™tre effectu√©es quand on appelle compute() et pas avant (lazy evaluation).\nAfin de voir la structure du dask.DataFrame on peut utiliser la m√©thode visualize()\n\ndvf_dd.visualize() # attention graphviz est requis\n\n\n\n\n\n\n Note\ngraphviz est requis pour ce graphique. S‚Äôil n‚Äôest pas install√© dans votre environnement, faire :\n!pip install graphviz\n\n\nPour construire de v√©ritables pipelines de donn√©es,\nles principes du pipe de pandas √©voqu√© dans cette partie du cours et celui des pipelines scikit, √©voqu√© dans un chapitre d√©di√©\nont √©t√© import√©s dans dask.\n\n\n44.2.3 Probl√®mes de lecture dus √† des types probl√©matiques\nLa m√©thode read_csv de dask va inf√©rer les types du DataFrame √† partir d‚Äô√©chantillon, et va les impl√©menter sur tout le DataFrame seulement au moment d‚Äôune √©tape compute.\nIl peut donc y avoir des erreurs de types d√ªs √† un √©chantillon ne prenant pas en compte certains cas particuliers, causant des erreurs dans la lecture du fichier.\nDans ce cas, et comme de mani√®re g√©n√©rale avec pandas, il peut √™tre recommand√© de faire appel au param√®tre dtype de read_csv - qui est un dict - (la doc de dask nous dit aussi que l‚Äôon peut augmenter la taille de l‚Äô√©chantllon sample)."
  },
  {
    "objectID": "content/course/manipulation/07_dask/index.html#utiliser-dask-avec-le-format-parquet",
    "href": "content/course/manipulation/07_dask/index.html#utiliser-dask-avec-le-format-parquet",
    "title": "44¬† Introduction √† dask gr√¢ce aux donn√©es DVF",
    "section": "44.3 Utiliser Dask avec le format parquet",
    "text": "44.3 Utiliser Dask avec le format parquet\nLe format parquet tend √† devenir le format\nde r√©f√©rence dans le monde de la data-science.\nUne pr√©sentation extensive de celui-ci est disponible\ndans le chapitre d√©di√©.\ndask permet de lire le format parquet, et plus pr√©cis√©ment d‚Äôutiliser des fonctionnalit√©s sp√©cifiques √† ce format. La lecture et l‚Äô√©criture en parquet reposent par d√©faut sur pyarrow. On peut aussi utiliser fastparquet et pr√©ciser dans la lecture/√©criture ce que l‚Äôon souhaite des deux.\n\ndvf_net = dvf.loc[:,[ 'Date mutation', 'Nature mutation', 'Valeur fonciere', 'Commune', \n       'Code commune', 'Type local', 'Identifiant local', 'Surface reelle bati',\n       'Nombre pieces principales', 'Nature culture',\n       'Nature culture speciale', 'Surface terrain', 'year']]\n\nOn va utiliser l‚Äôengine par d√©faut pour\nl‚Äô√©criture de parquet qui est pyarrow (faire pip install pyarrow si vous ne l‚Äôavez pas d√©j√† install√©). to_parquet qui est une m√©thode pandas a √©t√© √©galement √©tendue aux objets dask:\n\ndvf_net.to_parquet(\"dvf/\", partition_cols=\"year\")\n\nLorsqu‚Äôil est partitionn√©, le format parquet am√®ne √† une structure\nde fichiers similaire √† celle-ci:\npath\n‚îî‚îÄ‚îÄ to\n    ‚îî‚îÄ‚îÄ table\n        ‚îú‚îÄ‚îÄ gender=male\n        ‚îÇ   ‚îú‚îÄ‚îÄ ...\n        ‚îÇ   ‚îÇ\n        ‚îÇ   ‚îú‚îÄ‚îÄ country=US\n        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data.parquet\n        ‚îÇ   ‚îú‚îÄ‚îÄ country=CN\n        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data.parquet\n        ‚îÇ   ‚îî‚îÄ‚îÄ ...\n        ‚îî‚îÄ‚îÄ gender=female\n            ‚îú‚îÄ‚îÄ ...\n            ‚îÇ\n            ‚îú‚îÄ‚îÄ country=US\n            ‚îÇ   ‚îî‚îÄ‚îÄ data.parquet\n            ‚îú‚îÄ‚îÄ country=CN\n            ‚îÇ   ‚îî‚îÄ‚îÄ data.parquet\n            ‚îî‚îÄ‚îÄ ...\nOn peut alors facilement traiter un sous-√©chantillon des donn√©es,\npar exemple l‚Äôann√©e 2019:\n\ndvf_2019 = dd.read_parquet(\"dvf/year=2019/\", columns=[\"Date mutation\", \"Valeur fonciere\"]) # On peut s√©lectionner directement les deux colonnes\n\nLorsqu‚Äôil faudra passer √† l‚Äô√©chelle, on changera le chemin en \"dvf/\npour utiliser l‚Äôensemble des donn√©es."
  },
  {
    "objectID": "content/course/manipulation/07_dask/index.html#a-quoi-sert-persist",
    "href": "content/course/manipulation/07_dask/index.html#a-quoi-sert-persist",
    "title": "44¬† Introduction √† dask gr√¢ce aux donn√©es DVF",
    "section": "44.4 A quoi sert persist ?",
    "text": "44.4 A quoi sert persist ?\nPar d√©faut, compute ex√©cute l‚Äôensemble du plan et ne conserve\nen m√©moire que le r√©sultat de celui-ci. Les donn√©es interm√©diaires\nne sont pas conserv√©es. Si on d√©sire r√©utiliser une partie de celui-ci,\npar exemple les premi√®res √©tapes, on devra donc r√©-effectuer\nles calculs.\nIl est possible de garder une partie des donn√©es en m√©moire avec persist(). Les donn√©es sont sauvegard√©es dans des objets appel√©s Futures. Cela peut √™tre int√©ressant si un bloc particulier de donn√©es est utilis√© dans plusieurs compute ou si l‚Äôon a besoin de voir ce qu‚Äôil y a √† l‚Äôint√©rieur souvent.\n\ndvf_dd_mem = dvf_dd.persist()\n\n\nstart_time = time.time()\ndvf_dd_mem.head()\nprint(f\"{time.time() - start_time} seconds\")\n\n\nstart_time = time.time()\ndvf_dd.head()\nprint(f\"{time.time() - start_time} seconds\")\n\n0.004109621047973633 seconds\n\n\nOn a bien un temps plus important avec le dask.DataFrame initial, compar√© avec celui sur lequel on a utilis√© persist. L‚Äôop√©ration qu‚Äôon r√©alise ici √©tant peu complexe, la diff√©rence n‚Äôest pas substantielle. Elle serait beaucoup plus marqu√©e avec un jeu de donn√©es plus volumineux ou des √©tapes intensives en calcul."
  },
  {
    "objectID": "content/course/manipulation/07_dask/index.html#aller-plus-loin-utiliser-le-decorator-dask.delayed-pour-parall√©liser-du-code",
    "href": "content/course/manipulation/07_dask/index.html#aller-plus-loin-utiliser-le-decorator-dask.delayed-pour-parall√©liser-du-code",
    "title": "44¬† Introduction √† dask gr√¢ce aux donn√©es DVF",
    "section": "44.5 Aller plus loin: Utiliser le decorator dask.delayed pour parall√©liser du code",
    "text": "44.5 Aller plus loin: Utiliser le decorator dask.delayed pour parall√©liser du code\nIl est possible de parall√©liser des fonctions par exemple en utilisant le decorator dask.delayed. Cela permet de rendre les fonctions lazy. Cela signifie que lorsqu‚Äôon appelle la fonction, un delayed object est construit. Pour avoir le r√©sultat, il faut faire un compute. Pour aller plus loin: https://tutorial.dask.org/03_dask.delayed.html.\nPrenons par exemple des fonctions permettant de calculer\ndes aires et des p√©rim√®tres. Comme il s‚Äôagit d‚Äôune op√©ration\ntr√®s peu complexe, on ajoute un d√©lai de calcul avec time.sleep\npour que le timer ne nous sugg√®re pas que l‚Äôop√©ration est\ninstantan√©e.\n\ndef aire_carre(longueur):\n    time.sleep(1)\n    return longueur**2\n\ndef perimetre_carre(longueur):\n    time.sleep(1)\n    return 4*longueur\n\ndef ajout_aire_perim(a, b):\n    return a + b\n\nSans timer, c‚Äôest-√†-dire de mani√®re classique,\non ferait nos appels de fonctions de la\nmani√®re suivante:\n\nstart_time = time.time()\ncar1 = aire_carre(7)\ncar2 = perimetre_carre(9)\ncar3 = ajout_aire_perim(car1, car2)\ncar3\nprint(time.time() - start_time)\n\n2.0027363300323486\n\n\nAvec le d√©corateur dask.delayed, on d√©finit\nnos fonctions de la mani√®re suivante:\n\nimport dask\n\n@dask.delayed\ndef aire_carre(longueur):\n    time.sleep(1)\n    return longueur**2\n\n@dask.delayed\ndef perimetre_carre(longueur):\n    time.sleep(1)\n    return 4*longueur\n\n@dask.delayed\ndef ajout_aire_perim(a, b):\n    return a + b\n\nL‚Äôappel de fonctions est identique\n\ncar1 = aire_carre(7)\ncar2 = perimetre_carre(9)\ncar3 = ajout_aire_perim(car1, car2)\n\nCependant, en fait rien n‚Äôa √©t√© calcul√©, si l‚Äôon souhaite le r√©sultat, il faut appeler compute:\n\nstart_time = time.time()\ncar3.compute()\nprint(time.time() - start_time)\n\n1.002293348312378\n\n\nIci l‚Äôint√©r√™t est assez limit√©, mais on voit que l‚Äôon r√©duit quand m√™me de 2 √† 1 seconde le temps de calcul. Mais l‚Äôid√©e derri√®re est que l‚Äôon a transform√© car3 en un objet Delayed. Cela a g√©n√©r√© un task graph permettant de parall√©liser certaines op√©rations.\nIci il est important de noter que les fonctions que l‚Äôon parall√©lise doivent mettre un certain temps, sinon il n‚Äôy aura pas de gain de performance (si on retire le time.sleep il n‚Äôy a pas de gain de performance car le fait de parall√©liser rajoute en fait du temps vu que chaque fonction a un temps de calcul trop faible pour que la parall√©lisation soit int√©ressante).\n\ncar3.visualize() # on peut visualiser le task graph et voir ce qui est fait en parall√®le \n\n\n\n\nIl y a des exercices int√©ressants dans la doc de Dask sur les objets Delayed, notamment sur la parall√©lisation de s√©quence de traitement de donn√©es. Ils donnent l‚Äôexemple d‚Äôun ensemble de csv ayant le m√™me format dont on veut r√©sumer un indicateur final. On peut appliquer le decorator √† une fonction permettant de lire le csv, puis utiliser une boucle for pour lire chaque fichier et appliquer les traitements. Ensuite, il faudra appeler compute sur l‚Äôobjet final que l‚Äôon souhaite.\nPour aller plus loin sur l‚Äôutilisation de Dask sur un cluster voir https://tutorial.dask.org/04_distributed.html."
  },
  {
    "objectID": "content/course/manipulation/07_dask/index.html#remerciements",
    "href": "content/course/manipulation/07_dask/index.html#remerciements",
    "title": "44¬† Introduction √† dask gr√¢ce aux donn√©es DVF",
    "section": "44.6 Remerciements",
    "text": "44.6 Remerciements\nCe chapitre a √©t√© r√©dig√© avec Rapha√´le Adjerad."
  },
  {
    "objectID": "content/course/manipulation/03_geopandas_tutorial/index.html#donn√©es-spatiales-quelle-diff√©rence-avec-des-donn√©es-traditionnelles",
    "href": "content/course/manipulation/03_geopandas_tutorial/index.html#donn√©es-spatiales-quelle-diff√©rence-avec-des-donn√©es-traditionnelles",
    "title": "45¬† Donn√©es spatiales: d√©couverte de geopandas",
    "section": "45.1 Donn√©es spatiales: quelle diff√©rence avec des donn√©es traditionnelles ?",
    "text": "45.1 Donn√©es spatiales: quelle diff√©rence avec des donn√©es traditionnelles ?\nLe terme ‚Äúdonn√©es spatiales‚Äù d√©signe les donn√©es qui portent sur les caract√©ristiques g√©ographiques des objets (localisation, contours, liens).\nLes caract√©ristiques g√©ographiques des objets sont d√©crites √† l‚Äôaide d‚Äôun syst√®me de coordonn√©es\nqui permettent une repr√©sentation dans un espace euclidien (\\((x,y)\\)).\nLe passage de l‚Äôespace r√©el (la Terre, qui est une sph√®re) √† l‚Äôespace plan\nse fait gr√¢ce √† un syst√®me de projection. Voici quelques exemples\nde donn√©es spatiales :\n\nUne table d√©crivant des b√¢timents, avec les coordonn√©es g√©ographiques de chaque b√¢timent;\nLe d√©coupage communal du territoire, avec le contour du territoire de chaque commune;\nLes routes terrestres, avec les coordonn√©es d√©crivant leur parcours.\n\nLes donn√©es spatiales rassemblent classiquement deux types de donn√©es :\n\ndes donn√©es g√©ographiques (ou g√©om√©tries): objets g√©om√©triques tels que des points, des vecteurs, des polygones, ou des maillages (raster). Exemple: la forme de chaque chaque commune, les coordonn√©es d‚Äôun b√¢timent;\ndes donn√©es attributaires (ou attributs): des mesures et des caract√©ristiques associ√©s aux objets g√©om√©triques. Exemple: la population de chaque commune, le nombre de fen√™tres et le nombre d‚Äô√©tages d‚Äôun b√¢timent.\n\nLes donn√©es spatiales sont fr√©quemment trait√©es √† l‚Äôaide d‚Äôun syst√®me d‚Äôinformation g√©ographique (SIG), c‚Äôest-√†-dire un syst√®me d‚Äôinformation capable de stocker, d‚Äôorganiser et de pr√©senter des donn√©es alphanum√©riques spatialement r√©f√©renc√©es par des coordonn√©es dans un syst√®me de r√©f√©rence (CRS). Python dispose de fonctionnalit√©s lui permettant de r√©aliser les m√™mes t√¢ches qu‚Äôun SIG (traitement de donn√©es spatiales, repr√©sentations cartographiques).\nLes syst√®mes de projection font l‚Äôobjet de standards internationaux et sont souvent d√©sign√©s par des codes dits codes EPSG. Ce site est un bon aide-m√©moire. Les plus fr√©quents, pour les utilisateurs fran√ßais, sont les suivants (plus d‚Äôinfos ici):\n\n2154: syst√®me de projection Lambert 93. Il s‚Äôagit du syst√®me de projection officiel. La plupart des donn√©es diffus√©es par l‚Äôadministration pour la m√©tropole sont disponibles dans ce syst√®me de projection.\n27572: Lambert II √©tendu. Il s‚Äôagit de l‚Äôancien syst√®me de projection officiel. Les donn√©es spatiales anciennes peuvent √™tre dans ce format.\n4326: WGS 84 ou syst√®me de pseudo-Mercator. Attention, ce n‚Äôest en r√©alit√© pas un syst√®me de projection mais un syst√®me de coordonn√©es (longitude / latitude) qui permet simplement un rep√©rage angulaire sur l‚Äôellipso√Øde. Il est utilis√© pour les donn√©es GPS.\n\n\n45.1.1 De pandas √† geopandas\nLe package geopandas est une bo√Æte √† outils con√ßue pour faciliter la manipulation de donn√©es spatiales. La grande force de geopandas est qu‚Äôil permet de manipuler des donn√©es spatiales comme s‚Äôil s‚Äôagissait de donn√©es traditionnelles, car il repose sur le standard ISO 19125 simple feature access d√©fini conjointement par l‚ÄôOpen Geospatial Consortium (OGC) et l‚ÄôInternational Organization for Standardization (ISO).\nPar rapport √† un DataFrame standard, un objet geopandas comporte\nune colonne suppl√©mentaire: geometry. Elle stocke les coordonn√©es des\nobjets g√©ographiques (ou ensemble de coordonn√©es s‚Äôagissant de contours). Un objet geopandas h√©rite des propri√©t√©s d‚Äôun\nDataFrame pandas mais propose des m√©thodes adapt√©es au traitement des donn√©es spatiales.\nAinsi, gr√¢ce √† Geopandas, on pourra effectuer des manipulations sur les attributs des donn√©es comme avec pandas mais on pourra √©galement faire des manipulations sur la dimension spatiale des donn√©es. En particulier,\n\nCalculer des distances et des surfaces;\nAgr√©ger rapidement des zonages (regrouper les communes en d√©partement par exemple);\nTrouver dans quelle commune se trouve un b√¢timent √† partir de ses coordonn√©es g√©ographiques;\nRecalculer des coordonn√©es dans un autre syst√®me de projection.\nFaire une carte, rapidement et simplement\n\n\n\n Hint\nLes manipulations de donn√©es sur un objet Geopandas sont nettement plus lentes que sur\nun DataFrame traditionnel (car Python doit g√©rer les informations g√©ographiques pendant la manipulation des donn√©es).\nLorsque vous manipulez des donn√©es de grandes dimensions,\nil peut √™tre pr√©f√©rable d‚Äôeffectuer les op√©rations sur les donn√©es avant de joindre une g√©om√©trie √† celles-ci.\n\n\nPar rapport √† un logiciel sp√©cialis√© comme QGIS, Python permettra\nd‚Äôautomatiser le traitement et la repr√©sentation des donn√©es. D‚Äôailleurs,\nQGIS utilise lui-m√™me python‚Ä¶"
  },
  {
    "objectID": "content/course/manipulation/03_geopandas_tutorial/index.html#importer-des-donn√©es-spatiales",
    "href": "content/course/manipulation/03_geopandas_tutorial/index.html#importer-des-donn√©es-spatiales",
    "title": "45¬† Donn√©es spatiales: d√©couverte de geopandas",
    "section": "45.2 Importer des donn√©es spatiales",
    "text": "45.2 Importer des donn√©es spatiales\nLes donn√©es spatiales sont plus riches que les donn√©es traditionnelles car elles\nincluent, habituellement, des √©l√©ments suppl√©mentaires pour placer dans\nun espace cart√©sien les objets. Cette dimension suppl√©mentaire peut √™tre simple\n(un point comporte deux informations suppl√©mentaire: \\(x\\) et \\(y\\)) ou\nassez complexe (polygones, lignes avec direction, etc.)\nLes formats les plus communs de donn√©es spatiales sont les suivants :\n\nshapefile (.shp): format (propri√©taire) le plus commun de donn√©es g√©ographiques.\nLa table de donn√©es (attributs) est stock√©e dans un fichier s√©par√© des\ndonn√©es spatiales. En faisant geopandas.read_file(\"monfichier.shp\"), le\npackage fait lui-m√™me le lien entre les observations et leur repr√©sentation spatiale ;\ngeopackage (.gpkg) : ce (relativement) nouveau format libre en un seul fichier √©galement (lui recommand√© par l‚ÄôOGC) vise progressivement √† se substituer au shapefile. Il est par exemple le format par d√©faut dans QGIS.\ngeojson (.json) : ce format, non pr√©conis√© par l‚ÄôOGC est largement utilis√© pour le d√©veloppement web\ncomme dans la librairie leaflet.js.\nLa dimension spatiale est stock√©e dans le m√™me fichier que les attributs.\nCes fichiers sont g√©n√©ralement beaucoup plus l√©gers que les shapefiles mais poss√®dent des limites s‚Äôagissant de gros jeux de donn√©es.\n\nCette page compare plus en d√©tail ces trois types de formats de donn√©es g√©ographiques.\nL‚Äôaide de geopandas propose des bouts de code en fonction des diff√©rentes situations dans lesquelles on se trouve.\n\n45.2.1 Exemple: r√©cup√©rer les d√©coupages territoriaux\nL‚Äôun des fonds de carte les plus fr√©quents qu‚Äôon utilise est celui des\nlimites administratives des communes.\nCelui-ci peut √™tre r√©cup√©r√© de plusieurs mani√®res.\nEn premier lieu, pour r√©cup√©rer\nle fond de carte officiel, produit par l‚ÄôIGN, sous\nle nom d‚ÄôAdminExpress1,\nil est possible de se rendre sur le site de l‚ÄôIGN et de le t√©l√©charger depuis\nle serveur FTP.\nIl est √©galement possible d‚Äôutiliser l‚Äôune des API de l‚ÄôIGN\nmais ces derni√®res ne sont pas encore tr√®s document√©es pour des utilisateurs\nde Python.\nLe package pynsee propose notamment un module d√©di√© √† la r√©cup√©ration de fonds de carte officiels pour valoriser des donn√©es\nd‚Äôopen data. L‚ÄôAPI sur laquelle il repose √©tant parfois lente, nous pr√©sentons le code d√©di√© uniquement en annexe.\nNous proposons ici une m√©thode nouvelle de r√©cup√©ration de\nces donn√©es qui s‚Äôappuie sur le projet interminist√©riel\ncartiflette.\nCe projet vise √† faciliter la r√©cup√©ration des sources officielles, notamment\ncelles de l‚ÄôIGN, et leur association √† des jeux de donn√©es g√©ographiques.\n\n\n Note\nLe package cartiflette est exp√©rimental\net n‚Äôest disponible que sur\nGithub, pas sur PyPi.\nIl est amen√© √† √©voluer rapidement et cette page sera mise √† jour\nquand de nouvelles fonctionalit√©s (notamment l‚Äôutilisation d‚ÄôAPI)\nseront disponibles pour encore simplifier la r√©cup√©ration de\ncontours g√©ographiques.\nPour installer cartiflette, il est n√©cessaire d‚Äôutiliser les commandes suivantes\ndepuis un Jupyter Notebook (si vous utilisez la ligne de commande directement,\nvous pouvez retirer les ! et % en d√©but de ligne):\n\n!pip install --upgrade botocore==1.23.26  #Sur colab, sinon bug\n!pip install --upgrade urllib3==1.22.0 #Sur colab, sinon bug\n!pip install py7zr #Sur colab, sinon bug\n!pip install s3fs #Sur colab, sinon bug\n!git clone https://github.com/InseeFrLab/cartogether.git\n%cd ./cartogether\n!pip install -r requirements.txt\n!pip install .\n\nCes commandes permettent de r√©cup√©rer l‚Äôensemble du code\nsource depuis Github \n\n\nIci, nous sommes int√©ress√©s par les contours des communes\nde la petite couronne. On pourrait d√©sirer r√©cup√©rer\nl‚Äôensemble de la r√©gion Ile-de-France mais nous\nallons nous contenter de l‚Äôanalyse de Paris intra-muros\net des d√©partements limitrophes.\nC‚Äôest l‚Äôun des avantage de cartiflette que de faciliter\nla r√©cup√©ration de fonds de carte sur un ensemble de d√©partement.\nCela √©vite la r√©cup√©ration d‚Äôun fond de carte tr√®s\nvolumineux (plus de 500Mo) pour une analyse restreinte (quelques d√©partements).\nUn autre avantage de cartiflette est de faciliter la r√©cup√©ration de fonds\nde carte consolid√©s comme celui dont on a besoin ici: arrondissements\ndans Paris, communes ailleurs. Comme cela est expliqu√© dans un encadr√© √† part,\nil s‚Äôagirait d‚Äôune op√©ration p√©nible √† mettre en oeuvre sans cartiflette.\nLes contours de cet espace peuvent √™tre r√©cup√©r√©s de la mani√®re suivante:\n\nimport cartiflette.s3 as s3\n\nshp_communes = s3.download_vectorfile_url_all(\n    crs = 4326,\n    values = [\"75\", \"92\", \"93\", \"94\"],\n    borders=\"COMMUNE\",\n    vectorfile_format=\"topojson\",\n    filter_by=\"DEPARTEMENT\",\n    source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n    year=2022)\n\nshp_communes.head()\n\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=75/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=92/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=93/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=94/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\n\n\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 8.00kiB [00:00, 77.9kiB/s]Downloading: : 14.5kiB [00:00, 111kiB/s] \nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 8.00kiB [00:00, 75.2kiB/s]Downloading: : 36.0kiB [00:00, 145kiB/s] Downloading: : 62.3kiB [00:00, 198kiB/s]\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 12.0kiB [00:00, 87.5kiB/s]Downloading: : 30.0kiB [00:00, 112kiB/s] Downloading: : 66.0kiB [00:00, 180kiB/s]Downloading: : 119kiB [00:00, 285kiB/s] \nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 12.0kiB [00:00, 87.1kiB/s]Downloading: : 40.0kiB [00:00, 154kiB/s] Downloading: : 88.0kiB [00:00, 241kiB/s]Downloading: : 110kiB [00:00, 263kiB/s] \n\n\n\n\n\n\n\n\n\nid\nID\nNOM\nNOM_M\nINSEE_COM\nSTATUT\nPOPULATION\nINSEE_CAN\nINSEE_ARR\nINSEE_DEP\nINSEE_REG\nSIREN_EPCI\nsource\ngeometry\n\n\n\n\n0\nCOMMUNE_0000000009736048\nNaN\nParis\nPARIS\n75056\nCapitale d'√©tat\n2165423\nNR\n1\n75\n11\n200054781\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.36421 48.81640, 2.36333 48.81615, ...\n\n\n0\nCOMMUNE_0000000009736037\nNaN\nLevallois-Perret\nLEVALLOIS-PERRET\n92044\nCommune simple\n66082\n16\n2\n92\n11\n200054781/200057982\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.28739 48.90364, 2.28846 48.90302, ...\n\n\n1\nCOMMUNE_0000000009736055\nNaN\nBois-Colombes\nBOIS-COLOMBES\n92009\nCommune simple\n28841\n11\n2\n92\n11\n200054781/200057990\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.26639 48.90629, 2.26645 48.90615, ...\n\n\n2\nCOMMUNE_0000000009736538\nNaN\nMalakoff\nMALAKOFF\n92046\nCommune simple\n30950\n18\n1\n92\n11\n200054781/200057966\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.27818 48.81425, 2.28066 48.81469, ...\n\n\n3\nCOMMUNE_0000000009736038\nNaN\nClichy\nCLICHY\n92024\nCommune simple\n63089\n09\n2\n92\n11\n200054781/200057990\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.30377 48.89415, 2.30258 48.89487, ...\n\n\n\n\n\n\n\nOn reconna√Æt la structure d‚Äôun DataFrame Pandas. A cette structure s‚Äôajoute\nune colonne geometry qui enregistre la position des limites des polygones de chaque\nobservation.\nComme vu pr√©c√©demment, le syst√®me de projection est un √©l√©ment important. Il permet √† Python\nd‚Äôinterpr√©ter les valeurs des points (deux dimensions) en position sur\nla terre, qui n‚Äôest pas un espace plan.\n\nshp_communes.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nIci, les donn√©es sont dans le syst√®me WGS84 (code EPSG 4326).\nCe n‚Äôest pas le\nLambert-93 comme on pourrait s‚Äôy attendre, ce dernier\n√©tant le syst√®me l√©gal de projection pour la France\nm√©tropolitaine.\nPour s‚Äôassurer qu‚Äôon a bien r√©cup√©r√© les contours voulus,\non peut repr√©senter graphiquement\nles contours gr√¢ce √† la m√©thode plot sur laquelle nous\nreviendrons :\n\nax = shp_communes.boundary.plot()\nax.set_axis_off()\n\n\n\n\n\n\n Note\nSi on ne d√©sire pas utiliser le niveau COMMUNE_ARRONDISSEMENT,\nil est n√©cessaire de mettre en oeuvre une construction du fond de\ncarte en plusieurs phases.\nEn premier lieu, il est n√©cessaire de r√©cup√©rer le niveau des communes.\n\nimport cartiflette.s3 as s3\n\nshp_communes = s3.download_vectorfile_url_all(\n    crs = 4326,\n    values = [\"75\", \"92\", \"93\", \"94\"],\n    borders=\"COMMUNE\",\n    vectorfile_format=\"topojson\",\n    filter_by=\"DEPARTEMENT\",\n    source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n    year=2022)\n\nshp_communes.head()\n\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=75/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=92/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=93/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=94/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\n\n\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 8.00kiB [00:00, 75.1kiB/s]Downloading: : 14.5kiB [00:00, 109kiB/s] \nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 8.00kiB [00:00, 75.7kiB/s]Downloading: : 32.0kiB [00:00, 142kiB/s] Downloading: : 62.3kiB [00:00, 233kiB/s]\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 12.0kiB [00:00, 89.8kiB/s]Downloading: : 36.0kiB [00:00, 142kiB/s] Downloading: : 80.0kiB [00:00, 226kiB/s]Downloading: : 119kiB [00:00, 293kiB/s] \nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 8.00kiB [00:00, 70.0kiB/s]Downloading: : 32.0kiB [00:00, 130kiB/s] Downloading: : 80.0kiB [00:00, 223kiB/s]Downloading: : 110kiB [00:00, 248kiB/s] \n\n\n\n\n\n\n\n\n\nid\nID\nNOM\nNOM_M\nINSEE_COM\nSTATUT\nPOPULATION\nINSEE_CAN\nINSEE_ARR\nINSEE_DEP\nINSEE_REG\nSIREN_EPCI\nsource\ngeometry\n\n\n\n\n0\nCOMMUNE_0000000009736048\nNaN\nParis\nPARIS\n75056\nCapitale d'√©tat\n2165423\nNR\n1\n75\n11\n200054781\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.36421 48.81640, 2.36333 48.81615, ...\n\n\n0\nCOMMUNE_0000000009736037\nNaN\nLevallois-Perret\nLEVALLOIS-PERRET\n92044\nCommune simple\n66082\n16\n2\n92\n11\n200054781/200057982\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.28739 48.90364, 2.28846 48.90302, ...\n\n\n1\nCOMMUNE_0000000009736055\nNaN\nBois-Colombes\nBOIS-COLOMBES\n92009\nCommune simple\n28841\n11\n2\n92\n11\n200054781/200057990\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.26639 48.90629, 2.26645 48.90615, ...\n\n\n2\nCOMMUNE_0000000009736538\nNaN\nMalakoff\nMALAKOFF\n92046\nCommune simple\n30950\n18\n1\n92\n11\n200054781/200057966\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.27818 48.81425, 2.28066 48.81469, ...\n\n\n3\nCOMMUNE_0000000009736038\nNaN\nClichy\nCLICHY\n92024\nCommune simple\n63089\n09\n2\n92\n11\n200054781/200057990\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((2.30377 48.89415, 2.30258 48.89487, ...\n\n\n\n\n\n\n\n\nax = shp_communes.boundary.plot()\nax.set_axis_off()\n\n\n\n\nOn peut remarquer que la ville de Paris ne comporte pas d‚Äôarrondissements\nsur cette carte. Pour vous en convaincre, vous pouvez ex√©cuter la\ncommande :\n\nax = shp_communes.loc[shp_communes['INSEE_DEP']==\"75\"].boundary.plot()\nax.set_axis_off()\n\n\n\n\nIl faut donc utiliser une source compl√©mentaire.\nLe contour officiel des arrondissements est\nproduit par l‚ÄôIGN s√©paremment des contours de communes.\nLes contours d‚Äôarrondissements sont √©galement\ndisponibles\ngr√¢ce √† cartiflette:\n\narrondissements = s3.download_vectorfile_url_all(\n    crs = 4326,\n    values = [\"75\"],\n    borders=\"COMMUNE\",\n    vectorfile_format=\"topojson\",\n    filter_by=\"DEPARTEMENT\",\n    source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n    year=2022)\n\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=75/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\n\n\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 12.0kiB [00:00, 81.7kiB/s]Downloading: : 14.5kiB [00:00, 98.4kiB/s]\n\n\n\nax = arrondissements.plot(alpha = 0.8, edgecolor = \"k\")\nax.set_axis_off()\n\n\n\n\nIl ne reste plus qu‚Äô√† remplacer Paris par\nses arrondissements dans shp_communes.\nPour cela, on peut utiliser les m√©thodes\nvues dans le chapitre Pandas relatives\naux filtres et √† la concat√©nation\nde plusieurs DataFrames:\n\nimport pandas as pd\n\nshp_communes = pd.concat(\n  [\n    shp_communes.loc[shp_communes['INSEE_DEP'] != \"75\"].to_crs(2154),\n    arrondissements.to_crs(2154)\n  ])\n\nCette approche fonctionne mais elle n√©cessite un certain nombre\nde gestes, qui sont autant de risques d‚Äôerreurs. Il est\ndonc recommand√© de privil√©gier le niveau COMMUNE_ARRONDISSEMENT\nqui fait exactement ceci mais de mani√®re fiable."
  },
  {
    "objectID": "content/course/manipulation/03_geopandas_tutorial/index.html#op√©rations-sur-les-attributs-et-les-g√©om√©tries",
    "href": "content/course/manipulation/03_geopandas_tutorial/index.html#op√©rations-sur-les-attributs-et-les-g√©om√©tries",
    "title": "45¬† Donn√©es spatiales: d√©couverte de geopandas",
    "section": "45.3 Op√©rations sur les attributs et les g√©om√©tries",
    "text": "45.3 Op√©rations sur les attributs et les g√©om√©tries\n\n45.3.1 Import des donn√©es velib\nSouvent, le d√©coupage communal ne sert qu‚Äôen fond de cartes, pour donner des\nrep√®res. En compl√©ment de celui-ci, on peut d√©sirer exploiter\nun autre jeu de donn√©es. On va partir des donn√©es de localisation des\nstations velib,\ndisponibles sur le site d‚Äôopen data de la ville de Paris et\nrequ√™tables directement par l‚Äôurl\nhttps://opendata.paris.fr/explore/dataset/velib-emplacement-des-stations/download/?format=geojson&timezone=Europe/Berlin&lang=fr\n\nvelib_data = 'https://opendata.paris.fr/explore/dataset/velib-emplacement-des-stations/download/?format=geojson&timezone=Europe/Berlin&lang=fr'\nstations = gpd.read_file(velib_data)\nstations.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nLes donn√©es sont dans le syst√®me de projection WGS84 qui est celui du\nsyst√®me GPS. Celui-ci s‚Äôint√®gre bien avec les fonds de carte\nStamen, OpenStreetMap ou Google Maps. En toute rigueur, si on\nd√©sire effectuer certains calculs g√©om√©triques (mesurer des surfaces‚Ä¶), il est\nn√©cessaire de re-projeter les donn√©es dans un syst√®me qui pr√©serve la g√©om√©trie\n(c‚Äôest le cas du Lambert 93).\nPour avoir une intuition de la localisation des stations, et notamment de la\ndensit√© h√©t√©rog√®ne de celles-ci,\non peut afficher les donn√©es sur la carte des communes\nde la petite couronne. Il s‚Äôagit donc d‚Äôenrichir la carte\npr√©c√©dente d‚Äôune couche suppl√©mentaire, √† savoir la localisation\ndes stations. Au passage, on va utiliser un fond de carte\nplus esth√©tique:\n\nfig,ax = plt.subplots(figsize=(10, 10))\nstations.sample(200).to_crs(3857).plot(ax = ax, color = 'red', alpha = 0.4, zorder=2)\nshp_communes.to_crs(3857).plot(ax = ax, zorder=1, edgecolor = \"black\", facecolor=\"none\",\n                                                           color = None)\nctx.add_basemap(ax, source = ctx.providers.Stamen.Watercolor)\nax.set_axis_off()\n\n\n\n\nD√©couvrez ci-dessous par √©tape les diff√©rentes lignes de commandes permettant d‚Äôafficher cette carte compl√®te,\n√©tape par √©tape :\n:one:\nAfficher le nuage de points de 200 stations v√©libs prises au hasard\n\nfig,ax = plt.subplots(figsize=(10, 10))\nstations.sample(200).to_crs(3857).plot(ax = ax, color = 'red', alpha = 0.4, zorder=2)\n\n&lt;Axes: &gt;\n\n\n\n\n\n:two:\nAjouter √† cette couche, en-dessous, les contours des communes\n\nfig,ax = plt.subplots(figsize=(10, 10))\nstations.sample(200).to_crs(3857).plot(ax = ax, color = 'red', alpha = 0.4, zorder=2)\nshp_communes.to_crs(3857).plot(ax = ax, zorder=1, edgecolor = \"black\", facecolor=\"none\",\n                                                           color = None)\n\n\n\n\n\n\n:three:\nAjouter un fond de carte de type open street map gr√¢ce au package\ncontextily\n\nfig,ax = plt.subplots(figsize=(10, 10))\nstations.sample(200).to_crs(3857).plot(ax = ax, color = 'red', alpha = 0.4, zorder=2)\nshp_communes.to_crs(3857).plot(ax = ax, zorder=1, edgecolor = \"black\", facecolor=\"none\",\n                                                           color = None)\nctx.add_basemap(ax, source = ctx.providers.Stamen.Watercolor)\n\n\n\n\n\n\n:four:\nIl ne reste plus qu‚Äô√† retirer l‚Äôaxe des coordonn√©es, qui n‚Äôest pas tr√®s\nesth√©tique.\n\nfig,ax = plt.subplots(figsize=(10, 10))\nstations.sample(200).to_crs(3857).plot(ax = ax, color = 'red', alpha = 0.4, zorder=2)\nshp_communes.to_crs(3857).plot(ax = ax, zorder=1, edgecolor = \"black\", facecolor=\"none\",\n                                                           color = None)\nctx.add_basemap(ax, source = ctx.providers.Stamen.Watercolor)\nax.set_axis_off()\nax\n\n\n\n\n\n\nIn fine, on obtient la carte d√©sir√©e.\n\n\n45.3.2 Op√©rations sur les attributs\nToutes les op√©rations possibles sur un objet Pandas le sont √©galement\nsur un objet GeoPandas. Pour manipuler les donn√©es, et non la g√©om√©trie,\non parlera d‚Äôop√©rations sur les attributs.\nPar exemple, si on d√©sire\nconna√Ætre quelques statistiques sur la taille des stations:\n\nstations.describe()\n\n\n\n\n\n\n\n\ncapacity\n\n\n\n\ncount\n1465.000000\n\n\nmean\n31.054608\n\n\nstd\n12.017793\n\n\nmin\n0.000000\n\n\n25%\n23.000000\n\n\n50%\n29.000000\n\n\n75%\n37.000000\n\n\nmax\n74.000000\n\n\n\n\n\n\n\nPour conna√Ætre les plus grands d√©partements de France m√©tropolitaine,\nproc√©dons en deux √©tapes:\n\nR√©cup√©rons le contour des communes de France m√©tropolitaine dans son ensemble\ngr√¢ce √† cartiflette.\nNotons qu‚Äôon pourrait r√©cup√©rer directement les contours d√©partementaux mais\npour l‚Äôexercice, nous allons le cr√©er nous-m√™mes comme agr√©gation\ndes contours communaux\n(voir ce notebook observable pour la m√©thode plus\nl√©g√®re qui utilise pleinement les fonctionnalit√©s de cartiflette).\nCalculons la surface (m√©thode area sur un objet GeoPandas.GeoDataFrame ramen√©e en km¬≤, attention n√©amoins au syst√®me de projection comme cela est expliqu√© plus bas)\n\n\nfrom cartiflette.download import get_vectorfile_ign\nfrance = get_vectorfile_ign(\n  borders = \"COMMUNE\",\n  field = \"metropole\",\n  source = \"EXPRESS-COG-CARTO-TERRITOIRE\",\n  provider=\"IGN\"\n)\n\n\nfrance['surface'] = france.area.div(10**6)\n\nLes plus grands d√©partements s‚Äôobtiennent par une agr√©gation des\nsurfaces communales :\n\nfrance.groupby('INSEE_DEP').sum(numeric_only = True).sort_values('surface', ascending = False)\n\n\n\n\n\n\n\n\nPOPULATION\nsurface\n\n\nINSEE_DEP\n\n\n\n\n\n\n33\n1623749\n10367.786664\n\n\n40\n413690\n9354.208426\n\n\n24\n413223\n9210.826232\n\n\n21\n534124\n8787.757953\n\n\n12\n279595\n8770.152839\n\n\n...\n...\n...\n\n\n90\n141318\n610.126210\n\n\n94\n1407124\n244.811547\n\n\n93\n1644903\n236.867946\n\n\n92\n1624357\n175.570732\n\n\n75\n2165423\n105.431453\n\n\n\n\n96 rows √ó 2 columns\n\n\n\nSi on veut directement les plus\ngrandes communes de France m√©tropolitaine :\n\nfrance.sort_values('surface', ascending = False).head(10)\n\n\n\n\n\n\n\n\nID\nNOM\nNOM_M\nINSEE_COM\nSTATUT\nPOPULATION\nINSEE_CAN\nINSEE_ARR\nINSEE_DEP\nINSEE_REG\nSIREN_EPCI\ngeometry\nsource\nsurface\n\n\n\n\n240\nCOMMUNE_0000000009760125\nArles\nARLES\n13004\nSous-pr√©fecture\n50454\n04\n2\n13\n93\n241300417\nPOLYGON ((841785.400 6290116.800, 841787.600 6...\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n757.418542\n\n\n308\nCOMMUNE_0000000009753237\nVal-Cenis\nVAL-CENIS\n73290\nCommune simple\n2062\n10\n3\n73\n84\n200070340\nPOLYGON ((1017798.400 6466625.200, 1017573.700...\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n455.423629\n\n\n19074\nCOMMUNE_0000000009760352\nSaintes-Maries-de-la-Mer\nSAINTES-MARIES-DE-LA-MER\n13096\nCommune simple\n2144\n04\n2\n13\n93\n241300417\nMULTIPOLYGON (((813751.200 6261910.500, 813716...\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n371.214527\n\n\n274\nCOMMUNE_0000000009746086\nChemill√©-en-Anjou\nCHEMILLE-EN-ANJOU\n49092\nCommune simple\n20828\n11\n2\n49\n52\n200060010\nPOLYGON ((429946.200 6682848.700, 429939.900 6...\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n320.816158\n\n\n436\nCOMMUNE_0000000009744893\nNoyant-Villages\nNOYANT-VILLAGES\n49228\nCommune simple\n5546\n08\n3\n49\n52\n244900882\nPOLYGON ((490764.000 6714964.800, 490615.800 6...\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n300.457150\n\n\n14079\nCOMMUNE_0000000009744622\nBaug√©-en-Anjou\nBAUGE-EN-ANJOU\n49018\nCommune simple\n11829\n08\n3\n49\n52\n244900882\nPOLYGON ((456856.800 6728449.000, 456857.600 6...\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n270.223011\n\n\n1279\nCOMMUNE_0000000009762779\nLaruns\nLARUNS\n64320\nCommune simple\n1185\n15\n2\n64\n75\n246400337\nPOLYGON ((428969.700 6200124.400, 428966.500 6...\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n248.657511\n\n\n275\nCOMMUNE_0000000009750652\nChamonix-Mont-Blanc\nCHAMONIX-MONT-BLANC\n74056\nCommune simple\n8640\n10\n2\n74\n84\n200023372\nPOLYGON ((1000337.100 6532738.100, 1000258.500...\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n245.012587\n\n\n6206\nCOMMUNE_0000000009743670\nSegr√©-en-Anjou Bleu\nSEGRE-EN-ANJOU BLEU\n49331\nSous-pr√©fecture\n17462\n20\n4\n49\n52\n244900809\nMULTIPOLYGON (((421207.400 6742529.000, 421215...\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n243.945747\n\n\n473\nCOMMUNE_0000000009761156\nMarseille\nMARSEILLE\n13055\nPr√©fecture de r√©gion\n870731\n98\n3\n13\n93\n200054807\nMULTIPOLYGON (((894824.800 6233030.600, 894821...\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\n238.445688\n\n\n\n\n\n\n\nLors des √©tapes d‚Äôagr√©gation, groupby ne conserve pas les g√©om√©tries. Autrement\ndit, si on effectue, par exemple, une somme en fonction d‚Äôune variable de groupe avec\nle combo groupby(...).sum(...) , on perd\nla dimension g√©ographique.\nIl est n√©anmoins possible d‚Äôaggr√©ger √† la fois les g√©om√©tries et les\nattribus avec la m√©thode dissolve:\n\nfig,ax = plt.subplots(figsize=(10, 10))\nfrance.dissolve(by='INSEE_DEP', aggfunc='sum').plot(ax = ax, column = \"surface\")\nax.set_axis_off()\nax\n\n&lt;Axes: &gt;\n\n\n\n\n\nPour produire cette carte, il serait n√©anmoins plus simple de directement\nr√©cup√©rer les fonds officiels des d√©partements plut√¥t que d‚Äôagr√©ger les\ncontours des communes:\n\ndep = get_vectorfile_ign(\n  borders = \"DEPARTEMENT\",\n  field = \"metropole\",\n  source = \"EXPRESS-COG-CARTO-TERRITOIRE\",\n  provider=\"IGN\")\ndep[\"area\"] = dep.area\nax = dep.plot(column = \"area\")\nax.set_axis_off()\n\nhttps://wxs.ign.fr/x02uy2aiwjo9bm8ce5plwqmr/telechargement/prepackage/ADMINEXPRESS-COG_SHP_TERRITOIRES_PACK_2022-04-15$ADMIN-EXPRESS-COG_3-1__SHP_LAMB93_FXX_2022-04-15/file/ADMIN-EXPRESS-COG_3-1__SHP_LAMB93_FXX_2022-04-15.7z\nData have been previously downloaded and are still available in /tmp/EXPRESS-COG-CARTO-TERRITOIRE-2022/metropole\n\n\n\n\n\n\n\n45.3.3 Op√©rations sur les g√©om√©tries\nOutre la repr√©sentation graphique simplifi√©e,\nsur laquelle nous reviendrons ult√©rieurement, l‚Äôint√©r√™t principal d‚Äôutiliser\nGeoPandas est l‚Äôexistence de m√©thodes efficaces pour\nmanipuler la dimension spatiale. Un certain nombre proviennent du\npackage\nShapely.\n\n\n Warning\nLes donn√©es sont en syst√®me de coordonn√©es WGS 84 ou pseudo-Mercator (epsg: 4326) et ne sont pas projet√©es.\nC‚Äôest un format appropri√© lorsqu‚Äôil s‚Äôagit d‚Äôutiliser un fonds\nde carte OpenStreetMap, Stamen, Google Maps, etc.\nMais ce n‚Äôest pas un\nformat sur lequel on d√©sire faire des calculs car les distances sont fauss√©es sans utiliser de projection. D‚Äôailleurs, geopandas refusera certaines op√©rations\nsur des donn√©es dont le crs est 4326. On reprojete ainsi les donn√©es\ndans la projection officielle pour la m√©tropole, le Lambert 93\n(epsg: 2154).\n\n\nComme indiqu√© ci-dessus, nous reprojetons les donn√©es\ndans le syst√®me Lambert 93 qui ne fausse pas les\ncalculs de distance et d‚Äôaires.\n\ncommunes = shp_communes.to_crs(2154)\nstations = stations.to_crs(2154)\n\nPar exemple, on peut recalculer la taille d‚Äôune commune ou d‚Äôarrondissement\navec la m√©thode area (et diviser par \\(10^6\\) pour avoir des \\(km^2\\) au lieu\ndes \\(m^2\\)):\n\ncommunes['superficie'] = communes.area.div(10**6)\ncommunes.head(3)\n\n\n\n\n\n\n\n\nid\nID\nNOM\nNOM_M\nINSEE_COM\nSTATUT\nPOPULATION\nINSEE_CAN\nINSEE_ARR\nINSEE_DEP\nINSEE_REG\nSIREN_EPCI\nsource\ngeometry\nsuperficie\n\n\n\n\n0\nCOMMUNE_0000000009736037\nNaN\nLevallois-Perret\nLEVALLOIS-PERRET\n92044\nCommune simple\n66082\n16\n2\n92\n11\n200054781/200057982\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((647761.341 6867306.988, 647839.223 6...\n2.417491\n\n\n1\nCOMMUNE_0000000009736055\nNaN\nBois-Colombes\nBOIS-COLOMBES\n92009\nCommune simple\n28841\n11\n2\n92\n11\n200054781/200057990\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((646224.655 6867615.810, 646228.990 6...\n1.926619\n\n\n2\nCOMMUNE_0000000009736538\nNaN\nMalakoff\nMALAKOFF\n92046\nCommune simple\n30950\n18\n1\n92\n11\n200054781/200057966\nIGN:EXPRESS-COG-CARTO-TERRITOIRE\nPOLYGON ((646995.323 6857373.499, 647177.485 6...\n2.070953\n\n\n\n\n\n\n\nUne m√©thode qu‚Äôon utilise r√©guli√®rement est centroid qui, comme son nom l‚Äôindique,\nrecherche le centro√Øde de chaque polygone et transforme ainsi des donn√©es\nsurfaciques en donn√©es ponctuelles. Par exemple, pour\nrepr√©senter approximativement les centres des villages de la\nHaute-Garonne (31), apr√®s avoir t√©l√©charg√© le fonds de carte adapt√©,\nfera\n\ncommunes_31 = s3.download_vectorfile_url_all(\n      crs = 4326,\n      values = \"31\",\n      borders=\"COMMUNE\",\n      vectorfile_format=\"topojson\",\n      filter_by=\"DEPARTEMENT\",\n      source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n      year=2022)\n\n# on reprojete en 3857 pour le fond de carte\ncommunes_31 = communes_31.to_crs(3857)\n\n# on calcule le centroide\ndep_31 = communes_31.copy()\ncommunes_31['geometry'] = communes_31['geometry'].centroid\n\nax = communes_31.plot(figsize = (10,10), color = 'red', alpha = 0.4, zorder=2)\ndep_31.to_crs(3857).plot(ax = ax, zorder=1, edgecolor = \"black\", facecolor=\"none\",\n                                                           color = None)\nctx.add_basemap(ax, source = ctx.providers.Stamen.Toner)\nax.set_axis_off()\nax\n\nhttps://minio.lab.sspcloud.fr/projet-cartiflette/diffusion/shapefiles-test1/year=2022/administrative_level=COMMUNE/crs=4326/DEPARTEMENT=31/vectorfile_format=topojson/provider=IGN/source=EXPRESS-COG-CARTO-TERRITOIRE/raw.topojson\n\n\nDownloading: : 0.00iB [00:00, ?iB/s]Downloading: : 12.0kiB [00:00, 89.7kiB/s]Downloading: : 37.0kiB [00:00, 146kiB/s] Downloading: : 81.0kiB [00:00, 228kiB/s]Downloading: : 177kiB [00:00, 418kiB/s] Downloading: : 365kiB [00:00, 767kiB/s]Downloading: : 741kiB [00:00, 1.45MiB/s]Downloading: : 883kiB [00:01, 1.19MiB/s]Downloading: : 1.60MiB [00:01, 1.64MiB/s]\n\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "content/course/manipulation/03_geopandas_tutorial/index.html#g√©rer-le-syst√®me-de-projection",
    "href": "content/course/manipulation/03_geopandas_tutorial/index.html#g√©rer-le-syst√®me-de-projection",
    "title": "45¬† Donn√©es spatiales: d√©couverte de geopandas",
    "section": "45.4 G√©rer le syst√®me de projection",
    "text": "45.4 G√©rer le syst√®me de projection\nPr√©c√©demment, nous avons appliqu√© une m√©thode to_crs pour reprojeter\nles donn√©es dans un syst√®me de projection diff√©rent de celui du fichier\nd‚Äôorigine :\n\ncommunes = communes.to_crs(2154)\nstations = stations.to_crs(2154)\n\nLe syst√®me de projection est fondamental pour que la dimension\nspatiale soit bien interpr√©t√©e par Python. Un mauvais syst√®me de repr√©sentation\nfausse l‚Äôappr√©ciation visuelle mais peut aussi entra√Æner des erreurs dans\nles calculs sur la dimension spatiale.\nCe post propose de riches √©l√©ments sur le\nsujet, notamment l‚Äôimage suivante qui montre bien le principe d‚Äôune projection :\n\n\n\nLes diff√©rents types de projection\n\n\nLa Terre peut ainsi √™tre repr√©sent√©e de multiples mani√®res, ce qui n‚Äôest pas neutre sur la mani√®re de se repr√©senter\ncertains continents.\nL‚ÄôAfrique appara√Æt beaucoup moins vaste qu‚Äôelle ne l‚Äôest en r√©alit√© sur les cartes utilisant\ncette projection.\nL‚Äôune des d√©formations les mieux connue est celle provoqu√©e par la projection Mercator.\nLe Gro√´nland para√Æt avoir\nla m√™me surface que l‚ÄôAm√©rique du Sud. Pourtant, cette derni√®re est 8 fois\nplus grande.\nIl existe en fait de nombreuses repr√©sentations possibles du monde, plus ou moins\nalambiqu√©es. Les projections sont tr√®s nombreuses et certaines peuvent avoir une forme suprenante.\nPar exemple,\nla projection de Spillhaus\npropose de centrer la vue sur les oc√©ans et non une terre. C‚Äôest pour\ncette raison qu‚Äôon parle parfois de monde tel que vu par les poissons\n√† son propos.\n\nExemple de reprojection de pays depuis le site thetruesize.com\nConcernant la gestion des projections avec GeoPandas,\nla documentation officielle est tr√®s bien\nfaite. Elle fournit notamment l‚Äôavertissement suivant qu‚Äôil est\nbon d‚Äôavoir en t√™te:\n\nBe aware that most of the time you don‚Äôt have to set a projection. Data loaded from a reputable source (using the geopandas.read_file() command) should always include projection information. You can see an objects current CRS through the GeoSeries.crs attribute.\nFrom time to time, however, you may get data that does not include a projection. In this situation, you have to set the CRS so geopandas knows how to interpret the coordinates.\n\n\nImage emprunt√©e √† XKCD https://xkcd.com/2256/ qu‚Äôon peut √©galement trouver sur https://blog.chrislansdown.com/2020/01/17/a-great-map-projection-joke/\nPour d√©terminer le syst√®me de projection d‚Äôune base de donn√©es, on peut v√©rifier l‚Äôattribut crs:\n\ncommunes.crs\n\n&lt;Projected CRS: EPSG:2154&gt;\nName: RGF93 v1 / Lambert-93\nAxis Info [cartesian]:\n- X[east]: Easting (metre)\n- Y[north]: Northing (metre)\nArea of Use:\n- name: France - onshore and offshore, mainland and Corsica (France m√©tropolitaine including Corsica).\n- bounds: (-9.86, 41.15, 10.38, 51.56)\nCoordinate Operation:\n- name: Lambert-93\n- method: Lambert Conic Conformal (2SP)\nDatum: Reseau Geodesique Francais 1993 v1\n- Ellipsoid: GRS 1980\n- Prime Meridian: Greenwich\n\n\nLes deux principales m√©thodes pour d√©finir le syst√®me de projection utilis√© sont:\n\ndf.set_crs: cette commande sert √† pr√©ciser quel est le syst√®me de projection utilis√©, c‚Äôest-√†-dire comment les coordonn√©es (x,y) sont reli√©es √† la surface terrestre. Cette commande ne doit pas √™tre utilis√©e pour transformer le syst√®me de coordonn√©es, seulement pour le d√©finir.\ndf.to_crs: cette commande sert √† projeter les points d‚Äôune g√©om√©trie dans une autre, c‚Äôest-√†-dire √† recalculer les coordonn√©es selon un autre syst√®me de projection.\n\nDans le cas particulier de production de carte avec un fond OpenStreetMaps ou une carte dynamique leaflet, il est n√©cessaire de d√©-projeter les donn√©es (par exemple √† partir du Lambert-93) pour atterrir dans le syst√®me non-projet√© WGS 84 (code EPSG 4326). Ce site d√©di√© aux projections g√©ographiques peut-√™tre utile pour retrouver le syst√®me de projection d‚Äôun fichier o√π il n‚Äôest pas indiqu√©.\nLa d√©finition du syst√®me de projection se fait de la mani√®re suivante (:warning: avant de le faire, se souvenir de l‚Äôavertissement !):\ncommunes = communes.set_crs(2154)\nAlors que la reprojection (projection Albers : 5070) s‚Äôobtient de la mani√®re suivante :\n\nshp_region = get_vectorfile_ign(\n  borders = \"REGION\", field = \"metropole\",\n  source = \"EXPRESS-COG-CARTO-TERRITOIRE\", provider=\"IGN\")\n\nfig,ax = plt.subplots(figsize=(10, 10))\nshp_region.to_crs(5070).plot(ax = ax)\nax\n\nhttps://wxs.ign.fr/x02uy2aiwjo9bm8ce5plwqmr/telechargement/prepackage/ADMINEXPRESS-COG_SHP_TERRITOIRES_PACK_2022-04-15$ADMIN-EXPRESS-COG_3-1__SHP_LAMB93_FXX_2022-04-15/file/ADMIN-EXPRESS-COG_3-1__SHP_LAMB93_FXX_2022-04-15.7z\nData have been previously downloaded and are still available in /tmp/EXPRESS-COG-CARTO-TERRITOIRE-2022/metropole\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\nOn le voit, cela modifie totalement la repr√©sentation de l‚Äôobjet dans l‚Äôespace.\nClairement, cette projection n‚Äôest pas adapt√©e aux longitudes et latitudes fran√ßaises.\nC‚Äôest normal, il s‚Äôagit d‚Äôune projection adapt√©e au continent\nnord-am√©ricain (et encore, pas dans son ensemble !).\n\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n\nfig,ax = plt.subplots(figsize=(10, 10))\nworld[world.continent == \"North America\"].to_crs(5070).plot(alpha = 0.2, edgecolor = \"k\", ax = ax)\nax\n\n/tmp/ipykernel_1875/3485617699.py:1: FutureWarning:\n\nThe geopandas.dataset module is deprecated and will be removed in GeoPandas 1.0. You can get the original 'naturalearth_lowres' data from https://www.naturalearthdata.com/downloads/110m-cultural-vectors/.\n\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "content/course/manipulation/03_geopandas_tutorial/index.html#joindre-des-donn√©es",
    "href": "content/course/manipulation/03_geopandas_tutorial/index.html#joindre-des-donn√©es",
    "title": "45¬† Donn√©es spatiales: d√©couverte de geopandas",
    "section": "45.5 Joindre des donn√©es",
    "text": "45.5 Joindre des donn√©es\n\n45.5.1 Joindre des donn√©es sur des attributs\nCe type de jointure se fait entre un objet g√©ographique et un\ndeuxi√®me objet, g√©ographique ou non. A l‚Äôexception de la question\ndes g√©om√©tries, il n‚Äôy a pas de diff√©rence par rapport √† Pandas.\nLa seule diff√©rence avec Pandas est dans la dimension g√©ographique.\nSi on d√©sire conserver la dimension g√©ographique, il faut faire\nattention √† faire :\ngeopandas_object.merge(pandas_object)\nSi on utilise deux objets g√©ographiques mais ne d√©sire conserver qu‚Äôune seule\ndimension g√©ographique2, on fera\ngeopandas_object1.merge(geopandas_object2)\nSeule la g√©om√©trie de l‚Äôobjet de gauche\nsera conserv√©e, m√™me si on fait un right join.\n\n\n45.5.2 Prolongation possible : joindre des donn√©es sur dimension g√©ographique\nLe chapitre suivant permettra de mettre en oeuvre des\njointures g√©ographiques.\n\n\n Hint\nLes jointures spatiales peuvent √™tre tr√®s gourmandes en ressources (car il peut √™tre n√©cessaire de croiser toutes les g√©om√©tries de x avec toutes les g√©om√©tries de y). Voici deux conseils qui peuvent vous aider :\n\nIl est pr√©f√©rable de tester les jointures g√©ographiques sur un petit √©chantillon de donn√©es, pour estimer le temps et les ressources n√©cessaires √† la r√©alisation de la jointure.\nIl est parfois possible d‚Äô√©crire une fonction qui r√©duit la taille du probl√®me. Exemple: vous voulez d√©terminer dans quelle commune se situe un logement dont vous connaissez les coordonn√©es et le d√©partement; vous pouvez √©crire une fonction qui r√©alise pour chaque d√©partement une jointure spatiale entre les logements situ√©s dans ce d√©partement et les communes de ce d√©partement, puis empiler les 101 tables de sorties."
  },
  {
    "objectID": "content/course/manipulation/03_geopandas_tutorial/index.html#annexe",
    "href": "content/course/manipulation/03_geopandas_tutorial/index.html#annexe",
    "title": "45¬† Donn√©es spatiales: d√©couverte de geopandas",
    "section": "45.6 Annexe",
    "text": "45.6 Annexe\n\n45.6.1 R√©cup√©ration des donn√©es depuis data.gouv\nAvec cette m√©thode, les donn√©es des limites administratives demandent donc un peu de travail pour √™tre\nimport√©es car elles sont zipp√©es (mais c‚Äôest un bon exercice !).\nLe code suivant, dont les\nd√©tails appara√Ætront plus clairs apr√®s la lecture de la partie\nwebscraping permet de :\n\nT√©l√©charger les donn√©es avec requests dans un dossier temporaire\nLes d√©zipper avec le module zipfile\n\nLa fonction suivante automatise un peu le processus :\n\nimport requests\nimport tempfile\nimport zipfile\n\nurl = 'https://www.data.gouv.fr/fr/datasets/r/0e117c06-248f-45e5-8945-0e79d9136165'\ntemporary_location = tempfile.gettempdir()\n\ndef download_unzip(url, dirname = tempfile.gettempdir(), destname = \"borders\"):\n  myfile = requests.get(url)\n  open(\"{}/{}.zip\".format(dirname, destname), 'wb').write(myfile.content)\n  with zipfile.ZipFile(\"{}/{}.zip\".format(dirname, destname), 'r') as zip_ref:\n      zip_ref.extractall(dirname + '/' + destname)\n\n\ndownload_unzip(url)\n\n\nshp_communes = gpd.read_file(temporary_location + \"/borders/communes-20220101.shp\")\n\nIci, les donn√©es ne sont pas projet√©es puisqu‚Äôelles sont dans le\nsyst√®me WSG84 (epsg: 4326) ce qui permet de facilement ajouter\nun fonds de carte Openstreetmap ou Stamen pour rendre une repr√©sentation\ngraphique plus esth√©tique.\nEn toute rigueur, pour faire une carte statique d‚Äôun pays en particulier,\nil faudrait reprojeter les donn√©es dans un syst√®me de projection adapt√© √† la zone g√©ographique √©tudi√©e\n(par exemple le Lambert-93 pour la France m√©tropolitaine).\nOn peut ainsi repr√©senter Paris pour se donner une id√©e de la nature\ndu shapefile utilis√© :\n\nparis = shp_communes.loc[shp_communes['insee'].str.startswith(\"75\")]\n\nfig,ax = plt.subplots(figsize=(10, 10))\nparis.plot(ax = ax, alpha=0.5, edgecolor='blue')\nctx.add_basemap(ax, crs = paris.crs.to_string())\nax.set_axis_off()\nax\n\nOn voit ainsi que les donn√©es pour Paris ne comportent pas d‚Äôarrondissement,\nce qui est limitant pour une analyse focalis√©e sur Paris. On va donc les\nr√©cup√©rer sur le site d‚Äôopen data de la ville de Paris et les substituer\n√† Paris :\n\narrondissements = gpd.read_file(\"https://opendata.paris.fr/explore/dataset/arrondissements/download/?format=geojson&timezone=Europe/Berlin&lang=fr\")\narrondissements = arrondissements.rename(columns = {\"c_arinsee\": \"insee\"})\narrondissements['insee'] = arrondissements['insee'].astype(str)\nshp_communes = shp_communes[~shp_communes.insee.str.startswith(\"75\")].append(arrondissements)\n\nPour produire la carte, il faudrait faire:\n\nparis = shp_communes.loc[shp_communes.insee.str.startswith(\"75\")]\n\nfig,ax = plt.subplots(figsize=(10, 10))\n\nparis.plot(ax = ax, alpha=0.5, edgecolor='k')\nctx.add_basemap(ax, crs = paris.crs.to_string())\nax.set_axis_off()\nax\n\n\n\n45.6.2 R√©cup√©ration des donn√©es depuis le package pynsee\nPour conna√Ætre les contraintes d‚Äôinstallation du package pynsee,\nse r√©f√©rer √† la partie de cours d√©di√©e √† Pandas.\n\n#le t√©l√©chargement des donn√©es prend plusieurs minutes\nfrom pynsee.geodata import get_geodata\nshp_communes = gpd.GeoDataFrame(\n  get_geodata('ADMINEXPRESS-COG-CARTO.LATEST:commune')\n)\nshp_communes = shp_communes.rename({\"insee_com\": 'insee'}, axis = 'columns')\n#shp_communes = shp_communes.set_crs(3857)"
  },
  {
    "objectID": "content/course/manipulation/02b_pandas_TP/index.html#exploration-de-la-structure-des-donn√©es",
    "href": "content/course/manipulation/02b_pandas_TP/index.html#exploration-de-la-structure-des-donn√©es",
    "title": "46¬† Pratique de pandas: un exemple complet",
    "section": "46.1 Exploration de la structure des donn√©es",
    "text": "46.1 Exploration de la structure des donn√©es\nCommencer par importer les donn√©es de l‚ÄôAdeme √† l‚Äôaide du package pandas. Vous pouvez nommer le DataFrame obtenu df.\n\ndf = pd.read_csv(\"https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert\", sep=\",\")\n\nPour les donn√©es de cadrage au niveau communal (source Insee), le package pynsee facilite grandement la vie.\nLa liste des donn√©es disponibles est ici.\nEn l‚Äôoccurrence, on va utiliser les donn√©es Filosofi (donn√©es de revenus) au niveau communal de 2016.\nLe point d‚Äôentr√©e principal de la fonction pynsee est la fonction download_file.\nLe code pour t√©l√©charger les donn√©es est le suivant :\n\ndf_city = pynsee.download.download_file(\"FILOSOFI_COM_2016\")\n\n\n\n Note\nLa fonction download_file attend un identifiant unique\npour savoir quelle base de donn√©es aller chercher et\nrestructurer depuis le\nsite insee.fr.\nPour conna√Ætre la liste des bases disponibles, vous\npouvez utiliser la fonction meta = pynsee.get_file_list().\nCelle-ci renvoie un DataFrame dans lequel on peut\nrechercher, par exemple gr√¢ce √† une recherche\nde mot cl√©:\n\nmeta = pynsee.get_file_list()\nmeta.loc[meta['label'].str.contains(r\"Filosofi.*2016\")]\n\nIci, meta['label'].str.contains(r\"Filosofi.*2016\") signifie:\n‚Äúpandas trouve moi tous les labels o√π sont contenus les termes Filosofi et 2016.‚Äù\n(.* signifiant ‚Äúpeu m‚Äôimporte le nombre de mots ou caract√®res entre‚Äù)\n\n\n\n\n Exercice 1: Afficher des donn√©es\nL‚Äôobjectif de cet exercice est de vous amener √† afficher des informations sur les donn√©es dans un bloc de code (notebook) ou dans la console\nCommencer sur df:\n\nUtiliser les m√©thodes ad√©quates pour les 10 premi√®res valeurs, les 15 derni√®res et un √©chantillon al√©atoire de 10 valeurs\nTirer 5 pourcent de l‚Äô√©chantillon sans remise\nNe conserver que les 10 premi√®res lignes et tirer al√©atoirement dans celles-ci pour obtenir un DataFrame de 100 donn√©es.\nFaire 100 tirages √† partir des 6 premi√®res lignes avec une probabilit√© de 1/2 pour la premi√®re observation et une probabilit√© uniforme pour les autres\nFaire la m√™me chose sur df_city.\n\n\n\nCette premi√®re approche exploratoire donne une id√©e assez pr√©cise de la mani√®re dont les donn√©es sont organis√©es.\nOn remarque ainsi une diff√©rence entre df et df_city quant aux valeurs manquantes :\nla premi√®re base est relativement compl√®te, la seconde comporte beaucoup de valeurs manquantes.\nAutrement dit, si on d√©sire exploiter df_city, il faut faire attention √† la variable choisie.\n\n\n Exercice 2: structure des donn√©es\nLa premi√®re chose √† v√©rifier est le format des donn√©es,\nafin d‚Äôidentifier des types de variables qui ne conviennent pas\nIci, comme c‚Äôest pandas qui a g√©r√© automatiquement les types de variables,\nil y a peu de chances que les types ne soient pas ad√©quats mais une v√©rification ne fait pas de mal.\n\nV√©rifier les types des variables.\nS‚Äôassurer que les types des variables communes aux deux bases sont coh√©rents.\nPour les variables qui ne sont pas en type float alors qu‚Äôelles devraient l‚Äô√™tre, modifier leur type.\n\nEnsuite, on v√©rifie les dimensions des DataFrames et la structure de certaines variables cl√©s.\nEn l‚Äôoccurrence, les variables fondamentales pour lier nos donn√©es sont les variables communales.\nIci, on a deux variables g√©ographiques: un code commune et un nom de commune.\n\nV√©rifier les dimensions des DataFrames\nV√©rifier le nombre de valeurs uniques des variables g√©ographiques dans chaque base. Les r√©sultats apparaissent-ils coh√©rents ?\nIdentifier dans df_city les noms de communes qui correspondent √† plusieurs codes communes et s√©lectionner leurs codes. En d‚Äôautres termes, identifier les CODGEO tels qu‚Äôil existe des doublons de LIBGEO et les stocker dans un vecteur x (conseil: faire attention √† l‚Äôindex de x)\n\nOn se focalise temporairement sur les observations o√π le libell√© comporte plus de deux codes communes diff√©rents\n\nRegarder dans df_city ces observations\nPour mieux y voir, r√©ordonner la base obtenue par order alphab√©tique\nD√©terminer la taille moyenne (variable nombre de personnes: NBPERSMENFISC16) et quelques statistiques descriptives de ces donn√©es.\nComparer aux m√™mes statistiques sur les donn√©es o√π libell√©s et codes communes co√Øncident\nV√©rifier les grandes villes (plus de 100 000 personnes),\nla proportion de villes pour lesquelles un m√™me nom est associ√© √† diff√©rents codes commune.\nV√©rifier dans df_city les villes dont le libell√© est √©gal √† Montreuil.\nV√©rifier √©galement celles qui contiennent le terme ‚ÄòSaint-Denis‚Äô\n\n\n\nCe petit exercice permet de se rassurer car les libell√©s dupliqu√©s\nsont en fait des noms de commune identiques mais qui ne sont pas dans le m√™me d√©partement.\nIl ne s‚Äôagit donc pas d‚Äôobservations dupliqu√©es.\nOn se fiera ainsi aux codes communes, qui eux sont uniques."
  },
  {
    "objectID": "content/course/manipulation/02b_pandas_TP/index.html#les-indices",
    "href": "content/course/manipulation/02b_pandas_TP/index.html#les-indices",
    "title": "46¬† Pratique de pandas: un exemple complet",
    "section": "46.2 Les indices",
    "text": "46.2 Les indices\nLes indices sont des √©l√©ments sp√©ciaux d‚Äôun DataFrame puisqu‚Äôils permettent d‚Äôidentifier certaines observations.\nIl est tout √† fait possible d‚Äôutiliser plusieurs indices, par exemple si on a des niveaux imbriqu√©s.\n\n\n Exercice 3: Les indices\nA partir de l‚Äôexercice pr√©c√©dent, on peut se fier aux codes communes.\n\nFixer comme indice la variable de code commune dans les deux bases.\nRegarder le changement que cela induit sur le display du DataFrame\nLes deux premiers chiffres des codes communes sont le num√©ro de d√©partement.\nCr√©er une variable de d√©partement dep dans df et dans df_city\nCalculer les √©missions totales par secteur pour chaque d√©partement.\nMettre en log ces r√©sultats dans un objet df_log.\nGarder 5 d√©partements et produire un barplot\nRepartir de df.\nCalculer les √©missions totales par d√©partement et sortir la liste\ndes 10 principaux √©metteurs de CO2 et des 5 d√©partements les moins √©metteurs.\nSans faire de merge,\nregarder les caract√©ristiques de ces d√©partements (population et niveau de vie)\n\n\n\n\n\n Exercice 4: performance des indices\nUn des int√©r√™ts des indices est qu‚Äôils permettent des agr√©gations efficaces.\n\nRepartir de df et cr√©er une copie df_copy = df.copy() et df_copy2 = df.copy() afin de ne pas √©craser le DataFrame df\nUtiliser la variable dep comme indice pour df_copy et retirer tout index pour df_copy2\nImporter le module timeit et comparer le temps d‚Äôex√©cution de la somme par secteur, pour chaque d√©partement, des √©missions de CO2"
  },
  {
    "objectID": "content/course/manipulation/02b_pandas_TP/index.html#restructurer-les-donn√©es",
    "href": "content/course/manipulation/02b_pandas_TP/index.html#restructurer-les-donn√©es",
    "title": "46¬† Pratique de pandas: un exemple complet",
    "section": "46.3 Restructurer les donn√©es",
    "text": "46.3 Restructurer les donn√©es\nOn pr√©sente g√©n√©ralement deux types de donn√©es :\n\nformat wide: les donn√©es comportent des observations r√©p√©t√©es, pour un m√™me individu (ou groupe), dans des colonnes diff√©rentes\nformat long: les donn√©es comportent des observations r√©p√©t√©es, pour un m√™me individu, dans des lignes diff√©rentes avec une colonne permettant de distinguer les niveaux d‚Äôobservations\n\nUn exemple de la distinction entre les deux peut √™tre pris √† l‚Äôouvrage de r√©f√©rence d‚ÄôHadley Wickham, R for Data Science:\n\nL‚Äôaide m√©moire suivante aidera √† se rappeler les fonctions √† appliquer si besoin:\n\nLe fait de passer d‚Äôun format wide au format long (ou vice-versa)\npeut √™tre extr√™mement pratique car certaines fonctions sont plus ad√©quates sur une forme de donn√©es ou sur l‚Äôautre.\nEn r√®gle g√©n√©rale, avec Python comme avec R, les formats long sont souvent pr√©f√©rables.\n\n\n Exercice 5: Restructurer les donn√©es: wide to long\n\nCr√©er une copie des donn√©es de l‚ÄôADEME en faisant df_wide = df.copy()\nRestructurer les donn√©es au format long pour avoir des donn√©es d‚Äô√©missions par secteur en gardant comme niveau d‚Äôanalyse la commune (attention aux autres variables identifiantes).\nFaire la somme par secteur et repr√©senter graphiquement\nGarder, pour chaque d√©partement, le secteur le plus polluant\n\n\n\n\n\n Exercice 6: long to wide\nCette transformation est moins fr√©quente car appliquer des fonctions par groupe, comme nous le verrons par la suite, est tr√®s simple.\n\nRepartir de `df_wide = df.copy()\nReconstruire le DataFrame, au format long, des donn√©es d‚Äô√©missions par secteur en gardant comme niveau d‚Äôanalyse la commune puis faire la somme par d√©partement et secteur\nPasser au format wide pour avoir une ligne par secteur et une colonne par d√©partement\nCalculer, pour chaque secteur, la place du d√©partement dans la hi√©rarchie des √©missions nationales\nA partir de l√†, en d√©duire le rang m√©dian de chaque d√©partement dans la hi√©rarchie des √©missions et regarder les 10 plus mauvais √©l√®ves, selon ce crit√®re."
  },
  {
    "objectID": "content/course/manipulation/02b_pandas_TP/index.html#combiner-les-donn√©es",
    "href": "content/course/manipulation/02b_pandas_TP/index.html#combiner-les-donn√©es",
    "title": "46¬† Pratique de pandas: un exemple complet",
    "section": "46.4 Combiner les donn√©es",
    "text": "46.4 Combiner les donn√©es\nUne information que l‚Äôon cherche √† obtenir s‚Äôobtient de moins en moins √† partir d‚Äôune unique base de donn√©es. Il devient commun de devoir combiner des donn√©es issues de sources diff√©rentes. Nous allons ici nous focaliser sur le cas le plus favorable qui est la situation o√π une information permet d‚Äôapparier de mani√®re exacte deux bases de donn√©es (autrement nous serions dans une situation, beaucoup plus complexe, d‚Äôappariement flou). La situation typique est l‚Äôappariement entre deux sources de donn√©es selon un identifiant individuel ou un identifiant de code commune, ce qui est notre cas.\nIl est recommand√© de lire ce guide assez complet sur la question des jointures avec R qui donne des recommandations √©galement utiles en python.\nOn utilise de mani√®re indiff√©rente les termes merge ou join. Le deuxi√®me terme provient de la syntaxe SQL. En pandas, dans la plupart des cas, on peut utiliser indiff√©remment df.join et df.merge\n\n\n\n Exercice 7: Calculer l'empreinte carbone par habitant\n\nCr√©er une variable emissions qui correspond aux √©missions totales d‚Äôune commune\nFaire une jointure √† gauche entre les donn√©es d‚Äô√©missions et les donn√©es de cadrage. Comparer les √©missions moyennes des villes sans match (celles dont des variables bien choisies de la table de droite sont NaN) avec celles o√π on a bien une valeur correspondante dans la base Insee\nFaire un inner join puis calculer l‚Äôempreinte carbone (l‚Äô√©mission rapport√©e au nombre de m√©nages fiscaux) dans chaque commune. Sortir un histogramme en niveau puis en log et quelques statistiques descriptives sur le sujet.\nRegarder la corr√©lation entre les variables de cadrage et l‚Äôempreinte carbone. Certaines variables semblent-elles pouvoir potentiellement influer sur l‚Äôempreinte carbone ?"
  },
  {
    "objectID": "content/course/manipulation/02b_pandas_TP/index.html#exercices-bonus",
    "href": "content/course/manipulation/02b_pandas_TP/index.html#exercices-bonus",
    "title": "46¬† Pratique de pandas: un exemple complet",
    "section": "46.5 Exercices bonus",
    "text": "46.5 Exercices bonus\nLes plus rapides d‚Äôentre vous sont invit√©s √† aller un peu plus loin en s‚Äôentra√Ænant avec des exercices bonus qui proviennent du site de Xavier Dupr√©. 3 notebooks en lien avec numpy et pandas vous y sont propos√©s :\n\nCalcul Matriciel, Optimisation : √©nonc√© / corrig√©\nDataFrame et Graphes : √©nonc√© / corrig√©\nPandas et it√©rateurs : √©nonc√© / corrig√©"
  },
  {
    "objectID": "content/course/manipulation/04c_API_TP/index.html#introduction-quest-ce-quune-api",
    "href": "content/course/manipulation/04c_API_TP/index.html#introduction-quest-ce-quune-api",
    "title": "47¬† R√©cup√©rer des donn√©es avec des API depuis Python",
    "section": "47.1 Introduction : Qu‚Äôest-ce qu‚Äôune API ?",
    "text": "47.1 Introduction : Qu‚Äôest-ce qu‚Äôune API ?\n\n47.1.1 D√©finition\nPour expliquer le principe d‚Äôune API, je vais reprendre le d√©but de\nla fiche d√©di√©e dans la documentation collaborative\nutilitR que je recommande de lire :\n\nUne Application Programming Interface (ou API) est une interface de programmation qui permet d‚Äôutiliser une application existante pour restituer des donn√©es. Le terme d‚ÄôAPI peut √™tre para√Ætre intimidant, mais il s‚Äôagit simplement d‚Äôune fa√ßon de mettre √† disposition des donn√©es : plut√¥t que de laisser l‚Äôutilisateur consulter directement des bases de donn√©es (souvent volumineuses et complexes), l‚ÄôAPI lui propose de formuler une requ√™te qui est trait√©e par le serveur h√©bergeant la base de donn√©es, puis de recevoir des donn√©es en r√©ponse √† sa requ√™te.\nD‚Äôun point de vue informatique, une API est une porte d‚Äôentr√©e clairement identifi√©e par laquelle un logiciel offre des services √† d‚Äôautres logiciels (ou utilisateurs). L‚Äôobjectif d‚Äôune API est de fournir un point d‚Äôacc√®s √† une fonctionnalit√© qui soit facile √† utiliser et qui masque les d√©tails de la mise en oeuvre. Par exemple, l‚ÄôAPI Sirene permet de r√©cup√©rer la raison sociale d‚Äôune entreprise √† partir de son identifiant Siren en interrogeant le r√©f√©rentiel disponible sur Internet directement depuis un script R, sans avoir √† conna√Ætre tous les d√©tails du r√©pertoire Sirene.\n√Ä l‚ÄôInsee comme ailleurs, la connexion entre les bases de donn√©es pour les nouveaux projets tend √† se r√©aliser par des API. L‚Äôacc√®s √† des donn√©es par des API devient ainsi de plus en plus commun et est amen√© √† devenir une comp√©tence de base de tout utilisateur de donn√©es.\nutilitR\n\n\n\n47.1.2 Avantages des API\nA nouveau, citons la documentation utilitR\nLes API pr√©sentent de multiples avantages :\n\n\nLes API rendent les programmes plus reproductibles. En effet, gr√¢ce aux API, il est possible de mettre √† jour facilement les donn√©es utilis√©es par un programme si celles-ci √©voluent. Cette flexibilit√© accrue pour l‚Äôutilisateur √©vite au producteur de donn√©es d‚Äôavoir √† r√©aliser de multiples extractions, et r√©duit le probl√®me de la coexistence de versions diff√©rentes des donn√©es.\nGr√¢ce aux API, l‚Äôutilisateur peut extraire facilement une petite partie d‚Äôune base de donn√©es plus cons√©quente.\nLes API permettent de mettre √† disposition des donn√©es tout en limitant le nombre de personnes ayant acc√®s aux bases de donn√©es elles-m√™mes.\nGr√¢ce aux API, il est possible de proposer des services sur mesure pour les utilisateurs (par exemple, un acc√®s sp√©cifique pour les gros utilisateurs).\n\nutilitR\n\nL‚Äôutilisation accrue d‚ÄôAPI dans le cadre de strat√©gies open-data est l‚Äôun\ndes piliers des 15 feuilles de route minist√©rielles\nen mati√®re d‚Äôouverture, de circulation et de valorisation des donn√©es publiques.\n\n\n47.1.3 Utilisation des API\nCitons encore une fois\nla documentation utilitR\n\nUne API peut souvent √™tre utilis√©e de deux fa√ßons : par une interface Web, et par l‚Äôinterm√©diaire d‚Äôun logiciel (R, Python‚Ä¶). Par ailleurs, les API peuvent √™tre propos√©es avec un niveau de libert√© variable pour l‚Äôutilisateur :\n\nsoit en libre acc√®s (l‚Äôutilisation n‚Äôest pas contr√¥l√©e et l‚Äôutilisateur peut utiliser le service comme bon lui semble)‚ÄØ;\nsoit via la g√©n√©ration d‚Äôun compte et d‚Äôun jeton d‚Äôacc√®s qui permettent de s√©curiser l‚Äôutilisation de l‚ÄôAPI et de limiter le nombre de requ√™tes.\n\nutilitR\n\nDe nombreuses API n√©cessitent une authentification, c‚Äôest-√†-dire un\ncompte utilisateur afin de pouvoir acc√©der aux donn√©es.\nDans un premier temps,\nnous regarderons exclusivement les API ouvertes sans restriction d‚Äôacc√®s.\nCertains exercices et exemples permettront n√©anmoins d‚Äôessayer des API\navec restrictions d‚Äôacc√®s."
  },
  {
    "objectID": "content/course/manipulation/04c_API_TP/index.html#requ√™ter-une-api",
    "href": "content/course/manipulation/04c_API_TP/index.html#requ√™ter-une-api",
    "title": "47¬† R√©cup√©rer des donn√©es avec des API depuis Python",
    "section": "47.2 Requ√™ter une API",
    "text": "47.2 Requ√™ter une API\n\n47.2.1 Principe g√©n√©ral\n\nL‚Äôutilisation de l‚Äôinterface Web est utile dans une d√©marche exploratoire mais trouve rapidement ses limites, notamment lorsqu‚Äôon consulte r√©guli√®rement l‚ÄôAPI. L‚Äôutilisateur va rapidement se rendre compte qu‚Äôil est beaucoup plus commode d‚Äôutiliser une API via un logiciel de traitement pour automatiser la consultation ou pour r√©aliser du t√©l√©chargement de masse. De plus, l‚Äôinterface Web n‚Äôexiste pas syst√©matiquement pour toutes les API.\nLe mode principal de consultation d‚Äôune API consiste √† adresser une requ√™te √† cette API via un logiciel adapt√© (R, Python, Java‚Ä¶). Comme pour l‚Äôutilisation d‚Äôune fonction, l‚Äôappel d‚Äôune API comprend des param√®tres qui sont d√©taill√©es dans la documentation de l‚ÄôAPI.\nutilitR\n\nVoici les √©l√©ments importants √† avoir en t√™te sur les requ√™tes (j‚Äôemprunte encore\n√† utilitR):\n\nLe point d‚Äôentr√©e d‚Äôun service offert par une API se pr√©sente sous la forme d‚Äôune URL (adresse web).\nChaque service propos√© par une API a sa propre URL. Par exemple, dans le cas de l‚ÄôOpenFood Facts,\nl‚ÄôURL √† utiliser pour obtenir des informations sur un produit particulier (l‚Äôidentifiant 737628064502)\nest https://world.openfoodfacts.org/api/v0/product/737628064502.json\nCette URL doit √™tre compl√©t√©e avec diff√©rents param√®tres qui pr√©cisent la requ√™te (par exemple l‚Äôidentifiant Siren). Ces param√®tres viennent s‚Äôajouter √† l‚ÄôURL, souvent √† la suite de ?. Chaque service propos√© par une API a ses propres param√®tres, d√©taill√©s dans la documentation.\nLorsque l‚Äôutilisateur soumet sa requ√™te, l‚ÄôAPI lui renvoie une r√©ponse structur√©e contenant l‚Äôensemble des informations demand√©es. Le r√©sultat envoy√© par une API est majoritairement aux formats JSON ou XML (deux formats dans lesquels les informations sont hi√©rarchis√©es de mani√®re emboit√©e). Plus rarement, certains services proposent une information sous forme plate (de type csv).\n\nDu fait de la dimension hi√©rarchique des formats JSON ou XML, le r√©sultat n‚Äôest pas toujours facile √† r√©cup√©rer mais\npython propose d‚Äôexcellents outils pour cela (meilleurs que ceux de R). Certains packages, notamment json, facilitent l‚Äôextraction de champs d‚Äôune sortie d‚ÄôAPI. Dans certains cas, des packages sp√©cifiques √† une API ont √©t√© cr√©√©s pour simplifier l‚Äô√©criture d‚Äôune requ√™te ou la r√©cup√©ration du r√©sultat. Par exemple, le package\npynsee\npropose des options qui seront retranscrites automatiquement dans l‚ÄôURL de\nrequ√™te pour faciliter le travail sur les donn√©es Insee.\n\n\n47.2.2 Illustration avec une API de l‚ÄôAdeme pour obtenir des diagnostics energ√©tiques\nLe diagnostic de performance √©nerg√©tique (DPE)\nrenseigne sur la performance √©nerg√©tique d‚Äôun logement ou d‚Äôun b√¢timent,\nen √©valuant sa consommation d‚Äô√©nergie et son impact en terme d‚Äô√©missions de gaz √† effet de serre.\nLes donn√©es des performances √©nerg√©tiques des b√¢timents sont\nmises √† disposition par l‚ÄôAdeme.\nComme ces donn√©es sont relativement\nvolumineuses, une API peut √™tre utile lorsqu‚Äôon ne s‚Äôint√©resse\nqu‚Äô√† un sous-champ des donn√©es.\nUne documentation et un espace de test de l‚ÄôAPI sont disponibles\nsur le site API GOUV1.\nSupposons qu‚Äôon d√©sire r√©cup√©rer une centaine de valeurs pour la commune\nde Villieu-Loyes-Mollon dans l‚ÄôAin (code Insee 01450).\nL‚ÄôAPI comporte plusieurs points d‚Äôentr√©e. Globalement, la racine\ncommune est:\n\nhttps://koumoul.com/data-fair/api/v1/datasets/dpe-france\n\nEnsuite, en fonction de l‚ÄôAPI d√©sir√©e, on va ajouter des √©l√©ments\n√† cette racine. En l‚Äôoccurrence, on va utiliser\nl‚ÄôAPI field qui permet de r√©cup√©rer des lignes en fonction d‚Äôun\nou plusieurs crit√®res (pour nous, la localisation g√©ographique):\nL‚Äôexemple donn√© dans la documentation technique est\n\nGET https://koumoul.com/data-fair/api/v1/datasets/dpe-france/values/{field}\n\nce qui en python se traduira par l‚Äôutilisation de la m√©thode get du\npackage request\nsur un url dont la structure est la suivante:\n\nil commencera par https://koumoul.com/data-fair/api/v1/datasets/dpe-france/values/ ;\nil sera ensuite suivi par des param√®tres de recherche? Le champ {field}\ncommande ainsi g√©n√©ralement par un ? qui permet ensuite de sp√©cifier des param√®tres\nsous la forme nom_parameter=value\n\nA la lecture de la documentation, les premiers param√®tres qu‚Äôon d√©sire:\n\nLe nombre de pages, ce qui nous permet d‚Äôobtenir un certain nombre d‚Äô√©chos. On\nva seulement r√©cup√©rer 10 pages ce qui correspond √† une centaine d‚Äô√©chos. On va\nn√©anmoins pr√©ciser qu‚Äôon veut 100 √©chos\nLe format de sortie. On va privil√©gier le JSON qui est un format standard dans le\nmonde des API. Python offre beaucoup de flexibilit√© gr√¢ce √† l‚Äôun de\nses objets de base, √† savoir le dictionnaire (type dict), pour manipuler de tels\nfichiers\nLe code commune des donn√©es qu‚Äôon d√©sire obtenir. Comme on l‚Äôa √©voqu√©,\non va r√©cup√©rer les donn√©es dont le code commune est 01450. D‚Äôapr√®s la doc,\nil convient de passer le code commune sous le format:\ncode_insee_commune_actualise:{code_commune}. Pour √©viter tout risque de\nmauvais formatage, on va utiliser %3A% pour signifier :\nD‚Äôautres param√®tres annexes, sugg√©r√©s par la documentation\n\nCela nous donne ainsi un URL dont la structure est la suivante:\n\ncode_commune=\"01450\"\nsize = 100\napi_root=\"https://koumoul.com/data-fair/api/v1/datasets/dpe-france/lines\"\nurl_api = f\"{api_root}?page=1&after=10&format=json&q_mode=simple&qs=code_insee_commune_actualise\" + \"%3A%22\" + f\"{code_commune}\" + \"%22\" + f\"&size={size}&select=\" + \"%2A&sampling=neighbors\"\n\nSi vous introduisez cet URL dans votre navigateur, vous devriez aboutir\nsur un JSON non format√©2. En Python,\non peut utiliser requests pour r√©cup√©rer les donn√©es3:\n\nimport requests\nimport pandas as pd\n\nreq = requests.get(url_api)\nwb = req.json()\n\nPrenons par exemple les 1000 premiers caract√®res du r√©sultat, pour se donner\nune id√©e du r√©sultat et se convaincre que notre filtre au niveau\ncommunal est bien pass√© :\nprint(req.content[:1000])\nb‚Äô{‚Äútotal‚Äù: 121,‚Äúnext‚Äù: ‚Äúhttps://koumoul.com/data-fair/api/v1/datasets/dpe-france/lines?after=102721&format=json&q_mode=simple&qs=code_insee_commune_actualise%3A%2201450%22&size=100&select=*&sampling=neighbors‚Äù,‚Äúresults‚Äù: [\\n {‚Äúclasse_consommation_energie‚Äù: ‚ÄúD‚Äù,‚Äútr001_modele_dpe_type_libelle‚Äù: ‚ÄúVente‚Äù,‚Äúannee_construction‚Äù: 1947,‚Äú_geopoint‚Äù: ‚Äú45.925922,5.229964‚Äù,‚Äúlatitude‚Äù: 45.925922,‚Äúsurface_thermique_lot‚Äù: 117.16,‚Äú_i‚Äù: 487,‚Äútr002_type_batiment_description‚Äù: ‚ÄúMaison Individuelle‚Äù,‚Äúgeo_adresse‚Äù: ‚ÄúRue de la Brugni8re 01800 Villieu-Loyes-Mollon‚Äù,‚Äú_rand‚Äù: 23215,‚Äúcode_insee_commune_actualise‚Äù: ‚Äú01450‚Äù,‚Äúestimation_ges‚Äù: 53,‚Äúgeo_score‚Äù: 0.4,‚Äúclasse_estimation_ges‚Äù: ‚ÄúE‚Äù,‚Äúnom_methode_dpe‚Äù: ‚ÄúM9thode Facture‚Äù,‚Äútv016_departement_code‚Äù: ‚Äú01‚Äù,‚Äúconsommation_energie‚Äù: 178,‚Äúdate_etablissement_dpe‚Äù: ‚Äú2013-06-13‚Äù,‚Äúlongitude‚Äù: 5.229964,‚Äú_score‚Äù: null,‚Äô\nIci, il n‚Äôest m√™me pas n√©cessaire en premi√®re approche\nd‚Äôutiliser le package json, l‚Äôinformation\n√©tant d√©j√† tabul√©e dans l‚Äô√©cho renvoy√© (on a la m√™me information pour tous les pays):\nOn peut donc se contenter de pandas pour transformer nos donn√©es en\nDataFrame et geopandas pour convertir en donn√©es\ng√©ographiques :\n\nimport pandas as pandas\nimport geopandas as gpd\n\ndef get_dpe_from_url(url):\n\n    req = requests.get(url)\n    wb = req.json()\n    df = pd.json_normalize(wb[\"results\"])\n\n    dpe = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude), crs = 4326)\n    dpe = dpe.dropna(subset = ['longitude', 'latitude'])\n\n    return dpe\n\ndpe = get_dpe_from_url(url_api)\ndpe.head(2)\n\n/miniconda/envs/python-ENSAE/lib/python3.9/site-packages/geopandas/_compat.py:124: UserWarning:\n\nThe Shapely GEOS version (3.11.2-CAPI-1.17.2) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n\n/tmp/ipykernel_1978/2008334648.py:2: DeprecationWarning:\n\nShapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas still uses PyGEOS by default. However, starting with version 0.14, the default will switch to Shapely. To force to use Shapely 2.0 now, you can either uninstall PyGEOS or set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n\nimport os\nos.environ['USE_PYGEOS'] = '0'\nimport geopandas\n\nIn the next release, GeoPandas will switch to using Shapely by default, even if PyGEOS is installed. If you only have PyGEOS installed to get speed-ups, this switch should be smooth. However, if you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n\n\n\n\n\n\n\n\n\n\nclasse_consommation_energie\ntr001_modele_dpe_type_libelle\nannee_construction\n_geopoint\nlatitude\nsurface_thermique_lot\n_i\ntr002_type_batiment_description\ngeo_adresse\n_rand\n...\nclasse_estimation_ges\nnom_methode_dpe\ntv016_departement_code\nconsommation_energie\ndate_etablissement_dpe\nlongitude\n_score\n_id\nversion_methode_dpe\ngeometry\n\n\n\n\n0\nD\nVente\n1947\n45.925922,5.229964\n45.925922\n117.16\n487\nMaison Individuelle\nRue de la Brugni√®re 01800 Villieu-Loyes-Mollon\n23215\n...\nE\nM√©thode Facture\n01\n178.00\n2013-06-13\n5.229964\nNone\n04JZNel3WCJYcfsHpCcHv\nNaN\nPOINT (5.22996 45.92592)\n\n\n2\nD\nNeuf\n2006\n45.923421,5.223777\n45.923421\n90.53\n689\nMaison Individuelle\nChemin du Pont-vieux 01800 Villieu-Loyes-Mollon\n401672\n...\nC\nFACTURE - DPE\n01\n227.99\n2013-06-11\n5.223777\nNone\nrkdV2lJn2wxaidVBaHBFY\nV2012\nPOINT (5.22378 45.92342)\n\n\n\n\n2 rows √ó 23 columns\n\n\n\nEssayons de repr√©senter sur une carte ces DPE avec les\nann√©es de construction des logements.\nAvec folium, on obtient la carte interactive suivante:\n\nimport seaborn as sns\nimport folium\n\npalette = sns.color_palette(\"coolwarm\", 8)\n\ndef interactive_map_dpe(dpe):\n\n    # convert in number\n    dpe['color'] = [ord(dpe.iloc[i]['classe_consommation_energie'].lower()) - 96 for i in range(len(dpe))]\n    dpe = dpe.loc[dpe['color']&lt;=7]\n    dpe['color'] = [palette.as_hex()[x] for x in dpe['color']]\n\n\n    center = dpe[['latitude', 'longitude']].mean().values.tolist()\n    sw = dpe[['latitude', 'longitude']].min().values.tolist()\n    ne = dpe[['latitude', 'longitude']].max().values.tolist()\n\n    m = folium.Map(location = center, tiles='Stamen Toner')\n\n    # I can add marker one by one on the map\n    for i in range(0,len(dpe)):\n        folium.Marker([dpe.iloc[i]['latitude'], dpe.iloc[i]['longitude']],\n                    popup=f\"Ann√©e de construction: {dpe.iloc[i]['annee_construction']}, &lt;br&gt;DPE: {dpe.iloc[i]['classe_consommation_energie']}\",\n                    icon=folium.Icon(color=\"black\", icon=\"home\", icon_color = dpe.iloc[i]['color'])).add_to(m)\n\n    m.fit_bounds([sw, ne])\n\n    return m\n\nm = interactive_map_dpe(dpe)\n\n/miniconda/envs/python-ENSAE/lib/python3.9/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n# Afficher la carte\nm\n\nOn remarque un probl√®me dans les donn√©es: un logement qui n‚Äôa\nrien √† voir avec les autres. Il faudrait donc id√©alement\nnettoyer un peu le jeu de donn√©es pour filtrer en fonction de\nlimites g√©ographiques.\nUn des param√®tres qui peut permettre ceci est geo_distance.\nPour commencer, on va tricher un petit peu pour d√©terminer\nles longitudes et latitudes de d√©part. Id√©alement, on\nr√©cup√©rerait le d√©coupage de la commune et utiliserait, par\nexemple, le centroid de cette commune. Cela n√©cessite\nn√©anmoins l‚Äôappel √† une autre API que nous n‚Äôavons\npour le moment pas d√©crite. Nous allons donc\nnous contenter d‚Äôutiliser les longitudes et latitudes\ndu point m√©dian et fixer un rayon de plusieurs kilom√®tres\npour exclure les points aberrants.\n\nx_median = dpe['longitude'].median()\ny_median = dpe['latitude'].median()\n\nLa documentation nous informe du format √† utiliser:\n\nLe format est ‚Äòlon,lat,distance‚Äô. La distance optionnelle (0 par d√©faut) et est exprim√©e en m√®tres.\n\n\nparam_distance = f'{x_median},{y_median},1000'\nprint(param_distance)\n\n5.223777,45.922457,1000\n\n\nNotre requ√™te devient ainsi:\n\nurl_api = f\"{api_root}?page=1&after=10&format=json&q_mode=simple&qs=code_insee_commune_actualise\" + \"%3A%22\" + f\"{code_commune}\" + \"%22\" + f\"&size={size}&select=\" + \"%2A&sampling=neighbors\" + f\"&geodistance={param_distance}\"\nprint(url_api)\n\nhttps://koumoul.com/data-fair/api/v1/datasets/dpe-france/lines?page=1&after=10&format=json&q_mode=simple&qs=code_insee_commune_actualise%3A%2201450%22&size=100&select=%2A&sampling=neighbors&geodistance=5.223777,45.922457,1000\n\n\n\ndpe_geo_filter = get_dpe_from_url(url_api)\nm_geo_filter = interactive_map_dpe(dpe)\n\n/miniconda/envs/python-ENSAE/lib/python3.9/site-packages/geopandas/geodataframe.py:1538: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n# Afficher la carte\nm_geo_filter\n\n\n\n47.2.3 Un catalogue incomplet d‚ÄôAPI existantes\nDe plus en plus de sites mettent des API √† disposition des d√©veloppeurs et autres curieux.\nPour en citer quelques-unes tr√®s connues :\n\nTwitter  : https://dev.twitter.com/rest/public\nFacebook  : https://developers.facebook.com/\nInstagram  : https://www.instagram.com/developer/\nSpotify  : https://developer.spotify.com/web-api/\n\nCependant, il est int√©ressant de ne pas se restreindre √† celles-ci dont les\ndonn√©es ne sont pas toujours les plus int√©ressantes. Beaucoup\nde producteurs de donn√©es, priv√©s comme publics, mettent √† disposition\nleurs donn√©es sous forme d‚ÄôAPI\n\nAPI gouv: beaucoup d‚ÄôAPI officielles de l‚ÄôEtat fran√ßais\net acc√®s √† de la documentation\nInsee: https://api.insee.fr/catalogue/ et pynsee\nPole Emploi : https://www.emploi-store-dev.fr/portail-developpeur-cms/home.html\nSNCF : https://data.sncf.com/api\nBanque Mondiale : https://datahelpdesk.worldbank.org/knowledgebase/topics/125589"
  },
  {
    "objectID": "content/course/manipulation/04c_API_TP/index.html#lapi-dvf-acc√©der-√†-des-donn√©es-de-transactions-immobili√®res-simplement",
    "href": "content/course/manipulation/04c_API_TP/index.html#lapi-dvf-acc√©der-√†-des-donn√©es-de-transactions-immobili√®res-simplement",
    "title": "47¬† R√©cup√©rer des donn√©es avec des API depuis Python",
    "section": "47.3 L‚ÄôAPI DVF : acc√©der √† des donn√©es de transactions immobili√®res simplement",
    "text": "47.3 L‚ÄôAPI DVF : acc√©der √† des donn√©es de transactions immobili√®res simplement\n‚ö†Ô∏è Cette partie n√©cessite une mise √† jour pour privil√©gier l‚ÄôAPI DVF du Cerema\nLe site DVF (demandes de valeurs fonci√®res) permet de visualiser toutes les donn√©es relatives aux mutations √† titre on√©reux (ventes de maisons, appartements, garages‚Ä¶) r√©alis√©es durant les 5 derni√®res ann√©es.\nUn site de visualisation est disponible sur https://app.dvf.etalab.gouv.fr/.\nCe site est tr√®s complet quand il s‚Äôagit de conna√Ætre le prix moyen au m√®tre\ncarr√© d‚Äôun quartier ou de comparer des r√©gions entre elles.\nL‚ÄôAPI DVF permet d‚Äôaller plus loin afin de r√©cup√©rer les r√©sultats dans\nun logiciel de traitement de donn√©es. Elle a √©t√© r√©alis√©e par\nChristian Quest et le code\nsource est disponible sur Github .\nLes crit√®res de recherche sont les suivants :\n- code_commune = code INSEE de la commune (ex: 94068)\n- section = section cadastrale (ex: 94068000CQ)\n- numero_plan = identifiant de la parcelle, (ex: 94068000CQ0110)\n- lat + lon + dist (optionnel): pour une recherche g√©ographique, dist est par d√©faut un rayon de 500m\n- code_postal\nLes filtres de s√©lection compl√©mentaires :\n- nature_mutation (Vente, etc)\n- type_local (Maison, Appartement, Local, D√©pendance)\nLes requ√™tes sont de la forme : http://api.cquest.org/dvf?code_commune=29168.\n\n\n Exercice 1 : Exploiter l'API DVF\n\nRechercher toutes les transactions existantes dans DVF √† Plogoff (code commune 29168, en Bretagne).\nAfficher les cl√©s du JSON et en d√©duire le nombre de transactions r√©pertori√©es.\nN‚Äôafficher que les transactions portant sur des maisons.\nUtiliser l‚ÄôAPI geo pour\nr√©cup√©rer le d√©coupage communal de la ville de Plogoff\nRepr√©senter l‚Äôhistogramme des prix de vente\n\nN‚Äôh√©sitez pas √† aller plus loin en jouant sur des variables de\ngroupes par exemple\n\n\nLe r√©sultat de la question 2 devrait\nressembler au DataFrame suivant:\nL‚Äôhistogramme des prix de vente (question 4) aura l‚Äôaspect suivant:\nOn va faire une carte des ventes en affichant le prix de l‚Äôachat.\nLa cartographie r√©active sera pr√©sent√©e dans les chapitres\nconsacr√©s √† la visualisation de donn√©es.\nSupposons que le DataFrame des ventes s‚Äôappelle ventes. Il faut d‚Äôabord le\nconvertir\nen objet geopandas.\n\nventes = ventes.dropna(subset = ['lat','lon'])\nventes = gpd.GeoDataFrame(ventes, geometry=gpd.points_from_xy(ventes.lon, ventes.lat))\nventes\n\nAvant de faire une carte, on va convertir\nles limites de la commune de Plogoff en geoJSON pour faciliter\nsa repr√©sentation avec folium\n(voir la doc geopandas √† ce propos):\n\ngeo_j = plgf.to_json()\n\nPour repr√©senter graphiquement, on peut utiliser le code suivant (essayez de\nle comprendre et pas uniquement de l‚Äôex√©cuter).\n\nimport folium\nimport numpy as np\n\nventes['map_color'] = pd.qcut(ventes['valeur_fonciere'], [0,0.8,1], labels = ['lightblue','red'])\nventes['icon'] = np.where(ventes['type_local']== 'Maison', \"home\", \"\")\nventes['num_voie_clean'] = np.where(ventes['numero_voie'].isnull(), \"\", ventes['numero_voie'])\nventes['text'] = ventes.apply(lambda s: \"Adresse: {num} {voie} &lt;br&gt;Vente en {annee} &lt;br&gt;Prix {prix:.0f} ‚Ç¨\".format(\n                        num = s['num_voie_clean'],\n                        voie = s[\"voie\"],\n                        annee = s['date_mutation'].split(\"-\")[0],\n                        prix = s[\"valeur_fonciere\"]),\n             axis=1)\n             \ncenter = ventes[['lat', 'lon']].mean().values.tolist()\nsw = ventes[['lat', 'lon']].min().values.tolist()\nne = ventes[['lat', 'lon']].max().values.tolist()\n\nm = folium.Map(location = center, tiles='Stamen Toner')\n\n# I can add marker one by one on the map\nfor i in range(0,len(ventes)):\n    folium.Marker([ventes.iloc[i]['lat'], ventes.iloc[i]['lon']],\n                  popup=ventes.iloc[i]['text'],\n                  icon=folium.Icon(color=ventes.iloc[i]['map_color'], icon=ventes.iloc[i]['icon'])).add_to(m)\n\nm.fit_bounds([sw, ne])\n\n\n# Afficher la carte\nm"
  },
  {
    "objectID": "content/course/manipulation/04c_API_TP/index.html#g√©ocoder-des-donn√©es-gr√¢ce-aux-api-officielles",
    "href": "content/course/manipulation/04c_API_TP/index.html#g√©ocoder-des-donn√©es-gr√¢ce-aux-api-officielles",
    "title": "47¬† R√©cup√©rer des donn√©es avec des API depuis Python",
    "section": "47.4 G√©ocoder des donn√©es gr√¢ce aux API officielles",
    "text": "47.4 G√©ocoder des donn√©es gr√¢ce aux API officielles\nJusqu‚Äô√† pr√©sent, nous avons travaill√©s sur des donn√©es o√π la dimension\ng√©ographique √©tait d√©j√† pr√©sente ou relativement facile √† int√©grer.\nCe cas id√©al ne se rencontre pas n√©cessairement dans la pratique.\nOn dispose parfois de localisations plus ou moins pr√©cises et plus ou\nmoins bien formatt√©es pour d√©terminer la localisation de certains\nlieux.\nDepuis quelques ann√©es, un service officiel de g√©ocodage a √©t√© mis en place.\nCelui-ci est gratuit et permet de mani√®re efficace de coder des adresses\n√† partir d‚Äôune API. Cette API, connue sous le nom de la Base d‚ÄôAdresses Nationale\n(BAN) a b√©n√©fici√© de la mise en commun de donn√©es de plusieurs\nacteurs (collectivit√©s locales, Poste) et de comp√©tences d‚Äôacteurs\ncomme Etalab. La documentation de celle-ci est disponible √† l‚Äôadresse\nhttps://api.gouv.fr/les-api/base-adresse-nationale\nPour illustrer la mani√®re de g√©ocoder des donn√©es avec Python, nous\nallons partir de la base\ndes r√©sultats des auto-√©coles √† l‚Äôexamen du permis sur l‚Äôann√©e 2018.\nCes donn√©es n√©cessitent un petit peu de travail pour √™tre propres √† une\nanalyse statistique.\nApr√®s avoir renomm√© les colonnes, nous n‚Äôallons conserver que\nles informations relatives au permis B (permis voiture classique) et\nles auto-√©coles ayant pr√©sent√© au moins 20 personnes √† l‚Äôexamen.\n\nimport pandas as pd\nimport xlrd\nimport geopandas as gpd\n\ndf = pd.read_excel(\"https://www.data.gouv.fr/fr/datasets/r/d4b6b072-8a7d-4e04-a029-8cdbdbaf36a5\", header = [0,1])\n\nindex_0 = [\"\" if df.columns[i][0].startswith(\"Unnamed\") else df.columns[i][0] for i in range(len(df.columns))]\nindex_1 = [df.columns[i][1] for i in range(len(df.columns))]\nkeep_index = [True if el in ('', \"B\") else False for el in index_0] \n\ncols = [index_0[i] + \" \" + index_1[i].replace(\"+\", \"_\") for i in range(len(df.columns))]\ndf.columns = cols\ndf = df.loc[:, keep_index]\ndf.columns = df.columns.str.replace(\"(^ |¬∞)\", \"\", regex = True).str.replace(\" \", \"_\")\ndf = df.dropna(subset = ['B_NB'])\ndf = df.loc[~df[\"B_NB\"].astype(str).str.contains(\"(\\%|\\.)\"),:]\n\ndf['B_NB'] = df['B_NB'].astype(int)\ndf['B_TR'] = df['B_TR'].str.replace(\",\", \".\").str.replace(\"%\",\"\").astype(float)\n\ndf = df.loc[df[\"B_NB\"]&gt;20]\n\n/tmp/ipykernel_1978/1059845781.py:16: UserWarning:\n\nThis pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n\n\n\nSur cet √©chantillon, le taux de r√©ussite moyen √©tait, en 2018, de 58.02%\nNos informations g√©ographiques prennent la forme suivante:\n\ndf.loc[:,['Adresse','CP','Ville']].head(5)\n\n\n\n\n\n\n\n\nAdresse\nCP\nVille\n\n\n\n\n0\n56 RUE CHARLES ROBIN\n01000\nBOURG EN BRESSE\n\n\n2\n7, avenue Revermont\n01250\nCeyzeriat\n\n\n3\n72 PLACE DE LA MAIRIE\n01000\nSAINT-DENIS LES BOURG\n\n\n4\n6 RUE DU LYCEE\n01000\nBOURG EN BRESSE\n\n\n5\n9 place Edgard Quinet\n01000\nBOURG EN BRESSE\n\n\n\n\n\n\n\nAutrement dit, nous disposons d‚Äôune adresse, d‚Äôun code postal et d‚Äôun nom\nde ville. Ces informations peuvent servir √† faire une recherche\nsur la localisation d‚Äôune auto-√©cole.\n\n47.4.1 Utiliser l‚ÄôAPI BAN\nLa documentation officielle de l‚ÄôAPI\npropose un certain nombre d‚Äôexemples de mani√®re de g√©olocaliser des donn√©es.\nDans notre situation, deux points d‚Äôentr√©e paraissent int√©ressants:\n\nL‚ÄôAPI /search/ qui repr√©sente un point d‚Äôentr√©e avec des URL de la forme\nhttps://api-adresse.data.gouv.fr/search/?q=&lt;adresse&gt;&postcode=&lt;codepostal&gt;&limit=1\nL‚ÄôAPI /search/csv qui prend un CSV en entr√©e et retourne ce m√™me CSV avec\nles observations g√©ocod√©es. La requ√™te prend la forme suivante, en apparence\nmoins simple √† mettre en oeuvre :\ncurl -X POST -F data=@search.csv -F columns=adresse -F columns=postcode https://api-adresse.data.gouv.fr/search/csv/\n\nLa tentation serait forte d‚Äôutiliser la premi√®re m√©thode avec une boucle sur les\nlignes de notre DataFrame pour g√©ocoder l‚Äôensemble de notre jeu de donn√©es.\nCela serait n√©anmoins une mauvaise id√©e car les communications entre notre\nsession Python et les serveurs de l‚ÄôAPI seraient beaucoup trop nombreuses\npour offrir des performances satisfaisantes.\nPour vous en convaincre, vous pouvez ex√©cuter le code suivant sur un petit\n√©chantillon de donn√©es (par exemple 100 comme ici) et remarquer que le temps\nd‚Äôex√©cution est assez important\n\nimport time\n\ndfgeoloc = df.loc[:, ['Adresse','CP','Ville']].apply(lambda s: s.str.lower().str.replace(\",\",\" \"))\ndfgeoloc['url'] = (dfgeoloc['Adresse'] + \"+\" + dfgeoloc['Ville'].str.replace(\"-\",'+')).str.replace(\" \",\"+\")\ndfgeoloc['url'] = 'https://api-adresse.data.gouv.fr/search/?q=' + dfgeoloc['url'] + \"&postcode=\" + df['CP'] + \"&limit=1\"\ndfgeoloc = dfgeoloc.dropna()\n\nstart_time = time.time()\n\ndef get_geoloc(i):\n    print(i)\n    return gpd.GeoDataFrame.from_features(requests.get(dfgeoloc['url'].iloc[i]).json()['features'])\n\nlocal = [get_geoloc(i) for i in range(len(dfgeoloc.head(10)))]\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\nComme l‚Äôindique la documentation, si on d√©sire industrialiser notre processus\nde g√©ocodage, on va privil√©gier l‚ÄôAPI CSV.\nPour obtenir une requ√™te CURL coh√©rente avec le format d√©sir√© par l‚ÄôAPI\non va √† nouveau utiliser requests mais cette fois avec des param√®tres\nsuppl√©mentaires:\n\ndata va nous permettre de passer des param√®tres √† CURL (√©quivalents aux -F\nde la requ√™te CURL):\n\ncolumns: Les colonnes utilis√©es pour localiser une donn√©e. En l‚Äôoccurrence,\non utilise l‚Äôadresse et la ville (car les codes postaux n‚Äô√©tant pas uniques,\nun m√™me nom de voirie peut se trouver dans plusieurs villes partageant le m√™me\ncode postal)\npostcode: Le code postal de la ville. Id√©alement nous aurions utilis√©\nle code Insee mais nous ne l‚Äôavons pas dans nos donn√©es.\nresult_columns: on restreint les donn√©es √©chang√©es avec l‚ÄôAPI aux\ncolonnes qui nous int√©ressent. Cela permet d‚Äôacc√©l√©rer les processus (on\n√©change moins de donn√©es) et de r√©duire l‚Äôimpact carbone de notre activit√©\n(moins de transferts = moins d‚Äô√©nergie d√©pens√©e). En l‚Äôoccurrence, on ne ressort\nque les donn√©es g√©olocalis√©es et un score de confiance en la g√©olocalisation.\n\nfiles: permet d‚Äôenvoyer un fichier via CURL\n\nLes donn√©es sont r√©cup√©r√©es avec request.post. Comme il s‚Äôagit d‚Äôune\ncha√Æne de caract√®re, nous pouvons directement la lire avec pandas en\nutilisant io.StringIO pour √©viter d‚Äô√©crire des donn√©es interm√©diaires.\nLe nombre d‚Äô√©chos semblant √™tre limit√©, je propose de proc√©der par morceaux\n(ici je d√©coupe mon jeu de donn√©es en 5 morceaux).\n\nimport requests\nimport io   \nimport numpy as np\nimport time\n\nparams = {\n    'columns': ['Adresse', 'Ville'],\n    'postcode': 'CP',\n    'result_columns': ['result_score', 'latitude', 'longitude'],\n}\n\ndf[['Adresse','CP','Ville']] = df.loc[:, ['Adresse','CP','Ville']].apply(lambda s: s.str.lower().str.replace(\",\",\" \"))\n\ndef geoloc_chunk(x):\n    dfgeoloc = x.loc[:, ['Adresse','CP','Ville']]\n    dfgeoloc.to_csv(\"datageocodage.csv\", index=False)\n    response = requests.post('https://api-adresse.data.gouv.fr/search/csv/', data=params, files={'data': ('datageocodage.csv', open('datageocodage.csv', 'rb'))})\n    geoloc = pd.read_csv(io.StringIO(response.text), dtype = {'CP': 'str'})\n    return geoloc\n    \nstart_time = time.time()\ngeodata = [geoloc_chunk(dd) for dd in np.array_split(df, 10)]\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\n--- 21.894761323928833 seconds ---\n\n\nCette m√©thode est beaucoup plus rapide et permet ainsi, une fois retourn√© √† nos\ndonn√©es initiales, d‚Äôavoir un jeu de donn√©es g√©olocalis√©\n\ngeodata = pd.concat(geodata, ignore_index = True)\ndf_xy = df.merge(geodata, on = ['Adresse','CP','Ville'])\ndf_xy = df_xy.dropna(subset = ['latitude','longitude'])\ndf_xy['text'] = df_xy['Raison_Sociale'] + '&lt;br&gt;' + df_xy['Adresse'] + '&lt;br&gt;' + df_xy['Ville'] + '&lt;br&gt;Nombre de candidats:' + df_xy['B_NB'].astype(str)\n\ndf_xy.filter(['Raison_Sociale','Adresse','CP','Ville','latitude','longitude'], axis = \"columns\").sample(10)\n\n\n\n\n\n\n\n\nRaison_Sociale\nAdresse\nCP\nVille\nlatitude\nlongitude\n\n\n\n\n5476\nAUTO ECOLE SUPERDRIVE\n4 rue emile darthois\n59111\nbouchain\n50.285949\n3.315659\n\n\n11341\nECOLE DE FORMATION AUTODRIVE\n47 rue karl marx saint therese\n97419\nla possession\n-20.954168\n55.338699\n\n\n7913\nPERMIS PAS CHER\n16 rue du gal giraud\n76000\nrouen\n49.440586\n1.088880\n\n\n7602\nL'Albanais\n15 rue du pont neuf\n74150\nrumilly\n45.868800\n5.943710\n\n\n10436\nTURQUOISE LIBERTE\n99 avenue gallieni\n93800\nepinay sur seine\n48.963736\n2.297905\n\n\n10279\nCSR JAURES\n19 bis rue jean jaur√®s\n93240\nstains\n48.963726\n2.380392\n\n\n8306\nLa Bellifontaine I\n157 rue grande\n77300\nfontainebleau\n48.409174\n2.704196\n\n\n5665\nEFSR59\n10 avenue villars\n59300\nvalenciennes\n50.358704\n3.511668\n\n\n9359\nSARL La Bruffi√®re Auto-√©cole\n25 place jeanne d'arc\n85530\nla bruffi√®re\n47.013279\n-1.197267\n\n\n10523\nauto ecole formation d'ivry s/se\n106 avenue maurice thorez\n94200\nivry sur seine\n48.811398\n2.382815\n\n\n\n\n\n\n\nIl ne reste plus qu‚Äô√† utiliser geopandas\net nous serons en mesure de faire une carte des localisations des auto-√©coles :\n\nimport geopandas as gpd\ndfgeo = gpd.GeoDataFrame(df_xy, geometry=gpd.points_from_xy(df_xy.longitude, df_xy.latitude))\n\nNous allons repr√©senter les stations dans l‚ÄôEssonne avec un zoom initialement\nsur les villes de Massy et Palaiseau. Le code est le suivant:\n\nimport folium\n\n# Repr√©senter toutes les auto√©coles de l'Essonne\ndf_91 = df_xy.loc[df_xy[\"Dept\"] == \"091\"]\n\n# Centrer la vue initiale sur Massy-Palaiseau\ndf_pal = df_xy.loc[df_xy['Ville'].isin([\"massy\", \"palaiseau\"])]\ncenter = df_pal[['latitude', 'longitude']].mean().values.tolist()\nsw = df_pal[['latitude', 'longitude']].min().values.tolist()\nne = df_pal[['latitude', 'longitude']].max().values.tolist()\n\nm = folium.Map(location = center, tiles='Stamen Toner')\n\n# I can add marker one by one on the map\nfor i in range(0,len(df_91)):\n    folium.Marker([df_91.iloc[i]['latitude'], df_91.iloc[i]['longitude']],\n                  popup=df_91.iloc[i]['text'],\n                  icon=folium.Icon(icon='car', prefix='fa')).add_to(m)\n\nm.fit_bounds([sw, ne])\n\nCe qui permet d‚Äôobtenir la carte:\n\n# Afficher la carte\nm\n\nVous pouvez aller plus loin avec l‚Äôexercice suivant.\nCelui-ci n√©cessite une fonction pour cr√©er un cercle autour d‚Äôun point (source ici),\nla voici:\n\nfrom functools import partial\nimport pyproj\nfrom shapely.ops import transform\nfrom shapely.geometry import Point\n\nproj_wgs84 = pyproj.Proj('+proj=longlat +datum=WGS84')\n\n\ndef geodesic_point_buffer(lat, lon, km):\n    # Azimuthal equidistant projection\n    aeqd_proj = '+proj=aeqd +lat_0={lat} +lon_0={lon} +x_0=0 +y_0=0'\n    project = partial(\n        pyproj.transform,\n        pyproj.Proj(aeqd_proj.format(lat=lat, lon=lon)),\n        proj_wgs84)\n    buf = Point(0, 0).buffer(km * 1000)  # distance in metres\n    return transform(project, buf).exterior.coords[:]\n\n\n\n Exercice 2 : Quelles sont les auto-√©coles les plus proches de chez moi ?\nOn va supposer que vous cherchez, dans un rayon donn√© autour d‚Äôun centre ville,\nles auto-√©coles disponibles.\n\nPour commencer, utiliser l‚ÄôAPI geo\npour la ville de Palaiseau\nAppliquer la fonction geodesic_point_buffer au centre ville de Palaiseau\nNe conserver que les auto-√©coles dans ce cercle et les ordonner\n\nSi vous avez la r√©ponse √† la question 3, n‚Äôh√©sitez pas √† la soumettre sur Github afin que je compl√®te la correction :wink: !\n\n\n\n\nERROR 1: PROJ: proj_create_from_database: Open of /miniconda/envs/python-ENSAE/share/proj failed\n\n\n\n\n/miniconda/envs/python-ENSAE/lib/python3.9/site-packages/shapely/ops.py:276: FutureWarning:\n\nThis function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n\n\n\nPour se convaincre, de notre cercle constitu√© lors de\nla question 2, on peut repr√©senter une carte.\nOn a bien un cercle centr√© autour de Palaiseau:\n\nimport matplotlib.pyplot as plt\nimport contextily as ctx\n\nfig,ax = plt.subplots(figsize=(10, 10))\ncircle.to_crs(\"EPSG:3857\").plot(ax = ax, color = 'red')\npal.to_crs(\"EPSG:3857\").plot(ax = ax, color = 'green')\nctx.add_basemap(ax, source = ctx.providers.Stamen.Toner)\nax\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "content/course/manipulation/04c_API_TP/index.html#exercices-suppl√©mentaires",
    "href": "content/course/manipulation/04c_API_TP/index.html#exercices-suppl√©mentaires",
    "title": "47¬† R√©cup√©rer des donn√©es avec des API depuis Python",
    "section": "47.5 Exercices suppl√©mentaires",
    "text": "47.5 Exercices suppl√©mentaires\nPour vous aidez, vous pouvez regarder une exemple de structure du json ici : https://world.openfoodfacts.org/api/v0/product/3274080005003.json en particulier la cat√©gorie nutriments.\n\n\n Exercice 3 : Retrouver des produits dans l'openfood facts :pizza:\nVoici une liste de code-barres:\n3274080005003,  5449000000996, 8002270014901, 3228857000906, 3017620421006, 8712100325953\nUtiliser l‚ÄôAPI d‚Äôopenfoodfacts\n(l‚ÄôAPI, pas depuis le CSV !)\npour retrouver les produits correspondant\net leurs caract√©ristiques nutritionnelles.\nLe panier para√Æt-il √©quilibr√© ? :chocolate_bar:\nR√©cup√©rer l‚ÄôURL d‚Äôune des images et l‚Äôafficher dans votre navigateur.\n\n\nVoici par exemple la photo du produit ayant le code-barre 5449000000996. Vous le reconnaissez ?\n\n\n47.5.1 Exemple avec l‚ÄôAPI de la Banque Mondiale\nAvec l‚ÄôAPI de la Banque mondiale, voici comme s‚Äô√©crit une requ√™te :\n\nhttp://api.worldbank.org/v2/countries?incomeLevel=LMC\n\n\nLe point d‚Äôentr√©e est l‚ÄôURL http://api.worldbank.org/v24\nUn filtre est appliqu√© sur les pays (countries?) afin de ne conserver\nque celles telles que incomeLevel=LMC (‚ÄúLower middle income‚Äù)\n\nEn cliquant sur le lien, le site renvoie des donn√©es en XML,\nqui ressemblent pas mal √† ce qu‚Äôon a vu plus t√¥t avec le scraping : une structure avec des balises qui s‚Äôouvrent et qui se ferment.\nPour obtenir la m√™me information en Python, il faut revenir aux fondamentaux : on va avoir besoin du module requests. Suivant les API, nous avons soit besoin de rien de plus si nous parvenons directement √† obtenir un json, soit devoir utiliser un parser comme BeautifulSoup dans le cas contraire.\nAvec l‚ÄôAPI de la banque mondiale, on va utiliser le module requests et sa m√©thode get : on lui donne l‚Äôurl de l‚ÄôAPI qui nous int√©resse, on lui demande d‚Äôen faire un json et le tour est en apparence jou√©.\n\nimport requests\nreq = requests.get('http://api.worldbank.org/v2/countries?incomeLevel=LMC')\n\nQuand on regarde de plus pr√®s, on voit que les informations suivantes apparaissent :\n\nCode du pays\nNom du pays\nR√©gion\nClassification en termes de revenus\nLes types de pr√™t pour ces pays\nLa capitale\nLongitude\nLatitude\n\nLe format XML est fortement balis√©, ce qui n‚Äôest pas tr√®s pratique.\nEn utilisant d√©sormais un autre URL, on obtient un JSON, plus pratique pour travailler :\n\nhttp://api.worldbank.org/v2/countries?incomeLevel=LMC&format=json\n\n\nimport requests\nimport pandas as pd\n\nreq = requests.get('http://api.worldbank.org/v2/countries?incomeLevel=LMC&format=json')\nwb = req.json()\nwb = pd.json_normalize(wb[1])\nwb.head(5)\n\n\n\n\n\n\n\n\nid\niso2Code\nname\ncapitalCity\nlongitude\nlatitude\nregion.id\nregion.iso2code\nregion.value\nadminregion.id\nadminregion.iso2code\nadminregion.value\nincomeLevel.id\nincomeLevel.iso2code\nincomeLevel.value\nlendingType.id\nlendingType.iso2code\nlendingType.value\n\n\n\n\n0\nAGO\nAO\nAngola\nLuanda\n13.242\n-8.81155\nSSF\nZG\nSub-Saharan Africa\nSSA\nZF\nSub-Saharan Africa (excluding high income)\nLMC\nXN\nLower middle income\nIBD\nXF\nIBRD\n\n\n1\nBEN\nBJ\nBenin\nPorto-Novo\n2.6323\n6.4779\nSSF\nZG\nSub-Saharan Africa\nSSA\nZF\nSub-Saharan Africa (excluding high income)\nLMC\nXN\nLower middle income\nIDX\nXI\nIDA\n\n\n2\nBGD\nBD\nBangladesh\nDhaka\n90.4113\n23.7055\nSAS\n8S\nSouth Asia\nSAS\n8S\nSouth Asia\nLMC\nXN\nLower middle income\nIDX\nXI\nIDA\n\n\n3\nBOL\nBO\nBolivia\nLa Paz\n-66.1936\n-13.9908\nLCN\nZJ\nLatin America & Caribbean\nLAC\nXJ\nLatin America & Caribbean (excluding high income)\nLMC\nXN\nLower middle income\nIBD\nXF\nIBRD\n\n\n4\nBTN\nBT\nBhutan\nThimphu\n89.6177\n27.5768\nSAS\n8S\nSouth Asia\nSAS\n8S\nSouth Asia\nLMC\nXN\nLower middle income\nIDX\nXI\nIDA\n\n\n\n\n\n\n\nCependant, si on regarde la dimension de l‚Äôobjet obtenu, on obtient un\nchiffre rond (50 lignes). Ceci est suspect et un petit tour dans la\ndocumentation de l‚ÄôAPI nous apprendrait que c‚Äôest le nombre maximal de\nretour possible. Il faut donc faire attention √† la documentation et\najouter un param√®tre page=2 pour rattraper les derniers √©chos:\n\nwb2 = pd.json_normalize(\n    requests.get(\"http://api.worldbank.org/v2/countries?incomeLevel=LMC&format=json&page=2\").json()[1]\n    )\npd.concat([wb, wb2]).head(5)\n\n\n\n\n\n\n\n\nid\niso2Code\nname\ncapitalCity\nlongitude\nlatitude\nregion.id\nregion.iso2code\nregion.value\nadminregion.id\nadminregion.iso2code\nadminregion.value\nincomeLevel.id\nincomeLevel.iso2code\nincomeLevel.value\nlendingType.id\nlendingType.iso2code\nlendingType.value\n\n\n\n\n0\nAGO\nAO\nAngola\nLuanda\n13.242\n-8.81155\nSSF\nZG\nSub-Saharan Africa\nSSA\nZF\nSub-Saharan Africa (excluding high income)\nLMC\nXN\nLower middle income\nIBD\nXF\nIBRD\n\n\n1\nBEN\nBJ\nBenin\nPorto-Novo\n2.6323\n6.4779\nSSF\nZG\nSub-Saharan Africa\nSSA\nZF\nSub-Saharan Africa (excluding high income)\nLMC\nXN\nLower middle income\nIDX\nXI\nIDA\n\n\n2\nBGD\nBD\nBangladesh\nDhaka\n90.4113\n23.7055\nSAS\n8S\nSouth Asia\nSAS\n8S\nSouth Asia\nLMC\nXN\nLower middle income\nIDX\nXI\nIDA\n\n\n3\nBOL\nBO\nBolivia\nLa Paz\n-66.1936\n-13.9908\nLCN\nZJ\nLatin America & Caribbean\nLAC\nXJ\nLatin America & Caribbean (excluding high income)\nLMC\nXN\nLower middle income\nIBD\nXF\nIBRD\n\n\n4\nBTN\nBT\nBhutan\nThimphu\n89.6177\n27.5768\nSAS\n8S\nSouth Asia\nSAS\n8S\nSouth Asia\nLMC\nXN\nLower middle income\nIDX\nXI\nIDA\n\n\n\n\n\n\n\nSi on regarde l‚Äôinformation pr√©sente dans le DataFrame, on voit qu‚Äôelle se\npr√©sente sous forme lendingType.value. C‚Äôest parce que pandas a\nconcat√©n√© les diff√©rents niveaux de notre dictionnaire. Si on d√©sire\ns‚Äôen assurer, on peut regarder sur un exemple:\n\nreq.json()[1][0]['incomeLevel']['value'] == wb.loc[0, 'incomeLevel.value'] \n\nTrue"
  },
  {
    "objectID": "content/course/manipulation/04b_regex_TP/index.html#introduction",
    "href": "content/course/manipulation/04b_regex_TP/index.html#introduction",
    "title": "48¬† Ma√Ætriser les expressions r√©guli√®res",
    "section": "48.1 Introduction",
    "text": "48.1 Introduction\nPython offre √©norm√©ment de fonctionalit√©s tr√®s pratiques pour la manipulation de donn√©es\ntextuelles. C‚Äôest l‚Äôune des raisons de son\nsucc√®s dans la communaut√© du traitement automatis√© du langage (NLP, voir partie d√©di√©e).\nDans les chapitres pr√©c√©dents, nous avons parfois √©t√© amen√©s √† chercher des √©l√©ments textuels basiques. Cela √©tait possible avec la m√©thode str.find du package Pandas qui constitue une version vectoris√©e de la m√©thode find\nde base. Nous avons d‚Äôailleurs\npu utiliser cette derni√®re directement, notamment lorsqu‚Äôon a fait du webscraping.\nCependant, cette fonction de recherche\ntrouve rapidement ses limites.\nPar exemple, si on d√©sire trouver √† la fois les occurrences d‚Äôun terme au singulier\net au pluriel, il sera n√©cessaire d‚Äôutiliser\nau moins deux fois la m√©thode find.\nPour des verbes conjugu√©s, cela devient encore plus complexe, en particulier si ceux-ci changent de forme selon le sujet.\nPour des expressions compliqu√©es, il est conseill√© d‚Äôutiliser les expressions r√©guli√®res,\nou ‚Äúregex‚Äù. C‚Äôest une fonctionnalit√© qu‚Äôon retrouve dans beaucoup de langages. C‚Äôest une forme de grammaire qui permet de rechercher des expressions.\nUne partie du contenu de cette partie\nest une adaptation de la\ndocumentation collaborative sur R nomm√©e utilitR √† laquelle j‚Äôai particip√©. Ce chapitre reprend aussi du contenu du\nlivre R for Data Science qui pr√©sente un chapitre\ntr√®s p√©dagogique sur les regex.\nNous allons utiliser le package re pour illustrer nos exemples d‚Äôexpressions\nr√©guli√®res. Il s‚Äôagit du package de r√©f√©rence, qui est utilis√©, en arri√®re-plan,\npar Pandas pour vectoriser les recherches textuelles.\n\nimport re\nimport pandas as pd\n\n\n\n Hint\nLes expressions r√©guli√®res (regex) sont notoirement difficiles √† ma√Ætriser. Il existe des outils qui facilitent le travail avec les expressions r√©guli√®res.\n\nL‚Äôoutil de r√©f√©rence pour ceci est [https://regex101.com/] qui permet de tester des regex en Python\ntout en ayant une explication qui accompagne ce test\nDe m√™me pour ce site qui comporte une cheat sheet en bas de la page.\nLes jeux de Regex Crossword permettent d‚Äôapprendre les expressions r√©guli√®res en s‚Äôamusant\n\nIl peut √™tre pratique de demander √† des IA assistantes, comme Github Copilot ou ChatGPT, une\npremi√®re version d‚Äôune regex en expliquant le contenu qu‚Äôon veut extraire.\nCela peut faire √©conomiser pas mal de temps, sauf quand l‚ÄôIA fait preuve d‚Äôune confiance excessive\net vous propose avec aplomb une regex totalement fausse‚Ä¶"
  },
  {
    "objectID": "content/course/manipulation/04b_regex_TP/index.html#principe",
    "href": "content/course/manipulation/04b_regex_TP/index.html#principe",
    "title": "48¬† Ma√Ætriser les expressions r√©guli√®res",
    "section": "48.2 Principe",
    "text": "48.2 Principe\nLes expressions r√©guli√®res sont un outil permettant de d√©crire un ensemble de cha√Ænes de caract√®res possibles selon une syntaxe pr√©cise, et donc de d√©finir un motif (ou pattern). Les expressions r√©guli√®res servent par exemple lorsqu‚Äôon veut extraire une partie d‚Äôune cha√Æne de caract√®res, ou remplacer une partie d‚Äôune cha√Æne de caract√®res. Une expression r√©guli√®re prend la forme d‚Äôune cha√Æne de caract√®res, qui peut contenir √† la fois des √©l√©ments litt√©raux et des caract√®res sp√©ciaux qui ont un sens logique.\nPar exemple, \"ch.+n\" est une expression r√©guli√®re qui d√©crit le motif suivant: la cha√Æne litt√©rale ch, suivi de n‚Äôimporte quelle cha√Æne d‚Äôau moins un caract√®re (.+), suivie de la lettre n. Dans la cha√Æne \"J'ai un chien.\", la sous-cha√Æne \"chien\" correspond √† ce motif. De m√™me pour \"chapeau ron\" dans \"J'ai un chapeau rond\". En revanche, dans la cha√Æne \"La soupe est chaude.\", aucune sous-cha√Æne ne correpsond √† ce motif (car aucun n n‚Äôappara√Æt apr√®s le ch).\nPour s‚Äôen convaincre, nous pouvons d√©j√† regarder\nles deux premiers cas:\n\npattern = \"ch.+n\"\nprint(re.search(pattern, \"J'ai un chien.\"))\nprint(re.search(pattern, \"J'ai un chapeau rond.\"))\n\n&lt;re.Match object; span=(8, 13), match='chien'&gt;\n&lt;re.Match object; span=(8, 19), match='chapeau ron'&gt;\n\n\nCependant, dans le dernier cas, nous ne trouvons pas\nle pattern recherch√©:\n\nprint(re.search(pattern, \"La soupe est chaude.\"))\n\nNone\n\n\nLa regex pr√©c√©dente comportait deux types de caract√®res:\n\nles caract√®res litt√©raux: lettres et nombres qui sont reconnus de mani√®re litt√©rale\nles m√©ta-caract√®res: symboles qui ont un sens particulier dans les regex.\n\nLes principaux m√©ta-caract√®res sont ., +, *, [, ], ^ et $ mais il\nen existe beaucoup d‚Äôautres.\nParmi cet ensemble, on utilise principalement les quantifieurs (., +, *‚Ä¶),\nles classes de caract√®res (ensemble qui sont d√©limit√©s par [ et ])\nou les ancres (^, $‚Ä¶)\nDans l‚Äôexemple pr√©c√©dent,\nnous retrouvions deux quantifieurs accol√©s .+. Le premier (.) signifie n‚Äôimporte quel caract√®re1. Le deuxi√®me (+) signifie ‚Äúr√©p√®te le pattern pr√©c√©dent‚Äù.\nDans notre cas, la combinaison .+ permet ainsi de r√©p√©ter n‚Äôimporte quel caract√®re avant de trouver un n.\nLe nombre de fois est indetermin√©: cela peut ne pas √™tre pas n√©cessaire d‚Äôintercaler des caract√®res avant le n\nou cela peut √™tre n√©cessaire d‚Äôen intercepter plusieurs:\n\nprint(re.search(pattern, \"J'ai un chino\"))\nprint(re.search(pattern, \"J'ai un chiot tr√®s mignon.\"))\n\n&lt;re.Match object; span=(8, 12), match='chin'&gt;\n&lt;re.Match object; span=(8, 25), match='chiot tr√®s mignon'&gt;\n\n\n\n48.2.1 Classes de caract√®res\nLors d‚Äôune recherche, on s‚Äôint√©resse aux caract√®res et souvent aux classes de caract√®res : on cherche un chiffre, une lettre, un caract√®re dans un ensemble pr√©cis ou un caract√®re qui n‚Äôappartient pas √† un ensemble pr√©cis. Certains ensembles sont pr√©d√©finis, d‚Äôautres doivent √™tre d√©finis √† l‚Äôaide de crochets.\nPour d√©finir un ensemble de caract√®res, il faut √©crire cet ensemble entre crochets. Par exemple, [0123456789] d√©signe un chiffre. Comme c‚Äôest une s√©quence de caract√®res cons√©cutifs, on peut r√©sumer cette √©criture en [0-9].\nPar\nexemple, si on d√©sire trouver tous les pattern qui commencent par un c suivi\nd‚Äôun h puis d‚Äôune voyelle (a, e, i, o, u), on peut essayer\ncette expression r√©guli√®re.\n\nre.findall(\"[c][h][aeiou]\", \"chat, chien, veau, vache, ch√®vre\")\n\n['cha', 'chi', 'che']\n\n\nIl serait plus pratique d‚Äôutiliser Pandas dans ce cas pour isoler les\nlignes qui r√©pondent √† la condition logique (en ajoutant les accents\nqui ne sont pas compris sinon):\n\nimport pandas as pd\ntxt = pd.Series(\"chat, chien, veau, vache, ch√®vre\".split(\", \"))\ntxt.str.match(\"ch[ae√©√®iou]\")\n\n0     True\n1     True\n2    False\n3    False\n4     True\ndtype: bool\n\n\nCependant, l‚Äôusage ci-dessus des classes de caract√®res\nn‚Äôest pas le plus fr√©quent.\nOn privil√©gie celles-ci pour identifier des\npattern complexe plut√¥t qu‚Äôune suite de caract√®res litt√©raux.\nLes tableaux d‚Äôaide m√©moire illustrent une partie des\nclasses de caract√®res les plus fr√©quentes\n([:digit:] ou \\d‚Ä¶)\n\n\n48.2.2 Quantifieurs\nNous avons rencontr√© les quantifieurs avec notre premi√®re expression\nr√©guli√®re. Ceux-ci contr√¥lent le nombre de fois\nqu‚Äôun pattern est rencontr√©.\nLes plus fr√©quents sont:\n\n? : 0 ou 1 match ;\n+ : 1 ou plus de matches ;\n* : 0 or more matches.\n\nPar exemple, colou?r permettra de matcher √† la fois l‚Äô√©criture am√©ricaine et anglaise\n\nre.findall(\"colou?r\", \"Did you write color or colour?\")\n\n['color', 'colour']\n\n\nCes quantifiers peuvent bien-s√ªr √™tre associ√©s √†\nd‚Äôautres types de caract√®res, notamment les classes de caract√®res.\nCela peut √™tre extr√®mement pratique.\nPar exemple, \\d+ permettra de capturer un ou plusieurs chiffres, \\s?\npermettra d‚Äôajouter en option un espace,\n[\\w]{6,8} un mot entre six et huit lettres qu‚Äôon √©crira‚Ä¶\nIl est aussi possible de d√©finir le nombre de r√©p√©titions\navec {}:\n\n{n} matche exactement n fois ;\n{n,} matche au moins n fois ;\n{n,m} matche entre n et m fois.\n\nCependant, la r√©p√©tition des termes\nne s‚Äôapplique par d√©faut qu‚Äôau dernier\ncaract√®re pr√©c√©dent le quantifier.\nOn peut s‚Äôen convaincre avec l‚Äôexemple ci-dessus:\n\nprint(re.match(\"toc{4}\",\"toctoctoctoc\"))\n\nNone\n\n\nPour pallier ce probl√®me, il existe les parenth√®ses.\nLe principe est le m√™me qu‚Äôavec les r√®gles num√©riques:\nles parenth√®ses permettent d‚Äôintroduire une hi√©rarchie.\nPour reprendre l‚Äôexemple pr√©c√©dent, on obtient\nbien le r√©sultat attendu gr√¢ce aux parenth√®ses:\n\nprint(re.match(\"(toc){4}\",\"toctoctoctoc\"))\nprint(re.match(\"(toc){5}\",\"toctoctoctoc\"))\nprint(re.match(\"(toc){2,4}\",\"toctoctoctoc\"))\n\n&lt;re.Match object; span=(0, 12), match='toctoctoctoc'&gt;\nNone\n&lt;re.Match object; span=(0, 12), match='toctoctoctoc'&gt;\n\n\n\n\n Note\nL‚Äôalgorithme des expressions r√©guli√®res essaye toujours de faire correspondre le plus grand morceau √† l‚Äôexpression r√©guli√®re.\nPar exemple, soit une chaine de caract√®re HTML:\n\ns = \"&lt;h1&gt;Super titre HTML&lt;/h1&gt;\"\n\nL‚Äôexpression r√©guli√®re re.findall(\"&lt;.*&gt;\", s) correspond, potentiellement,\n√† trois morceaux :\n\n&lt;h1&gt;\n&lt;/h1&gt;\n&lt;h1&gt;Super titre HTML&lt;/h1&gt;\n\nC‚Äôest ce dernier qui sera choisi, car le plus grand. Pour\ns√©lectionner le plus petit,\nil faudra √©crire les multiplicateurs comme ceci : *?, +?.\nEn voici quelques exemples:\n\ns = \"&lt;h1&gt;Super titre HTML&lt;/h1&gt;\\n&lt;p&gt;&lt;code&gt;Python&lt;/code&gt; est un langage tr√®s flexible&lt;/p&gt;\"\nprint(re.findall(\"&lt;.*&gt;\", s))\nprint(re.findall(\"&lt;p&gt;.*&lt;/p&gt;\", s))\nprint(re.findall(\"&lt;p&gt;.*?&lt;/p&gt;\", s))\nprint(re.compile(\"&lt;.*?&gt;\").findall(s))\n\n['&lt;h1&gt;Super titre HTML&lt;/h1&gt;', '&lt;p&gt;&lt;code&gt;Python&lt;/code&gt; est un langage tr√®s flexible&lt;/p&gt;']\n['&lt;p&gt;&lt;code&gt;Python&lt;/code&gt; est un langage tr√®s flexible&lt;/p&gt;']\n['&lt;p&gt;&lt;code&gt;Python&lt;/code&gt; est un langage tr√®s flexible&lt;/p&gt;']\n['&lt;h1&gt;', '&lt;/h1&gt;', '&lt;p&gt;', '&lt;code&gt;', '&lt;/code&gt;', '&lt;/p&gt;']\n\n\n\n\n\n\n48.2.3 Aide-m√©moire\nLe tableau ci-dessous peut servir d‚Äôaide-m√©moire\nsur les regex:\n\n\n\n\n\n\n\nExpression r√©guli√®re\nSignification\n\n\n\n\n\"^\"\nD√©but de la cha√Æne de caract√®res\n\n\n\"$\"\nFin de la cha√Æne de caract√®res\n\n\n\"\\\\.\"\nUn point\n\n\n\".\"\nN‚Äôimporte quel caract√®re\n\n\n\".+\"\nN‚Äôimporte quelle suite de caract√®res non vide\n\n\n\".*\"\nN‚Äôimporte quelle suite de caract√®res, √©ventuellement vi\n\n\n\"[:alnum:]\"\nUn caract√®re alphanum√©rique\n\n\n\"[:alpha:]\"\nUne lettre\n\n\n\"[:digit:]\"\nUn chiffre\n\n\n\"[:lower:]\"\nUne lettre minuscule\n\n\n\"[:punct:]\"\nUn signe de ponctuation\n\n\n\"[:space:]\"\nun espace\n\n\n\"[:upper:]\"\nUne lettre majuscule\n\n\n\"[[:alnum:]]+\"\nUne suite d‚Äôau moins un caract√®re alphanum√©rique\n\n\n\"[[:alpha:]]+\"\nUne suite d‚Äôau moins une lettre\n\n\n\"[[:digit:]]+\"\nUne suite d‚Äôau moins un chiffre\n\n\n\"[[:lower:]]+\"\nUne suite d‚Äôau moins une lettre minuscule\n\n\n\"[[:punct:]]+\"\nUne suite d‚Äôau moins un signe de ponctuation\n\n\n\"[[:space:]]+\"\nUne suite d‚Äôau moins un espace\n\n\n\"[[:upper:]]+\"\nUne suite d‚Äôau moins une lettre majuscule\n\n\n\"[[:alnum:]]*\"\nUne suite de caract√®res alphanum√©riques, √©ventuellement vide\n\n\n\"[[:alpha:]]*\"\nUne suite de lettres, √©ventuellement vide\n\n\n\"[[:digit:]]*\"\nUne suite de chiffres, √©ventuellement vide\n\n\n\"[[:lower:]]*\"\nUne suite de lettres minuscules, √©ventuellement vide\n\n\n\"[[:upper:]]*\"\nUne suite de lettres majuscules, √©ventuellement vide\n\n\n\"[[:punct:]]*\"\nUne suite de signes de ponctuation, √©ventuellement vide\n\n\n\"[^[:alpha:]]+\"\nUne suite d‚Äôau moins un caract√®re autre qu‚Äôune lettre\n\n\n\"[^[:digit:]]+\"\nUne suite d‚Äôau moins un caract√®re autre qu‚Äôun chiffre\n\n\n\"\\|\"\nL‚Äôune des expressions x ou y est pr√©sente\n\n\n[abyz]\nUn seul des caract√®res sp√©cifi√©s\n\n\n[abyz]+\nUn ou plusieurs des caract√®res sp√©cifi√©s (√©ventuellement r√©p√©t√©s)\n\n\n[^abyz]\nAucun des caract√®res sp√©cifi√©s n‚Äôest pr√©sent\n\n\n\nCertaines classes de caract√®res b√©n√©ficient d‚Äôune syntaxe plus l√©g√®re car\nelles sont tr√®s fr√©quentes. Parmi-celles:\n\n\n\n\n\n\n\nExpression r√©guli√®re\nSignification\n\n\n\n\n\\d\nN‚Äôimporte quel chiffre\n\n\n\\D\nN‚Äôimporte quel caract√®re qui n‚Äôest pas un caract√®re\n\n\n\\s\nN‚Äôimporte quel espace (espace, tabulation, retour √† la ligne)\n\n\n\\S\nN‚Äôimporte quel caract√®re qui n‚Äôest pas un espace\n\n\n\\w\nN‚Äôimporte quel type de mot (lettres et nombres)\n\n\n\\W\nN‚Äôimporte quel ensemble qui n‚Äôest pas un mot (lettres et nombres)\n\n\n\nDans l‚Äôexercice suivant, vous allez pouvoir mettre en pratique\nles exemples pr√©c√©dents sur une regex un peu plus compl√®te.\nCet exercice ne n√©cessite pas la connaissance des subtilit√©s\ndu package re, vous n‚Äôaurez besoin que de re.findall.\nCet exercice utilisera la chaine de caract√®re suivante:\n\ns = \"\"\"date 0 : 14/9/2000\ndate 1 : 20/04/1971     date 2 : 14/09/1913     date 3 : 2/3/1978\ndate 4 : 1/7/1986     date 5 : 7/3/47     date 6 : 15/10/1914\ndate 7 : 08/03/1941     date 8 : 8/1/1980     date 9 : 30/6/1976\"\"\"\ns\n\n'date 0 : 14/9/2000\\ndate 1 : 20/04/1971     date 2 : 14/09/1913     date 3 : 2/3/1978\\ndate 4 : 1/7/1986     date 5 : 7/3/47     date 6 : 15/10/1914\\ndate 7 : 08/03/1941     date 8 : 8/1/1980     date 9 : 30/6/1976'\n\n\n\n\n Exercice 1\n\nOn va d‚Äôabord s‚Äôoccuper d‚Äôextraire le jour de naissance.\n\nLe premier chiffre du jour est 0, 1, 2 ou 3. Traduire cela sous la forme d‚Äôune s√©quence [X-X]\nLe deuxi√®me chiffre du jour est lui entre 0 et 9. Traduire cela sous la s√©quence ad√©quate\nRemarquez que le premier jour est facultatif. Intercaler entre les deux classes de caract√®re ad√©quate\nle quantifieur qui convient\nAjouter le slash √† la suite du motif\nTester avec re.findall. Vous devriez obtenir beaucoup plus d‚Äô√©chos que n√©cessaire.\nC‚Äôest normal, √† ce stade la\nregex n‚Äôest pas encore finalis√©e\n\nSuivre la m√™me logique pour les mois en notant que les mois du calendrier gr√©gorien ne d√©passent\njamais la premi√®re dizaine. Tester avec re.findall\nDe m√™me pour les ann√©es de naissance en notant que jusqu‚Äô√† preuve du contraire, pour des personnes vivantes\naujourd‚Äôhui, les mill√©naires concern√©s sont restreints. Tester avec re.findall\nCette regex n‚Äôest pas naturelle, on pourrait tr√®s bien se satisfaire de classes de\ncaract√®res g√©n√©riques \\d m√™me si elles pourraient, en pratique, nous s√©lectionner des\ndates de naissance non possibles (43/78/4528 par exemple). Cela permettrait\nd‚Äôall√©ger la regex afin de la rendre plus intelligible. Ne pas oublier l‚Äôutilit√© des quantifieurs.\nComment adapter la regex pour qu‚Äôelle soit toujours valide pour nos cas mais permette aussi de\ncapturer les dates de type YYYY/MM/DD ? Tester sur 1998/07/12\n\n\n\nA l‚Äôissue de la question 1, vous devriez avoir ce r√©sultat :\n\n\n['14/',\n '9/',\n '20/',\n '04/',\n '14/',\n '09/',\n '2/',\n '3/',\n '1/',\n '7/',\n '7/',\n '3/',\n '15/',\n '10/',\n '08/',\n '03/',\n '8/',\n '1/',\n '30/',\n '6/']\n\n\nA l‚Äôissue de la question 2, vous devriez avoir ce r√©sultat, qui\ncommence √† prendre forme:\n\n\n['14/9',\n '20/04',\n '14/09',\n '2/3',\n '1/7',\n '7/3',\n '15/10',\n '08/03',\n '8/1',\n '30/6']\n\n\nA l‚Äôissue de la question 3, on parvient bien\n√† extraire les dates :\n\n\n['14/9/2000',\n '20/04/1971',\n '14/09/1913',\n '2/3/1978',\n '1/7/1986',\n '7/3/47',\n '15/10/1914',\n '08/03/1941',\n '8/1/1980',\n '30/6/1976']\n\n\nSi tout va bien, √† la question 5, votre regex devrait\nfonctionner:\n\n\n['14/9/2000',\n '20/04/1971',\n '14/09/1913',\n '2/3/1978',\n '1/7/1986',\n '7/3/47',\n '15/10/1914',\n '08/03/1941',\n '8/1/1980',\n '30/6/1976',\n '1998/07/12']"
  },
  {
    "objectID": "content/course/manipulation/04b_regex_TP/index.html#principales-fonctions-de-re",
    "href": "content/course/manipulation/04b_regex_TP/index.html#principales-fonctions-de-re",
    "title": "48¬† Ma√Ætriser les expressions r√©guli√®res",
    "section": "48.3 Principales fonctions de re",
    "text": "48.3 Principales fonctions de re\nVoici un tableau r√©capitulatif des principales\nfonctions du package re suivi d‚Äôexemples.\nNous avons principalement\nutilis√© jusqu‚Äô√† pr√©sent re.findall qui est\nl‚Äôune des fonctions les plus pratiques du package.\nre.sub et re.search sont √©galement bien pratiques.\nLes autres sont moins vitales mais peuvent dans des\ncas pr√©cis √™tre utiles.\n\n\n\n\n\n\n\nFonction\nObjectif\n\n\n\n\nre.match(&lt;regex&gt;, s)\nTrouver et renvoyer le premier match de l‚Äôexpression r√©guli√®re &lt;regex&gt; √† partir du d√©but du string s\n\n\nre.search(&lt;regex&gt;, s)\nTrouver et renvoyer le premier match de l‚Äôexpression r√©guli√®re &lt;regex&gt; quelle que soit sa position dans le string s\n\n\nre.finditer(&lt;regex&gt;, s)\nTrouver et renvoyer un it√©rateur stockant tous les matches de l‚Äôexpression r√©guli√®re &lt;regex&gt; quelle que soit leur(s) position(s) dans le string s. En g√©n√©ral, on effectue ensuite une boucle sur cet it√©rateur\n\n\nre.findall(&lt;regex&gt;, s)\nTrouver et renvoyer tous les matches de l‚Äôexpression r√©guli√®re &lt;regex&gt; quelle que soit leur(s) position(s) dans le string s sous forme de liste\n\n\nre.sub(&lt;regex&gt;, new_text, s)\nTrouver et remplacer tous les matches de l‚Äôexpression r√©guli√®re &lt;regex&gt; quelle que soit leur(s) position(s) dans le string s\n\n\n\nPour illustrer ces fonctions, voici quelques exemples:\n\nExemple de re.match üëá\nre.match ne peut servir qu‚Äô√† capturer un pattern en d√©but\nde string. Son utilit√© est donc limit√©e.\nCapturons n√©anmoins toto :\n\nre.match(\"(to){2}\", \"toto √† la plage\")\n\n&lt;re.Match object; span=(0, 4), match='toto'&gt;\n\n\n\n\n\nExemple de re.search üëá\nre.search est plus puissant que re.match, on peut\ncapturer des termes quelle que soit leur position\ndans un string. Par exemple, pour capturer age:\n\nre.search(\"age\", \"toto a l'age d'aller √† la plage\")\n\n&lt;re.Match object; span=(9, 12), match='age'&gt;\n\n\nEt pour capturer exclusivement ‚Äúage‚Äù en fin\nde string:\n\nre.search(\"age$\", \"toto a l'age d'aller √† la plage\")\n\n&lt;re.Match object; span=(28, 31), match='age'&gt;\n\n\n\n\n\nExemple de re.finditer üëá\nre.finditer est, √† mon avis,\nmoins pratique que re.findall. Son utilit√©\nprincipale par rapport √† re.findall\nest de capturer la position dans un champ textuel:\n\ns = \"toto a l'age d'aller √† la plage\"\nfor match in re.finditer(\"age\", s):\n    start = match.start()\n    end = match.end()\n    print(f'String match \"{s[start:end]}\" at {start}:{end}')\n\nString match \"age\" at 9:12\nString match \"age\" at 28:31\n\n\n\n\n\nExemple de re.sub üëá\nre.sub permet de capturer et remplacer des expressions.\nPar exemple, rempla√ßons ‚Äúage‚Äù par ‚Äú√¢ge‚Äù. Mais attention,\nil ne faut pas le faire lorsque le motif est pr√©sent dans ‚Äúplage‚Äù.\nOn va donc mettre une condition n√©gative: capturer ‚Äúage‚Äù seulement\ns‚Äôil n‚Äôest pas en fin de string (ce qui se traduit en regex par ?!$)\n\nre.sub(\"age(?!$)\", \"√¢ge\", \"toto a l'age d'aller √† la plage\")\n\n\"toto a l'√¢ge d'aller √† la plage\"\n\n\n\n\n\n\n Quand utiliser re.compile et les raw strings ?\nre.compile peut √™tre int√©ressant lorsque\nvous utilisez une expression r√©guli√®re plusieurs fois dans votre code.\nCela permet de compiler l‚Äôexpression r√©guli√®re en un objet reconnu par re,\nce qui peut √™tre plus efficace en termes de performance lorsque l‚Äôexpression r√©guli√®re\nest utilis√©e √† plusieurs reprises ou sur des donn√©es volumineuses.\nLes cha√Ænes brutes (raw string) sont des cha√Ænes de caract√®res sp√©ciales en Python,\nqui commencent par r. Par exemple r\"toto √† la plage\".\nElles peuvent √™tre int√©ressantes\npour √©viter que les caract√®res d‚Äô√©chappement ne soient interpr√©t√©s par Python\nPar exemple, si vous voulez chercher une cha√Æne qui contient une barre oblique inverse \\ dans une cha√Æne, vous devez utiliser une cha√Æne brute pour √©viter que la barre oblique inverse ne soit interpr√©t√©e comme un caract√®re d‚Äô√©chappement (\\t, \\n, etc.).\nLe testeur https://regex101.com/ suppose d‚Äôailleurs que\nvous utilisez des raw string, cela peut donc √™tre utile de s‚Äôhabituer √† les utiliser."
  },
  {
    "objectID": "content/course/manipulation/04b_regex_TP/index.html#g√©n√©ralisation-avec-pandas",
    "href": "content/course/manipulation/04b_regex_TP/index.html#g√©n√©ralisation-avec-pandas",
    "title": "48¬† Ma√Ætriser les expressions r√©guli√®res",
    "section": "48.4 G√©n√©ralisation avec Pandas",
    "text": "48.4 G√©n√©ralisation avec Pandas\nLes m√©thodes de Pandas sont des extensions de celles de re\nqui √©vitent de faire une boucle pour regarder,\nligne √† ligne, une regex. En pratique, lorsqu‚Äôon traite des\nDataFrames, on utilise plut√¥t l‚ÄôAPI Pandas que re. Les\ncodes de la forme df.apply(lambda x: re.&lt;fonction&gt;(&lt;regex&gt;,x), axis = 1)\nsont √† bannir car tr√®s peu efficaces.\nLes noms changent parfois l√©g√®rement par rapport √† leur\n√©quivalent re.\n\n\n\n\n\n\n\nM√©thode\nDescription\n\n\n\n\nstr.count()\nCompter le nombre d‚Äôoccurrences du pattern dans chaque ligne\n\n\nstr.replace()\nRemplacer le pattern par une autre valeur. Version vectoris√©e de re.sub()\n\n\nstr.contains()\nTester si le pattern appara√Æt, ligne √† ligne. Version vectoris√©e de re.search()\n\n\nstr.extract()\nExtraire les groupes qui r√©pondent √† un pattern et les renvoyer dans une colonne\n\n\nstr.findall()\nTrouver et renvoyer toutes les occurrences d‚Äôun pattern. Si une ligne comporte plusieurs √©chos, une liste est renvoy√©e. Version vectoris√©e de re.findall()\n\n\n\nA ces fonctions, s‚Äôajoutent les m√©thodes str.split() et str.rsplit() qui sont bien pratiques.\n\nExemple de str.count üëá\nOn peut compter le nombre de fois qu‚Äôun pattern appara√Æt avec\nstr.count\n\ndf = pd.DataFrame({\"a\": [\"toto\", \"titi\"]})\ndf['a'].str.count(\"to\")\n\n0    2\n1    0\nName: a, dtype: int64\n\n\n\n\n\nExemple de str.replace üëá\nRempla√ßons le motif ‚Äúti‚Äù en fin de phrase\n\ndf = pd.DataFrame({\"a\": [\"toto\", \"titi\"]})\ndf['a'].str.replace(\"ti$\", \" punch\")\n\n0    toto\n1    titi\nName: a, dtype: object\n\n\n\n\n\nExemple de str.contains üëá\nV√©rifions les cas o√π notre ligne termine par ‚Äúti‚Äù:\n\ndf = pd.DataFrame({\"a\": [\"toto\", \"titi\"]})\ndf['a'].str.contains(\"ti$\")\n\n0    False\n1     True\nName: a, dtype: bool\n\n\n\n\n\nExemple de str.findall üëá\n\ndf = pd.DataFrame({\"a\": [\"toto\", \"titi\"]})\ndf['a'].str.findall(\"to\")\n\n0    [to, to]\n1          []\nName: a, dtype: object\n\n\n\n\n\n\n Warning\nA l‚Äôheure actuelle, il n‚Äôest pas n√©cessaire d‚Äôajouter l‚Äôargument regex = True mais cela\ndevrait √™tre le cas dans une future version de Pandas.\nCela peut valoir le coup de s‚Äôhabituer √† l‚Äôajouter."
  },
  {
    "objectID": "content/course/manipulation/04b_regex_TP/index.html#pour-en-savoir-plus",
    "href": "content/course/manipulation/04b_regex_TP/index.html#pour-en-savoir-plus",
    "title": "48¬† Ma√Ætriser les expressions r√©guli√®res",
    "section": "48.5 Pour en savoir plus",
    "text": "48.5 Pour en savoir plus\n\ndocumentation collaborative sur R nomm√©e utilitR\nR for Data Science\nRegular Expression HOWTO dans la documentation officielle de Python\nL‚Äôoutil de r√©f√©rence [https://regex101.com/] pour tester des expressions r√©guli√®res\nCe site qui comporte une cheat sheet en bas de la page.\nLes jeux de Regex Crossword permettent d‚Äôapprendre les expressions r√©guli√®res en s‚Äôamusant"
  },
  {
    "objectID": "content/course/manipulation/04b_regex_TP/index.html#exercices-suppl√©mentaires",
    "href": "content/course/manipulation/04b_regex_TP/index.html#exercices-suppl√©mentaires",
    "title": "48¬† Ma√Ætriser les expressions r√©guli√®res",
    "section": "48.6 Exercices suppl√©mentaires",
    "text": "48.6 Exercices suppl√©mentaires\n\n48.6.1 Extraction d‚Äôadresses email\nIl s‚Äôagit d‚Äôun usage classique des regex\n\ntext_emails = 'Hello from toto@gmail.com to titi.grominet@yahoo.com about the meeting @2PM'\n\n\n\n Exercice : extraction d'adresses email\nUtiliser la structure d‚Äôune adresse mail [XXXX]@[XXXX] pour r√©cup√©rer\nce contenu\n\n\n\n\n['toto@gmail.com', 'titi.grominet@yahoo.com']\n\n\n\n\n48.6.2 Extraire des ann√©es depuis un DataFrame Pandas\nL‚Äôobjectif g√©n√©ral de l‚Äôexercice est de nettoyer des colonnes d‚Äôun DataFrame en utilisant des expressions r√©guli√®res.\n\n\n Exercice\nLa base en question contient des livres de la British Library et quelques informations les concernant. Le jeu de donn√©es est disponible ici : https://raw.githubusercontent.com/realpython/python-data-cleaning/master/Datasets/BL-Flickr-Images-Book.csv\nLa colonne ‚ÄúDate de Publication‚Äù n‚Äôest pas toujours une ann√©e, il y a parfois d‚Äôautres informations. Le but de l‚Äôexercice est d‚Äôavoir une date de publication du livre propre et de regarder la distribution des ann√©es de publications.\nPour ce faire, vous pouvez :\n\nSoit choisir de r√©aliser l‚Äôexercice sans aide. Votre lecture de l‚Äô√©nonc√© s‚Äôarr√™te donc ici. Vous devez alors faire attention √† bien regarder vous-m√™me la base de donn√©es et la transformer avec attention.\nSoit suivre les diff√©rentes √©tapes qui suivent pas √† pas.\n\nVersion guid√©e üëá\n\nLire les donn√©es depuis l‚Äôurl https://raw.githubusercontent.com/realpython/python-data-cleaning/master/Datasets/BL-Flickr-Images-Book.csv. Attention au s√©parateur\nNe garder que les colonnes ['Identifier', 'Place of Publication', 'Date of Publication', 'Publisher', 'Title', 'Author']\nObserver la colonne ‚ÄòDate of Publication‚Äô et remarquer le probl√®me sur certaines lignes (par exemple la ligne 13)\nCommencez par regarder le nombre d‚Äôinformations manquantes. On ne pourra pas avoir mieux apr√®s la regex, et normalement on ne devrait pas avoir moins‚Ä¶\nD√©terminer la forme de la regex pour une date de publication. A priori, il y a 4 chiffres qui forment une ann√©e.\nUtiliser la m√©thode str.extract() avec l‚Äôargument expand = False (pour ne conserver que la premi√®re date concordant avec notre pattern)?\nOn a 2 NaN qui n‚Äô√©taient pas pr√©sents au d√©but de l‚Äôexercice. Quels sont-ils et pourquoi ?\nQuelle est la r√©partition des dates de publications dans le jeu de donn√©es ? Vous pouvez par exemple afficher un histogramme gr√¢ce √† la m√©thode plot avec l‚Äôargument kind =\"hist\".\n\n\n\n\nVoici par exemple le probl√®me qu‚Äôon demande de d√©tecter √† la question 3 :\n\n\n\n\n\n\n\n\n\nDate of Publication\nTitle\n\n\n\n\n13\n1839, 38-54\nDe Aardbol. Magazijn van hedendaagsche land- e...\n\n\n14\n1897\nCronache Savonesi dal 1500 al 1570 ... Accresc...\n\n\n15\n1865\nSee-Saw; a novel ... Edited [or rather, writte...\n\n\n16\n1860-63\nGeÃÅodeÃÅsie d'une partie de la Haute EÃÅthiopie,...\n\n\n17\n1873\n[With eleven maps.]\n\n\n18\n1866\n[Historia geograÃÅfica, civil y politica de la ...\n\n\n19\n1899\nThe Crisis of the Revolution, being the story ...\n\n\n\n\n\n\n\n\n\n181\n\n\nGr√¢ce √† notre regex (question 5), on obtient ainsi un DataFrame plus conforme √† nos attentes\n\n\n\n\n\n\n\n\n\nDate of Publication\nyear\n\n\n\n\n0\n1879 [1878]\n1879\n\n\n7\nNaN\nNaN\n\n\n13\n1839, 38-54\n1839\n\n\n16\n1860-63\n1860\n\n\n23\n1847, 48 [1846-48]\n1847\n\n\n...\n...\n...\n\n\n8278\n1883, [1884]\n1883\n\n\n8279\n1898-1912\n1898\n\n\n8283\n1831, 32\n1831\n\n\n8284\n[1806]-22\n1806\n\n\n8286\n1834-43\n1834\n\n\n\n\n1759 rows √ó 2 columns\n\n\n\nQuant aux nouveaux NaN,\nil s‚Äôagit de lignes qui ne contenaient pas de cha√Ænes de caract√®res qui ressemblaient √† des ann√©es:\n\n\n\n\n\n\n\n\n\nDate of Publication\nyear\n\n\n\n\n1081\n112. G. & W. B. Whittaker\nNaN\n\n\n7391\n17 vols. University Press\nNaN\n\n\n\n\n\n\n\nEnfin, on obtient l‚Äôhistogramme suivant des dates de publications:\n\n\n&lt;Axes: ylabel='Frequency'&gt;"
  }
]