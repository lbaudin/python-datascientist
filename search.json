[
  {
    "objectID": "content/modelisation/7_mlapi.html",
    "href": "content/modelisation/7_mlapi.html",
    "title": "Mettre à disposition un modèle par le biais d’une API",
    "section": "",
    "text": "Ce chapitre présente la deuxième application\nd’une journée de cours que j’ai\ndonné à l’Université Dauphine dans le cadre\ndes PSL Data Week.\nL’objectif de ce chapitre est d’amener à développer\nune API du type de celle-ci.\nLe chapitre précédent constituait une introduction à la création\nde pipelines de machine learning.\nCe chapitre va aller plus loin en montrant la démarche pour le rendre\ndisponible à plus grande échelle par le biais d’une API pouvant\nêtre consommée avec de nouvelles données. L’objectif de celle-ci est\nde ne pas contraindre les réutilisateurs d’un modèle\nà disposer d’un environnement technique complexe\npour pouvoir utiliser le même modèle que celui entraîné précédemment."
  },
  {
    "objectID": "content/modelisation/7_mlapi.html#exemple-de-réutilisation-dun-modèle-sous-forme-dapi",
    "href": "content/modelisation/7_mlapi.html#exemple-de-réutilisation-dun-modèle-sous-forme-dapi",
    "title": "Mettre à disposition un modèle par le biais d’une API",
    "section": "Exemple de réutilisation d’un modèle sous forme d’API",
    "text": "Exemple de réutilisation d’un modèle sous forme d’API\nUn exemple d’API obtenue à l’issue de ce chapitre est\nmis à disposition sur https://dvf-simple-api.lab.sspcloud.fr/.\nLa documentation de l’API est disponible ici.\nCette API est utilisable dans plusieurs langages.\nEn Python, par exemple, cela donnera:\n\nimport requests\n\npieces_principales = 6\nsurface = 50\nurl = f\"https://dvf-simple-api.lab.sspcloud.fr/predict?month=4&nombre_lots=1&code_type_local=2&nombre_pieces_principales={pieces_principales}&surface={surface}\"\nrequests.get(url).json()\n\n728358.5461884077\n\n\nNéanmoins, l’un des intérêts de proposer\nune API est que les utilisateurs du modèle\nne sont pas obligés d’être des pythonistes.\nCela accroît grandement la cible des ré-utilisateurs\npotentiels.\nCette approche ouvre notamment la possibilité de\nfaire des applications interactives qui utilisent,\nen arrière plan, notre modèle entraîné avec Python.\nVoici un exemple, minimaliste, d’une réutilisation\nde notre modèle avec deux sélecteurs Javascript\nqui mettent à jour le prix estimé du bien.\n\n\n\nhtml`&lt;div&gt;Nombre de pièces&lt;/div&gt;&lt;div&gt;${viewof pieces_principales}&lt;/div&gt;`\n\n\n\n\n\n\n\n\nhtml`&lt;div&gt;Surface de l'appartement&lt;/div&gt;&lt;div&gt;${surface}&lt;/div&gt;`\n\n\n\n\n\n\n\n\n\nviewof pieces_principales = Inputs.range([1, 12], {step: 1, value: 6})\n\n\n\n\n\n\n\nviewof surface = Inputs.range([1, 300], {step: 1, value: 50})\n\n\n\n\n\n\n\nmd`${return_message}`\n\n\n\n\n\n\n\nhtml`${url_api_print}`\n\n\n\n\n\n\n\nurl_api_dvf = `https://corsproxy.io/?https://dvf-simple-api.lab.sspcloud.fr/predict?month=4&nombre_lots=1&code_type_local=2&nombre_pieces_principales=${pieces_principales}&surface=${surface}`\n\n\n\n\n\n\n\nurl_api_print = md`[&lt;span class=\"blue-underlined\"&gt;https://dvf-simple-api.lab.sspcloud.fr/predict?&lt;/span&gt;month=4&nombre_lots=1&code_type_local=2&nombre_pieces_principales=&lt;span class=\"blue-underlined\"&gt;${pieces_principales}&lt;/span&gt;&surface=&lt;span class=\"blue-underlined\"&gt;${surface}&lt;/span&gt;](${url_api_dvf})`\n\n\n\n\n\n\n\nvalue = d3.json(url_api_dvf).then(data =&gt; {\n    // Access the 'value' property from the object\n    let originalNumber = data;\n\n    // Convert it to a floating-point number\n    let numericValue = parseFloat(originalNumber);\n\n    // Round the number\n    let roundedNumber = Math.round(numericValue).toLocaleString();\n\n    return roundedNumber;\n}).catch(error =&gt; console.error('Error:', error));\n\n\n\n\n\n\n\nreturn_message = `Valeur estimée de l'appartement: &lt;span class=\"blue2\"&gt;__${value} €__&lt;/span&gt;`"
  },
  {
    "objectID": "content/modelisation/7_mlapi.html#etape-1-créer-une-application-en-local",
    "href": "content/modelisation/7_mlapi.html#etape-1-créer-une-application-en-local",
    "title": "Mettre à disposition un modèle par le biais d’une API",
    "section": "Etape 1: créer une application en local",
    "text": "Etape 1: créer une application en local\nMettre en place une API consiste à gravir une marche\ndans l’échelle de la reproductibilité par rapport\nà fournir un notebook. Ces derniers\nne sont pas les outils les plus adaptés\npour partager autre chose que du code, à faire tourner\nde son côté.\nIl est donc naturel de sortir des notebooks\nlorsqu’on commence à aller vers ce niveau de mise à\ndisposition.\nPar le biais de\nscripts Python lancés en ligne de commande,\nconstruits en exportant le code du chapitre précédent\nde nos notebooks, on pourra\ncréer une base de départ propre.\nIl est plus naturel de privilégier une interface de développement\ngénéraliste comme VSCode à Jupyter lorsqu’on franchit\nce rubicon. L’exercice suivant permettra donc\nde créer cette première application minimale, à\nexécuter en ligne de commande.\n\n\n Exercice 1: créer des scripts pour entraîner le modèle\nLe dépôt Github qui permet de construire l’API from scratch\nest disponible ici.\nNous allons emprunter quelques éléments, par-ci par-là,\npour faire notre application en local.\n\nCréer un nouveau service VSCode sur le SSPCloud en paramétrant dans l’onglet\nNetworking le port 5000 ;\nUtiliser la commande suivante depuis le terminal:\n\nmkdir app\ncd app\nDepuis le menu des fichiers, créer quatre fichiers dont le contenu\nsuit:\n\nrequirements.txt: récupérer le contenu sur cette page ;\ngetdvf.py: récupérer le contenu sur cette page ;\ntrain.py: récupérer le contenu sur cette page ;\napi.py: récupérer le contenu sur cette page.\nExécuter getdvf.py puis train.py pour stocker en local le modèle entraîné\nAjouter model.joblib au .gitignore (si vous utilisez Git)\nCréer un script test.py qui contient la fonction suivante et la teste après avoir importé votre modèle (load('pipe.joblib') en n’oubliant pas from joblib import load):\n\n\nimport pandas as pd\n\ndef predict(\n    month: int = 3,\n    nombre_lots: int = 1,\n    code_type_local: int = 2,\n    nombre_pieces_principales: int = 3,\n    surface: float = 75\n) -&gt; float:\n    \"\"\"\n    \"\"\"\n\n    df = pd.DataFrame(\n        {\n            \"month\": [month],\n            \"Nombre_de_lots\": [nombre_lots],\n            \"Code_type_local\": [code_type_local],\n            \"Nombre_pieces_principales\": [nombre_pieces_principales],\n            \"surface\": [surface]\n        }\n    )\n\n    prediction = model.predict(df)\n\n    return prediction"
  },
  {
    "objectID": "content/modelisation/7_mlapi.html#etape-2-créer-une-api-en-local",
    "href": "content/modelisation/7_mlapi.html#etape-2-créer-une-api-en-local",
    "title": "Mettre à disposition un modèle par le biais d’une API",
    "section": "Etape 2: créer une API en local",
    "text": "Etape 2: créer une API en local\nLe script précédent constitue déjà un progrès dans\nla reproductibilité. Il rend plus facile le réentraînement\nd’un modèle sur le même jeu de données. Néanmoins,\nil reste tributaire du fait que la personne désirant\nutiliser du modèle utilise Python et sache réentrainer\nle modèle dans les mêmes conditions que vous.\nAvec FastAPI, nous allons très facilement pouvoir\ntransformer cette application Python en une API.\n\n\n Exercice 2: créer des scripts pour entraîner le modèle\n\nLa ligne ci-dessous du script api.py récupère un modèle pré-entraîné enregistré sur un espace de stockage\n\ndownload_file(\"https://minio.lab.sspcloud.fr/projet-formation/diffusion/python-datascientist/pipe.joblib\", 'pipe.joblib')\nRetirer cette ligne de votre script, pour utiliser\nle modèle que vous venez d’entraîner.\n\nDéployer en local l’API avec la commande\n\nuvicorn api:app --reload --host \"0.0.0.0\" --port 5000\n\nA partir du README du service VSCode,\nse rendre sur l’URL de déploiement,\najouter /docs/ à celui-ci et observer la documentation de l’API\nSe servir de la documentation pour tester les requêtes /predict\nRécupérer l’URL d’une des requêtes proposées. La tester dans le navigateur\net depuis Python avec Requests (requests.get(url).json())\nOptionnel: faire tourner le même code dans un autre environnement que le SSPCloud (par exemple une installation de Python en local) pour voir que ça fonctionne de manière identique."
  },
  {
    "objectID": "content/modelisation/7_mlapi.html#aller-plus-loin-mettre-à-disposition-cette-api-de-manière-pérenne",
    "href": "content/modelisation/7_mlapi.html#aller-plus-loin-mettre-à-disposition-cette-api-de-manière-pérenne",
    "title": "Mettre à disposition un modèle par le biais d’une API",
    "section": "Aller plus loin: mettre à disposition cette API de manière pérenne",
    "text": "Aller plus loin: mettre à disposition cette API de manière pérenne\nL’étape précédente permettait de créer un point d’accès\nà votre modèle depuis n’importe quel type de client. A chaque\nrequête de l’API, le script api.py était exécuté et\nrenvoyait son output.\nCeci est déjà un saut de géant dans l’échelle de la\nreproductibilité. Néanmoins, cela reste artisanal: si votre\nserveur local connait un problème (par exemple, vous killez l’application), les clients ne recevront plus de réponse,\nsans comprendre pourquoi.\nIl est donc plus fiable de mettre en production sur des\nserveurs dédiés, qui tournent 24h/24 et qui peuvent\négalement se répartir la charge de travail s’il y a\nbeaucoup de demandes instantanées.\nCeci dépasse néanmoins\nle cadre de ce cours et sera l’objet\nd’un cours dédié en 3e année de l’ENSAE: “Mise en production de projets data science” donné par Romain Avouac et moi."
  },
  {
    "objectID": "content/getting-started/index.html",
    "href": "content/getting-started/index.html",
    "title": "Introduction",
    "section": "",
    "text": "Ce cours rassemble l’ensemble du contenu du cours\nPython  pour la data science que je donne\nà l’ENSAE\ndepuis 2018.\nCe cours était auparavant donné par Xavier Dupré.\nQuelques éléments supplémentaires sont disponibles dans\nles slides d’introduction.\nDes éléments plus avancés sont présents dans un autre cours consacré\nà la mise en production de projets data science\nque je donne avec Romain Avouac\nà l’ENSAE (ensae-reproductibilite.github.io/website)\nPython est un langage qui a déjà plus de trente ans\nmais qui a connu, au cours de la décennie 2010, une\nnouvelle jeunesse du fait de l’engouement pour\nla data science.\nPython, plus que tout autre\nlangage informatique, réunit des communautés aussi\ndiverses que des statisticiens, des développeurs,\ndes gestionnaires\nd’applications ou d’infrastructures informatiques,\ndes lycées - Python est au programme du bac français\ndepuis quelques années - ou des chercheurs\ndans des champs à la fois théoriques et appliqués. Contrairement\nà beaucoup de langages informatiques qui fédèrent\nune communauté assez homogène, Python est parvenu à réunir\nlargement grâce à quelques principes centraux : la lisibilité\ndu langage, la simplicité à utiliser des modules,\nla simplicité à l’associer à des langages plus performants\npour certaines tâches données, l’énorme volume de documentation\ndisponible en ligne…\nÊtre le deuxième meilleur langage pour réaliser telle ou telle\ntâche\npeut ainsi être une source de succès lorsque la concurrence ne dispose\npas d’un éventail aussi large d’avantages.\nLe succès de Python, de par sa nature de\nlangage couteau-suisse, est indissociable\nde l’émergence du profil du data scientist, individu\ncapable de s’intégrer à différents niveaux dans la valorisation\nde données.\nDavenport and Patil (2012a), dans la Harvard Business Review,\nont ainsi pu parler du “boulot le plus sexy du 21e siècle”\net ont pu, dix ans plus tard, faire un panorama complet de l’évolution\ndes compétences attendues d’un data scientist dans\nla même revue (Davenport and Patil 2012b).\nLa richesse de Python permet de l’utiliser dans toutes les phases\ndu traitement de la donnée, de sa récupération et structuration à partir de\nsources diverses à sa valorisation.\nPar le prisme de la data science, nous verrons que Python est\nun très bon candidat pour assister les data scientists dans tous\nles aspects du travail de données.\nCe cours introduit différents outils qui permettent de mettre en relation\ndes données et des théories grâce à Python. Néanmoins, ce cours\nva au-delà d’une simple introduction au langage et propose\ndes éléments plus approfondis, notamment sur les dernières\ninnovations permises par la data science dans les méthodes de travail.\n\n\nLe succès de scikit-learn et\nde Tensorflow dans la communauté\nde la Data-Science ont beaucoup contribué à l’adoption de Python. Cependant,\nrésumer Python à ces quelques librairies serait réducteur tant il s’agit\nd’un véritable couteau-suisse pour les data scientists,\nles social scientists ou les économistes.\nL’intérêt de Python pour un data scientist ou data economist\nva au-delà du champ du Machine Learning.\nComme pour R, l’intérêt de Python est son rôle central dans un\nécosystème plus large autour d’outils puissants, flexibles et open-source.\nPython concurrence très bien R dans son domaine de prédilection, à\nsavoir l’analyse statistique sur des bases de données structurées.\nComme dans R, les dataframes sont un concept central de Python.\nPython est néanmoins bien plus complet dans certains domaines.\nOutre le Machine Learning,\nPython est mieux adapté aux données volumineuses que\nR. Python est également meilleur que R pour faire\ndu webscraping ou accéder à des données par le biais d’API.\nDans le domaine de l’économétrie, Python offre\nl’avantage de la simplicité avec un nombre restreint de packages (scikit et\nstatsmodels) permettant d’avoir des modèles très généraux\n(les generalized estimating equations)\nalors qu’il faut\nchoisir parmi une grande variété de packages en R pour obtenir les\nmodèles équivalents. Dans le domaine du Deep Learning, Python écrase\nla concurrence.\nAu contraire, dans certains domaines, R reste meilleur, même si les\névolutions très récentes de certains outils peuvent amener à réviser\nce constat. Historiquement,\nR était très bien intégré au langage de publication Markdown ce qui,\npermet la construction de documents reproductibles très raffinés.\nL’émergence récente de Quarto, héritier de R Markdown développé par\nla société Posit permet aux utilisateur de Python de bénéficier\négalement de la richesse de cette approche pour leur langage de prédilection.\nCe site web, à l’arborescence relativement complexe, est ainsi\nconstruit grâce à cet outil qui permet à la fois de tester les blocs\nde code présentés mais aussi de produire de manière automatisée les\ntableaux et graphiques présentés. S’il fallait trouver un point faible\nà Python par rapport à R dans le domaine de la data science\nc’est sur la production de graphiques. matplotlib et seaborn, qui sont\nprésentés dans la partie visualisation, sont d’excellents outils. Néanmoins,\nggplot2, l’équivalent en R est plus facile de prise en main et\npropose une syntaxe extrêmement flexible, qu’il est difficile de ne pas\napprécier. Cependant, l’écosystème de la\nvisualisation de données est en pleine révolution avec le succès\nd’Observable qui\nrapproche l’écosystème JavaScript des développeurs web\nde la communauté des analystes de données.\nUn des avantages comparatifs de Python par rapport à d’autres\nlangages (notamment R et Julia) est sa dynamique,\nce que montre l’explosion du nombre de questions\nsur Stack Overflow.\nCependant, il ne s’agit pas bêtement d’enterrer R.\nAu contraire, outre leur logique très proche,\nles deux langages sont dans une phase de convergence avec des initiatives comme\nreticulate,\nquarto ou\nsnakemake qui\npermettent, de manière différente, de créer des chaînes de traitement\nmélangeant R et Python.\nUne autre raison pour laquelle cette guéguerre R/Python n’a pas\nde sens est que les bonnes\npratiques peuvent être transposées de manière presque transparente d’un\nlangage à l’autre. Il s’agit d’un point qui est développé plus amplement\ndans le cours plus avancé que je donne avec Romain Avouac en dernière année\nd’ENSAE : ensae-reproductibilite.github.io/website.\nA terme, les data scientists et chercheurs en sciences sociales ou\néconomie utiliseront\nde manière presque indifférente, et en alternance, Python et R. Ce cours\nprésentera ainsi régulièrement des analogies avec R pour aider les\npersonnes découvrant Python, mais connaissant déjà bien R, à\nmieux comprendre certains messages.\n\n\n\nLe but de ce cours est de rendre autonome sur\nl’utilisation de Python\ndans un contexte de travail de data scientist ou de\nsocial scientist (économie, sociologie, géographie…).\nAutrement dit,\nil présuppose qu’on désire faire un usage intense\nde données dans un cadre statistique rigoureux.\nLa data science est un ensemble de techniques\nvisant à donner du sens à des sources de données\ndiverses. Selon les organisations,\nles data scientists peuvent ainsi être à\nl’interface de projets nécessitant un\nlarge spectre de compétences\n(analyse\nde données textuelles, représentation\ngraphique interactive…),\navoir des interactions avec des profils\ntrès différents (experts métiers,\ndéveloppeurs, data architect,\ndata engineer…) voire adopter\nun peu tous ces rôles.\nLes innovations\nrécentes de la data science ne se réduisent\nnéanmoins\npas qu’à des découvertes méthodologiques.\nLa data science propose un ensemble de\ntechniques et de méthodes de travail\npour réduire les coûts de passage\nd’un protype à une chaine\nde production pérenne.\nCe cours introduit à quelques notions\nsur le sujet, notamment les\npipelines scikit, pour adopter\ndès l’apprentissage du langage\nquelques bons réflexes (ensae-reproductibilite.github.io/website).\n\n\n\nCe cours ne revient que de manière secondaire\nsur les fondements statistiques ou algorithmiques\nderrière certaines des techniques évoquées.\nNe pas connaître ces notions n’empêche néanmoins pas de comprendre\nle contenu de ce site web. En effet, la facilité d’usage de Python\névite de devoir programmer soi-même un modèle, ce qui rend\npossible l’application\nde modèles dont on n’est pas expert. La connaissance des modèles sera\nplutôt nécessaire dans l’interprétation des résultats.\nCependant, la facilité avec laquelle il est possible de construire des modèles complexes\navec Python peut laisser apparaître que connaître les spécifités de chaque\nmodèle est inutile. Il\ns’agirait d’une grave erreur : même si l’implémentation de modèles est aisée, il\nest nécessaire de bien comprendre la structure des données et leur adéquation\navec les hypothèses d’un modèle.\n\n\n\nCe cours donne une place centrale à\nla notion de reproductibilité. Cette exigence se traduit de diverses\nmanières dans cet enseignement, en particulier en insistant sur un\noutil indispensable pour favoriser le partage de codes informatiques,\nà savoir Git.\nL’ensemble du contenu du site web est reproductible dans des environnements\ninformatiques divers. Il est bien sûr possible de copier-coller les morceaux\nde code présents dans ce site. Cette méthode montrant rapidement ses limites,\nle site présente un certain nombre de boutons disponibles pour\nouvrir la page sous un format Jupyter Notebook sur divers\npages web:\n\nSur l’ensemble du site web,\nil est possible de cliquer\nsur la petite icone \npour être redirigé vers le dépôt Github associé à ce cours.\nUn certain nombre de boutons permettent de transformer chaque\npage web en Jupyter Notebooks s’il est nécessaire de\nvisualiser ou exécuter du code Python.\n\nVoici, par exemple, ces boutons pour le tutoriel numpy :\n\n\n\n\n\n\n\n\n\n\n\nPour les agents de la fonction publique, ou\nles élèves des écoles partenaires, il est recommandé\nde privilégier le bouton SSPCloud qui est\nune infrastructure cloud moderne, puissante et flexible\ndéveloppée par l’Insee et accessible à l’url\nhttps://datalab.sspcloud.fr1.\nL’ensemble du contenu de ce site s’appuie sur des données\nouvertes, qu’il s’agisse de données françaises (principalement\nissues de la plateforme\ncentralisatrice data.gouv ou du site\nweb de l’Insee) ou de données\naméricaines. Les résultats sont donc reproductibles pour quelqu’un\ndisposant d’un environnement identique.\n\n\n\nCe cours présente\ndes tutoriels et des exercices complets.\nChaque page est structurée sous la forme\nd’un problème concret et présente la\ndémarche générique pour résoudre ce problème général.\nVous pouvez naviguer dans l’architecture du site via la table des matières\nou par les liens vers le contenu antérieur ou postérieur à la fin de chaque\npage. Certaines parties, notamment celle consacrée à la modélisation,\nproposent des exemples fil-rouge pour illustrer la démarche de manière\nplus extensive.\n\n\n\nLes élèves de l’ENSAE valident le cours grâce à\nun projet approfondi.\nLes éléments relatifs à l’évaluation du cours, ainsi qu’une\nliste des projets déjà effectués, sont disponibles dans la\nSection Evaluation.\n\n\n\n\n\nDavenport, Thomas H, and DJ Patil. 2012a. “Data Scientist, the Sexiest Job of the 21st Century.” Harvard Business Review 90 (5): 70–76. https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century.\n\n\n———. 2012b. “Is Data Scientist Still the Sexiest Job of the 21st Century?” Harvard Business Review 90 (5): 70–76. https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century."
  },
  {
    "objectID": "content/getting-started/index.html#pourquoi-faire-du-python-pour-lanalyse-de-données",
    "href": "content/getting-started/index.html#pourquoi-faire-du-python-pour-lanalyse-de-données",
    "title": "Introduction",
    "section": "",
    "text": "Le succès de scikit-learn et\nde Tensorflow dans la communauté\nde la Data-Science ont beaucoup contribué à l’adoption de Python. Cependant,\nrésumer Python à ces quelques librairies serait réducteur tant il s’agit\nd’un véritable couteau-suisse pour les data scientists,\nles social scientists ou les économistes.\nL’intérêt de Python pour un data scientist ou data economist\nva au-delà du champ du Machine Learning.\nComme pour R, l’intérêt de Python est son rôle central dans un\nécosystème plus large autour d’outils puissants, flexibles et open-source.\nPython concurrence très bien R dans son domaine de prédilection, à\nsavoir l’analyse statistique sur des bases de données structurées.\nComme dans R, les dataframes sont un concept central de Python.\nPython est néanmoins bien plus complet dans certains domaines.\nOutre le Machine Learning,\nPython est mieux adapté aux données volumineuses que\nR. Python est également meilleur que R pour faire\ndu webscraping ou accéder à des données par le biais d’API.\nDans le domaine de l’économétrie, Python offre\nl’avantage de la simplicité avec un nombre restreint de packages (scikit et\nstatsmodels) permettant d’avoir des modèles très généraux\n(les generalized estimating equations)\nalors qu’il faut\nchoisir parmi une grande variété de packages en R pour obtenir les\nmodèles équivalents. Dans le domaine du Deep Learning, Python écrase\nla concurrence.\nAu contraire, dans certains domaines, R reste meilleur, même si les\névolutions très récentes de certains outils peuvent amener à réviser\nce constat. Historiquement,\nR était très bien intégré au langage de publication Markdown ce qui,\npermet la construction de documents reproductibles très raffinés.\nL’émergence récente de Quarto, héritier de R Markdown développé par\nla société Posit permet aux utilisateur de Python de bénéficier\négalement de la richesse de cette approche pour leur langage de prédilection.\nCe site web, à l’arborescence relativement complexe, est ainsi\nconstruit grâce à cet outil qui permet à la fois de tester les blocs\nde code présentés mais aussi de produire de manière automatisée les\ntableaux et graphiques présentés. S’il fallait trouver un point faible\nà Python par rapport à R dans le domaine de la data science\nc’est sur la production de graphiques. matplotlib et seaborn, qui sont\nprésentés dans la partie visualisation, sont d’excellents outils. Néanmoins,\nggplot2, l’équivalent en R est plus facile de prise en main et\npropose une syntaxe extrêmement flexible, qu’il est difficile de ne pas\napprécier. Cependant, l’écosystème de la\nvisualisation de données est en pleine révolution avec le succès\nd’Observable qui\nrapproche l’écosystème JavaScript des développeurs web\nde la communauté des analystes de données.\nUn des avantages comparatifs de Python par rapport à d’autres\nlangages (notamment R et Julia) est sa dynamique,\nce que montre l’explosion du nombre de questions\nsur Stack Overflow.\nCependant, il ne s’agit pas bêtement d’enterrer R.\nAu contraire, outre leur logique très proche,\nles deux langages sont dans une phase de convergence avec des initiatives comme\nreticulate,\nquarto ou\nsnakemake qui\npermettent, de manière différente, de créer des chaînes de traitement\nmélangeant R et Python.\nUne autre raison pour laquelle cette guéguerre R/Python n’a pas\nde sens est que les bonnes\npratiques peuvent être transposées de manière presque transparente d’un\nlangage à l’autre. Il s’agit d’un point qui est développé plus amplement\ndans le cours plus avancé que je donne avec Romain Avouac en dernière année\nd’ENSAE : ensae-reproductibilite.github.io/website.\nA terme, les data scientists et chercheurs en sciences sociales ou\néconomie utiliseront\nde manière presque indifférente, et en alternance, Python et R. Ce cours\nprésentera ainsi régulièrement des analogies avec R pour aider les\npersonnes découvrant Python, mais connaissant déjà bien R, à\nmieux comprendre certains messages."
  },
  {
    "objectID": "content/getting-started/index.html#objectif-du-cours",
    "href": "content/getting-started/index.html#objectif-du-cours",
    "title": "Introduction",
    "section": "",
    "text": "Le but de ce cours est de rendre autonome sur\nl’utilisation de Python\ndans un contexte de travail de data scientist ou de\nsocial scientist (économie, sociologie, géographie…).\nAutrement dit,\nil présuppose qu’on désire faire un usage intense\nde données dans un cadre statistique rigoureux.\nLa data science est un ensemble de techniques\nvisant à donner du sens à des sources de données\ndiverses. Selon les organisations,\nles data scientists peuvent ainsi être à\nl’interface de projets nécessitant un\nlarge spectre de compétences\n(analyse\nde données textuelles, représentation\ngraphique interactive…),\navoir des interactions avec des profils\ntrès différents (experts métiers,\ndéveloppeurs, data architect,\ndata engineer…) voire adopter\nun peu tous ces rôles.\nLes innovations\nrécentes de la data science ne se réduisent\nnéanmoins\npas qu’à des découvertes méthodologiques.\nLa data science propose un ensemble de\ntechniques et de méthodes de travail\npour réduire les coûts de passage\nd’un protype à une chaine\nde production pérenne.\nCe cours introduit à quelques notions\nsur le sujet, notamment les\npipelines scikit, pour adopter\ndès l’apprentissage du langage\nquelques bons réflexes (ensae-reproductibilite.github.io/website)."
  },
  {
    "objectID": "content/getting-started/index.html#public-cible",
    "href": "content/getting-started/index.html#public-cible",
    "title": "Introduction",
    "section": "",
    "text": "Ce cours ne revient que de manière secondaire\nsur les fondements statistiques ou algorithmiques\nderrière certaines des techniques évoquées.\nNe pas connaître ces notions n’empêche néanmoins pas de comprendre\nle contenu de ce site web. En effet, la facilité d’usage de Python\névite de devoir programmer soi-même un modèle, ce qui rend\npossible l’application\nde modèles dont on n’est pas expert. La connaissance des modèles sera\nplutôt nécessaire dans l’interprétation des résultats.\nCependant, la facilité avec laquelle il est possible de construire des modèles complexes\navec Python peut laisser apparaître que connaître les spécifités de chaque\nmodèle est inutile. Il\ns’agirait d’une grave erreur : même si l’implémentation de modèles est aisée, il\nest nécessaire de bien comprendre la structure des données et leur adéquation\navec les hypothèses d’un modèle."
  },
  {
    "objectID": "content/getting-started/index.html#reproductibilité",
    "href": "content/getting-started/index.html#reproductibilité",
    "title": "Introduction",
    "section": "",
    "text": "Ce cours donne une place centrale à\nla notion de reproductibilité. Cette exigence se traduit de diverses\nmanières dans cet enseignement, en particulier en insistant sur un\noutil indispensable pour favoriser le partage de codes informatiques,\nà savoir Git.\nL’ensemble du contenu du site web est reproductible dans des environnements\ninformatiques divers. Il est bien sûr possible de copier-coller les morceaux\nde code présents dans ce site. Cette méthode montrant rapidement ses limites,\nle site présente un certain nombre de boutons disponibles pour\nouvrir la page sous un format Jupyter Notebook sur divers\npages web:\n\nSur l’ensemble du site web,\nil est possible de cliquer\nsur la petite icone \npour être redirigé vers le dépôt Github associé à ce cours.\nUn certain nombre de boutons permettent de transformer chaque\npage web en Jupyter Notebooks s’il est nécessaire de\nvisualiser ou exécuter du code Python.\n\nVoici, par exemple, ces boutons pour le tutoriel numpy :\n\n\n\n\n\n\n\n\n\n\n\nPour les agents de la fonction publique, ou\nles élèves des écoles partenaires, il est recommandé\nde privilégier le bouton SSPCloud qui est\nune infrastructure cloud moderne, puissante et flexible\ndéveloppée par l’Insee et accessible à l’url\nhttps://datalab.sspcloud.fr1.\nL’ensemble du contenu de ce site s’appuie sur des données\nouvertes, qu’il s’agisse de données françaises (principalement\nissues de la plateforme\ncentralisatrice data.gouv ou du site\nweb de l’Insee) ou de données\naméricaines. Les résultats sont donc reproductibles pour quelqu’un\ndisposant d’un environnement identique."
  },
  {
    "objectID": "content/getting-started/index.html#architecture-du-site-web",
    "href": "content/getting-started/index.html#architecture-du-site-web",
    "title": "Introduction",
    "section": "",
    "text": "Ce cours présente\ndes tutoriels et des exercices complets.\nChaque page est structurée sous la forme\nd’un problème concret et présente la\ndémarche générique pour résoudre ce problème général.\nVous pouvez naviguer dans l’architecture du site via la table des matières\nou par les liens vers le contenu antérieur ou postérieur à la fin de chaque\npage. Certaines parties, notamment celle consacrée à la modélisation,\nproposent des exemples fil-rouge pour illustrer la démarche de manière\nplus extensive."
  },
  {
    "objectID": "content/getting-started/index.html#evaluation",
    "href": "content/getting-started/index.html#evaluation",
    "title": "Introduction",
    "section": "",
    "text": "Les élèves de l’ENSAE valident le cours grâce à\nun projet approfondi.\nLes éléments relatifs à l’évaluation du cours, ainsi qu’une\nliste des projets déjà effectués, sont disponibles dans la\nSection Evaluation."
  },
  {
    "objectID": "content/getting-started/index.html#références",
    "href": "content/getting-started/index.html#références",
    "title": "Introduction",
    "section": "",
    "text": "Davenport, Thomas H, and DJ Patil. 2012a. “Data Scientist, the Sexiest Job of the 21st Century.” Harvard Business Review 90 (5): 70–76. https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century.\n\n\n———. 2012b. “Is Data Scientist Still the Sexiest Job of the 21st Century?” Harvard Business Review 90 (5): 70–76. https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century."
  },
  {
    "objectID": "content/getting-started/index.html#footnotes",
    "href": "content/getting-started/index.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPour les utilisateurs de cette infrastructure, les notebooks\nsont également listés, parmi de nombreuses autres\nressources de qualité, sur la\npage Formation↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Thèmes en vrac",
    "section": "",
    "text": "Python pour la data science \n\n\nLino Galiana\n\nStar this website on Github\n\nCe site web rend public le contenu du cours de deuxième année (Master 1) de l’ENSAE :\nPython pour la data science\n\nTout est présent sur ce site web ! Des Notebooks Jupyter peuvent être récupérés pour s’exercer. L’ensemble des codes sources est stocké sur Github\n\n\n\n\n\n\n\n\n\n\n\n\n\nPour découvrir Python  de manière thématique\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\nCette introduction propose quelques éléments de\nrévision des concepts de base en Python et\nprésente l’écosystème Python que nous allons\ndécouvrir tout au long de ce…\n\n\n\nLino Galiana\n\n\nJun 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPremier pas vers l’industrialisation avec les pipelines scikit\n\n\n\nModélisation\n\n\nTutoriel\n\n\n\nLes pipelines Scikit permettent d’intégrer de manière très flexible\nun ensemble d’opérations de pre-processing et d’entraînement de modèles\ndans une chaîne d’opérations.…\n\n\n\nLino Galiana\n\n\nOct 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMettre à disposition un modèle par le biais d’une API\n\n\n\nModélisation\n\n\nTutoriel\n\n\n\nTO BE COMPLETED\n\n\n\nLino Galiana\n\n\nOct 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n Back to topCitationBibTeX citation:@book{galiana2023,\n  author = {Galiana, Lino},\n  title = {Python Pour La Data Science},\n  date = {2023},\n  url = {https://pythonds.linogaliana.fr/},\n  doi = {10.5281/zenodo.8229676},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGaliana, Lino. 2023. Python Pour La Data Science. https://doi.org/10.5281/zenodo.8229676."
  },
  {
    "objectID": "content/modelisation/6_pipeline.html",
    "href": "content/modelisation/6_pipeline.html",
    "title": "Premier pas vers l’industrialisation avec les pipelines scikit",
    "section": "",
    "text": "Ce chapitre présente la première application\nd’une journée de cours que j’ai\ndonné à l’Université Dauphine dans le cadre\ndes PSL Data Week.\nPour lire les données de manière efficace, nous\nproposons d’utiliser le package duckdb.\nPour l’installer, voici la commande:\n!pip install duckdb"
  },
  {
    "objectID": "content/modelisation/6_pipeline.html#pourquoi-utiliser-les-pipelines",
    "href": "content/modelisation/6_pipeline.html#pourquoi-utiliser-les-pipelines",
    "title": "Premier pas vers l’industrialisation avec les pipelines scikit",
    "section": "Pourquoi utiliser les pipelines ?",
    "text": "Pourquoi utiliser les pipelines ?\n\nDéfinitions préalables\nCe chapitre nous amènera à explorer plusieurs écosystèmes, pour lesquels on retrouve quelques buzz-words dont voici les définitions :\n\n\n\n\n\n\n\nTerme\nDéfinition\n\n\n\n\nDevOps\nMouvement en ingénierie informatique et une pratique technique visant à l’unification du développement logiciel (dev) et de l’administration des infrastructures informatiques (ops)\n\n\nMLOps\nEnsemble de pratiques qui vise à déployer et maintenir des modèles de machine learning en production de manière fiable et efficace\n\n\n\nCe chapitre fera des références régulières au cours\nde 3e année de l’ENSAE\n“Mise en production de projets data science”.\n\n\nObjectif\nLes chapitres précédents ont permis de montrer des bouts de code\népars pour entraîner des modèles ou faire du preprocessing.\nCette démarche est intéressante pour tâtonner mais risque d’être coûteuse\nultérieurement s’il est nécessaire d’ajouter une étape de preprocessing\nou de changer d’algorithme.\nLes pipelines sont pensés pour simplifier la mise en production\nultérieure d’un modèle de machine learning.\nIls sont au coeur de la démarche de MLOps qui est\nprésentée\ndans le cours de 3e année de l’ENSAE\nde “Mise en production de projets data science”.\nqui vise à simplifier la mise en oeuvre opérationnelle de\nprojets utilisant des techniques de machine learning.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\n\n\nLes pipelines Scikit\nHeureusement, Scikit propose un excellent outil pour proposer un cadre\ngénéral pour créer une chaîne de production machine learning. Il\ns’agit des\npipelines.\nIls présentent de nombreux intérêts, parmi lesquels:\n\nIls sont très pratiques et lisibles. On rentre des données en entrée, on n’appelle qu’une seule fois les méthodes fit et predict ce qui permet de s’assurer une gestion cohérente des transformations de variables, par exemple après l’appel d’un StandardScaler ;\nLa modularité rend aisée la mise à jour d’un pipeline et renforce la capacité à le réutiliser ;\nIls permettent de facilement chercher les hyperparamètres d’un modèle. Sans pipeline, écrire un code qui fait du tuning d’hyperparamètres peut être pénible. Avec les pipelines, c’est une ligne de code ;\nLa sécurité d’être certain que les étapes de preprocessing sont bien appliquées aux jeux de données désirés avant l’estimation.\n\n\n\n Hint\nUn des intérêts des pipelines scikit est qu’ils fonctionnent aussi avec\ndes méthodes qui ne sont pas issues de scikit.\nIl est possible d’introduire un modèle de réseau de neurone Keras dans\nun pipeline scikit.\nPour introduire un modèle économétrique statsmodels\nc’est un peu plus coûteux mais nous allons proposer des exemples\nqui peuvent servir de modèle et qui montrent que c’est faisable\nsans trop de difficulté."
  },
  {
    "objectID": "content/modelisation/6_pipeline.html#comment-créer-un-pipeline",
    "href": "content/modelisation/6_pipeline.html#comment-créer-un-pipeline",
    "title": "Premier pas vers l’industrialisation avec les pipelines scikit",
    "section": "Comment créer un pipeline",
    "text": "Comment créer un pipeline\nUn pipeline est un enchaînement d’opérations qu’on code en enchainant\ndes pairs (clé, valeur):\n\nla clé est le nom du pipeline, cela peut être utile lorsqu’on va\nreprésenter le pipeline sous forme de diagramme acyclique (visualisation DAG)\nou qu’on veut afficher des informations sur une étape\nla valeur représente la transformation à mettre en oeuvre dans le pipeline\n(c’est-à-dire, à l’exception de la dernière étape,\nmettre en oeuvre une méthode transform et éventuellement une\ntransformation inverse).\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\n\nestimators = [('reduce_dim', PCA()), ('clf', SVC())]\npipe = Pipeline(estimators)\npipe\n\nPipeline(steps=[('reduce_dim', PCA()), ('clf', SVC())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('reduce_dim', PCA()), ('clf', SVC())])PCAPCA()SVCSVC()\n\n\nAu sein d’une étape de pipeline, les paramètres d’un estimateur\nsont accessibles avec la notation &lt;estimator&gt;__&lt;parameter&gt;.\nCela permet de fixer des valeurs pour les arguments des fonctions scikit\nqui sont appelées au sein d’un pipeline.\nC’est cela qui rendra l’approche des pipelines particulièrement utile\npour la grid search:\n\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {\"reduce_dim__n_components\":[2, 5, 10], \"clf__C\":[0.1, 10, 100]}\ngrid_search = GridSearchCV(pipe, param_grid=param_grid)\ngrid_search\n\nGridSearchCV(estimator=Pipeline(steps=[('reduce_dim', PCA()), ('clf', SVC())]),\n             param_grid={'clf__C': [0.1, 10, 100],\n                         'reduce_dim__n_components': [2, 5, 10]})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(estimator=Pipeline(steps=[('reduce_dim', PCA()), ('clf', SVC())]),\n             param_grid={'clf__C': [0.1, 10, 100],\n                         'reduce_dim__n_components': [2, 5, 10]})estimator: PipelinePipeline(steps=[('reduce_dim', PCA()), ('clf', SVC())])PCAPCA()SVCSVC()\n\n\nCes pipelines sont initialisés sans données, il s’agit d’une structure formelle\nque nous allons ensuite ajuster en entraînant des modèles.\n\nDonnées utilisées\nNous allons utiliser les données\nde transactions immobilières DVF pour chercher\nla meilleure manière de prédire, sachant les caractéristiques d’un bien, son\nprix.\nCes données sont mises à disposition\nsur data.gouv.\nNéanmoins, le format csv n’étant pas pratique pour importer des jeux de données\nvolumineux, nous proposons de privilégier la version Parquet mise à\ndisposition par Eric Mauvière sur data.gouv.\nL’approche la plus efficace pour lire ces données est\nd’utiliser DuckDB afin de lire le fichier, extraire les colonnes\nd’intérêt puis passer à Pandas (pour en savoir plus sur\nl’intérêt de DuckDB pour lire des fichiers volumineux, vous pouvez\nconsulter ce post de blog ou\ncelui-ci écrit\npar Eric Mauvière).\nMême si, en soi, les gains de temps sont faibles car DuckDB optimise\nles requêtes HTTPS nécessaires à l’import des données, nous proposons\nde télécharger les données pour réduire les besoins de bande passante.\n\nimport requests\nimport os\n\nurl = \"https://www.data.gouv.fr/fr/datasets/r/56bde1e9-e214-408b-888d-34c57ff005c4\"\nfile_name = \"dvf.parquet\"\n\n# Check if the file already exists\nif not os.path.exists(file_name):\n    response = requests.get(url)\n\n    if response.status_code == 200:\n        with open(file_name, \"wb\") as f:\n            f.write(response.content)\n        print(\"Téléchargement réussi.\")\n    else:\n        print(f\"Échec du téléchargement. Code d'état : {response.status_code}\")\nelse:\n    print(f\"Le fichier '{file_name}' existe déjà. Aucun téléchargement nécessaire.\")\n\nEn premier lieu, puisque cela va faciliter les requêtes SQL ultérieures, on crée\nune vue:\n\nimport duckdb\nduckdb.sql(f'CREATE OR REPLACE VIEW dvf AS SELECT * FROM read_parquet(\"dvf.parquet\")')\n\nLes données prennent la forme suivante:\n\nduckdb.sql(f\"SELECT * FROM dvf LIMIT 5\")\n\n┌──────────────────────┬────────────────────┬───┬────────────────┬──────────────────────┬─────────────────┐\n│ Identifiant de doc…  │ Reference document │ … │ Nature culture │ Nature culture spe…  │ Surface terrain │\n│       varchar        │      varchar       │   │    varchar     │       varchar        │      int64      │\n├──────────────────────┼────────────────────┼───┼────────────────┼──────────────────────┼─────────────────┤\n│ NULL                 │ NULL               │ … │ NULL           │ NULL                 │            NULL │\n│ NULL                 │ NULL               │ … │ S              │ NULL                 │              84 │\n│ NULL                 │ NULL               │ … │ S              │ NULL                 │              88 │\n│ NULL                 │ NULL               │ … │ NULL           │ NULL                 │            NULL │\n│ NULL                 │ NULL               │ … │ T              │ NULL                 │             510 │\n├──────────────────────┴────────────────────┴───┴────────────────┴──────────────────────┴─────────────────┤\n│ 5 rows                                                                             43 columns (5 shown) │\n└─────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n\n\nLes variables que nous allons conserver sont les suivantes,\nnous allons les reformater\npour la suite de l’exercice\n\nxvars = [\n    \"Date mutation\", \"Valeur fonciere\",\n    'Nombre de lots', 'Code type local',\n    'Nombre pieces principales'\n]\nxvars = \", \".join([f'\"{s}\"' for s in xvars])\n\n\nmutations = duckdb.sql(\n    f'''\n    SELECT\n    date_part('month', \"Date mutation\") AS month,\n    substring(\"Code postal\", 1, 2) AS dep,\n    {xvars},\n    COLUMNS('Surface Carrez.*')\n    FROM dvf\n    '''\n).to_df()\n\ncolonnes_surface = mutations.columns[mutations.columns.str.startswith('Surface Carrez')]\nmutations.loc[:, colonnes_surface] = mutations.loc[:, colonnes_surface].replace({',': '.'}, regex=True).astype(float).fillna(0)\n\n\n\n\n\n\n Note\nLe fichier Parquet mis à disposition sur data.gouv présente une incohérence de mise en forme de\ncertaines colonnes à cause des virgules qui empêchent le formattage sous forme de colonne\nnumérique.\nLe code ci-dessus effectue la conversion adéquate au niveau de Pandas.\n\n\n\nmutations.head(2)\n\n\n\n\n\n\n\n\nmonth\ndep\nDate mutation\nValeur fonciere\nNombre de lots\nCode type local\nNombre pieces principales\nSurface Carrez du 1er lot\nSurface Carrez du 2eme lot\nSurface Carrez du 3eme lot\nSurface Carrez du 4eme lot\nSurface Carrez du 5eme lot\n\n\n\n\n0\n1\n01\n2022-01-03\n55000.0\n1\n2.0\n1.0\n24.1\n0.0\n0.0\n0.0\n0.0\n\n\n1\n1\n01\n2022-01-03\n143000.0\n0\nNaN\nNaN\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\n\nIntroduire un effet confinement\n\nSi vous travaillez avec les données de 2020, n’oubliez pas\nd’intégrer l’effet\nconfinement dans vos modèles puisque cela a lourdement\naffecté les possibilités de transaction sur cette période, donc\nl’effet potentiel de certaines variables explicatives du prix.\nPour introduire cet effet, vous pouvez créer une variable\nindicatrice entre les dates en question:\nmutations['confinement'] = (\n    mutations['Date mutation']\n    .between(pd.to_datetime(\"2020-03-17\"), pd.to_datetime(\"2020-05-03\"))\n    .astype(int)\n)\nComme nous travaillons sur les données de 2022,\nnous pouvons nous passer de cette variable.\n\n\nLes données DVF proposent une observation par transaction.\nCes transactions\npeuvent concerner plusieurs lots. Par exemple, un appartement\navec garage et cave comportera trois lots.\nPour simplifier,\non va créer une variable de surface qui agrège les différentes informations\nde surface disponibles dans le jeu de données.\nLes agréger revient à supposer que le modèle de fixation des prix est le même\nentre chaque lot. C’est une hypothèse simplificatrice qu’une personne plus\nexperte du marché immobilier, ou qu’une approche propre de sélection\nde variable pourrait amener à nier. En effet, les variables\nen question sont faiblement corrélées les unes entre elles, à quelques\nexceptions près (Figure 1):\n\ncorr = mutations.loc[\n    :,\n    mutations.columns[mutations.columns.str.startswith('Surface Carrez')].tolist()\n]\ncorr.columns = corr.columns.str.replace(\"Carrez du \", \"\")\ncorr = corr.corr()\n\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n\n\nfig, ax = plt.subplots(1)\ng = sns.heatmap(\n    corr, ax=ax, \n    mask=mask,\n    vmax=.3, center=0,\n    square=True, linewidths=.5, cbar_kws={\"shrink\": .5},\n    xticklabels=corr.columns.values,\n    yticklabels=corr.columns.values, cmap=cmap, annot=True, fmt=\".2f\"\n)\ng\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/matrix.py:260: FutureWarning:\n\nFormat strings passed to MaskedConstant are ignored, but in future may error or produce different behavior\n\n\n\n\n&lt;Axes: &gt;\n(a) Matrice de corrélation des variables de surface\n\n\n\n\n\n\n(b)\n\n\n\nFigure 1: ?(caption)\n\n\n\nmutations['lprix'] = np.log(mutations[\"Valeur fonciere\"])\nmutations['surface'] = mutations.loc[:, colonnes_surface].sum(axis = 1).astype(int)\n\n/opt/mamba/lib/python3.9/site-packages/pandas/core/arraylike.py:396: RuntimeWarning:\n\ndivide by zero encountered in log\n\n\n\n\nmutations['surface'] = mutations.loc[:, mutations.columns[mutations.columns.str.startswith('Surface Carrez')].tolist()].sum(axis = 1)"
  },
  {
    "objectID": "content/modelisation/6_pipeline.html#un-premier-pipeline-random-forest-sur-des-variables-standardisées",
    "href": "content/modelisation/6_pipeline.html#un-premier-pipeline-random-forest-sur-des-variables-standardisées",
    "title": "Premier pas vers l’industrialisation avec les pipelines scikit",
    "section": "Un premier pipeline: random forest sur des variables standardisées",
    "text": "Un premier pipeline: random forest sur des variables standardisées\nNotre premier pipeline va nous permettre d’intégrer ensemble:\n\nUne étape de preprocessing avec la standardisation de variables\nUne étape d’estimation du prix en utilisant un modèle de random forest\n\nPour le moment, on va prendre comme acquis un certain nombre de variables\nexplicatives (les features) et les hyperparamètres du modèle.\nL’algorithme des random forest est une technique statistique basée sur\nles arbres de décision. Elle a été définie explicitement par l’un\ndes pionniers du machine learning, Breiman (2001).\nIl s’agit d’une méthode ensembliste\npuisqu’elle consiste à utiliser plusieurs algorithmes (en l’occurrence des arbres\nde décision) pour obtenir une meilleure prédiction que ne le permettraient\nchaque modèle isolément.\nLes random forest sont une méthode d’aggrégation1 d’arbres de décision.\nOn calcule \\(K\\) arbres de décision et en tire, par une méthode d’agrégation,\nune règle de décision moyenne qu’on va appliquer pour tirer une\nprédiction de nos données.\n\n\n\n\n\nL’un des intérêts\ndes random forest est qu’il existe des méthodes pour déterminer\nl’importance relative de chaque variable dans la prédiction.\nNous allons ici partir d’un random forest avec des valeurs d’hyperparamètres\ndonnées, à savoir la profondeur de l’arbre.\n\nDéfinition des ensembles train et test\nNous allons donc nous restreindre à un sous-ensemble de colonnes dans un\npremier temps.\nNous allons également ne conserver que les\ntransactions inférieures à 5 millions\nd’euros (on anticipe que celles ayant un montant supérieur sont des transactions\nexceptionnelles dont le mécanisme de fixation du prix diffère)\n\nmutations2 = mutations.drop(\n    colonnes_surface.tolist() + [\"Date mutation\", \"lprix\"], # ajouter \"confinement\" si données 2020\n    axis = \"columns\"\n    ).copy()\n\nmutations2 = mutations2.loc[mutations2['Valeur fonciere'] &lt; 5e6] #keep only values below 5 millions\n\nmutations2.columns = mutations2.columns.str.replace(\" \", \"_\")\nmutations2  = mutations2.dropna(subset = ['dep','Code_type_local','month'])\n\nNotre pipeline va incorporer deux types de variables: les variables\ncatégorielles et les variables numériques.\nCes différents types vont bénéficier d’étapes de preprocessing\ndifférentes.\n\nnumeric_features = mutations2.columns[~mutations2.columns.isin(['dep','Code_type_local', 'month', 'Valeur_fonciere'])].tolist()\ncategorical_features = ['dep','Code_type_local','month']\n\nAu passage, nous avons abandonné la variable de code postal pour privilégier\nle département afin de réduire la dimension de notre jeu de données. Si on voulait\nvraiment avoir un bon modèle, il faudrait faire autrement car le code postal\nest probablement un très bon prédicteur du prix d’un bien, une fois que\nles caractéristiques du bien sont contrôlées.\n\n\n Exercice 1 : Découpage des échantillons\nNous allons stratifier notre échantillonage de train/test par département\nafin de tenir compte, de manière minimale, de la géographie.\nPour accélérer les calculs pour ce tutoriel, nous n’allons considérer que\n30% des transactions observées sur chaque département.\nVoici le code pour le faire:\nmutations2 = mutations2.groupby('dep').sample(frac = 0.1, random_state = 123)\nAvec la fonction adéquate de Scikit, faire un découpage de mutations2\nen train et test sets\nen suivant les consignes suivantes:\n\n20% des données dans l’échantillon de test ;\nL’échantillonnage est stratifié par départements ;\nPour avoir des résultats reproductibles, choisir une racine égale à 123.\n\n\n\n\n\nDéfinition du premier pipeline\nPour commencer, nous allons fixer la taille des arbres de décision avec\nl’hyperparamètre max_depth = 2.\nNotre pipeline va intégrer les étapes suivantes :\n\nPreprocessing :\n\nLes variables numériques vont être standardisées avec un StandardScaler.\nPour cela, nous allons utiliser la liste numeric_features définie précédemment.\nLes variables catégorielles vont être explosées avec un one hot encoding\n(méthode OneHotEncoder de scikit)\nPour cela, nous allons utiliser la liste categorical_features\n\nRandom forest : nous allons appliquer l’estimateur ad hoc de Scikit.\n\n\n\n Exercice 2 : Construction d'un premier pipeline formel\n\nInitialiser un random forest de profondeur 2. Fixer la racine à 123 pour avoir des résultats reproductibles.\nLa première étape du pipeline (nommer cette couche preprocessor) consiste à appliquer les étapes de preprocessing adaptées à chaque type de variables:\n\nPour les variables numériques, appliquer une étape d’imputation à la moyenne puis standardiser celles-ci\nPour les variables catégorielles, appliquer un one hot encoding\n\nAppliquer comme couche de sortie le modèle défini plus tôt.\n\n💡 Il est recommandé de s’aider de la documentation de Scikit. Si vous avez besoin d’un indice supplémentaire, consulter le pipeline présenté ci-dessous.\n\n\nA l’issue de cet exercice, nous devrions obtenir le pipeline suivant.\n\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer()),\n                                                                  ('standardscaler',\n                                                                   StandardScaler())]),\n                                                  ['Nombre_de_lots',\n                                                   'Nombre_pieces_principales',\n                                                   'surface']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse=False),\n                                                  ['dep', 'Code_type_local',\n                                                   'month'])])),\n                ('randomforest',\n                 RandomForestRegressor(max_depth=2, random_state=123))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer()),\n                                                                  ('standardscaler',\n                                                                   StandardScaler())]),\n                                                  ['Nombre_de_lots',\n                                                   'Nombre_pieces_principales',\n                                                   'surface']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse=False),\n                                                  ['dep', 'Code_type_local',\n                                                   'month'])])),\n                ('randomforest',\n                 RandomForestRegressor(max_depth=2, random_state=123))])preprocessor: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer()),\n                                                 ('standardscaler',\n                                                  StandardScaler())]),\n                                 ['Nombre_de_lots', 'Nombre_pieces_principales',\n                                  'surface']),\n                                ('onehotencoder',\n                                 OneHotEncoder(handle_unknown='ignore',\n                                               sparse=False),\n                                 ['dep', 'Code_type_local', 'month'])])pipeline['Nombre_de_lots', 'Nombre_pieces_principales', 'surface']SimpleImputerSimpleImputer()StandardScalerStandardScaler()onehotencoder['dep', 'Code_type_local', 'month']OneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse=False)RandomForestRegressorRandomForestRegressor(max_depth=2, random_state=123)\n\n\nNous avons construit ce pipeline sous forme de couches successives. La couche\nrandomforest prendra automatiquement le résultat de la couche preprocessor\nen input. La couche features permet d’introduire de manière relativement\nsimple (quand on a les bonnes méthodes) la complexité du preprocessing\nsur données réelles dont les types divergent.\nA cette étape, rien n’a encore été estimé.\nC’est très simple à mettre en oeuvre\navec un pipeline.\n\n\n Exercice 3 : Mise en oeuvre du pipeline\n\nEstimer les paramètres du modèle sur le jeu d’entraînement\nObserver la manière dont les données d’entraînement sont transformées\npar l’étape de preprocessing avec les méthodes adéquates sur 4 observations de X_train\ntirées aléatoirement\nUtiliser ce modèle pour prédire le prix sur l’échantillon de test. A partir de ces quelques prédictions,\nquel semble être le problème ?\nObserver la manière dont ce preprocessing peut s’appliquer sur deux exemples fictifs :\n\nUn appartement (code_type_local = 2) dans le 75, vendu au mois de mai, unique lot de la vente avec 3 pièces, faisant 75m² ;\nUne maison (code_type_local = 1) dans le 06, vendue en décembre, dans une transaction avec 2 lots. La surface complète est de 180m² et le bien comporte 6 pièces.\n\nDéduire sur ces deux exemples le prix prédit par le modèle.\nCalculer et interpréter le RMSE sur l’échantillon de test. Ce modèle est-il satisfaisant ?\n\n\n\n\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning:\n\n`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n\n\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer()),\n                                                                  ('standardscaler',\n                                                                   StandardScaler())]),\n                                                  ['Nombre_de_lots',\n                                                   'Nombre_pieces_principales',\n                                                   'surface']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse=False),\n                                                  ['dep', 'Code_type_local',\n                                                   'month'])])),\n                ('randomforest',\n                 RandomForestRegressor(max_depth=2, random_state=123))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer()),\n                                                                  ('standardscaler',\n                                                                   StandardScaler())]),\n                                                  ['Nombre_de_lots',\n                                                   'Nombre_pieces_principales',\n                                                   'surface']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse=False),\n                                                  ['dep', 'Code_type_local',\n                                                   'month'])])),\n                ('randomforest',\n                 RandomForestRegressor(max_depth=2, random_state=123))])preprocessor: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer()),\n                                                 ('standardscaler',\n                                                  StandardScaler())]),\n                                 ['Nombre_de_lots', 'Nombre_pieces_principales',\n                                  'surface']),\n                                ('onehotencoder',\n                                 OneHotEncoder(handle_unknown='ignore',\n                                               sparse=False),\n                                 ['dep', 'Code_type_local', 'month'])])pipeline['Nombre_de_lots', 'Nombre_pieces_principales', 'surface']SimpleImputerSimpleImputer()StandardScalerStandardScaler()onehotencoder['dep', 'Code_type_local', 'month']OneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse=False)RandomForestRegressorRandomForestRegressor(max_depth=2, random_state=123)\n\n\n\n\narray([282871.63598981, 301165.65351098, 301165.65351098, ...,\n       282871.63598981, 471048.40037679, 282871.63598981])\n\n\n\n\narray([642280.20111587, 282871.63598981])\n\n\n\n\n433497.6437239088\n\n\n\n\nVariable importance\nLes prédictions semblent avoir une assez faible variance, comme si des variables\nde seuils intervenaient. Nous allons donc devoir essayer de comprendre pourquoi.\nLa “variable importance”\nse réfère à la mesure de l’influence de chaque variable d’entrée sur la performance du modèle.\nL’impureté fait référence à l’incertitude ou à l’entropie présente dans un ensemble de données.\nDans le contexte des random forest, cette mesure est souvent calculée en évaluant la réduction moyenne de l’impureté des nœuds de décision causée par une variable spécifique.\nCette approche permet de quantifier l’importance des variables dans le processus de prise de décision du modèle, offrant ainsi des intuitions sur les caractéristiques les plus informatives pour la prédiction (plus de détails sur ce blog).\nOn ne va représenter, parmi notre ensemble important de colonnes, que celles\nqui ont une importance non nulle.\n\n\n Exercice 4 : Compréhension du modèle\n\nRécupérer la feature importance directement depuis la couche adaptée de votre pipeline\nUtiliser le code suivant pour calculer l’intervalle de confiance de cette mesure d’importance:\n\nstd = np.std([tree.feature_importances_ for tree in pipe['randomforest'].estimators_], axis=0)\n\nReprésenter les variables d’importance non nulle. Qu’en concluez-vous ?\n\n\n\nLe graphique d’importance des variables que vous devriez obtenir à l’issue\nde cet exercice est le suivant.\n\n\n\n\n\n\n\n\n\n\n\n&lt;Axes: title={'center': 'Feature importances using MDI'}, ylabel='Mean decrease in impurity'&gt;\n\n\nLes statistiques obtenues par le biais de cette variable importance\nsont un peu rudimentaires mais permettent déjà de comprendre\nle problème de notre modèle.\nOn voit donc que deux de nos variables déterminantes sont des effets fixes\ngéographiques (qui servent à ajuster de la différence de prix entre\nParis et les Hauts de Seine et le reste de la France), une autre variable\nest un effet fixe type de bien. Les deux variables qui pourraient introduire\nde la variabilité, à savoir la surface et, dans une moindre mesure, le\nnombre de lots, ont une importance moindre.\n\n\n Note\nIdéalement, on utiliserait Yellowbrick pour représenter l’importance des variables\nMais en l’état actuel du pipeline on a beaucoup de variables dont le poids\nest nul qui viennent polluer la visualisation. Vous pouvez\nconsulter la\ndocumentation de Yellowbrick sur ce sujet\n\n\nLes prédictions peuvent nous suggérer également\nqu’il y a un problème:\n\ncompar = pd.DataFrame([y_test, pipe.predict(X_test)]).T\ncompar.columns = ['obs','pred']\ncompar['diff'] = compar.obs - compar.pred\n\ng = sns.relplot(data = compar, x = 'obs', y = 'pred', color = \"royalblue\", alpha = 0.8)\ng.set(ylim=(0, 2e6), xlim=(0, 2e6),\n      title='Evaluating estimation error on test sample',\n      xlabel='Observed values',\n      ylabel='Predicted values')\ng.ax.axline(xy1=(0, 0), slope=1, color=\"red\", dashes=(5, 2))\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n&lt;matplotlib.lines.AxLine at 0x7f2b8a6defa0&gt;"
  },
  {
    "objectID": "content/modelisation/6_pipeline.html#restriction-du-champ-du-modèle",
    "href": "content/modelisation/6_pipeline.html#restriction-du-champ-du-modèle",
    "title": "Premier pas vers l’industrialisation avec les pipelines scikit",
    "section": "Restriction du champ du modèle",
    "text": "Restriction du champ du modèle\nMettre en oeuvre un bon modèle de prix au niveau France entière\nest complexe. Nous allons donc nous restreindre au champ suivant:\nles appartements dans Paris.\n\nmutations_paris = mutations.drop(\n    colonnes_surface.tolist() + [\"Date mutation\", \"lprix\"], # ajouter \"confinement\" si données 2020\n    axis = \"columns\"\n    ).copy()\n\nmutations_paris = mutations_paris.loc[mutations_paris['Valeur fonciere'] &lt; 5e6] #keep only values below 5 millions\n\nmutations_paris.columns = mutations_paris.columns.str.replace(\" \", \"_\")\nmutations_paris  = mutations_paris.dropna(subset = ['dep','Code_type_local','month'])\nmutations_paris = mutations_paris.loc[mutations_paris['dep'] == \"75\"]\nmutations_paris = mutations_paris.loc[mutations_paris['Code_type_local'] == 2].drop(['dep','Code_type_local'], axis = \"columns\")\nmutations_paris.loc[mutations_paris['surface']&gt;0]\n\n\n\n\n\n\n\n\nmonth\nValeur_fonciere\nNombre_de_lots\nNombre_pieces_principales\nsurface\n\n\n\n\n4272176\n1\n686000.0\n1\n4.0\n73.45\n\n\n4273807\n2\n400000.0\n1\n2.0\n41.16\n\n\n4275450\n3\n475000.0\n3\n2.0\n47.7\n\n\n4275883\n2\n396200.0\n2\n2.0\n47.14\n\n\n4275884\n2\n299500.0\n1\n1.0\n34.02\n\n\n...\n...\n...\n...\n...\n...\n\n\n4617521\n12\n1650000.0\n3\n6.0\n154.75\n\n\n4617532\n12\n525000.0\n2\n3.0\n46.65\n\n\n4617540\n12\n425000.0\n2\n1.0\n27.85\n\n\n4617549\n12\n330000.0\n2\n2.0\n34.32\n\n\n4617577\n3\n232650.0\n1\n1.0\n28.25\n\n\n\n\n27971 rows × 5 columns\n\n\n\n\n\n Exercice 4 : Pipeline plus simple\nReprendre les codes précédents et reconstruire notre pipeline sur\nla nouvelle base en mettant en oeuvre une méthode de boosting\nplutôt qu’une forêt aléatoire.\n\n\nA l’issue de cet exercice, vous devriez avoir des MDI proches\nde celles-ci:\n\n\n/opt/mamba/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning:\n\n`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n\n\n\n\n\n\n\n\n\n\n0\n\n\n\n\npipeline__Nombre_pieces_principales\n0.268176\n\n\npipeline__surface\n0.716371\n\n\nonehotencoder__month_1\n0.000309\n\n\nonehotencoder__month_2\n0.001060\n\n\nonehotencoder__month_3\n0.000967\n\n\nonehotencoder__month_4\n0.007498\n\n\nonehotencoder__month_5\n0.000000\n\n\nonehotencoder__month_6\n0.000153\n\n\nonehotencoder__month_7\n0.000156\n\n\nonehotencoder__month_8\n0.000819\n\n\nonehotencoder__month_9\n0.000026\n\n\nonehotencoder__month_10\n0.002170\n\n\nonehotencoder__month_11\n0.001690\n\n\nonehotencoder__month_12\n0.000605"
  },
  {
    "objectID": "content/modelisation/6_pipeline.html#recherche-des-hyperparamètres-optimaux-avec-une-validation-croisée",
    "href": "content/modelisation/6_pipeline.html#recherche-des-hyperparamètres-optimaux-avec-une-validation-croisée",
    "title": "Premier pas vers l’industrialisation avec les pipelines scikit",
    "section": "Recherche des hyperparamètres optimaux avec une validation croisée",
    "text": "Recherche des hyperparamètres optimaux avec une validation croisée\nOn détecte que le premier modèle n’est pas très bon et ne nous aidera\npas vraiment à évaluer de manière fiable l’appartement de nos rêves.\nOn va essayer de voir si notre modèle ne serait pas meilleur avec des\nhyperparamètres plus adaptés. Après tout, nous avons choisi par défaut\nla profondeur de l’arbre mais c’était un choix au doigt mouillé.\n❓️ Quels sont les hyperparamètres qu’on peut essayer d’optimiser ?\n\n\n{'alpha': 0.9,\n 'ccp_alpha': 0.0,\n 'criterion': 'friedman_mse',\n 'init': None,\n 'learning_rate': 0.1,\n 'loss': 'squared_error',\n 'max_depth': 3,\n 'max_features': None,\n 'max_leaf_nodes': None,\n 'min_impurity_decrease': 0.0,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'n_estimators': 100,\n 'n_iter_no_change': None,\n 'random_state': 0,\n 'subsample': 1.0,\n 'tol': 0.0001,\n 'validation_fraction': 0.1,\n 'verbose': 0,\n 'warm_start': False}\n\n\nUn détour par la documentation\nnous aide à comprendre ceux sur lesquels on va jouer. Par exemple, il serait\nabsurde de jouer sur le paramètre random_state qui est la racine du générateur\npseudo-aléatoire.\nNous allons nous contenter de jouer sur les paramètres:\n\nn_estimators: Le nombre d’arbres de décision que notre forêt contient\nmax_depth: La profondeur de chaque arbre\n\nIl existe plusieurs manières de faire de la validation croisée. Nous allons ici\nutiliser la grid search qui consiste à estimer et tester le modèle sur chaque\ncombinaison d’une grille de paramètres et sélectionner le couple de valeurs\ndes hyperparamètres amenant à la meilleure prédiction. Par défaut, scikit\neffectue une 5-fold cross validation. Nous n’allons pas changer\nce comportement.\nComme expliqué précédemment, les paramètres s’appelent sous la forme\n&lt;step&gt;__&lt;parameter_name&gt;\nLa validation croisée pouvant être très consommatrice de temps, nous\nn’allons l’effectuer que sur un nombre réduit de valeurs de notre grille.\nIl est possible de passer la liste des valeurs à passer au crible sous\nforme de liste\n(comme nous allons le proposer pour l’argument max_depth dans l’exercice ci-dessous) ou\nsous forme d’array (comme nous allons le proposer pour l’argument n_estimators) ce qui est\nsouvent pratique pour générer un criblage d’un intervalle avec np.linspace.\n\n\n Hint \nLes estimations sont, par défaut, menées de manière séquentielle (l’une après\nl’autre). Nous sommes cependant face à un problème\nembarassingly parallel.\nPour gagner en performance, il est recommandé d’utiliser l’argument\nn_jobs=-1.\n\n\n\n\nGridSearchCV(estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('pipeline',\n                                                                         Pipeline(steps=[('simpleimputer',\n                                                                                          SimpleImputer()),\n                                                                                         ('standardscaler',\n                                                                                          StandardScaler())]),\n                                                                         ['Nombre_pieces_principales',\n                                                                          'surface']),\n                                                                        ('onehotencoder',\n                                                                         OneHotEncoder(handle_unknown='ignore',\n                                                                                       sparse=False),\n                                                                         ['month'])])),\n                                       ('boosting',\n                                        GradientBoostingRegressor(random_state=0))]),\n             param_grid={'boosting__max_depth': [2, 4],\n                         'boosting__n_estimators': array([ 5, 10, 15, 20, 25])})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('pipeline',\n                                                                         Pipeline(steps=[('simpleimputer',\n                                                                                          SimpleImputer()),\n                                                                                         ('standardscaler',\n                                                                                          StandardScaler())]),\n                                                                         ['Nombre_pieces_principales',\n                                                                          'surface']),\n                                                                        ('onehotencoder',\n                                                                         OneHotEncoder(handle_unknown='ignore',\n                                                                                       sparse=False),\n                                                                         ['month'])])),\n                                       ('boosting',\n                                        GradientBoostingRegressor(random_state=0))]),\n             param_grid={'boosting__max_depth': [2, 4],\n                         'boosting__n_estimators': array([ 5, 10, 15, 20, 25])})estimator: PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer()),\n                                                                  ('standardscaler',\n                                                                   StandardScaler())]),\n                                                  ['Nombre_pieces_principales',\n                                                   'surface']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse=False),\n                                                  ['month'])])),\n                ('boosting', GradientBoostingRegressor(random_state=0))])preprocessor: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer()),\n                                                 ('standardscaler',\n                                                  StandardScaler())]),\n                                 ['Nombre_pieces_principales', 'surface']),\n                                ('onehotencoder',\n                                 OneHotEncoder(handle_unknown='ignore',\n                                               sparse=False),\n                                 ['month'])])pipeline['Nombre_pieces_principales', 'surface']SimpleImputerSimpleImputer()StandardScalerStandardScaler()onehotencoder['month']OneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse=False)GradientBoostingRegressorGradientBoostingRegressor(random_state=0)\n\n\n\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer()),\n                                                                  ('standardscaler',\n                                                                   StandardScaler())]),\n                                                  ['Nombre_pieces_principales',\n                                                   'surface']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse=False),\n                                                  ['month'])])),\n                ('boosting',\n                 GradientBoostingRegressor(max_depth=4, n_estimators=25,\n                                           random_state=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('pipeline',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer()),\n                                                                  ('standardscaler',\n                                                                   StandardScaler())]),\n                                                  ['Nombre_pieces_principales',\n                                                   'surface']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse=False),\n                                                  ['month'])])),\n                ('boosting',\n                 GradientBoostingRegressor(max_depth=4, n_estimators=25,\n                                           random_state=0))])preprocessor: ColumnTransformerColumnTransformer(transformers=[('pipeline',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer()),\n                                                 ('standardscaler',\n                                                  StandardScaler())]),\n                                 ['Nombre_pieces_principales', 'surface']),\n                                ('onehotencoder',\n                                 OneHotEncoder(handle_unknown='ignore',\n                                               sparse=False),\n                                 ['month'])])pipeline['Nombre_pieces_principales', 'surface']SimpleImputerSimpleImputer()StandardScalerStandardScaler()onehotencoder['month']OneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse=False)GradientBoostingRegressorGradientBoostingRegressor(max_depth=4, n_estimators=25, random_state=0)\n\n\nToutes les performances sur les ensembles d’échantillons et de test sur la grille\nd’hyperparamètres sont disponibles dans l’attribut:\nRegardons les résultats moyens pour chaque valeur des hyperparamètres:\n\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n/opt/mamba/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n\n\n&lt;matplotlib.legend.Legend at 0x7f2b8a2ed970&gt;\n\n\n\n\n\n\n\n\n\nGlobalement, à profondeur d’arbre donnée, le nombre d’arbres affecte\nla performance. Changer la profondeur de l’arbre améliore la\nperformance de manière plus marquée.\nMaintenant, il nous reste à re-entraîner le modèle avec ces nouveaux\nparamètres sur l’ensemble du jeu de train et l’évaluer sur l’ensemble\ndu jeu de test:\n\npipe_optimal = grid_search.best_estimator_\npipe_optimal.fit(X_train, y_train)\n\ncompar = pd.DataFrame([y_test, pipe_optimal.predict(X_test)]).T\ncompar.columns = ['obs','pred']\ncompar['diff'] = compar.obs - compar.pred\n\nOn obtient le RMSE suivant :\n\n\nLe RMSE sur le jeu de test est 562,317\n\n\nEt si on regarde la qualité en prédiction:\nOn obtient plus de variance dans la prédiction, c’est déjà un peu mieux.\nCependant, cela reste décevant pour plusieurs raisons:\n\nnous n’avons pas fait d’étape de sélection de variable\nnous n’avons pas chercher à déterminer si la variable à prédire la plus\npertinente était le prix ou une transformation de celle-ci\n(par exemple le prix au \\(m^2\\))"
  },
  {
    "objectID": "content/modelisation/6_pipeline.html#prochaine-étape",
    "href": "content/modelisation/6_pipeline.html#prochaine-étape",
    "title": "Premier pas vers l’industrialisation avec les pipelines scikit",
    "section": "Prochaine étape",
    "text": "Prochaine étape\nNous avons un modèle certes perfectible mais fonctionnel.\nLa question qui se pose maintenant c’est d’essayer d’en faire\nquelque chose au service des utilisateurs. Cela nous amène vers\nla question de la mise en production.\nCeci est l’objet du prochain chapitre. Il s’agira d’une version introductive\ndes enjeux évoqués dans le cadre du cours de\n3e année de mise en production de projets de data science."
  },
  {
    "objectID": "content/modelisation/6_pipeline.html#footnotes",
    "href": "content/modelisation/6_pipeline.html#footnotes",
    "title": "Premier pas vers l’industrialisation avec les pipelines scikit",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLes random forest sont l’une des principales méthodes\nensemblistes. Outre cette approche, les plus connues sont\nle bagging (boostrap aggregating) et le boosting\nqui consistent à choisir la prédiction à privilégier\nselon des algorithmes de choix différens.\nPar exemple le bagging est une technique basée sur le vote majoritaire (Breiman 1996).\nCette technique s’inspire du bootstrap qui, en économétrie,\nconsiste à ré-estimer sur K sous-échantillons\naléatoires des données un estimateur afin d’en tirer, par exemple, un intervalle\nde confiance empirique à 95%. Le principe du bagging est le même. On ré-estime\nK fois notre estimateur (par exemple un arbre de décision) et propose une\nrègle d’agrégation pour en tirer une règle moyennisée et donc une prédiction.\nLe boosting fonctionne selon un principe différent, basé sur\nl’optimisation de combinaisons de classifieurs faibles.↩︎"
  }
]