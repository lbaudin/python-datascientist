<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Lino Galiana">
<meta name="dcterms.date" content="2020-10-29">
<meta name="description" content="Pour pouvoir utiliser des données textuelles dans des algorithmes de machine learning, il faut les vectoriser, c’est à dire transformer le texte en données numériques. Dans ce TP, nous allons comparer différentes méthodes de vectorisation, à travers une tâche de prédiction : peut-on prédire un auteur littéraire à partir d’extraits de ses textes ? Parmi ces méthodes, on va notamment explorer le modèle Word2Vec, qui permet d’exploiter les structures latentes d’un texte en construisant des word embeddings (plongements de mots).">

<title>Python pour la data science - Méthodes de vectorisation : comptages et word embeddings</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../content/NLP/05_exo_supp.html" rel="next">
<link href="../../content/NLP/03_lda.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>

      .quarto-title-block .quarto-title-banner {
        background: #e9f3fa;
      }
</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
<meta name="twitter:title" content="Python pour la data science - Méthodes de vectorisation : comptages et word embeddings">
<meta name="twitter:description" content="Pour pouvoir utiliser des données textuelles dans des algorithmes
de machine learning, il faut les vectoriser, c’est à dire transformer
le texte en données numériques. Dans ce TP, nous allons comparer
différentes méthodes de vectorisation, à travers une tâche de prédiction :
peut-on prédire un auteur littéraire à partir d’extraits de ses textes ?
Parmi ces méthodes, on va notamment explorer le modèle Word2Vec, qui
permet d’exploiter les structures latentes d’un texte en construisant
des word embeddings (plongements de mots).">
<meta name="twitter:image" content="https://pythonds.linogaliana.fr/content/NLP/featured_intro.png">
<meta name="twitter:image-height" content="746">
<meta name="twitter:image-width" content="1182">
<meta name="twitter:card" content="summary_large_image">
<meta name="citation_title" content="Python pour la data science">
<meta name="citation_author" content="Lino Galiana">
<meta name="citation_publication_date" content="2023">
<meta name="citation_cover_date" content="2023">
<meta name="citation_year" content="2023">
<meta name="citation_online_date" content="2020-10-29">
<meta name="citation_fulltext_html_url" content="https://pythonds.linogaliana.fr/">
<meta name="citation_doi" content="10.5281/zenodo.8229676">
<meta name="citation_language" content="en">
</head>

<body class="nav-sidebar floating nav-fixed">


<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Python pour la data science</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-introduction" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Introduction</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-introduction">    
        <li>
    <a class="dropdown-item" href="../../content/getting-started/index.html" rel="" target="">
 <span class="dropdown-text">Introduction</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/getting-started/01_installation.html" rel="" target="">
 <span class="dropdown-text">Configuration de Python</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/getting-started/02_DS_environment.html" rel="" target="">
 <span class="dropdown-text">L’environnement Python pour la data science</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/getting-started/03_data_analysis.html" rel="" target="">
 <span class="dropdown-text">Comment aborder un jeu de données</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/getting-started/04_python_practice.html" rel="" target="">
 <span class="dropdown-text">Bonne pratique de Python</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/getting-started/05_rappels_types.html" rel="" target="">
 <span class="dropdown-text">Quelques rappels sur les principes de base de Python</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/getting-started/06_rappels_fonctions.html" rel="" target="">
 <span class="dropdown-text">Modules, tests, boucles, fonctions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/getting-started/07_rappels_classes.html" rel="" target="">
 <span class="dropdown-text">Les classes en Python</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-manipuler" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Manipuler</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-manipuler">    
        <li>
    <a class="dropdown-item" href="../../content/manipulation/index.html" rel="" target="">
 <span class="dropdown-text">Introduction</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/manipulation/01_numpy.html" rel="" target="">
 <span class="dropdown-text">Numpy, la brique de base de la data science</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/manipulation/02a_pandas_tutorial.html" rel="" target="">
 <span class="dropdown-text">Introduction à Pandas</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/manipulation/02b_pandas_TP.html" rel="" target="">
 <span class="dropdown-text">Pratique de pandas : un exemple complet</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/manipulation/03_geopandas_tutorial.html" rel="" target="">
 <span class="dropdown-text">Données spatiales : découverte de geopandas</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/manipulation/03_geopandas_TP.html" rel="" target="">
 <span class="dropdown-text">Pratique de geopandas avec les données vélib</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/manipulation/04a_webscraping_TP.html" rel="" target="">
 <span class="dropdown-text">Web scraping avec Python</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/manipulation/04c_API_TP.html" rel="" target="">
 <span class="dropdown-text">Récupérer des données avec des API depuis Python</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/manipulation/04b_regex_TP.html" rel="" target="">
 <span class="dropdown-text">Maîtriser les expressions régulières</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/manipulation/07_dask.html" rel="" target="">
 <span class="dropdown-text">Introduction à dask grâce aux données DVF</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-communiquer" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Communiquer</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-communiquer">    
        <li>
    <a class="dropdown-item" href="../../content/visualisation/index.html" rel="" target="">
 <span class="dropdown-text">Introduction</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/visualisation/matplotlib.html" rel="" target="">
 <span class="dropdown-text">De beaux graphiques avec python : mise en pratique</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/visualisation/maps.html" rel="" target="">
 <span class="dropdown-text">De belles cartes avec python : mise en pratique</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-modéliser" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Modéliser</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-modéliser">    
        <li>
    <a class="dropdown-item" href="../../content/modelisation/index.html" rel="" target="">
 <span class="dropdown-text">Introduction</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/modelisation/0_preprocessing.html" rel="" target="">
 <span class="dropdown-text">Préparation des données pour construire un modèle</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/modelisation/1_modelevaluation.html" rel="" target="">
 <span class="dropdown-text">Evaluer la qualité d’un modèle</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/modelisation/2_SVM.html" rel="" target="">
 <span class="dropdown-text">Classification: premier modèle avec les SVM</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/modelisation/3_regression.html" rel="" target="">
 <span class="dropdown-text">Régression : une introduction</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/modelisation/4_featureselection.html" rel="" target="">
 <span class="dropdown-text">Sélection de variables : une introduction</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/modelisation/5_clustering.html" rel="" target="">
 <span class="dropdown-text">Clustering</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/modelisation/6_pipeline.html" rel="" target="">
 <span class="dropdown-text">Premier pas vers l’industrialisation avec les pipelines scikit</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-approfondissements" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Approfondissements</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-approfondissements">    
        <li>
    <a class="dropdown-item" href="../../content/modern-ds/index.html" rel="" target="">
 <span class="dropdown-text">Introduction</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/modern-ds/continuous_integration.html" rel="" target="">
 <span class="dropdown-text">Intégration continue avec Python</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/modern-ds/dallE.html" rel="" target="">
 <span class="dropdown-text">Génération d’images avec Python, DALL-E et StableDiffusion</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/modern-ds/s3.html" rel="" target="">
 <span class="dropdown-text">Les nouveaux modes d’accès aux données : le format parquet et les données sur le cloud</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/modern-ds/elastic_approfondissement.html" rel="" target="">
 <span class="dropdown-text">Approfondissement ElasticSearch pour des recherches de proximité géographique</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/modern-ds/elastic_intro.html" rel="" target="">
 <span class="dropdown-text">Introduction à ElasticSearch pour la recherche textuelle</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-git" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Git</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-git">    
        <li>
    <a class="dropdown-item" href="../../content/git/index.html" rel="" target="">
 <span class="dropdown-text">Introduction</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/git/introgit.html" rel="" target="">
 <span class="dropdown-text">Git : un élément essentiel au quotidien</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/git/exogit.html" rel="" target="">
 <span class="dropdown-text">Un cadavre exquis pour découvrir Git</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-annexes" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Annexes</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-annexes">    
        <li>
    <a class="dropdown-item" href="../../content/annexes/evaluation.html" rel="" target="">
 <span class="dropdown-text">Evaluation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../content/annexes/corrections.html" rel="" target="">
 <span class="dropdown-text">Corrections</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://linogaliana.github.io/python-datascientist-slides/#/title-slide" rel="" target="">
 <span class="dropdown-text">Slides de présentation</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <div class="quarto-navbar-tools ms-auto">
    <div class="dropdown">
      <a href="" title="" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label=""><i class="bi bi-github"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="https://github.com/linogaliana/python-datascientist">
            Code source
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../content/NLP/04_word2vec.html">Méthodes de vectorisation : comptages et word embeddings</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns">
                <div id="title-block-header-title" class="quarto-title page-columns page-full page-layout-full featured-image p-4" style="background-image: url(featured_intro.png);">
                <h1 class="title">Méthodes de vectorisation : comptages et word embeddings</h1>
                                    <div class="quarto-categories">
                        <div class="quarto-category">Tutoriel</div>
                        <div class="quarto-category">NLP</div>
                      </div>
                          </div>
        
                <div>
          <div id="title-block-title-desc" class="description pt-4">
            <p>Pour pouvoir utiliser des données textuelles dans des algorithmes
            de <em>machine learning</em>, il faut les vectoriser, c’est à dire transformer
            le texte en données numériques. Dans ce TP, nous allons comparer
            différentes méthodes de vectorisation, à travers une tâche de prédiction :
            <em>peut-on prédire un auteur littéraire à partir d’extraits de ses textes ?</em>
            Parmi ces méthodes, on va notamment explorer le modèle <code>Word2Vec</code>, qui
            permet d’exploiter les structures latentes d’un texte en construisant
            des <em>word embeddings</em> (plongements de mots).</p>
          </div>
        </div>
                
        
        <div class="quarto-title-meta">

            <div>
            <div class="quarto-title-meta-heading">Author</div>
            <div class="quarto-title-meta-contents">
                     <p>Lino Galiana </p>
                  </div>
          </div>
            
            <div>
            <div class="quarto-title-meta-heading">Published</div>
            <div class="quarto-title-meta-contents">
              <p class="date">October 29, 2020</p>
            </div>
          </div>
          
            
          </div>
          
        
        
        

        </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../content/NLP/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../content/NLP/01_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quelques éléments pour comprendre les enjeux du NLP</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../content/NLP/02_exoclean.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Nettoyer un texte: des exercices pour découvrir l’approche bag-of-words</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../content/NLP/03_lda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Latent Dirichlet Allocation (LDA)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../content/NLP/04_word2vec.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Méthodes de vectorisation : comptages et word embeddings</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../content/NLP/05_exo_supp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercices supplémentaires</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#nettoyage-des-données" id="toc-nettoyage-des-données" class="nav-link active" data-scroll-target="#nettoyage-des-données">Nettoyage des données</a>
  <ul class="collapse">
  <li><a href="#preprocessing" id="toc-preprocessing" class="nav-link" data-scroll-target="#preprocessing">Preprocessing</a></li>
  <li><a href="#encodage-de-la-variable-à-prédire" id="toc-encodage-de-la-variable-à-prédire" class="nav-link" data-scroll-target="#encodage-de-la-variable-à-prédire">Encodage de la variable à prédire</a></li>
  <li><a href="#construction-des-bases-dentraînement-et-de-test" id="toc-construction-des-bases-dentraînement-et-de-test" class="nav-link" data-scroll-target="#construction-des-bases-dentraînement-et-de-test">Construction des bases d’entraînement et de test</a></li>
  </ul></li>
  <li><a href="#statistiques-exploratoires" id="toc-statistiques-exploratoires" class="nav-link" data-scroll-target="#statistiques-exploratoires">Statistiques exploratoires</a>
  <ul class="collapse">
  <li><a href="#répartition-des-labels" id="toc-répartition-des-labels" class="nav-link" data-scroll-target="#répartition-des-labels">Répartition des labels</a></li>
  <li><a href="#mots-les-plus-fréquemment-utilisés-par-chaque-auteur" id="toc-mots-les-plus-fréquemment-utilisés-par-chaque-auteur" class="nav-link" data-scroll-target="#mots-les-plus-fréquemment-utilisés-par-chaque-auteur">Mots les plus fréquemment utilisés par chaque auteur</a></li>
  </ul></li>
  <li><a href="#prédiction-sur-le-set-dentraînement" id="toc-prédiction-sur-le-set-dentraînement" class="nav-link" data-scroll-target="#prédiction-sur-le-set-dentraînement">Prédiction sur le set d’entraînement</a>
  <ul class="collapse">
  <li><a href="#démarche" id="toc-démarche" class="nav-link" data-scroll-target="#démarche">Démarche</a></li>
  <li><a href="#pipeline-de-prédiction" id="toc-pipeline-de-prédiction" class="nav-link" data-scroll-target="#pipeline-de-prédiction">Pipeline de prédiction</a></li>
  </ul></li>
  <li><a href="#approche-bag-of-words" id="toc-approche-bag-of-words" class="nav-link" data-scroll-target="#approche-bag-of-words">Approche <em>bag-of-words</em></a></li>
  <li><a href="#tf-idf" id="toc-tf-idf" class="nav-link" data-scroll-target="#tf-idf">TF-IDF</a></li>
  <li><a href="#word2vec-avec-averaging" id="toc-word2vec-avec-averaging" class="nav-link" data-scroll-target="#word2vec-avec-averaging">Word2vec avec averaging</a></li>
  <li><a href="#word2vec-pré-entraîné-averaging" id="toc-word2vec-pré-entraîné-averaging" class="nav-link" data-scroll-target="#word2vec-pré-entraîné-averaging">Word2vec pré-entraîné + averaging</a></li>
  <li><a href="#contextual-embeddings" id="toc-contextual-embeddings" class="nav-link" data-scroll-target="#contextual-embeddings">Contextual embeddings</a></li>
  <li><a href="#aller-plus-loin" id="toc-aller-plus-loin" class="nav-link" data-scroll-target="#aller-plus-loin">Aller plus loin</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/linogaliana/python-datascientist/edit/master/content/NLP/04_word2vec.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/linogaliana/python-datascientist/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">

<!-- source: https://github.com/gadenbuie/garrickadenbuie-com/blob/main/_partials/title-block-link-buttons/title-block.html -->
<!-- source: https://raw.githubusercontent.com/spcanelon/silvia/32853dd0e70a71514b0922ee306f80ed5897f776/_partials/title-block-link-buttons/title-block.html -->
<!-- 
<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title"> 
-->
<!-- <header id="title-block-header" class="quarto-title-block default page-columns"> -->
<!-- <div class="quarto-title page-columns page-full featured-image p-4" style="background-image: url(featured.png), url(featured.jpg), url(../featured.jpg);"> -->
    

<div class="cell markdown">
<p class="badges">
<a href="https://github.com/linogaliana/python-datascientist-notebooks/blob/main/notebooks/NLP/04_word2vec.ipynb" class="github"><i class="fab fa-github"></i></a>
<a href="https://downgit.github.io/#/home?url=https://github.com/linogaliana/python-datascientist-notebooks/blob/main/notebooks/NLP/04_word2vec.ipynb" target="_blank" rel="noopener"><img src="https://img.shields.io/badge/Download-Notebook-important?logo=Jupyter" alt="Download"></a>
<a href="https://nbviewer.jupyter.org/github/linogaliana/python-datascientist-notebooksblob/main/notebooks/NLP/04_word2vec.ipynb" target="_blank" rel="noopener"><img src="https://img.shields.io/badge/Visualize-nbviewer-blue?logo=Jupyter" alt="nbviewer"></a>
<a href="https://datalab.sspcloud.fr/launcher/ide/jupyter-python?autoLaunch=true&amp;onyxia.friendlyName=%C2%ABpython-datascience%C2%BB&amp;init.personalInit=%C2%ABhttps%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmaster%2Fsspcloud%2Finit-jupyter.sh%C2%BB&amp;init.personalInitArgs=%C2%ABNLP%2004_word2vec%C2%BB&amp;security.allowlist.enabled=false" target="_blank" rel="noopener"><img src="https://img.shields.io/badge/SSP%20Cloud-Tester_avec_Jupyter-orange?logo=Jupyter&amp;logoColor=orange" alt="Onyxia"></a>
<a href="https://datalab.sspcloud.fr/launcher/ide/vscode-python?autoLaunch=true&amp;onyxia.friendlyName=%C2%ABpython-datascience%C2%BB&amp;init.personalInit=%C2%ABhttps%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmaster%2Fsspcloud%2Finit-vscode.sh%C2%BB&amp;init.personalInitArgs=%C2%ABNLP%2004_word2vec%C2%BB&amp;security.allowlist.enabled=false" target="_blank" rel="noopener"><img src="https://img.shields.io/badge/SSP%20Cloud-Tester_avec_VSCode-blue?logo=visualstudiocode&amp;logoColor=blue" alt="Onyxia"></a><br>
<a href="https://mybinder.org/v2/gh/linogaliana/python-datascientist-notebooks/main?filepath={binder_path}" target="_blank" rel="noopener"><img src="https://img.shields.io/badge/Launch-Binder-E66581.svg?logo=data:image/png;base64,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" alt="Binder"></a>
<a href="https://colab.research.google.com/github/linogaliana/python-datascientist-notebooks/blob/main/notebooks/NLP/04_word2vec.ipynb" target="_blank" rel="noopener"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a>
<a href="https://github.dev/linogaliana/python-datascientist-notebooks/notebooks/NLP/04_word2vec.ipynb" target="_blank" rel="noopener"><img src="https://img.shields.io/static/v1?logo=visualstudiocode&amp;label=&amp;message=Open%20in%20Visual%20Studio%20Code&amp;labelColor=2c2c32&amp;color=007acc&amp;logoColor=007acc" alt="githubdev"></a>
</p>
<p></p>
</div>
<p>Cette page approfondit certains aspects présentés dans la
<a href="#nlp">partie introductive</a>. Après avoir travaillé sur le
<em>Comte de Monte Cristo</em>, on va continuer notre exploration de la littérature
avec cette fois des auteurs anglophones:</p>
<ul>
<li>Edgar Allan Poe, (EAP) ;</li>
<li>HP Lovecraft (HPL) ;</li>
<li>Mary Wollstonecraft Shelley (MWS).</li>
</ul>
<p>Les données sont disponibles ici : <a href="https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/blob/master/data/spooky.csv">spooky.csv</a> et peuvent être requétées via l’url
<a href="https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv" class="uri">https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv</a>.</p>
<p>Le but va être dans un premier temps de regarder dans le détail les termes les plus fréquents utilisés par les auteurs, de les représenter graphiquement puis on va ensuite essayer de prédire quel texte correspond à quel auteur à partir de différents modèles de vectorisation, notamment les <em>word embeddings</em>.</p>
<p>Ce notebook est librement inspiré de :</p>
<ul>
<li>https://www.kaggle.com/enerrio/scary-nlp-with-spacy-and-keras</li>
<li>https://github.com/GU4243-ADS/spring2018-project1-ginnyqg</li>
<li>https://www.kaggle.com/meiyizi/spooky-nlp-and-topic-modelling-tutorial/notebook</li>
</ul>
<div class="cell markdown">
<div class="alert alert-danger" role="alert">
<i class="fa-solid fa-triangle-exclamation"></i> Warning
<p>Comme dans la <a href="#nlp">partie précédente</a>, il faut télécharger quelques éléments
pour que nos librairies de NLP puissent fonctionner correctement.</p>
<p>En premier lieu, il convient d’installer les librairies adéquates
(<code>spacy</code>, <code>gensim</code> et <code>sentence_transformers</code>):</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install spacy gensim sentence_transformers</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Ensuite, comme nous allons utiliser également <code>spacy</code>, il convient de télécharger
le corpus Anglais. Pour cela, on peut se référer à
<a href="https://spacy.io/usage/models">la documentation de <code>spacy</code></a>,
extrêmement bien faite.</p>
<ul>
<li>Idéalement, il faut installer le module via la ligne de commande. Dans
une cellule de notebook <code>Jupyter</code>, faire :</li>
</ul>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>python <span class="op">-</span>m spacy download en_core_web_sm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>Sans accès à la ligne de commande (depuis une instance <code>Docker</code> par exemple),
faire :</li>
</ul>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>spacy.cli.download(<span class="st">"en_core_web_sm"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>Sinon, il est également possible d’installer le module en faisant pointer
<code>pip install</code> vers le fichier adéquat sur
<a href="https://github.com/explosion/spacy-models"><code>Github</code></a>. Pour cela, taper</li>
</ul>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>pip install https:<span class="op">//</span>github.com<span class="op">/</span>explosion<span class="op">/</span>spacy<span class="op">-</span>models<span class="op">/</span>releases<span class="op">/</span>download<span class="op">/</span>en_core_web_sm<span class="op">-</span><span class="fl">3.0.0</span><span class="op">/</span>en_core_web_sm<span class="op">-</span><span class="fl">3.0.0</span><span class="op">-</span>py3<span class="op">-</span>none<span class="op">-</span><span class="bu">any</span>.whl</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> LinearSVC</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> CountVectorizer, TfidfVectorizer</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV, cross_val_score</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models.word2vec <span class="im">import</span> Word2Vec</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim.downloader</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sentence_transformers <span class="im">import</span> SentenceTransformer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="nettoyage-des-données" class="level2">
<h2 class="anchored" data-anchor-id="nettoyage-des-données">Nettoyage des données</h2>
<p>Nous allons ainsi à nouveau utiliser le jeu de données <code>spooky</code>:</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>data_url <span class="op">=</span> <span class="st">'https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv'</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>spooky_df <span class="op">=</span> pd.read_csv(data_url)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Le jeu de données met ainsi en regard un auteur avec une phrase qu’il a écrite:</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>spooky_df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="preprocessing" class="level3">
<h3 class="anchored" data-anchor-id="preprocessing">Preprocessing</h3>
<p>En NLP, la première étape est souvent celle du <em>preprocessing</em>, qui inclut notamment les étapes de tokenization et de nettoyage du texte. Comme celles-ci ont été vues en détail dans le précédent chapitre, on se contentera ici d’un <em>preprocessing</em> minimaliste : suppression de la ponctuation et des <em>stop words</em> (pour la visualisation et les méthodes de vectorisation basées sur des comptages).</p>
<p>Jusqu’à présent, nous avons utilisé principalement <code>nltk</code> pour le
<em>preprocessing</em> de données textuelles. Cette fois, nous proposons
d’utiliser la librairie <code>spaCy</code> qui permet de mieux automatiser sous forme de
<em>pipelines</em> de <em>preprocessing</em>.</p>
<p>Pour initialiser le processus de nettoyage,
on va utiliser le corpus <code>en_core_web_sm</code> (voir plus
haut pour l’installation de ce corpus):</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">'en_core_web_sm'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>On va utiliser un <code>pipe</code> <code>spacy</code> qui permet d’automatiser, et de paralléliser,
un certain nombre d’opérations. Les <em>pipes</em> sont l’équivalent, en NLP, de
nos <em>pipelines</em> <code>scikit</code> ou des <em>pipes</em> <code>pandas</code>. Il s’agit donc d’un outil
très approprié pour industrialiser un certain nombre d’opérations de
<em>preprocessing</em> :</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> clean_docs(texts, remove_stopwords<span class="op">=</span><span class="va">False</span>, n_process <span class="op">=</span> <span class="dv">4</span>):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    docs <span class="op">=</span> nlp.pipe(texts, </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>                    n_process<span class="op">=</span>n_process,</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>                    disable<span class="op">=</span>[<span class="st">'parser'</span>, <span class="st">'ner'</span>,</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>                             <span class="st">'lemmatizer'</span>, <span class="st">'textcat'</span>])</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    stopwords <span class="op">=</span> nlp.Defaults.stop_words</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    docs_cleaned <span class="op">=</span> []</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> doc <span class="kw">in</span> docs:</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> [tok.text.lower().strip() <span class="cf">for</span> tok <span class="kw">in</span> doc <span class="cf">if</span> <span class="kw">not</span> tok.is_punct]</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> remove_stopwords:</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>            tokens <span class="op">=</span> [tok <span class="cf">for</span> tok <span class="kw">in</span> tokens <span class="cf">if</span> tok <span class="kw">not</span> <span class="kw">in</span> stopwords]</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        doc_clean <span class="op">=</span> <span class="st">' '</span>.join(tokens)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        docs_cleaned.append(doc_clean)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> docs_cleaned</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>On applique la fonction <code>clean_docs</code> à notre colonne <code>pandas</code>.
Les <code>pandas.Series</code> étant itérables, elles se comportent comme des listes et
fonctionnent ainsi très bien avec notre <code>pipe</code> <code>spacy</code></p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>spooky_df[<span class="st">'text_clean'</span>] <span class="op">=</span> clean_docs(spooky_df[<span class="st">'text'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>spooky_df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="encodage-de-la-variable-à-prédire" class="level3">
<h3 class="anchored" data-anchor-id="encodage-de-la-variable-à-prédire">Encodage de la variable à prédire</h3>
<p>On réalise un simple encodage de la variable à prédire :
il y a trois catégories (auteurs), représentées par des entiers 0, 1 et 2.</p>
<p>Pour cela, on utilise le <code>LabelEncoder</code> de <code>scikit</code> déjà présenté
dans la <a href="#preprocessing">partie modélisation</a>. On va utiliser la méthode
<code>fit_transform</code> qui permet, en un tour de main, d’appliquer à la fois
l’entraînement (<code>fit</code>), à savoir la création d’une correspondance entre valeurs
numériques et <em>labels</em>, et l’appliquer (<code>transform</code>) à la même colonne.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>le <span class="op">=</span> LabelEncoder()</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>spooky_df[<span class="st">'author_encoded'</span>] <span class="op">=</span> le.fit_transform(spooky_df[<span class="st">'author'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>On peut vérifier les classes de notre <code>LabelEncoder</code> :</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>le.classes_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="construction-des-bases-dentraînement-et-de-test" class="level3">
<h3 class="anchored" data-anchor-id="construction-des-bases-dentraînement-et-de-test">Construction des bases d’entraînement et de test</h3>
<p>On met de côté un échantillon de test (20 %) avant toute analyse (même descriptive).
Cela permettra d’évaluer nos différents modèles toute à la fin de manière très rigoureuse,
puisque ces données n’auront jamais utilisées pendant l’entraînement.</p>
<p>Notre échantillon initial n’est pas équilibré (<em>balanced</em>) : on retrouve plus d’oeuvres de
certains auteurs que d’autres. Afin d’obtenir un modèle qui soit évalué au mieux, nous allons donc stratifier notre échantillon de manière à obtenir une répartition similaire d’auteurs dans nos
ensembles d’entraînement et de test.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(spooky_df[<span class="st">'text_clean'</span>].values, </span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>                                                    spooky_df[<span class="st">'author_encoded'</span>].values, </span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>                                                    test_size<span class="op">=</span><span class="fl">0.2</span>, </span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>                                                    random_state<span class="op">=</span><span class="dv">33</span>,</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>                                                    stratify <span class="op">=</span> spooky_df[<span class="st">'author_encoded'</span>].values)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Par exemple, les textes d’EAP représentent 40 % des échantillons d’entraînement et de test :</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="dv">100</span><span class="op">*</span>y_train.tolist().count(<span class="dv">0</span>)<span class="op">/</span>(<span class="bu">len</span>(y_train)))</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="dv">100</span><span class="op">*</span>y_test.tolist().count(<span class="dv">0</span>)<span class="op">/</span>(<span class="bu">len</span>(y_test)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Aperçu du premier élément de <code>X_train</code> :</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>X_train[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>On peut aussi vérifier qu’on est capable de retrouver
la correspondance entre nos auteurs initiaux avec
la méthode <code>inverse_transform</code></p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(y_train[<span class="dv">0</span>], le.inverse_transform([y_train[<span class="dv">0</span>]])[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="statistiques-exploratoires" class="level2">
<h2 class="anchored" data-anchor-id="statistiques-exploratoires">Statistiques exploratoires</h2>
<section id="répartition-des-labels" class="level3">
<h3 class="anchored" data-anchor-id="répartition-des-labels">Répartition des labels</h3>
<p>Refaisons un graphique que nous avons déjà produit précédemment pour voir
la répartition de notre corpus entre auteurs:</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> pd.Series(le.inverse_transform(y_train)).value_counts().plot(kind<span class="op">=</span><span class="st">'bar'</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>fig</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>On observe une petite asymétrie : les passages des livres d’Edgar Allen Poe sont plus nombreux que ceux des autres auteurs dans notre corpus d’entraînement, ce qui peut être problématique dans le cadre d’une tâche de classification.
L’écart n’est pas dramatique, mais on essaiera d’en tenir compte dans l’analyse en choisissant une métrique d’évaluation pertinente.</p>
</section>
<section id="mots-les-plus-fréquemment-utilisés-par-chaque-auteur" class="level3">
<h3 class="anchored" data-anchor-id="mots-les-plus-fréquemment-utilisés-par-chaque-auteur">Mots les plus fréquemment utilisés par chaque auteur</h3>
<p>On va supprimer les <em>stopwords</em> pour réduire le bruit dans notre jeu
de données.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Suppression des stop words</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>X_train_no_sw <span class="op">=</span> clean_docs(X_train, remove_stopwords<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>X_train_no_sw <span class="op">=</span> np.array(X_train_no_sw)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Pour visualiser rapidement nos corpus, on peut utiliser la technique des
nuages de mots déjà vue à plusieurs reprises.</p>
<p>Vous pouvez essayer de faire vous-même les nuages ci-dessous
ou cliquer sur la ligne ci-dessous pour afficher le code ayant
généré les figures :</p>
<div class="cell markdown">
<details><summary><code>Cliquer pour afficher le code</code> 👇</summary>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_top_words(initials, ax, n_words<span class="op">=</span><span class="dv">20</span>):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calcul des mots les plus fréquemment utilisés par l'auteur</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    texts <span class="op">=</span> X_train_no_sw[le.inverse_transform(y_train) <span class="op">==</span> initials]</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    all_tokens <span class="op">=</span> <span class="st">' '</span>.join(texts).split()</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    counts <span class="op">=</span> Counter(all_tokens)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    top_words <span class="op">=</span> [word[<span class="dv">0</span>] <span class="cf">for</span> word <span class="kw">in</span> counts.most_common(n_words)]</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    top_words_counts <span class="op">=</span> [word[<span class="dv">1</span>] <span class="cf">for</span> word <span class="kw">in</span> counts.most_common(n_words)]</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Représentation sous forme de barplot</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> sns.barplot(ax <span class="op">=</span> ax, x<span class="op">=</span>top_words, y<span class="op">=</span>top_words_counts)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f'Most Common Words used by </span><span class="sc">{</span>initials_to_author[initials]<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>initials_to_author <span class="op">=</span> {</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">'EAP'</span>: <span class="st">'Edgar Allen Poe'</span>,</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'HPL'</span>: <span class="st">'H.P. Lovecraft'</span>,</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'MWS'</span>: <span class="st">'Mary Wollstonecraft Shelley'</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">3</span>, <span class="dv">1</span>, figsize <span class="op">=</span> (<span class="dv">12</span>,<span class="dv">12</span>))</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>plot_top_words(<span class="st">'EAP'</span>, ax <span class="op">=</span> axs[<span class="dv">0</span>])</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>plot_top_words(<span class="st">'HPL'</span>, ax <span class="op">=</span> axs[<span class="dv">1</span>])</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>plot_top_words(<span class="st">'MWS'</span>, ax <span class="op">=</span> axs[<span class="dv">2</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</details>
</div>
<p>Beaucoup de mots se retrouvent très utilisés par les trois auteurs.
Il y a cependant des différences notables : le mot <em>“life”</em>
est le plus employé par MWS, alors qu’il n’apparaît pas dans les deux autres tops.
De même, le mot <em>“old”</em> est le plus utilisé par HPL
là où les deux autres ne l’utilisent pas de manière surreprésentée.</p>
<p>Il semble donc qu’il y ait des particularités propres à chacun des auteurs
en termes de vocabulaire,
ce qui laisse penser qu’il est envisageable de prédire les auteurs à partir
de leurs textes dans une certaine mesure.</p>
</section>
</section>
<section id="prédiction-sur-le-set-dentraînement" class="level2">
<h2 class="anchored" data-anchor-id="prédiction-sur-le-set-dentraînement">Prédiction sur le set d’entraînement</h2>
<p>Nous allons à présent vérifier cette conjecture en comparant
plusieurs modèles de vectorisation,
<em>i.e.</em> de transformation du texte en objets numériques
pour que l’information contenue soit exploitable dans un modèle de classification.</p>
<section id="démarche" class="level3">
<h3 class="anchored" data-anchor-id="démarche">Démarche</h3>
<p>Comme nous nous intéressons plus à l’effet de la vectorisation qu’à la tâche de classification en elle-même,
nous allons utiliser un algorithme de classification simple (un SVM linéaire), avec des paramètres non fine-tunés (c’est-à-dire des paramètres pas nécessairement choisis pour être les meilleurs de tous).</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> LinearSVC(max_iter<span class="op">=</span><span class="dv">10000</span>, C<span class="op">=</span><span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Ce modèle est connu pour être très performant sur les tâches de classification de texte, et nous fournira donc un bon modèle de référence (<em>baseline</em>). Cela nous permettra également de comparer de manière objective l’impact des méthodes de vectorisation sur la performance finale.</p>
<!-- KA : dit plus bas. -->
<!-- On va utiliser au maximum les objets de type pipeline de `sklearn`, -->
<!-- qui permettent de réaliser des analyses reproductibles -->
<!-- et de fine-tuner proprement les différents hyperparamètres. -->
<p>Pour les deux premières méthodes de vectorisation
(basées sur des fréquences et fréquences relatives des mots),
on va simplement normaliser les données d’entrée, ce qui va permettre au SVM de converger plus rapidement, ces modèles étant sensibles aux différences d’échelle dans les données.</p>
<p>On va également <em>fine-tuner</em> via <em>grid-search</em>
certains hyperparamètres liés à ces méthodes de vectorisation :</p>
<ul>
<li>on teste différents <em>ranges</em> de <code>n-grams</code> (unigrammes et unigrammes + bigrammes)</li>
<li>on teste avec et sans <em>stop-words</em></li>
</ul>
<p>Afin d’éviter le surapprentissage,
on va évaluer les différents modèles via validation croisée, calculée sur 4 blocs.</p>
<p>On récupère à la fin le meilleur modèle selon une métrique spécifiée.
On choisit le <code>score F1</code>,
moyenne harmonique de la précision et du rappel,
qui donne un poids équilibré aux deux métriques, tout en pénalisant fortement le cas où l’une des deux est faible.
Précisément, on retient le <code>score F1 *micro-averaged*</code> :
les contributions des différentes classes à prédire sont agrégées,
puis on calcule le <code>score F1</code> sur ces données agrégées.
L’avantage de ce choix est qu’il permet de tenir compte des différences
de fréquences des différentes classes.</p>
</section>
<section id="pipeline-de-prédiction" class="level3">
<h3 class="anchored" data-anchor-id="pipeline-de-prédiction">Pipeline de prédiction</h3>
<p>On va utiliser un <em>pipeline</em> <code>scikit</code> ce qui va nous permettre d’avoir
un code très concis pour effectuer cet ensemble de tâches cohérentes.
De plus, cela va nous assurer de gérer de manière cohérentes nos différentes
transformations (cf.&nbsp;<a href="#pipelines">partie sur les pipelines</a>)</p>
<p>Pour se faciliter la vie, on définit une fonction <code>fit_vectorizers</code> qui
intègre dans un <em>pipeline</em> générique une méthode d’estimation <code>scikit</code>
et fait de la validation croisée en cherchant le meilleur modèle
(en excluant/incluant les <em>stopwords</em> et avec unigrammes/bigrammes)</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit_vectorizers(vectorizer):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    pipeline <span class="op">=</span> Pipeline(</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    [</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"vect"</span>, vectorizer()),</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"scaling"</span>, StandardScaler(with_mean<span class="op">=</span><span class="va">False</span>)),</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"clf"</span>, clf),</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    parameters <span class="op">=</span> {</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">"vect__ngram_range"</span>: ((<span class="dv">1</span>, <span class="dv">1</span>), (<span class="dv">1</span>, <span class="dv">2</span>)),  <span class="co"># unigrams or bigrams</span></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>        <span class="st">"vect__stop_words"</span>: (<span class="st">"english"</span>, <span class="va">None</span>)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>    grid_search <span class="op">=</span> GridSearchCV(pipeline, parameters, scoring<span class="op">=</span><span class="st">'f1_micro'</span>,</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>                               cv<span class="op">=</span><span class="dv">4</span>, n_jobs<span class="op">=</span><span class="dv">4</span>, verbose<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>    grid_search.fit(X_train, y_train)</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>    best_parameters <span class="op">=</span> grid_search.best_estimator_.get_params()</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> param_name <span class="kw">in</span> <span class="bu">sorted</span>(parameters.keys()):</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"</span><span class="ch">\t</span><span class="sc">%s</span><span class="st">: </span><span class="sc">%r</span><span class="st">"</span> <span class="op">%</span> (param_name, best_parameters[param_name]))</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"CV scores </span><span class="sc">{</span>grid_search<span class="sc">.</span>cv_results_[<span class="st">'mean_test_score'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Mean F1 </span><span class="sc">{</span>np<span class="sc">.</span>mean(grid_search.cv_results_[<span class="st">'mean_test_score'</span>])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> grid_search</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="approche-bag-of-words" class="level2">
<h2 class="anchored" data-anchor-id="approche-bag-of-words">Approche <em>bag-of-words</em></h2>
<p>On commence par une approche <strong>“bag-of-words”</strong>,
i.e.&nbsp;qui revient simplement à représenter chaque document par un vecteur
qui compte le nombre d’apparitions de chaque mot du vocabulaire dans le document.</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>cv_bow <span class="op">=</span> fit_vectorizers(CountVectorizer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="tf-idf" class="level2">
<h2 class="anchored" data-anchor-id="tf-idf">TF-IDF</h2>
<p>On s’intéresse ensuite à l’approche <strong>TF-IDF</strong>,
qui permet de tenir compte des fréquences <em>relatives</em> des mots.</p>
<p>Ainsi, pour un mot donné, on va multiplier la fréquence d’apparition du mot dans le document (calculé comme dans la méthode précédente) par un terme qui pénalise une fréquence élevée du mot dans le corpus. L’image ci-dessous, empruntée à Chris Albon, illustre cette mesure:</p>
<p><a href="https://chrisalbon.com/images/machine_learning_flashcards/TF-IDF_print.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="https://chrisalbon.com/images/machine_learning_flashcards/TF-IDF_print.png" class="img-fluid"></a>
<em>Source: <a href="https://chrisalbon.com/code/machine_learning/preprocessing_text/tf-idf/">Chris Albon</a></em></p>
<p>La vectorisation <code>TF-IDF</code> permet donc de limiter l’influence des <em>stop-words</em>
et donc de donner plus de poids aux mots les plus salients d’un document.
On observe clairement que la performance de classification est bien supérieure,
ce qui montre la pertinence de cette technique.</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>cv_tfidf <span class="op">=</span> fit_vectorizers(TfidfVectorizer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="word2vec-avec-averaging" class="level2">
<h2 class="anchored" data-anchor-id="word2vec-avec-averaging">Word2vec avec averaging</h2>
<p>On va maintenant explorer les techniques de vectorisation basées sur les
<em>embeddings</em> de mots, et notamment la plus populaire : <code>Word2Vec</code>.</p>
<p>L’idée derrière est simple, mais a révolutionné le NLP :
au lieu de représenter les documents par des
vecteurs <em>sparse</em> de très grande dimension (la taille du vocabulaire)
comme on l’a fait jusqu’à présent,
on va les représenter par des vecteurs <em>dense</em> (continus)
de dimension réduite (en général, autour de 100-300).</p>
<p>Chacune de ces dimensions va représenter un facteur latent,
c’est à dire une variable inobservée,
de la même manière que les composantes principales produites par une ACP.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="w2v_vecto.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="w2v_vecto.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p><em>Source: https://medium.com/<span class="citation" data-cites="zafaralibagh6/simple-tutorial-on-word-embedding-and-word2vec-43d477624b6d">@zafaralibagh6/simple-tutorial-on-word-embedding-and-word2vec-43d477624b6d</span></em></p>
<p><strong>Pourquoi est-ce intéressant ?</strong>
Pour de nombreuses raisons, mais pour résumer :
cela permet de beaucoup mieux capturer la similarité sémantique entre les documents.</p>
<p>Par exemple, un humain sait qu’un document contenant le mot <em>“Roi”</em>
et un autre document contenant le mot <em>“Reine”</em> ont beaucoup de chance
d’aborder des sujets semblables.</p>
<p>Pourtant, une vectorisation de type comptage ou TF-IDF
ne permet pas de saisir cette similarité :
le calcul d’une mesure de similarité (norme euclidienne ou similarité cosinus)
entre les deux vecteurs ne prendra en compte la similarité des deux concepts, puisque les mots utilisés sont différents.</p>
<p>A l’inverse, un modèle <code>word2vec</code> bien entraîné va capter
qu’il existe un facteur latent de type <em>“royauté”</em>,
et la similarité entre les vecteurs associés aux deux mots sera forte.</p>
<p>La magie va même plus loin : le modèle captera aussi qu’il existe un
facteur latent de type <em>“genre”</em>,
et va permettre de construire un espace sémantique dans lequel les
relations arithmétiques entre vecteurs ont du sens ;
par exemple :
<span class="math display">\[\text{king} - \text{man} + \text{woman} ≈ \text{queen}\]</span></p>
<p><strong>Comment ces modèles sont-ils entraînés ?</strong>
Via une tâche de prédiction résolue par un réseau de neurones simple.</p>
<p>L’idée fondamentale est que la signification d’un mot se comprend
en regardant les mots qui apparaissent fréquemment dans son voisinage.</p>
<p>Pour un mot donné, on va donc essayer de prédire les mots
qui apparaissent dans une fenêtre autour du mot cible.</p>
<p>En répétant cette tâche de nombreuses fois et sur un corpus suffisamment varié,
on obtient finalement des <em>embeddings</em> pour chaque mot du vocabulaire,
qui présentent les propriétés discutées précédemment.</p>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>X_train_tokens <span class="op">=</span> [text.split() <span class="cf">for</span> text <span class="kw">in</span> X_train]</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>w2v_model <span class="op">=</span> Word2Vec(X_train_tokens, vector_size<span class="op">=</span><span class="dv">200</span>, window<span class="op">=</span><span class="dv">5</span>, </span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>                     min_count<span class="op">=</span><span class="dv">1</span>, workers<span class="op">=</span><span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>w2v_model.wv.most_similar(<span class="st">"mother"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>On voit que les mots les plus similaires à <em>“mother”</em>
sont souvent des mots liés à la famille, mais pas toujours.</p>
<p>C’est lié à la taille très restreinte du corpus sur lequel on entraîne le modèle,
qui ne permet pas de réaliser des associations toujours pertinentes.</p>
<p>L’<em>embedding</em> (la représentation vectorielle) de chaque document correspond à la moyenne des <em>word-embeddings</em> des mots qui le composent :</p>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_mean_vector(w2v_vectors, words):</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> [word <span class="cf">for</span> word <span class="kw">in</span> words <span class="cf">if</span> word <span class="kw">in</span> w2v_vectors]</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> words:</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>        avg_vector <span class="op">=</span> np.mean(w2v_vectors[words], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>        avg_vector <span class="op">=</span> np.zeros_like(w2v_vectors[<span class="st">'hi'</span>])</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> avg_vector</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit_w2v_avg(w2v_vectors):</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>    X_train_vectors <span class="op">=</span> np.array([get_mean_vector(w2v_vectors, words)</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>                                <span class="cf">for</span> words <span class="kw">in</span> X_train_tokens])</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> cross_val_score(clf, X_train_vectors, y_train, </span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>                         cv<span class="op">=</span><span class="dv">4</span>, scoring<span class="op">=</span><span class="st">'f1_micro'</span>, n_jobs<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"CV scores </span><span class="sc">{</span>scores<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Mean F1 </span><span class="sc">{</span>np<span class="sc">.</span>mean(scores)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>cv_w2vec <span class="op">=</span> fit_w2v_avg(w2v_model.wv)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La performance chute fortement ;
la faute à la taille très restreinte du corpus, comme annoncé précédemment.</p>
</section>
<section id="word2vec-pré-entraîné-averaging" class="level2">
<h2 class="anchored" data-anchor-id="word2vec-pré-entraîné-averaging">Word2vec pré-entraîné + averaging</h2>
<p>Quand on travaille avec des corpus de taille restreinte,
c’est généralement une mauvaise idée d’entraîner son propre modèle <code>word2vec</code>.</p>
<p>Heureusement, des modèles pré-entraînés sur de très gros corpus sont disponibles.
Ils permettent de réaliser du <em>transfer learning</em>,
c’est-à-dire de bénéficier de la performance d’un modèle qui a été entraîné sur une autre tâche ou bien sur un autre corpus.</p>
<p>L’un des modèles les plus connus pour démarrer est le <code>glove_model</code> de
<code>Gensim</code> (Glove pour <em>Global Vectors for Word Representation</em>)<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>:</p>
<blockquote class="blockquote">
<p>GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.</p>
<p><em>Source</em> : https://nlp.stanford.edu/projects/glove/</p>
</blockquote>
<p>On peut le charger directement grâce à l’instruction suivante :</p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>glove_model <span class="op">=</span> gensim.downloader.load(<span class="st">'glove-wiki-gigaword-200'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Par exemple, la représentation vectorielle de roi est l’objet
multidimensionnel suivant :</p>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>glove_model[<span class="st">'king'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Comme elle est peu intelligible, on va plutôt rechercher les termes les
plus similaires. Par exemple,</p>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>glove_model.most_similar(<span class="st">'mother'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>On peut retrouver notre formule précédente</p>
<p><span class="math display">\[\text{king} - \text{man} + \text{woman} ≈ \text{queen}\]</span>
dans ce plongement de mots:</p>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>glove_model.most_similar(positive <span class="op">=</span> [<span class="st">'king'</span>, <span class="st">'woman'</span>], negative <span class="op">=</span> [<span class="st">'man'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Vous pouvez vous référer à <a href="https://jalammar.github.io/illustrated-word2vec/">ce tutoriel</a>
pour en découvrir plus sur <code>Word2Vec</code>.</p>
<p>Faisons notre apprentissage par transfert :</p>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>cv_w2vec_transfert <span class="op">=</span> fit_w2v_avg(glove_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La performance remonte substantiellement.
Cela étant, on ne parvient pas à faire mieux que les approches basiques,
on arrive à peine aux performances de la vectorisation par comptage.</p>
<p>En effet, pour rappel, les performances sont les suivantes :</p>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>perfs <span class="op">=</span> pd.DataFrame(</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>    [np.mean(cv_bow.cv_results_[<span class="st">'mean_test_score'</span>]),</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>     np.mean(cv_tfidf.cv_results_[<span class="st">'mean_test_score'</span>]),</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    np.mean(cv_w2vec),</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>    np.mean(cv_w2vec_transfert)],</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>    index <span class="op">=</span> [<span class="st">'Bag-of-Words'</span>,<span class="st">'TF-IDF'</span>, <span class="st">'Word2Vec non pré-entraîné'</span>, <span class="st">'Word2Vec pré-entraîné'</span>],</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>    columns <span class="op">=</span> [<span class="st">"Mean F1 score"</span>]</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>).sort_values(<span class="st">"Mean F1 score"</span>,ascending <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>perfs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Les performences limitées du modèle <em>Word2Vec</em> sont cette fois certainement dues à la manière dont
les <em>word-embeddings</em> sont exploités : ils sont moyennés pour décrire chaque document.</p>
<p>Cela a plusieurs limites :</p>
<ul>
<li>on ne tient pas compte de l’ordre et donc du contexte des mots</li>
<li>lorsque les documents sont longs, la moyennisation peut créer
des représentation bruitées.</li>
</ul>
</section>
<section id="contextual-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="contextual-embeddings">Contextual embeddings</h2>
<p>Les <em>embeddings</em> contextuels visent à pallier les limites des <em>embeddings</em>
traditionnels évoquées précédemment.</p>
<p>Cette fois, les mots n’ont plus de représentation vectorielle fixe,
celle-ci est calculée dynamiquement en fonction des mots du voisinage, et ainsi de suite.
Cela permet de tenir compte de la structure des phrases
et de tenir compte du fait que le sens d’un mot est fortement dépendant des mots
qui l’entourent.
Par exemple, dans les expressions “le président Macron” et “le camembert Président” le mot président n’a pas du tout le même rôle.</p>
<p>Ces <em>embeddings</em> sont produits par des architectures très complexes,
de type Transformer (<code>BERT</code>, etc.).</p>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SentenceTransformer(<span class="st">'all-mpnet-base-v2'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>X_train_vectors <span class="op">=</span> model.encode(X_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> cross_val_score(clf, X_train_vectors, y_train, </span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>                         cv<span class="op">=</span><span class="dv">4</span>, scoring<span class="op">=</span><span class="st">'f1_micro'</span>, n_jobs<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"CV scores </span><span class="sc">{</span>scores<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mean F1 </span><span class="sc">{</span>np<span class="sc">.</span>mean(scores)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>perfs <span class="op">=</span> pd.concat(</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>  [perfs,</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>  pd.DataFrame(</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    [np.mean(scores)],</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>    index <span class="op">=</span> [<span class="st">'Contextual Embedding'</span>],</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>    columns <span class="op">=</span> [<span class="st">"Mean F1 score"</span>])]</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>).sort_values(<span class="st">"Mean F1 score"</span>,ascending <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>perfs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Verdict : on fait très légèrement mieux que la vectorisation TF-IDF.
On voit donc l’importance de tenir compte du contexte.</p>
<p><strong>Mais pourquoi, avec une méthode très compliquée, ne parvenons-nous pas à battre une méthode toute simple ?</strong></p>
<p>On peut avancer plusieurs raisons :</p>
<ul>
<li>le <code>TF-IDF</code> est un modèle simple, mais toujours très performant
(on parle de <em>“tough-to-beat baseline”</em>).</li>
<li>la classification d’auteurs est une tâche très particulière et très ardue,
qui ne fait pas justice aux <em>embeddings</em>. Comme on l’a dit précédemment, ces derniers se révèlent particulièrement pertinents lorsqu’il est question de similarité sémantique entre des textes (<em>clustering</em>, etc.).</li>
</ul>
<p>Dans le cas de notre tâche de classification, il est probable que
certains mots (noms de personnage, noms de lieux) soient suffisants pour classifier de manière pertinente,
ce que ne permettent pas de capter les <em>embeddings</em> qui accordent à tous les mots la même importance.</p>
</section>
<section id="aller-plus-loin" class="level2">
<h2 class="anchored" data-anchor-id="aller-plus-loin">Aller plus loin</h2>
<ul>
<li>Nous avons entraîné différents modèles sur l’échantillon d’entraînement par validation croisée, mais nous n’avons toujours pas utilisé l’échantillon test que nous avons mis de côté au début. Réaliser la prédiction sur les données de test, et vérifier si l’on obtient le même classement des méthodes de vectorisation.</li>
<li>Faire un <em>vrai</em> split train/test : faire l’entraînement avec des textes de certains auteurs, et faire la prédiction avec des textes d’auteurs différents. Cela permettrait de neutraliser la présence de noms de lieux, de personnages, etc.</li>
<li>Comparer avec d’autres algorithmes de classification qu’un SVM</li>
<li>(Avancé) : fine-tuner le modèle d’embeddings contextuels sur la tâche de classification</li>
</ul>


</section>


<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. <em>GloVe: Global Vectors for Word Representation</em>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@book{galiana2023,
  author = {Galiana, Lino},
  title = {Python Pour La Data Science},
  date = {2023},
  url = {https://pythonds.linogaliana.fr/},
  doi = {10.5281/zenodo.8229676},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-galiana2023" class="csl-entry quarto-appendix-citeas" role="listitem">
Galiana, Lino. 2023. <em>Python Pour La Data Science</em>. <a href="https://doi.org/10.5281/zenodo.8229676">https://doi.org/10.5281/zenodo.8229676</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="linogaliana/python-datascientist" data-repo-id="MDEwOlJlcG9zaXRvcnkyODAxNjE2Nzc=" data-category="General" data-category-id="DIC_kwDOELLtjc4B-5TX" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../content/NLP/03_lda.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Latent Dirichlet Allocation (LDA)</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../content/NLP/05_exo_supp.html" class="pagination-link">
        <span class="nav-page-text">Exercices supplémentaires</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Python pour la <em>data science</em>, Lino Galiana.<br>
Licence <i class="fa-brands fa-creative-commons" aria-label="creative-commons"></i><br>
Code source disponible sur <a href="https://github.com/linogaliana/python-datascientist"><code>Github</code></a></div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">Site construit avec <i class="fa-brands fa-python" aria-label="python"></i> et <a href="https://quarto.org/"><code>Quarto</code></a><br>
Inspiration pour la mise en forme du site <a href="https://www.andreashandel.com">ici</a><br>
<a href="https://github.com/linogaliana/python-datascientist">Code source disponible sur <i class="fa-brands fa-github" aria-label="github"></i> <code>GitHub</code></a></div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"loop":true,"openEffect":"zoom","selector":".lightbox","closeEffect":"zoom","descPosition":"bottom"});</script>



</body></html>